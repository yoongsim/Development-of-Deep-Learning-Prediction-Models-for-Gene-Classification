{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixed random seed for reproducibility \n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of dataGene:\n",
      "        log_2FoldChange            ET  CoExpression           PCC  \\\n",
      "count     41110.000000  41110.000000  41110.000000  41110.000000   \n",
      "mean         -0.037332      1.407395      0.991997     -0.361737   \n",
      "std           0.391444      0.784327      0.089101      0.463979   \n",
      "min          -1.000000      0.000000      0.000000     -1.000000   \n",
      "25%          -0.251534      1.000000      1.000000     -0.747963   \n",
      "50%           0.030675      2.000000      1.000000     -0.449089   \n",
      "75%           0.251534      2.000000      1.000000     -0.051646   \n",
      "max           1.000000      2.000000      1.000000      1.000000   \n",
      "\n",
      "                PPI  Root10DaysSeedling  Root14DaysSeedling  \\\n",
      "count  41110.000000        41110.000000        41110.000000   \n",
      "mean       0.914668           -0.522040           -0.646982   \n",
      "std        0.279379            0.498568            0.393549   \n",
      "min        0.000000           -1.000000           -1.000000   \n",
      "25%        1.000000           -0.901371           -0.965084   \n",
      "50%        1.000000           -0.663664           -0.680003   \n",
      "75%        1.000000           -0.378497           -0.559627   \n",
      "max        1.000000            1.000000            1.000000   \n",
      "\n",
      "       Root17DaysSeedling  Root21DaysSeedling  Root24DaysSeedling  ...  \\\n",
      "count        41110.000000        41110.000000        41110.000000  ...   \n",
      "mean            -0.700869           -0.669349           -0.670048  ...   \n",
      "std              0.378219            0.405860            0.390751  ...   \n",
      "min             -1.000000           -1.000000           -1.000000  ...   \n",
      "25%             -0.980226           -1.000000           -0.982003  ...   \n",
      "50%             -0.795609           -0.726665           -0.708584  ...   \n",
      "75%             -0.601266           -0.543621           -0.482133  ...   \n",
      "max              1.000000            1.000000            1.000000  ...   \n",
      "\n",
      "       Root52DaysSeedling  Shoot3DaysSeedling  Shoot10DaysSeedling  \\\n",
      "count        41110.000000        41110.000000         41110.000000   \n",
      "mean            -0.670345           -0.590806            -0.545055   \n",
      "std              0.478222            0.443552             0.477438   \n",
      "min             -1.000000           -1.000000            -1.000000   \n",
      "25%             -1.000000           -1.000000            -0.906055   \n",
      "50%             -0.853382           -0.676286            -0.698864   \n",
      "75%             -0.542371           -0.409775            -0.250588   \n",
      "max              1.000000            0.955179             1.000000   \n",
      "\n",
      "       Shoot14DaysSeedling  Shoot17DaysSeedling  Shoot21DaysSeedling  \\\n",
      "count         41110.000000         41110.000000         41110.000000   \n",
      "mean             -0.734141            -0.680810            -0.659443   \n",
      "std               0.413716             0.478189             0.463838   \n",
      "min              -1.000000            -1.000000            -1.000000   \n",
      "25%              -1.000000            -1.000000            -1.000000   \n",
      "50%              -0.924976            -0.954040            -0.874080   \n",
      "75%              -0.513759            -0.420386            -0.440577   \n",
      "max               0.997390             1.000000             1.000000   \n",
      "\n",
      "       Shoot35DaysSeedling  Leaf21DaysSeedling  Leaf45DaysOldPlant  \\\n",
      "count         41110.000000        41110.000000        41110.000000   \n",
      "mean             -0.558906           -0.828778           -0.585144   \n",
      "std               0.506423            0.327542            0.399046   \n",
      "min              -1.000000           -1.000000           -1.000000   \n",
      "25%              -0.962199           -1.000000           -0.901444   \n",
      "50%              -0.699035           -0.951894           -0.643376   \n",
      "75%              -0.352995           -0.883755           -0.451900   \n",
      "max               0.993958            1.000000            1.000000   \n",
      "\n",
      "              class  \n",
      "count  41110.000000  \n",
      "mean      59.092703  \n",
      "std       77.624892  \n",
      "min        0.000000  \n",
      "25%        8.000000  \n",
      "50%       25.000000  \n",
      "75%       77.000000  \n",
      "max      372.000000  \n",
      "\n",
      "[8 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# load dataset (input variables = X, output variables = Y)\n",
    "df = pd.read_csv(\"2.csv\")\n",
    "\n",
    "#count the number of occurances for each osID\n",
    "OsID_counts = df['OsID'].value_counts()\n",
    "\n",
    "#filter for osIDs that have 10 or more occurances\n",
    "OsID_counts_filtered = OsID_counts[OsID_counts >= 10]\n",
    "\n",
    "#assign a label for each osID \n",
    "OsID_labels = {}\n",
    "class_no = 0\n",
    "for osID in OsID_counts_filtered.index:\n",
    "    OsID_labels[osID] = class_no\n",
    "    class_no +=1\n",
    "\n",
    "#filter the dataset with osID that contain 10 or more occurances\n",
    "dataGene = df[df['OsID'].isin(OsID_counts_filtered.index)]\n",
    "\n",
    "dataGene = dataGene.drop(['Class', 'Trait'],axis=1)\n",
    "\n",
    "# Add a new column 'class' to the filtered dataset\n",
    "dataGene['class'] = dataGene['OsID'].map(OsID_labels)\n",
    "\n",
    "print(\"Summary of dataGene:\\n\",dataGene.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X:\n",
      " (41110, 20)\n",
      "Shape of Y:\n",
      " (41110,)\n",
      "Summary of X:\n",
      "        CoExpression           PCC           PPI  Root10DaysSeedling  \\\n",
      "count  41110.000000  41110.000000  41110.000000        41110.000000   \n",
      "mean       0.991997     -0.361737      0.914668           -0.522040   \n",
      "std        0.089101      0.463979      0.279379            0.498568   \n",
      "min        0.000000     -1.000000      0.000000           -1.000000   \n",
      "25%        1.000000     -0.747963      1.000000           -0.901371   \n",
      "50%        1.000000     -0.449089      1.000000           -0.663664   \n",
      "75%        1.000000     -0.051646      1.000000           -0.378497   \n",
      "max        1.000000      1.000000      1.000000            1.000000   \n",
      "\n",
      "       Leaf21DaysSeedling  Leaf45DaysOldPlant  log_2FoldChange            ET  \\\n",
      "count        41110.000000        41110.000000     41110.000000  41110.000000   \n",
      "mean            -0.828778           -0.585144        -0.037332      1.407395   \n",
      "std              0.327542            0.399046         0.391444      0.784327   \n",
      "min             -1.000000           -1.000000        -1.000000      0.000000   \n",
      "25%             -1.000000           -0.901444        -0.251534      1.000000   \n",
      "50%             -0.951894           -0.643376         0.030675      2.000000   \n",
      "75%             -0.883755           -0.451900         0.251534      2.000000   \n",
      "max              1.000000            1.000000         1.000000      2.000000   \n",
      "\n",
      "       Shoot10DaysSeedling  Shoot3DaysSeedling  Shoot35DaysSeedling  \\\n",
      "count         41110.000000        41110.000000         41110.000000   \n",
      "mean             -0.545055           -0.590806            -0.558906   \n",
      "std               0.477438            0.443552             0.506423   \n",
      "min              -1.000000           -1.000000            -1.000000   \n",
      "25%              -0.906055           -1.000000            -0.962199   \n",
      "50%              -0.698864           -0.676286            -0.699035   \n",
      "75%              -0.250588           -0.409775            -0.352995   \n",
      "max               1.000000            0.955179             0.993958   \n",
      "\n",
      "       Shoot14DaysSeedling  Root17DaysSeedling  Shoot17DaysSeedling  \\\n",
      "count         41110.000000        41110.000000         41110.000000   \n",
      "mean             -0.734141           -0.700869            -0.680810   \n",
      "std               0.413716            0.378219             0.478189   \n",
      "min              -1.000000           -1.000000            -1.000000   \n",
      "25%              -1.000000           -0.980226            -1.000000   \n",
      "50%              -0.924976           -0.795609            -0.954040   \n",
      "75%              -0.513759           -0.601266            -0.420386   \n",
      "max               0.997390            1.000000             1.000000   \n",
      "\n",
      "       Shoot21DaysSeedling  Root24DaysSeedling  Root14DaysSeedling  \\\n",
      "count         41110.000000        41110.000000        41110.000000   \n",
      "mean             -0.659443           -0.670048           -0.646982   \n",
      "std               0.463838            0.390751            0.393549   \n",
      "min              -1.000000           -1.000000           -1.000000   \n",
      "25%              -1.000000           -0.982003           -0.965084   \n",
      "50%              -0.874080           -0.708584           -0.680003   \n",
      "75%              -0.440577           -0.482133           -0.559627   \n",
      "max               1.000000            1.000000            1.000000   \n",
      "\n",
      "       Root21DaysSeedling  Root52DaysSeedling  Root35DaysSeedling  \n",
      "count        41110.000000        41110.000000        41110.000000  \n",
      "mean            -0.669349           -0.670345           -0.596196  \n",
      "std              0.405860            0.478222            0.461679  \n",
      "min             -1.000000           -1.000000           -1.000000  \n",
      "25%             -1.000000           -1.000000           -0.937286  \n",
      "50%             -0.726665           -0.853382           -0.769184  \n",
      "75%             -0.543621           -0.542371           -0.323664  \n",
      "max              1.000000            1.000000            1.000000  \n",
      "Summary of Y:\n",
      " count    41110.000000\n",
      "mean        59.092703\n",
      "std         77.624892\n",
      "min          0.000000\n",
      "25%          8.000000\n",
      "50%         25.000000\n",
      "75%         77.000000\n",
      "max        372.000000\n",
      "Name: class, dtype: float64\n",
      "class\n",
      "0.0      1800\n",
      "1.0      1296\n",
      "2.0      1260\n",
      "3.0      1218\n",
      "4.0      1026\n",
      "         ... \n",
      "368.0      10\n",
      "369.0      10\n",
      "370.0      10\n",
      "371.0      10\n",
      "372.0      10\n",
      "Length: 373, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X = dataGene.drop(['class','OsID'], axis=1) # exclude class & OsID column\n",
    "Y = dataGene['class']\n",
    "\n",
    "#input feature names in order of descending importance scores in PCC feature selection method\n",
    "feature_names = ['CoExpression', 'PCC', 'PPI', 'Root10DaysSeedling', 'Leaf21DaysSeedling',\n",
    "                 'Leaf45DaysOldPlant', 'log_2FoldChange', 'ET', 'Shoot10DaysSeedling', 'Shoot3DaysSeedling', \n",
    "                 'Shoot35DaysSeedling', 'Shoot14DaysSeedling', 'Root17DaysSeedling', 'Shoot17DaysSeedling', 'Shoot21DaysSeedling',\n",
    "                 'Root24DaysSeedling', 'Root14DaysSeedling', 'Root21DaysSeedling', 'Root52DaysSeedling', 'Root35DaysSeedling']\n",
    "\n",
    "X_fs = X.reindex(columns=feature_names)\n",
    "\n",
    "print(\"Shape of X:\\n\",X_fs.shape)\n",
    "print(\"Shape of Y:\\n\",Y.shape)\n",
    "\n",
    "# Statistical summary of the variables\n",
    "print(\"Summary of X:\\n\",X_fs.describe())\n",
    "print(\"Summary of Y:\\n\",Y.describe())\n",
    "\n",
    "# Check for class imbalance\n",
    "print(df.groupby(Y).size())\n",
    "\n",
    "# change both input and target variables datatype to ndarray\n",
    "\n",
    "X_fs = X_fs.values # 2-D array\n",
    "\n",
    "# select target variable \n",
    "\n",
    "Y = Y.values #1-D array\n",
    "Y = Y.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class=0, n=1800 (4.378%)\n",
      "Class=1, n=1296 (3.153%)\n",
      "Class=2, n=1260 (3.065%)\n",
      "Class=3, n=1218 (2.963%)\n",
      "Class=4, n=1026 (2.496%)\n",
      "Class=5, n=1008 (2.452%)\n",
      "Class=6, n=930 (2.262%)\n",
      "Class=7, n=912 (2.218%)\n",
      "Class=8, n=880 (2.141%)\n",
      "Class=9, n=798 (1.941%)\n",
      "Class=10, n=792 (1.927%)\n",
      "Class=11, n=759 (1.846%)\n",
      "Class=12, n=729 (1.773%)\n",
      "Class=13, n=720 (1.751%)\n",
      "Class=14, n=702 (1.708%)\n",
      "Class=15, n=693 (1.686%)\n",
      "Class=16, n=672 (1.635%)\n",
      "Class=17, n=640 (1.557%)\n",
      "Class=18, n=625 (1.520%)\n",
      "Class=19, n=570 (1.387%)\n",
      "Class=20, n=546 (1.328%)\n",
      "Class=21, n=506 (1.231%)\n",
      "Class=22, n=483 (1.175%)\n",
      "Class=23, n=448 (1.090%)\n",
      "Class=24, n=432 (1.051%)\n",
      "Class=25, n=384 (0.934%)\n",
      "Class=26, n=360 (0.876%)\n",
      "Class=27, n=360 (0.876%)\n",
      "Class=28, n=320 (0.778%)\n",
      "Class=29, n=312 (0.759%)\n",
      "Class=30, n=312 (0.759%)\n",
      "Class=31, n=306 (0.744%)\n",
      "Class=32, n=304 (0.739%)\n",
      "Class=33, n=299 (0.727%)\n",
      "Class=34, n=297 (0.722%)\n",
      "Class=35, n=296 (0.720%)\n",
      "Class=36, n=280 (0.681%)\n",
      "Class=37, n=264 (0.642%)\n",
      "Class=38, n=260 (0.632%)\n",
      "Class=39, n=253 (0.615%)\n",
      "Class=40, n=252 (0.613%)\n",
      "Class=41, n=248 (0.603%)\n",
      "Class=42, n=242 (0.589%)\n",
      "Class=43, n=228 (0.555%)\n",
      "Class=44, n=216 (0.525%)\n",
      "Class=45, n=210 (0.511%)\n",
      "Class=46, n=200 (0.486%)\n",
      "Class=47, n=192 (0.467%)\n",
      "Class=48, n=180 (0.438%)\n",
      "Class=49, n=171 (0.416%)\n",
      "Class=50, n=168 (0.409%)\n",
      "Class=51, n=168 (0.409%)\n",
      "Class=52, n=162 (0.394%)\n",
      "Class=53, n=150 (0.365%)\n",
      "Class=54, n=148 (0.360%)\n",
      "Class=55, n=138 (0.336%)\n",
      "Class=56, n=135 (0.328%)\n",
      "Class=57, n=135 (0.328%)\n",
      "Class=58, n=133 (0.324%)\n",
      "Class=59, n=132 (0.321%)\n",
      "Class=60, n=132 (0.321%)\n",
      "Class=61, n=130 (0.316%)\n",
      "Class=62, n=130 (0.316%)\n",
      "Class=63, n=130 (0.316%)\n",
      "Class=64, n=128 (0.311%)\n",
      "Class=65, n=128 (0.311%)\n",
      "Class=66, n=126 (0.306%)\n",
      "Class=67, n=124 (0.302%)\n",
      "Class=68, n=124 (0.302%)\n",
      "Class=69, n=124 (0.302%)\n",
      "Class=70, n=120 (0.292%)\n",
      "Class=71, n=120 (0.292%)\n",
      "Class=72, n=118 (0.287%)\n",
      "Class=73, n=116 (0.282%)\n",
      "Class=74, n=114 (0.277%)\n",
      "Class=75, n=105 (0.255%)\n",
      "Class=76, n=104 (0.253%)\n",
      "Class=77, n=102 (0.248%)\n",
      "Class=78, n=99 (0.241%)\n",
      "Class=79, n=98 (0.238%)\n",
      "Class=80, n=98 (0.238%)\n",
      "Class=81, n=98 (0.238%)\n",
      "Class=82, n=98 (0.238%)\n",
      "Class=83, n=96 (0.234%)\n",
      "Class=84, n=96 (0.234%)\n",
      "Class=85, n=96 (0.234%)\n",
      "Class=86, n=93 (0.226%)\n",
      "Class=87, n=92 (0.224%)\n",
      "Class=88, n=92 (0.224%)\n",
      "Class=89, n=91 (0.221%)\n",
      "Class=90, n=88 (0.214%)\n",
      "Class=91, n=88 (0.214%)\n",
      "Class=92, n=86 (0.209%)\n",
      "Class=93, n=86 (0.209%)\n",
      "Class=94, n=84 (0.204%)\n",
      "Class=95, n=84 (0.204%)\n",
      "Class=96, n=84 (0.204%)\n",
      "Class=97, n=78 (0.190%)\n",
      "Class=98, n=78 (0.190%)\n",
      "Class=99, n=76 (0.185%)\n",
      "Class=100, n=75 (0.182%)\n",
      "Class=101, n=75 (0.182%)\n",
      "Class=102, n=73 (0.178%)\n",
      "Class=103, n=72 (0.175%)\n",
      "Class=104, n=72 (0.175%)\n",
      "Class=105, n=70 (0.170%)\n",
      "Class=106, n=69 (0.168%)\n",
      "Class=107, n=68 (0.165%)\n",
      "Class=108, n=67 (0.163%)\n",
      "Class=109, n=66 (0.161%)\n",
      "Class=110, n=66 (0.161%)\n",
      "Class=111, n=66 (0.161%)\n",
      "Class=112, n=66 (0.161%)\n",
      "Class=113, n=66 (0.161%)\n",
      "Class=114, n=65 (0.158%)\n",
      "Class=115, n=64 (0.156%)\n",
      "Class=116, n=63 (0.153%)\n",
      "Class=117, n=63 (0.153%)\n",
      "Class=118, n=62 (0.151%)\n",
      "Class=119, n=61 (0.148%)\n",
      "Class=120, n=60 (0.146%)\n",
      "Class=121, n=60 (0.146%)\n",
      "Class=122, n=60 (0.146%)\n",
      "Class=123, n=60 (0.146%)\n",
      "Class=124, n=60 (0.146%)\n",
      "Class=125, n=60 (0.146%)\n",
      "Class=126, n=60 (0.146%)\n",
      "Class=127, n=60 (0.146%)\n",
      "Class=128, n=60 (0.146%)\n",
      "Class=129, n=60 (0.146%)\n",
      "Class=130, n=59 (0.144%)\n",
      "Class=131, n=59 (0.144%)\n",
      "Class=132, n=58 (0.141%)\n",
      "Class=133, n=56 (0.136%)\n",
      "Class=134, n=56 (0.136%)\n",
      "Class=135, n=56 (0.136%)\n",
      "Class=136, n=56 (0.136%)\n",
      "Class=137, n=56 (0.136%)\n",
      "Class=138, n=56 (0.136%)\n",
      "Class=139, n=56 (0.136%)\n",
      "Class=140, n=56 (0.136%)\n",
      "Class=141, n=56 (0.136%)\n",
      "Class=142, n=55 (0.134%)\n",
      "Class=143, n=55 (0.134%)\n",
      "Class=144, n=54 (0.131%)\n",
      "Class=145, n=54 (0.131%)\n",
      "Class=146, n=54 (0.131%)\n",
      "Class=147, n=54 (0.131%)\n",
      "Class=148, n=54 (0.131%)\n",
      "Class=149, n=53 (0.129%)\n",
      "Class=150, n=52 (0.126%)\n",
      "Class=151, n=52 (0.126%)\n",
      "Class=152, n=52 (0.126%)\n",
      "Class=153, n=52 (0.126%)\n",
      "Class=154, n=50 (0.122%)\n",
      "Class=155, n=50 (0.122%)\n",
      "Class=156, n=49 (0.119%)\n",
      "Class=157, n=49 (0.119%)\n",
      "Class=158, n=48 (0.117%)\n",
      "Class=159, n=48 (0.117%)\n",
      "Class=160, n=48 (0.117%)\n",
      "Class=161, n=46 (0.112%)\n",
      "Class=162, n=45 (0.109%)\n",
      "Class=163, n=44 (0.107%)\n",
      "Class=164, n=44 (0.107%)\n",
      "Class=165, n=44 (0.107%)\n",
      "Class=166, n=42 (0.102%)\n",
      "Class=167, n=42 (0.102%)\n",
      "Class=168, n=42 (0.102%)\n",
      "Class=169, n=42 (0.102%)\n",
      "Class=170, n=42 (0.102%)\n",
      "Class=171, n=42 (0.102%)\n",
      "Class=172, n=42 (0.102%)\n",
      "Class=173, n=41 (0.100%)\n",
      "Class=174, n=41 (0.100%)\n",
      "Class=175, n=40 (0.097%)\n",
      "Class=176, n=40 (0.097%)\n",
      "Class=177, n=39 (0.095%)\n",
      "Class=178, n=39 (0.095%)\n",
      "Class=179, n=38 (0.092%)\n",
      "Class=180, n=37 (0.090%)\n",
      "Class=181, n=36 (0.088%)\n",
      "Class=182, n=35 (0.085%)\n",
      "Class=183, n=35 (0.085%)\n",
      "Class=184, n=35 (0.085%)\n",
      "Class=185, n=35 (0.085%)\n",
      "Class=186, n=34 (0.083%)\n",
      "Class=187, n=34 (0.083%)\n",
      "Class=188, n=34 (0.083%)\n",
      "Class=189, n=34 (0.083%)\n",
      "Class=190, n=32 (0.078%)\n",
      "Class=191, n=32 (0.078%)\n",
      "Class=192, n=32 (0.078%)\n",
      "Class=193, n=32 (0.078%)\n",
      "Class=194, n=32 (0.078%)\n",
      "Class=195, n=32 (0.078%)\n",
      "Class=196, n=31 (0.075%)\n",
      "Class=197, n=31 (0.075%)\n",
      "Class=198, n=31 (0.075%)\n",
      "Class=199, n=31 (0.075%)\n",
      "Class=200, n=30 (0.073%)\n",
      "Class=201, n=30 (0.073%)\n",
      "Class=202, n=30 (0.073%)\n",
      "Class=203, n=30 (0.073%)\n",
      "Class=204, n=30 (0.073%)\n",
      "Class=205, n=30 (0.073%)\n",
      "Class=206, n=30 (0.073%)\n",
      "Class=207, n=30 (0.073%)\n",
      "Class=208, n=30 (0.073%)\n",
      "Class=209, n=29 (0.071%)\n",
      "Class=210, n=29 (0.071%)\n",
      "Class=211, n=28 (0.068%)\n",
      "Class=212, n=28 (0.068%)\n",
      "Class=213, n=28 (0.068%)\n",
      "Class=214, n=28 (0.068%)\n",
      "Class=215, n=28 (0.068%)\n",
      "Class=216, n=28 (0.068%)\n",
      "Class=217, n=27 (0.066%)\n",
      "Class=218, n=27 (0.066%)\n",
      "Class=219, n=27 (0.066%)\n",
      "Class=220, n=27 (0.066%)\n",
      "Class=221, n=27 (0.066%)\n",
      "Class=222, n=27 (0.066%)\n",
      "Class=223, n=26 (0.063%)\n",
      "Class=224, n=26 (0.063%)\n",
      "Class=225, n=26 (0.063%)\n",
      "Class=226, n=26 (0.063%)\n",
      "Class=227, n=26 (0.063%)\n",
      "Class=228, n=25 (0.061%)\n",
      "Class=229, n=25 (0.061%)\n",
      "Class=230, n=25 (0.061%)\n",
      "Class=231, n=25 (0.061%)\n",
      "Class=232, n=24 (0.058%)\n",
      "Class=233, n=24 (0.058%)\n",
      "Class=234, n=24 (0.058%)\n",
      "Class=235, n=24 (0.058%)\n",
      "Class=236, n=24 (0.058%)\n",
      "Class=237, n=24 (0.058%)\n",
      "Class=238, n=24 (0.058%)\n",
      "Class=239, n=24 (0.058%)\n",
      "Class=240, n=24 (0.058%)\n",
      "Class=241, n=24 (0.058%)\n",
      "Class=242, n=24 (0.058%)\n",
      "Class=243, n=24 (0.058%)\n",
      "Class=244, n=23 (0.056%)\n",
      "Class=245, n=23 (0.056%)\n",
      "Class=246, n=22 (0.054%)\n",
      "Class=247, n=22 (0.054%)\n",
      "Class=248, n=22 (0.054%)\n",
      "Class=249, n=22 (0.054%)\n",
      "Class=250, n=22 (0.054%)\n",
      "Class=251, n=22 (0.054%)\n",
      "Class=252, n=22 (0.054%)\n",
      "Class=253, n=22 (0.054%)\n",
      "Class=254, n=22 (0.054%)\n",
      "Class=255, n=22 (0.054%)\n",
      "Class=256, n=22 (0.054%)\n",
      "Class=257, n=22 (0.054%)\n",
      "Class=258, n=22 (0.054%)\n",
      "Class=259, n=22 (0.054%)\n",
      "Class=260, n=22 (0.054%)\n",
      "Class=261, n=22 (0.054%)\n",
      "Class=262, n=22 (0.054%)\n",
      "Class=263, n=22 (0.054%)\n",
      "Class=264, n=21 (0.051%)\n",
      "Class=265, n=21 (0.051%)\n",
      "Class=266, n=21 (0.051%)\n",
      "Class=267, n=21 (0.051%)\n",
      "Class=268, n=21 (0.051%)\n",
      "Class=269, n=20 (0.049%)\n",
      "Class=270, n=20 (0.049%)\n",
      "Class=271, n=20 (0.049%)\n",
      "Class=272, n=20 (0.049%)\n",
      "Class=273, n=20 (0.049%)\n",
      "Class=274, n=20 (0.049%)\n",
      "Class=275, n=20 (0.049%)\n",
      "Class=276, n=20 (0.049%)\n",
      "Class=277, n=20 (0.049%)\n",
      "Class=278, n=20 (0.049%)\n",
      "Class=279, n=20 (0.049%)\n",
      "Class=280, n=19 (0.046%)\n",
      "Class=281, n=19 (0.046%)\n",
      "Class=282, n=19 (0.046%)\n",
      "Class=283, n=18 (0.044%)\n",
      "Class=284, n=18 (0.044%)\n",
      "Class=285, n=18 (0.044%)\n",
      "Class=286, n=18 (0.044%)\n",
      "Class=287, n=18 (0.044%)\n",
      "Class=288, n=18 (0.044%)\n",
      "Class=289, n=18 (0.044%)\n",
      "Class=290, n=18 (0.044%)\n",
      "Class=291, n=18 (0.044%)\n",
      "Class=292, n=17 (0.041%)\n",
      "Class=293, n=17 (0.041%)\n",
      "Class=294, n=17 (0.041%)\n",
      "Class=295, n=17 (0.041%)\n",
      "Class=296, n=17 (0.041%)\n",
      "Class=297, n=17 (0.041%)\n",
      "Class=298, n=16 (0.039%)\n",
      "Class=299, n=16 (0.039%)\n",
      "Class=300, n=16 (0.039%)\n",
      "Class=301, n=16 (0.039%)\n",
      "Class=302, n=16 (0.039%)\n",
      "Class=303, n=16 (0.039%)\n",
      "Class=304, n=16 (0.039%)\n",
      "Class=305, n=16 (0.039%)\n",
      "Class=306, n=15 (0.036%)\n",
      "Class=307, n=15 (0.036%)\n",
      "Class=308, n=15 (0.036%)\n",
      "Class=309, n=15 (0.036%)\n",
      "Class=310, n=15 (0.036%)\n",
      "Class=311, n=14 (0.034%)\n",
      "Class=312, n=14 (0.034%)\n",
      "Class=313, n=14 (0.034%)\n",
      "Class=314, n=14 (0.034%)\n",
      "Class=315, n=14 (0.034%)\n",
      "Class=316, n=14 (0.034%)\n",
      "Class=317, n=14 (0.034%)\n",
      "Class=318, n=14 (0.034%)\n",
      "Class=319, n=14 (0.034%)\n",
      "Class=320, n=14 (0.034%)\n",
      "Class=321, n=14 (0.034%)\n",
      "Class=322, n=14 (0.034%)\n",
      "Class=323, n=14 (0.034%)\n",
      "Class=324, n=14 (0.034%)\n",
      "Class=325, n=14 (0.034%)\n",
      "Class=326, n=14 (0.034%)\n",
      "Class=327, n=14 (0.034%)\n",
      "Class=328, n=13 (0.032%)\n",
      "Class=329, n=13 (0.032%)\n",
      "Class=330, n=13 (0.032%)\n",
      "Class=331, n=13 (0.032%)\n",
      "Class=332, n=13 (0.032%)\n",
      "Class=333, n=13 (0.032%)\n",
      "Class=334, n=13 (0.032%)\n",
      "Class=335, n=13 (0.032%)\n",
      "Class=336, n=13 (0.032%)\n",
      "Class=337, n=12 (0.029%)\n",
      "Class=338, n=12 (0.029%)\n",
      "Class=339, n=12 (0.029%)\n",
      "Class=340, n=12 (0.029%)\n",
      "Class=341, n=12 (0.029%)\n",
      "Class=342, n=12 (0.029%)\n",
      "Class=343, n=12 (0.029%)\n",
      "Class=344, n=12 (0.029%)\n",
      "Class=345, n=12 (0.029%)\n",
      "Class=346, n=12 (0.029%)\n",
      "Class=347, n=12 (0.029%)\n",
      "Class=348, n=12 (0.029%)\n",
      "Class=349, n=12 (0.029%)\n",
      "Class=350, n=12 (0.029%)\n",
      "Class=351, n=12 (0.029%)\n",
      "Class=352, n=12 (0.029%)\n",
      "Class=353, n=12 (0.029%)\n",
      "Class=354, n=12 (0.029%)\n",
      "Class=355, n=11 (0.027%)\n",
      "Class=356, n=11 (0.027%)\n",
      "Class=357, n=11 (0.027%)\n",
      "Class=358, n=11 (0.027%)\n",
      "Class=359, n=11 (0.027%)\n",
      "Class=360, n=11 (0.027%)\n",
      "Class=361, n=10 (0.024%)\n",
      "Class=362, n=10 (0.024%)\n",
      "Class=363, n=10 (0.024%)\n",
      "Class=364, n=10 (0.024%)\n",
      "Class=365, n=10 (0.024%)\n",
      "Class=366, n=10 (0.024%)\n",
      "Class=367, n=10 (0.024%)\n",
      "Class=368, n=10 (0.024%)\n",
      "Class=369, n=10 (0.024%)\n",
      "Class=370, n=10 (0.024%)\n",
      "Class=371, n=10 (0.024%)\n",
      "Class=372, n=10 (0.024%)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# summarize distribution\n",
    "counter = Counter(Y.flatten())\n",
    "\n",
    "# sort counter by keys\n",
    "counter = dict(sorted(counter.items()))\n",
    "\n",
    "for k,v in counter.items():\n",
    " per = v / len(Y.flatten()) * 100\n",
    " print('Class=%d, n=%d (%.3f%%)' % (k, v, per))\n",
    "# plot the distribution\n",
    "plt.bar( counter.keys(), counter.values())\n",
    "\n",
    "plt.ylabel('No of gene samples')\n",
    "plt.xlabel('Class Index of OsID')\n",
    "plt.title('Distribution of Target Classes (OsID)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_confusion_matrix(cm):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    sns.heatmap(cm,annot=True,fmt=\"d\", center=0, cmap='autumn') \n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare target data\n",
    "def prepare_targets(y_train, y_test):\n",
    "\tle = LabelEncoder()\n",
    "\t\n",
    "\t#fit the encoders only to the training data and then transform both train and test data\n",
    "\ty_train_enc = le.fit_transform(y_train)\n",
    "\ty_test_enc = le.transform(y_test)\n",
    "\n",
    "\treturn y_train_enc, y_test_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define baseline model (MLP)\n",
    "def MLP_model(input_dim):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_dim=input_dim,bias_initializer='normal', activation='relu',kernel_regularizer=regularizers.l2(0.0001)))\n",
    "    model.add(Dense(20,bias_initializer='normal',activation='relu',kernel_regularizer=regularizers.l2(0.0001)))\n",
    "    model.add(Dense(20,bias_initializer='normal',activation='relu',kernel_regularizer=regularizers.l2(0.0001)))\n",
    "    model.add(Dense(373,kernel_initializer='normal', activation='softmax')) #softmax for multi-class classification, num_classes = 373\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input features: 1\n",
      "Fold: 1\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 9s 4ms/step - loss: 5.0754 - accuracy: 0.0425 - val_loss: 5.0166 - val_accuracy: 0.0451\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 4.9463 - accuracy: 0.0454 - val_loss: 5.0037 - val_accuracy: 0.0451\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 4.9323 - accuracy: 0.0453 - val_loss: 5.0030 - val_accuracy: 0.0451\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 4.9294 - accuracy: 0.0448 - val_loss: 4.9953 - val_accuracy: 0.0451\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.9268 - accuracy: 0.0453 - val_loss: 5.0019 - val_accuracy: 0.0451\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.9245 - accuracy: 0.0453 - val_loss: 5.0077 - val_accuracy: 0.0451\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 4.9240 - accuracy: 0.0451 - val_loss: 5.0016 - val_accuracy: 0.0451\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 4.9234 - accuracy: 0.0452 - val_loss: 5.0044 - val_accuracy: 0.0451\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 4.9216 - accuracy: 0.0447 - val_loss: 5.0067 - val_accuracy: 0.0451\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.9212 - accuracy: 0.0454 - val_loss: 5.0115 - val_accuracy: 0.0451\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 4.9206 - accuracy: 0.0454 - val_loss: 5.0102 - val_accuracy: 0.0451\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 4.9204 - accuracy: 0.0453 - val_loss: 5.0097 - val_accuracy: 0.0451\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 4.9192 - accuracy: 0.0453 - val_loss: 5.0094 - val_accuracy: 0.0451\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 4.9190 - accuracy: 0.0452 - val_loss: 5.0126 - val_accuracy: 0.0451\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 4.9185 - accuracy: 0.0452 - val_loss: 5.0129 - val_accuracy: 0.0451\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 4.9179 - accuracy: 0.0452 - val_loss: 5.0133 - val_accuracy: 0.0451\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 4.9178 - accuracy: 0.0453 - val_loss: 5.0087 - val_accuracy: 0.0451\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.9167 - accuracy: 0.0454 - val_loss: 5.0125 - val_accuracy: 0.0451\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 4.9163 - accuracy: 0.0454 - val_loss: 5.0118 - val_accuracy: 0.0451\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 4.9158 - accuracy: 0.0452 - val_loss: 5.0161 - val_accuracy: 0.0451\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 10s 4ms/step - loss: 5.0718 - accuracy: 0.0422 - val_loss: 5.0313 - val_accuracy: 0.0460\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 4.9516 - accuracy: 0.0451 - val_loss: 5.0041 - val_accuracy: 0.0460\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 4.9389 - accuracy: 0.0451 - val_loss: 5.0014 - val_accuracy: 0.0460\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 4.9360 - accuracy: 0.0448 - val_loss: 5.0039 - val_accuracy: 0.0460\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 4.9329 - accuracy: 0.0451 - val_loss: 5.0032 - val_accuracy: 0.0460\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.9321 - accuracy: 0.0451 - val_loss: 4.9994 - val_accuracy: 0.0460\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 4.9298 - accuracy: 0.0448 - val_loss: 4.9985 - val_accuracy: 0.0460\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 13s 7ms/step - loss: 4.9289 - accuracy: 0.0447 - val_loss: 5.0033 - val_accuracy: 0.0460\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 13s 7ms/step - loss: 4.9279 - accuracy: 0.0451 - val_loss: 5.0055 - val_accuracy: 0.0460\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 13s 7ms/step - loss: 4.9277 - accuracy: 0.0451 - val_loss: 5.0013 - val_accuracy: 0.0440\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 12s 7ms/step - loss: 4.9257 - accuracy: 0.0449 - val_loss: 5.0161 - val_accuracy: 0.0460\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 14s 8ms/step - loss: 4.9259 - accuracy: 0.0447 - val_loss: 5.0007 - val_accuracy: 0.0460\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 14s 8ms/step - loss: 4.9249 - accuracy: 0.0451 - val_loss: 5.0107 - val_accuracy: 0.0460\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 4.9256 - accuracy: 0.0447 - val_loss: 5.0006 - val_accuracy: 0.0460\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 12s 7ms/step - loss: 4.9250 - accuracy: 0.0448 - val_loss: 5.0050 - val_accuracy: 0.0460\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 20s 11ms/step - loss: 4.9233 - accuracy: 0.0451 - val_loss: 5.0033 - val_accuracy: 0.0460\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 20s 11ms/step - loss: 4.9238 - accuracy: 0.0451 - val_loss: 5.0017 - val_accuracy: 0.0460\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 24s 13ms/step - loss: 4.9224 - accuracy: 0.0448 - val_loss: 5.0057 - val_accuracy: 0.0460\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 23s 13ms/step - loss: 4.9229 - accuracy: 0.0451 - val_loss: 5.0008 - val_accuracy: 0.0460\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 24s 13ms/step - loss: 4.9227 - accuracy: 0.0451 - val_loss: 5.0041 - val_accuracy: 0.0460\n",
      "Average Validation Accuracy: 0.04534727334976196\n",
      "Average Validation Loss: 4.941268682479858\n",
      "Average Test Accuracy: 0.045035749673843384\n",
      "Final Test Accuracy for each fold: 0.045035749673843384\n",
      "Number of input features: 2\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 22s 9ms/step - loss: 5.0638 - accuracy: 0.0444 - val_loss: 4.9754 - val_accuracy: 0.0510\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 18s 10ms/step - loss: 4.8790 - accuracy: 0.0535 - val_loss: 4.9109 - val_accuracy: 0.0517\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 16s 9ms/step - loss: 4.8203 - accuracy: 0.0547 - val_loss: 4.9307 - val_accuracy: 0.0517\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 21s 11ms/step - loss: 4.7877 - accuracy: 0.0572 - val_loss: 4.8857 - val_accuracy: 0.0482\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 16s 9ms/step - loss: 4.7684 - accuracy: 0.0600 - val_loss: 4.8893 - val_accuracy: 0.0559\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 12s 7ms/step - loss: 4.7536 - accuracy: 0.0594 - val_loss: 4.8879 - val_accuracy: 0.0561\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 12s 7ms/step - loss: 4.7368 - accuracy: 0.0613 - val_loss: 4.9014 - val_accuracy: 0.0552\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 12s 7ms/step - loss: 4.7257 - accuracy: 0.0630 - val_loss: 4.8891 - val_accuracy: 0.0565\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 13s 7ms/step - loss: 4.7138 - accuracy: 0.0611 - val_loss: 4.8968 - val_accuracy: 0.0537\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 12s 7ms/step - loss: 4.7053 - accuracy: 0.0607 - val_loss: 4.9197 - val_accuracy: 0.0609\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 13s 7ms/step - loss: 4.6976 - accuracy: 0.0635 - val_loss: 4.9160 - val_accuracy: 0.0607\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 13s 7ms/step - loss: 4.6879 - accuracy: 0.0634 - val_loss: 4.9551 - val_accuracy: 0.0592\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 13s 7ms/step - loss: 4.6812 - accuracy: 0.0661 - val_loss: 4.9539 - val_accuracy: 0.0656\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 13s 7ms/step - loss: 4.6716 - accuracy: 0.0668 - val_loss: 4.9439 - val_accuracy: 0.0596\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 13s 7ms/step - loss: 4.6626 - accuracy: 0.0669 - val_loss: 4.9715 - val_accuracy: 0.0552\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 13s 7ms/step - loss: 4.6550 - accuracy: 0.0679 - val_loss: 4.9701 - val_accuracy: 0.0684\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 13s 7ms/step - loss: 4.6483 - accuracy: 0.0676 - val_loss: 4.9963 - val_accuracy: 0.0563\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 12s 7ms/step - loss: 4.6408 - accuracy: 0.0694 - val_loss: 5.0178 - val_accuracy: 0.0570\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 12s 6ms/step - loss: 4.6338 - accuracy: 0.0692 - val_loss: 4.9756 - val_accuracy: 0.0638\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 18s 10ms/step - loss: 4.6282 - accuracy: 0.0664 - val_loss: 5.0219 - val_accuracy: 0.0614\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 27s 12ms/step - loss: 5.0391 - accuracy: 0.0493 - val_loss: 4.9454 - val_accuracy: 0.0508\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 17s 9ms/step - loss: 4.8557 - accuracy: 0.0567 - val_loss: 4.9288 - val_accuracy: 0.0557\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 18s 10ms/step - loss: 4.8170 - accuracy: 0.0570 - val_loss: 4.9048 - val_accuracy: 0.0590\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 17s 9ms/step - loss: 4.7901 - accuracy: 0.0599 - val_loss: 4.8852 - val_accuracy: 0.0541\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 15s 8ms/step - loss: 4.7713 - accuracy: 0.0603 - val_loss: 4.9088 - val_accuracy: 0.0598\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 17s 9ms/step - loss: 4.7556 - accuracy: 0.0582 - val_loss: 4.8929 - val_accuracy: 0.0590\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 17s 9ms/step - loss: 4.7421 - accuracy: 0.0585 - val_loss: 4.8770 - val_accuracy: 0.0568\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 21s 11ms/step - loss: 4.7297 - accuracy: 0.0603 - val_loss: 4.9005 - val_accuracy: 0.0645\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 20s 11ms/step - loss: 4.7207 - accuracy: 0.0585 - val_loss: 4.8885 - val_accuracy: 0.0568\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 19s 10ms/step - loss: 4.7077 - accuracy: 0.0604 - val_loss: 4.9041 - val_accuracy: 0.0554\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 17s 9ms/step - loss: 4.6986 - accuracy: 0.0617 - val_loss: 4.8997 - val_accuracy: 0.0638\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 20s 11ms/step - loss: 4.6892 - accuracy: 0.0601 - val_loss: 4.9125 - val_accuracy: 0.0627\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 17s 9ms/step - loss: 4.6794 - accuracy: 0.0631 - val_loss: 4.9100 - val_accuracy: 0.0482\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 16s 9ms/step - loss: 4.6703 - accuracy: 0.0611 - val_loss: 4.9296 - val_accuracy: 0.0590\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 15s 8ms/step - loss: 4.6624 - accuracy: 0.0643 - val_loss: 4.9404 - val_accuracy: 0.0543\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 15s 8ms/step - loss: 4.6541 - accuracy: 0.0650 - val_loss: 4.9543 - val_accuracy: 0.0594\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 17s 9ms/step - loss: 4.6470 - accuracy: 0.0667 - val_loss: 4.9803 - val_accuracy: 0.0539\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 4.6387 - accuracy: 0.0675 - val_loss: 4.9827 - val_accuracy: 0.0603\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 4.6333 - accuracy: 0.0681 - val_loss: 5.0187 - val_accuracy: 0.0557\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 4.6277 - accuracy: 0.0686 - val_loss: 5.0509 - val_accuracy: 0.0614\n",
      "Average Validation Accuracy: 0.06477161683142185\n",
      "Average Validation Loss: 4.902704238891602\n",
      "Average Test Accuracy: 0.06604260578751564\n",
      "Final Test Accuracy for each fold: 0.0684012696146965\n",
      "Number of input features: 3\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 11s 4ms/step - loss: 4.8938 - accuracy: 0.0767 - val_loss: 4.7439 - val_accuracy: 0.0772\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 4.6348 - accuracy: 0.0838 - val_loss: 4.6980 - val_accuracy: 0.0785\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 4.5803 - accuracy: 0.0848 - val_loss: 4.6661 - val_accuracy: 0.0772\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 4.5466 - accuracy: 0.0854 - val_loss: 4.6687 - val_accuracy: 0.0788\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 4.5179 - accuracy: 0.0857 - val_loss: 4.6414 - val_accuracy: 0.0790\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 4.4977 - accuracy: 0.0861 - val_loss: 4.6702 - val_accuracy: 0.0752\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 4.4819 - accuracy: 0.0868 - val_loss: 4.6346 - val_accuracy: 0.0799\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 4.4661 - accuracy: 0.0887 - val_loss: 4.6456 - val_accuracy: 0.0759\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 4.4563 - accuracy: 0.0900 - val_loss: 4.6560 - val_accuracy: 0.0887\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 4.4493 - accuracy: 0.0905 - val_loss: 4.6652 - val_accuracy: 0.0752\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 4.4411 - accuracy: 0.0886 - val_loss: 4.6592 - val_accuracy: 0.0845\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 4.4320 - accuracy: 0.0900 - val_loss: 4.6600 - val_accuracy: 0.0779\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 4.4240 - accuracy: 0.0910 - val_loss: 4.6800 - val_accuracy: 0.0834\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 4.4176 - accuracy: 0.0921 - val_loss: 4.6783 - val_accuracy: 0.0801\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 4.4099 - accuracy: 0.0922 - val_loss: 4.6747 - val_accuracy: 0.0851\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 4.4039 - accuracy: 0.0923 - val_loss: 4.6791 - val_accuracy: 0.0805\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 4.3989 - accuracy: 0.0936 - val_loss: 4.6852 - val_accuracy: 0.0783\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 4.3922 - accuracy: 0.0949 - val_loss: 4.7233 - val_accuracy: 0.0805\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 4.3863 - accuracy: 0.0962 - val_loss: 4.6978 - val_accuracy: 0.0801\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 4.3808 - accuracy: 0.0947 - val_loss: 4.7189 - val_accuracy: 0.0818\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 12s 6ms/step - loss: 4.9317 - accuracy: 0.0700 - val_loss: 4.7902 - val_accuracy: 0.0854\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 4.6565 - accuracy: 0.0833 - val_loss: 4.7141 - val_accuracy: 0.0854\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 4.5899 - accuracy: 0.0832 - val_loss: 4.6773 - val_accuracy: 0.0854\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 4.5491 - accuracy: 0.0825 - val_loss: 4.6639 - val_accuracy: 0.0904\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 4.5193 - accuracy: 0.0819 - val_loss: 4.6434 - val_accuracy: 0.0829\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 4.4988 - accuracy: 0.0861 - val_loss: 4.6703 - val_accuracy: 0.0810\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 4.4802 - accuracy: 0.0861 - val_loss: 4.6368 - val_accuracy: 0.0944\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 4.4668 - accuracy: 0.0861 - val_loss: 4.6448 - val_accuracy: 0.0902\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 4.4515 - accuracy: 0.0921 - val_loss: 4.6447 - val_accuracy: 0.0920\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 4.4394 - accuracy: 0.0907 - val_loss: 4.6509 - val_accuracy: 0.0988\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 4.4293 - accuracy: 0.0926 - val_loss: 4.6699 - val_accuracy: 0.0944\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 4.4189 - accuracy: 0.0929 - val_loss: 4.6983 - val_accuracy: 0.0988\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 4.4097 - accuracy: 0.0941 - val_loss: 4.6633 - val_accuracy: 0.0970\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 4.4025 - accuracy: 0.0948 - val_loss: 4.6856 - val_accuracy: 0.0959\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 4.3946 - accuracy: 0.0966 - val_loss: 4.7048 - val_accuracy: 0.0953\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 4.3895 - accuracy: 0.0933 - val_loss: 4.7578 - val_accuracy: 0.0882\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 4.3807 - accuracy: 0.0954 - val_loss: 4.7090 - val_accuracy: 0.0968\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 4.3756 - accuracy: 0.0960 - val_loss: 4.7472 - val_accuracy: 0.0924\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 4.3705 - accuracy: 0.0933 - val_loss: 4.7784 - val_accuracy: 0.0869\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 4.3654 - accuracy: 0.0939 - val_loss: 4.7656 - val_accuracy: 0.1054\n",
      "Average Validation Accuracy: 0.09552355855703354\n",
      "Average Validation Loss: 4.618311166763306\n",
      "Average Test Accuracy: 0.09556276351213455\n",
      "Final Test Accuracy for each fold: 0.09928502887487411\n",
      "Number of input features: 4\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 11s 5ms/step - loss: 4.4312 - accuracy: 0.1275 - val_loss: 3.9421 - val_accuracy: 0.1943\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 3.5189 - accuracy: 0.2202 - val_loss: 3.4162 - val_accuracy: 0.2332\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 3.0758 - accuracy: 0.2821 - val_loss: 3.1074 - val_accuracy: 0.3153\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 2.7908 - accuracy: 0.3311 - val_loss: 2.9172 - val_accuracy: 0.3241\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 2.5876 - accuracy: 0.3554 - val_loss: 2.7488 - val_accuracy: 0.3573\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 2.4309 - accuracy: 0.3814 - val_loss: 2.6558 - val_accuracy: 0.3395\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 2.3150 - accuracy: 0.3950 - val_loss: 2.5919 - val_accuracy: 0.3562\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 2.2324 - accuracy: 0.4125 - val_loss: 2.4924 - val_accuracy: 0.4150\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 2.1618 - accuracy: 0.4228 - val_loss: 2.4795 - val_accuracy: 0.4062\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 8s 5ms/step - loss: 2.1086 - accuracy: 0.4319 - val_loss: 2.3580 - val_accuracy: 0.4196\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 2.0682 - accuracy: 0.4441 - val_loss: 2.3445 - val_accuracy: 0.4510\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 2.0230 - accuracy: 0.4484 - val_loss: 2.3008 - val_accuracy: 0.4471\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 1.9898 - accuracy: 0.4563 - val_loss: 2.2421 - val_accuracy: 0.4169\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 1.9667 - accuracy: 0.4581 - val_loss: 2.2308 - val_accuracy: 0.4587\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.9362 - accuracy: 0.4651 - val_loss: 2.2174 - val_accuracy: 0.4541\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 1.9178 - accuracy: 0.4726 - val_loss: 2.1762 - val_accuracy: 0.4814\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 1.8922 - accuracy: 0.4734 - val_loss: 2.2010 - val_accuracy: 0.4629\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 1.8794 - accuracy: 0.4776 - val_loss: 2.1197 - val_accuracy: 0.4779\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.8490 - accuracy: 0.4856 - val_loss: 2.1585 - val_accuracy: 0.4675\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 1.8395 - accuracy: 0.4848 - val_loss: 2.0990 - val_accuracy: 0.4913\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 11s 4ms/step - loss: 4.5246 - accuracy: 0.1026 - val_loss: 4.0272 - val_accuracy: 0.1758\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 3.6071 - accuracy: 0.2087 - val_loss: 3.4816 - val_accuracy: 0.2796\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 3.0816 - accuracy: 0.2889 - val_loss: 3.2017 - val_accuracy: 0.2638\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 2.7829 - accuracy: 0.3198 - val_loss: 2.9806 - val_accuracy: 0.3287\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 2.5773 - accuracy: 0.3484 - val_loss: 2.7948 - val_accuracy: 0.3736\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 2.4168 - accuracy: 0.3798 - val_loss: 2.7070 - val_accuracy: 0.3776\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 2.2998 - accuracy: 0.4008 - val_loss: 2.5902 - val_accuracy: 0.4086\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 2.2140 - accuracy: 0.4191 - val_loss: 2.5048 - val_accuracy: 0.4196\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 2.1508 - accuracy: 0.4248 - val_loss: 2.4466 - val_accuracy: 0.4334\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 2.1044 - accuracy: 0.4325 - val_loss: 2.4152 - val_accuracy: 0.4174\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 2.0650 - accuracy: 0.4408 - val_loss: 2.3561 - val_accuracy: 0.4614\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 2.0239 - accuracy: 0.4478 - val_loss: 2.3322 - val_accuracy: 0.4361\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 1.9951 - accuracy: 0.4569 - val_loss: 2.2890 - val_accuracy: 0.4343\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 1.9716 - accuracy: 0.4520 - val_loss: 2.2810 - val_accuracy: 0.4218\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 1.9414 - accuracy: 0.4619 - val_loss: 2.2332 - val_accuracy: 0.4510\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 1.9188 - accuracy: 0.4658 - val_loss: 2.2210 - val_accuracy: 0.4620\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 1.8994 - accuracy: 0.4731 - val_loss: 2.1951 - val_accuracy: 0.4865\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 1.8790 - accuracy: 0.4825 - val_loss: 2.1940 - val_accuracy: 0.4673\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.8663 - accuracy: 0.4842 - val_loss: 2.2097 - val_accuracy: 0.4603\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 1.8467 - accuracy: 0.4820 - val_loss: 2.1388 - val_accuracy: 0.4869\n",
      "Average Validation Accuracy: 0.4976942390203476\n",
      "Average Validation Loss: 1.9177134037017822\n",
      "Average Test Accuracy: 0.4973096400499344\n",
      "Final Test Accuracy for each fold: 0.5031325817108154\n",
      "Number of input features: 5\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 10s 4ms/step - loss: 4.3353 - accuracy: 0.1325 - val_loss: 3.8964 - val_accuracy: 0.1881\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 3.4055 - accuracy: 0.2429 - val_loss: 3.2178 - val_accuracy: 0.3122\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 2.7651 - accuracy: 0.3535 - val_loss: 2.7409 - val_accuracy: 0.3608\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 2.3451 - accuracy: 0.4105 - val_loss: 2.4129 - val_accuracy: 0.3813\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 2.0598 - accuracy: 0.4677 - val_loss: 2.2007 - val_accuracy: 0.4893\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 1.8651 - accuracy: 0.4998 - val_loss: 2.0391 - val_accuracy: 0.4944\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.7135 - accuracy: 0.5313 - val_loss: 1.9056 - val_accuracy: 0.5567\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 1.5906 - accuracy: 0.5559 - val_loss: 1.8975 - val_accuracy: 0.5465\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 1.5005 - accuracy: 0.5712 - val_loss: 1.7306 - val_accuracy: 0.5831\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.4085 - accuracy: 0.6007 - val_loss: 1.6089 - val_accuracy: 0.6064\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 1.3462 - accuracy: 0.6116 - val_loss: 1.5797 - val_accuracy: 0.5817\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.2905 - accuracy: 0.6237 - val_loss: 1.5054 - val_accuracy: 0.6099\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 1.2440 - accuracy: 0.6325 - val_loss: 1.4587 - val_accuracy: 0.6301\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.2131 - accuracy: 0.6390 - val_loss: 1.4263 - val_accuracy: 0.6519\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 1.1763 - accuracy: 0.6482 - val_loss: 1.4702 - val_accuracy: 0.5872\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 1.1567 - accuracy: 0.6536 - val_loss: 1.3821 - val_accuracy: 0.6473\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.1212 - accuracy: 0.6584 - val_loss: 1.3350 - val_accuracy: 0.6449\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 1.1119 - accuracy: 0.6662 - val_loss: 1.3276 - val_accuracy: 0.6590\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.0870 - accuracy: 0.6714 - val_loss: 1.2877 - val_accuracy: 0.6330\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.0689 - accuracy: 0.6739 - val_loss: 1.3752 - val_accuracy: 0.6332\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 10s 4ms/step - loss: 4.4241 - accuracy: 0.1312 - val_loss: 3.9352 - val_accuracy: 0.2048\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 3.5045 - accuracy: 0.2469 - val_loss: 3.4585 - val_accuracy: 0.2944\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 2.9912 - accuracy: 0.3171 - val_loss: 3.0813 - val_accuracy: 0.3199\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 2.6041 - accuracy: 0.3679 - val_loss: 2.8042 - val_accuracy: 0.3723\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 2.3265 - accuracy: 0.4239 - val_loss: 2.6031 - val_accuracy: 0.4673\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 2.1115 - accuracy: 0.4605 - val_loss: 2.4945 - val_accuracy: 0.4733\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 1.9601 - accuracy: 0.4804 - val_loss: 2.3869 - val_accuracy: 0.5056\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 1.8456 - accuracy: 0.4968 - val_loss: 2.2254 - val_accuracy: 0.5190\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.7526 - accuracy: 0.5164 - val_loss: 2.3221 - val_accuracy: 0.4653\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.6853 - accuracy: 0.5294 - val_loss: 2.0924 - val_accuracy: 0.5164\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 1.6251 - accuracy: 0.5399 - val_loss: 1.9970 - val_accuracy: 0.5443\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.5840 - accuracy: 0.5456 - val_loss: 1.9675 - val_accuracy: 0.5177\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.5238 - accuracy: 0.5645 - val_loss: 1.8684 - val_accuracy: 0.5738\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.4829 - accuracy: 0.5772 - val_loss: 1.8497 - val_accuracy: 0.5670\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 1.4455 - accuracy: 0.5844 - val_loss: 1.7883 - val_accuracy: 0.6073\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.4035 - accuracy: 0.5928 - val_loss: 1.7057 - val_accuracy: 0.6196\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 1.3670 - accuracy: 0.5969 - val_loss: 1.7351 - val_accuracy: 0.6055\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 1.3431 - accuracy: 0.6026 - val_loss: 1.6905 - val_accuracy: 0.6053\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.3008 - accuracy: 0.6142 - val_loss: 1.6654 - val_accuracy: 0.6163\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.2837 - accuracy: 0.6234 - val_loss: 1.6548 - val_accuracy: 0.6101\n",
      "Average Validation Accuracy: 0.6299237310886383\n",
      "Average Validation Loss: 1.3169591426849365\n",
      "Average Test Accuracy: 0.6250460743904114\n",
      "Final Test Accuracy for each fold: 0.6400825381278992\n",
      "Number of input features: 6\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 11s 4ms/step - loss: 3.9872 - accuracy: 0.2185 - val_loss: 3.2254 - val_accuracy: 0.3289\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 2.5459 - accuracy: 0.4553 - val_loss: 2.3111 - val_accuracy: 0.5591\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.7972 - accuracy: 0.5986 - val_loss: 1.8103 - val_accuracy: 0.6559\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.3962 - accuracy: 0.6658 - val_loss: 1.5129 - val_accuracy: 0.6788\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.1584 - accuracy: 0.7043 - val_loss: 1.3728 - val_accuracy: 0.7105\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.0097 - accuracy: 0.7325 - val_loss: 1.2205 - val_accuracy: 0.7399\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.9122 - accuracy: 0.7514 - val_loss: 1.1393 - val_accuracy: 0.7707\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.8381 - accuracy: 0.7656 - val_loss: 1.0681 - val_accuracy: 0.7710\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.7777 - accuracy: 0.7811 - val_loss: 1.0447 - val_accuracy: 0.7778\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.7300 - accuracy: 0.7902 - val_loss: 0.9788 - val_accuracy: 0.7934\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.6973 - accuracy: 0.7971 - val_loss: 0.9797 - val_accuracy: 0.7481\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.6704 - accuracy: 0.7985 - val_loss: 0.8725 - val_accuracy: 0.8103\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.6388 - accuracy: 0.8071 - val_loss: 0.9510 - val_accuracy: 0.7703\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.6099 - accuracy: 0.8190 - val_loss: 0.9004 - val_accuracy: 0.7749\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.6003 - accuracy: 0.8203 - val_loss: 0.9016 - val_accuracy: 0.7848\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.5647 - accuracy: 0.8269 - val_loss: 0.7855 - val_accuracy: 0.8308\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.5497 - accuracy: 0.8311 - val_loss: 0.7454 - val_accuracy: 0.8350\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.5308 - accuracy: 0.8383 - val_loss: 0.8160 - val_accuracy: 0.8095\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.5191 - accuracy: 0.8422 - val_loss: 0.7958 - val_accuracy: 0.8211\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.5100 - accuracy: 0.8447 - val_loss: 0.7502 - val_accuracy: 0.8279\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 11s 4ms/step - loss: 4.1050 - accuracy: 0.2110 - val_loss: 3.3247 - val_accuracy: 0.3465\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 2.5699 - accuracy: 0.4543 - val_loss: 2.5306 - val_accuracy: 0.4915\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 1.8645 - accuracy: 0.5853 - val_loss: 2.0677 - val_accuracy: 0.5881\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.4563 - accuracy: 0.6486 - val_loss: 1.8196 - val_accuracy: 0.6739\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.2090 - accuracy: 0.7020 - val_loss: 1.5932 - val_accuracy: 0.7058\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.0353 - accuracy: 0.7397 - val_loss: 1.4423 - val_accuracy: 0.7494\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.9281 - accuracy: 0.7535 - val_loss: 1.3397 - val_accuracy: 0.7523\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.8494 - accuracy: 0.7645 - val_loss: 1.2681 - val_accuracy: 0.7386\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.7822 - accuracy: 0.7819 - val_loss: 1.2190 - val_accuracy: 0.7556\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.7389 - accuracy: 0.7900 - val_loss: 1.1344 - val_accuracy: 0.7828\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.6912 - accuracy: 0.8007 - val_loss: 1.0744 - val_accuracy: 0.7732\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.6572 - accuracy: 0.8052 - val_loss: 0.9752 - val_accuracy: 0.8079\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.6270 - accuracy: 0.8155 - val_loss: 0.9848 - val_accuracy: 0.7936\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.6043 - accuracy: 0.8210 - val_loss: 0.9329 - val_accuracy: 0.8079\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.5921 - accuracy: 0.8232 - val_loss: 0.9273 - val_accuracy: 0.8209\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.5635 - accuracy: 0.8279 - val_loss: 0.9889 - val_accuracy: 0.8011\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.5541 - accuracy: 0.8288 - val_loss: 0.8579 - val_accuracy: 0.8330\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.5377 - accuracy: 0.8414 - val_loss: 0.8532 - val_accuracy: 0.8416\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.5217 - accuracy: 0.8414 - val_loss: 0.9039 - val_accuracy: 0.8125\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.5146 - accuracy: 0.8435 - val_loss: 0.8612 - val_accuracy: 0.8156\n",
      "Average Validation Accuracy: 0.8318626582622528\n",
      "Average Validation Loss: 0.6011684536933899\n",
      "Average Test Accuracy: 0.8300287425518036\n",
      "Final Test Accuracy for each fold: 0.835777997970581\n",
      "Number of input features: 7\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 11s 4ms/step - loss: 4.1049 - accuracy: 0.2207 - val_loss: 3.3237 - val_accuracy: 0.3397\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 2.6672 - accuracy: 0.4522 - val_loss: 2.4420 - val_accuracy: 0.4948\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.9003 - accuracy: 0.5791 - val_loss: 1.9058 - val_accuracy: 0.6136\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.4055 - accuracy: 0.6728 - val_loss: 1.5216 - val_accuracy: 0.7006\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.0885 - accuracy: 0.7253 - val_loss: 1.3029 - val_accuracy: 0.7426\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.8823 - accuracy: 0.7639 - val_loss: 1.1652 - val_accuracy: 0.7648\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.7550 - accuracy: 0.7920 - val_loss: 1.0406 - val_accuracy: 0.7888\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.6705 - accuracy: 0.8090 - val_loss: 0.9678 - val_accuracy: 0.8145\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.6023 - accuracy: 0.8258 - val_loss: 0.9204 - val_accuracy: 0.8055\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.5555 - accuracy: 0.8349 - val_loss: 0.8251 - val_accuracy: 0.8260\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.5173 - accuracy: 0.8447 - val_loss: 0.8139 - val_accuracy: 0.8407\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4859 - accuracy: 0.8550 - val_loss: 0.7560 - val_accuracy: 0.8451\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4561 - accuracy: 0.8608 - val_loss: 0.7475 - val_accuracy: 0.8519\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4356 - accuracy: 0.8670 - val_loss: 0.7041 - val_accuracy: 0.8623\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4229 - accuracy: 0.8667 - val_loss: 0.7299 - val_accuracy: 0.8513\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3963 - accuracy: 0.8788 - val_loss: 0.6954 - val_accuracy: 0.8603\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3956 - accuracy: 0.8750 - val_loss: 0.6741 - val_accuracy: 0.8708\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3781 - accuracy: 0.8860 - val_loss: 0.6558 - val_accuracy: 0.8796\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3614 - accuracy: 0.8866 - val_loss: 0.8160 - val_accuracy: 0.8352\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3569 - accuracy: 0.8937 - val_loss: 0.6883 - val_accuracy: 0.8653\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 13s 4ms/step - loss: 4.1990 - accuracy: 0.1898 - val_loss: 3.4233 - val_accuracy: 0.3298\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 2.6833 - accuracy: 0.4334 - val_loss: 2.5021 - val_accuracy: 0.5195\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.8252 - accuracy: 0.5928 - val_loss: 1.8999 - val_accuracy: 0.6552\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.3154 - accuracy: 0.6885 - val_loss: 1.5504 - val_accuracy: 0.7278\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 1.0245 - accuracy: 0.7435 - val_loss: 1.4644 - val_accuracy: 0.7446\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.8578 - accuracy: 0.7752 - val_loss: 1.2304 - val_accuracy: 0.7912\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.7412 - accuracy: 0.8004 - val_loss: 1.1504 - val_accuracy: 0.8046\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.6572 - accuracy: 0.8156 - val_loss: 1.0807 - val_accuracy: 0.8123\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.5933 - accuracy: 0.8295 - val_loss: 0.9985 - val_accuracy: 0.8334\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.5387 - accuracy: 0.8469 - val_loss: 0.9394 - val_accuracy: 0.8440\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.5073 - accuracy: 0.8509 - val_loss: 0.9020 - val_accuracy: 0.8557\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4791 - accuracy: 0.8570 - val_loss: 0.8705 - val_accuracy: 0.8414\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4389 - accuracy: 0.8712 - val_loss: 0.9130 - val_accuracy: 0.8315\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4284 - accuracy: 0.8723 - val_loss: 0.8450 - val_accuracy: 0.8651\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4027 - accuracy: 0.8800 - val_loss: 0.8017 - val_accuracy: 0.8739\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4027 - accuracy: 0.8750 - val_loss: 0.7938 - val_accuracy: 0.8717\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3720 - accuracy: 0.8897 - val_loss: 0.8401 - val_accuracy: 0.8535\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3746 - accuracy: 0.8867 - val_loss: 0.7913 - val_accuracy: 0.8700\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3506 - accuracy: 0.8964 - val_loss: 0.7581 - val_accuracy: 0.8836\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3502 - accuracy: 0.8941 - val_loss: 0.7894 - val_accuracy: 0.8759\n",
      "Average Validation Accuracy: 0.8828380107879639\n",
      "Average Validation Loss: 0.5014304220676422\n",
      "Average Test Accuracy: 0.8765386343002319\n",
      "Final Test Accuracy for each fold: 0.8879634141921997\n",
      "Number of input features: 8\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 10s 4ms/step - loss: 3.9437 - accuracy: 0.2118 - val_loss: 2.9470 - val_accuracy: 0.4293\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 2.2758 - accuracy: 0.4904 - val_loss: 2.0766 - val_accuracy: 0.5723\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.5679 - accuracy: 0.6201 - val_loss: 1.6061 - val_accuracy: 0.6519\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.2095 - accuracy: 0.6864 - val_loss: 1.3261 - val_accuracy: 0.7065\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.9967 - accuracy: 0.7271 - val_loss: 1.1350 - val_accuracy: 0.7547\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.8565 - accuracy: 0.7563 - val_loss: 1.0100 - val_accuracy: 0.7820\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.7623 - accuracy: 0.7746 - val_loss: 0.9629 - val_accuracy: 0.7802\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.6897 - accuracy: 0.7895 - val_loss: 0.9471 - val_accuracy: 0.7575\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.6384 - accuracy: 0.8065 - val_loss: 0.8276 - val_accuracy: 0.7960\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.6055 - accuracy: 0.8100 - val_loss: 0.8021 - val_accuracy: 0.8183\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.5612 - accuracy: 0.8250 - val_loss: 0.8331 - val_accuracy: 0.8013\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.5291 - accuracy: 0.8323 - val_loss: 0.7488 - val_accuracy: 0.8216\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.5097 - accuracy: 0.8351 - val_loss: 0.7713 - val_accuracy: 0.8000\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.4970 - accuracy: 0.8408 - val_loss: 0.7050 - val_accuracy: 0.8400\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.4727 - accuracy: 0.8486 - val_loss: 0.6792 - val_accuracy: 0.8447\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4642 - accuracy: 0.8483 - val_loss: 0.7270 - val_accuracy: 0.8359\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4428 - accuracy: 0.8567 - val_loss: 0.6631 - val_accuracy: 0.8462\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4355 - accuracy: 0.8592 - val_loss: 0.6485 - val_accuracy: 0.8598\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4093 - accuracy: 0.8654 - val_loss: 0.6193 - val_accuracy: 0.8537\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.4040 - accuracy: 0.8677 - val_loss: 0.5633 - val_accuracy: 0.8887\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 11s 4ms/step - loss: 3.9807 - accuracy: 0.2046 - val_loss: 3.1572 - val_accuracy: 0.3630\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 2.4292 - accuracy: 0.4843 - val_loss: 2.3299 - val_accuracy: 0.5514\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.7482 - accuracy: 0.6064 - val_loss: 1.8806 - val_accuracy: 0.6466\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.3602 - accuracy: 0.6745 - val_loss: 1.5463 - val_accuracy: 0.6933\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.1101 - accuracy: 0.7195 - val_loss: 1.3988 - val_accuracy: 0.7166\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.9449 - accuracy: 0.7457 - val_loss: 1.2343 - val_accuracy: 0.7377\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.8366 - accuracy: 0.7646 - val_loss: 1.1970 - val_accuracy: 0.7410\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.7623 - accuracy: 0.7806 - val_loss: 1.0598 - val_accuracy: 0.7718\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.7051 - accuracy: 0.7897 - val_loss: 0.9669 - val_accuracy: 0.7809\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.6560 - accuracy: 0.7959 - val_loss: 0.9782 - val_accuracy: 0.7969\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.6173 - accuracy: 0.8111 - val_loss: 0.8689 - val_accuracy: 0.8174\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.5793 - accuracy: 0.8186 - val_loss: 0.8407 - val_accuracy: 0.8211\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.5460 - accuracy: 0.8301 - val_loss: 0.8386 - val_accuracy: 0.8163\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.5328 - accuracy: 0.8304 - val_loss: 0.8184 - val_accuracy: 0.8174\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.5070 - accuracy: 0.8384 - val_loss: 0.7696 - val_accuracy: 0.8271\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4842 - accuracy: 0.8482 - val_loss: 0.7366 - val_accuracy: 0.8502\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4661 - accuracy: 0.8555 - val_loss: 0.7132 - val_accuracy: 0.8550\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4512 - accuracy: 0.8563 - val_loss: 0.6933 - val_accuracy: 0.8715\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4348 - accuracy: 0.8640 - val_loss: 0.6808 - val_accuracy: 0.8693\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4366 - accuracy: 0.8630 - val_loss: 0.6860 - val_accuracy: 0.8675\n",
      "Average Validation Accuracy: 0.8934028744697571\n",
      "Average Validation Loss: 0.43993282318115234\n",
      "Average Test Accuracy: 0.8892164826393127\n",
      "Final Test Accuracy for each fold: 0.8929018974304199\n",
      "Number of input features: 9\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 10s 4ms/step - loss: 3.6504 - accuracy: 0.2904 - val_loss: 2.5217 - val_accuracy: 0.5102\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.7860 - accuracy: 0.6299 - val_loss: 1.6183 - val_accuracy: 0.6845\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.1285 - accuracy: 0.7411 - val_loss: 1.1381 - val_accuracy: 0.7822\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.7993 - accuracy: 0.7985 - val_loss: 0.8899 - val_accuracy: 0.8119\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.6377 - accuracy: 0.8284 - val_loss: 0.7907 - val_accuracy: 0.8436\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.5527 - accuracy: 0.8487 - val_loss: 0.7054 - val_accuracy: 0.8528\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4879 - accuracy: 0.8584 - val_loss: 0.6909 - val_accuracy: 0.8420\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4465 - accuracy: 0.8713 - val_loss: 0.6510 - val_accuracy: 0.8486\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4085 - accuracy: 0.8807 - val_loss: 0.6472 - val_accuracy: 0.8537\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3879 - accuracy: 0.8852 - val_loss: 0.5215 - val_accuracy: 0.8957\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3673 - accuracy: 0.8913 - val_loss: 0.5005 - val_accuracy: 0.8999\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3382 - accuracy: 0.9001 - val_loss: 0.5232 - val_accuracy: 0.8922\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3278 - accuracy: 0.9030 - val_loss: 0.5781 - val_accuracy: 0.8750\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3221 - accuracy: 0.9055 - val_loss: 0.4465 - val_accuracy: 0.9188\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3063 - accuracy: 0.9109 - val_loss: 0.4567 - val_accuracy: 0.9129\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2948 - accuracy: 0.9159 - val_loss: 0.4428 - val_accuracy: 0.9140\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2837 - accuracy: 0.9160 - val_loss: 0.4470 - val_accuracy: 0.9153\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2775 - accuracy: 0.9210 - val_loss: 0.5260 - val_accuracy: 0.8900\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2560 - accuracy: 0.9251 - val_loss: 0.4872 - val_accuracy: 0.9041\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2596 - accuracy: 0.9259 - val_loss: 0.4299 - val_accuracy: 0.9226\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 10s 4ms/step - loss: 3.7979 - accuracy: 0.2585 - val_loss: 2.8477 - val_accuracy: 0.4733\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 2.0183 - accuracy: 0.5979 - val_loss: 1.9295 - val_accuracy: 0.6755\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.3214 - accuracy: 0.7227 - val_loss: 1.4982 - val_accuracy: 0.7575\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.9509 - accuracy: 0.7879 - val_loss: 1.2288 - val_accuracy: 0.7712\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.7336 - accuracy: 0.8250 - val_loss: 1.0359 - val_accuracy: 0.8405\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.6021 - accuracy: 0.8555 - val_loss: 0.9148 - val_accuracy: 0.8579\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.5051 - accuracy: 0.8727 - val_loss: 0.8485 - val_accuracy: 0.8565\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4465 - accuracy: 0.8864 - val_loss: 0.7980 - val_accuracy: 0.8783\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4109 - accuracy: 0.8876 - val_loss: 0.7572 - val_accuracy: 0.8746\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.3705 - accuracy: 0.8983 - val_loss: 0.6824 - val_accuracy: 0.8939\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3397 - accuracy: 0.9065 - val_loss: 0.6240 - val_accuracy: 0.9061\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3193 - accuracy: 0.9098 - val_loss: 0.5917 - val_accuracy: 0.9096\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 6s 4ms/step - loss: 0.2933 - accuracy: 0.9200 - val_loss: 0.6091 - val_accuracy: 0.9021\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2775 - accuracy: 0.9252 - val_loss: 0.5657 - val_accuracy: 0.9140\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2732 - accuracy: 0.9205 - val_loss: 0.5826 - val_accuracy: 0.9111\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2670 - accuracy: 0.9260 - val_loss: 0.5990 - val_accuracy: 0.9063\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2442 - accuracy: 0.9322 - val_loss: 0.4961 - val_accuracy: 0.9314\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2506 - accuracy: 0.9274 - val_loss: 0.5581 - val_accuracy: 0.9076\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2304 - accuracy: 0.9352 - val_loss: 0.5286 - val_accuracy: 0.9278\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2306 - accuracy: 0.9364 - val_loss: 0.5044 - val_accuracy: 0.9344\n",
      "Average Validation Accuracy: 0.9364269077777863\n",
      "Average Validation Loss: 0.30578602850437164\n",
      "Average Test Accuracy: 0.9360949397087097\n",
      "Final Test Accuracy for each fold: 0.9429498314857483\n",
      "Number of input features: 10\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 10s 4ms/step - loss: 3.7867 - accuracy: 0.2657 - val_loss: 2.5359 - val_accuracy: 0.5351\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.7376 - accuracy: 0.6636 - val_loss: 1.4887 - val_accuracy: 0.7287\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.0193 - accuracy: 0.7896 - val_loss: 1.0286 - val_accuracy: 0.8251\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.6882 - accuracy: 0.8432 - val_loss: 0.8400 - val_accuracy: 0.8546\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.5136 - accuracy: 0.8783 - val_loss: 0.7550 - val_accuracy: 0.8559\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4142 - accuracy: 0.8953 - val_loss: 0.6437 - val_accuracy: 0.8805\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3589 - accuracy: 0.9057 - val_loss: 0.5499 - val_accuracy: 0.9080\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3046 - accuracy: 0.9222 - val_loss: 0.5193 - val_accuracy: 0.9168\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2664 - accuracy: 0.9312 - val_loss: 0.5179 - val_accuracy: 0.8981\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2540 - accuracy: 0.9334 - val_loss: 0.4483 - val_accuracy: 0.9254\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2383 - accuracy: 0.9350 - val_loss: 0.4872 - val_accuracy: 0.9111\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2207 - accuracy: 0.9398 - val_loss: 0.4312 - val_accuracy: 0.9281\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2100 - accuracy: 0.9426 - val_loss: 0.4418 - val_accuracy: 0.9353\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1953 - accuracy: 0.9488 - val_loss: 0.4306 - val_accuracy: 0.9265\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1974 - accuracy: 0.9475 - val_loss: 0.4032 - val_accuracy: 0.9492\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1868 - accuracy: 0.9502 - val_loss: 0.3845 - val_accuracy: 0.9443\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1830 - accuracy: 0.9521 - val_loss: 0.4213 - val_accuracy: 0.9406\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1737 - accuracy: 0.9538 - val_loss: 0.3987 - val_accuracy: 0.9476\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1632 - accuracy: 0.9582 - val_loss: 0.4309 - val_accuracy: 0.9353\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1640 - accuracy: 0.9569 - val_loss: 0.3738 - val_accuracy: 0.9562\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 10s 4ms/step - loss: 3.6713 - accuracy: 0.2980 - val_loss: 2.6199 - val_accuracy: 0.5243\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 1.7711 - accuracy: 0.6663 - val_loss: 1.5967 - val_accuracy: 0.7494\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.0554 - accuracy: 0.7926 - val_loss: 1.2398 - val_accuracy: 0.8013\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.7301 - accuracy: 0.8418 - val_loss: 1.0503 - val_accuracy: 0.8174\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.5497 - accuracy: 0.8754 - val_loss: 0.8421 - val_accuracy: 0.8801\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4500 - accuracy: 0.8890 - val_loss: 0.7330 - val_accuracy: 0.8953\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.3737 - accuracy: 0.9083 - val_loss: 0.6710 - val_accuracy: 0.8891\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3277 - accuracy: 0.9128 - val_loss: 0.5877 - val_accuracy: 0.9116\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2890 - accuracy: 0.9232 - val_loss: 0.5638 - val_accuracy: 0.9100\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2688 - accuracy: 0.9287 - val_loss: 0.5533 - val_accuracy: 0.9001\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2439 - accuracy: 0.9343 - val_loss: 0.5049 - val_accuracy: 0.9160\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2260 - accuracy: 0.9374 - val_loss: 0.4566 - val_accuracy: 0.9360\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2210 - accuracy: 0.9390 - val_loss: 0.4653 - val_accuracy: 0.9206\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2068 - accuracy: 0.9441 - val_loss: 0.4412 - val_accuracy: 0.9413\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1989 - accuracy: 0.9469 - val_loss: 0.4398 - val_accuracy: 0.9309\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1868 - accuracy: 0.9503 - val_loss: 0.4346 - val_accuracy: 0.9285\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1823 - accuracy: 0.9514 - val_loss: 0.4047 - val_accuracy: 0.9419\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1808 - accuracy: 0.9522 - val_loss: 0.4293 - val_accuracy: 0.9336\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1709 - accuracy: 0.9550 - val_loss: 0.4501 - val_accuracy: 0.9188\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1699 - accuracy: 0.9540 - val_loss: 0.4180 - val_accuracy: 0.9419\n",
      "Average Validation Accuracy: 0.956467866897583\n",
      "Average Validation Loss: 0.23365896940231323\n",
      "Average Test Accuracy: 0.9572860598564148\n",
      "Final Test Accuracy for each fold: 0.9652097225189209\n",
      "Number of input features: 11\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 10s 4ms/step - loss: 3.6500 - accuracy: 0.2876 - val_loss: 2.5465 - val_accuracy: 0.5215\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.7161 - accuracy: 0.6826 - val_loss: 1.4981 - val_accuracy: 0.7454\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.9810 - accuracy: 0.8148 - val_loss: 1.0996 - val_accuracy: 0.8172\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.6529 - accuracy: 0.8655 - val_loss: 0.8484 - val_accuracy: 0.8777\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4731 - accuracy: 0.8953 - val_loss: 0.7221 - val_accuracy: 0.8931\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3637 - accuracy: 0.9155 - val_loss: 0.6602 - val_accuracy: 0.9083\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3039 - accuracy: 0.9258 - val_loss: 0.5750 - val_accuracy: 0.9243\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2684 - accuracy: 0.9310 - val_loss: 0.5409 - val_accuracy: 0.9292\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2327 - accuracy: 0.9408 - val_loss: 0.5111 - val_accuracy: 0.9221\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2143 - accuracy: 0.9430 - val_loss: 0.4999 - val_accuracy: 0.9248\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1987 - accuracy: 0.9478 - val_loss: 0.4592 - val_accuracy: 0.9439\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1878 - accuracy: 0.9519 - val_loss: 0.4569 - val_accuracy: 0.9413\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1723 - accuracy: 0.9568 - val_loss: 0.4442 - val_accuracy: 0.9430\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1682 - accuracy: 0.9586 - val_loss: 0.4262 - val_accuracy: 0.9553\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1552 - accuracy: 0.9620 - val_loss: 0.4734 - val_accuracy: 0.9353\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1532 - accuracy: 0.9613 - val_loss: 0.4461 - val_accuracy: 0.9529\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1408 - accuracy: 0.9661 - val_loss: 0.3773 - val_accuracy: 0.9663\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1366 - accuracy: 0.9673 - val_loss: 0.4156 - val_accuracy: 0.9597\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1446 - accuracy: 0.9644 - val_loss: 0.3997 - val_accuracy: 0.9595\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1223 - accuracy: 0.9718 - val_loss: 0.4000 - val_accuracy: 0.9569\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 14s 4ms/step - loss: 3.5434 - accuracy: 0.3412 - val_loss: 2.3331 - val_accuracy: 0.6077\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.5429 - accuracy: 0.7196 - val_loss: 1.4236 - val_accuracy: 0.7701\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.9282 - accuracy: 0.8209 - val_loss: 1.0916 - val_accuracy: 0.8220\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.6366 - accuracy: 0.8629 - val_loss: 0.8523 - val_accuracy: 0.8700\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.4759 - accuracy: 0.8932 - val_loss: 0.7348 - val_accuracy: 0.8770\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3754 - accuracy: 0.9138 - val_loss: 0.5892 - val_accuracy: 0.9175\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.3171 - accuracy: 0.9218 - val_loss: 0.5429 - val_accuracy: 0.9171\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2733 - accuracy: 0.9351 - val_loss: 0.4575 - val_accuracy: 0.9272\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.2411 - accuracy: 0.9392 - val_loss: 0.4677 - val_accuracy: 0.9208\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.2176 - accuracy: 0.9470 - val_loss: 0.4402 - val_accuracy: 0.9278\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2060 - accuracy: 0.9475 - val_loss: 0.3748 - val_accuracy: 0.9507\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1884 - accuracy: 0.9518 - val_loss: 0.3712 - val_accuracy: 0.9452\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1778 - accuracy: 0.9546 - val_loss: 0.3422 - val_accuracy: 0.9525\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1750 - accuracy: 0.9547 - val_loss: 0.3988 - val_accuracy: 0.9386\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1619 - accuracy: 0.9606 - val_loss: 0.3354 - val_accuracy: 0.9562\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1624 - accuracy: 0.9589 - val_loss: 0.3399 - val_accuracy: 0.9525\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1491 - accuracy: 0.9636 - val_loss: 0.3095 - val_accuracy: 0.9604\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1534 - accuracy: 0.9614 - val_loss: 0.3059 - val_accuracy: 0.9617\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1397 - accuracy: 0.9654 - val_loss: 0.3323 - val_accuracy: 0.9483\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1427 - accuracy: 0.9659 - val_loss: 0.3113 - val_accuracy: 0.9586\n",
      "Average Validation Accuracy: 0.9660893976688385\n",
      "Average Validation Loss: 0.2053138092160225\n",
      "Average Test Accuracy: 0.9655413925647736\n",
      "Final Test Accuracy for each fold: 0.9664627313613892\n",
      "Number of input features: 12\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 10s 4ms/step - loss: 3.5096 - accuracy: 0.3323 - val_loss: 2.2086 - val_accuracy: 0.5864\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.4109 - accuracy: 0.7447 - val_loss: 1.2306 - val_accuracy: 0.7974\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.7877 - accuracy: 0.8465 - val_loss: 0.8898 - val_accuracy: 0.8471\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.5210 - accuracy: 0.8875 - val_loss: 0.6886 - val_accuracy: 0.8792\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4004 - accuracy: 0.9043 - val_loss: 0.6011 - val_accuracy: 0.8957\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3278 - accuracy: 0.9202 - val_loss: 0.5276 - val_accuracy: 0.9201\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2880 - accuracy: 0.9281 - val_loss: 0.4768 - val_accuracy: 0.9171\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2481 - accuracy: 0.9381 - val_loss: 0.4438 - val_accuracy: 0.9245\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2363 - accuracy: 0.9371 - val_loss: 0.4387 - val_accuracy: 0.9186\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.2057 - accuracy: 0.9486 - val_loss: 0.4161 - val_accuracy: 0.9281\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1956 - accuracy: 0.9491 - val_loss: 0.3897 - val_accuracy: 0.9245\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1874 - accuracy: 0.9505 - val_loss: 0.3383 - val_accuracy: 0.9492\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1700 - accuracy: 0.9564 - val_loss: 0.3521 - val_accuracy: 0.9424\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1688 - accuracy: 0.9564 - val_loss: 0.3244 - val_accuracy: 0.9591\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1622 - accuracy: 0.9576 - val_loss: 0.3081 - val_accuracy: 0.9562\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1534 - accuracy: 0.9612 - val_loss: 0.3279 - val_accuracy: 0.9553\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1467 - accuracy: 0.9631 - val_loss: 0.2797 - val_accuracy: 0.9696\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1420 - accuracy: 0.9647 - val_loss: 0.3271 - val_accuracy: 0.9465\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1371 - accuracy: 0.9679 - val_loss: 0.2950 - val_accuracy: 0.9586\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1445 - accuracy: 0.9630 - val_loss: 0.2936 - val_accuracy: 0.9628\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 11s 5ms/step - loss: 3.5295 - accuracy: 0.3352 - val_loss: 2.2967 - val_accuracy: 0.6081\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.5271 - accuracy: 0.7133 - val_loss: 1.3880 - val_accuracy: 0.7771\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.8843 - accuracy: 0.8215 - val_loss: 1.0361 - val_accuracy: 0.8361\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.6086 - accuracy: 0.8662 - val_loss: 0.8369 - val_accuracy: 0.8693\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4715 - accuracy: 0.8896 - val_loss: 0.7123 - val_accuracy: 0.8972\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3928 - accuracy: 0.9046 - val_loss: 0.6273 - val_accuracy: 0.8891\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3342 - accuracy: 0.9169 - val_loss: 0.5213 - val_accuracy: 0.9094\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2897 - accuracy: 0.9248 - val_loss: 0.4824 - val_accuracy: 0.9085\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2596 - accuracy: 0.9349 - val_loss: 0.4163 - val_accuracy: 0.9314\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2469 - accuracy: 0.9327 - val_loss: 0.4383 - val_accuracy: 0.9133\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2205 - accuracy: 0.9414 - val_loss: 0.4054 - val_accuracy: 0.9245\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2072 - accuracy: 0.9456 - val_loss: 0.3434 - val_accuracy: 0.9439\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.2009 - accuracy: 0.9464 - val_loss: 0.3454 - val_accuracy: 0.9426\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1835 - accuracy: 0.9522 - val_loss: 0.3329 - val_accuracy: 0.9452\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1854 - accuracy: 0.9490 - val_loss: 0.3150 - val_accuracy: 0.9525\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1683 - accuracy: 0.9571 - val_loss: 0.3225 - val_accuracy: 0.9492\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1711 - accuracy: 0.9575 - val_loss: 0.3568 - val_accuracy: 0.9384\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1604 - accuracy: 0.9593 - val_loss: 0.3006 - val_accuracy: 0.9518\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1558 - accuracy: 0.9594 - val_loss: 0.2864 - val_accuracy: 0.9589\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1502 - accuracy: 0.9614 - val_loss: 0.3398 - val_accuracy: 0.9344\n",
      "Average Validation Accuracy: 0.959081768989563\n",
      "Average Validation Loss: 0.20096292346715927\n",
      "Average Test Accuracy: 0.957654595375061\n",
      "Final Test Accuracy for each fold: 0.9697796106338501\n",
      "Number of input features: 13\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 10s 4ms/step - loss: 3.4809 - accuracy: 0.3507 - val_loss: 2.1556 - val_accuracy: 0.6220\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.4085 - accuracy: 0.7475 - val_loss: 1.2209 - val_accuracy: 0.7958\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.7758 - accuracy: 0.8438 - val_loss: 0.8620 - val_accuracy: 0.8480\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.5044 - accuracy: 0.8892 - val_loss: 0.6663 - val_accuracy: 0.8924\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3696 - accuracy: 0.9148 - val_loss: 0.5721 - val_accuracy: 0.9113\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2967 - accuracy: 0.9248 - val_loss: 0.5187 - val_accuracy: 0.9223\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2529 - accuracy: 0.9367 - val_loss: 0.4526 - val_accuracy: 0.9362\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2243 - accuracy: 0.9428 - val_loss: 0.4495 - val_accuracy: 0.9303\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.2026 - accuracy: 0.9498 - val_loss: 0.4120 - val_accuracy: 0.9424\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1879 - accuracy: 0.9510 - val_loss: 0.3957 - val_accuracy: 0.9463\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1706 - accuracy: 0.9566 - val_loss: 0.3686 - val_accuracy: 0.9503\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1610 - accuracy: 0.9588 - val_loss: 0.4116 - val_accuracy: 0.9404\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1608 - accuracy: 0.9588 - val_loss: 0.3801 - val_accuracy: 0.9492\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1485 - accuracy: 0.9640 - val_loss: 0.3535 - val_accuracy: 0.9553\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1495 - accuracy: 0.9634 - val_loss: 0.3375 - val_accuracy: 0.9591\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1384 - accuracy: 0.9653 - val_loss: 0.3375 - val_accuracy: 0.9604\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1313 - accuracy: 0.9676 - val_loss: 0.3864 - val_accuracy: 0.9483\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1353 - accuracy: 0.9684 - val_loss: 0.3355 - val_accuracy: 0.9593\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1264 - accuracy: 0.9704 - val_loss: 0.3338 - val_accuracy: 0.9595\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1196 - accuracy: 0.9738 - val_loss: 0.3235 - val_accuracy: 0.9639\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 11s 5ms/step - loss: 3.6353 - accuracy: 0.3011 - val_loss: 2.3575 - val_accuracy: 0.5696\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 1.4469 - accuracy: 0.7209 - val_loss: 1.2976 - val_accuracy: 0.7908\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.7505 - accuracy: 0.8478 - val_loss: 0.9013 - val_accuracy: 0.8515\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4733 - accuracy: 0.8986 - val_loss: 0.6651 - val_accuracy: 0.8997\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3444 - accuracy: 0.9215 - val_loss: 0.5346 - val_accuracy: 0.9243\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2764 - accuracy: 0.9330 - val_loss: 0.4954 - val_accuracy: 0.9129\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2374 - accuracy: 0.9421 - val_loss: 0.4524 - val_accuracy: 0.9250\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2128 - accuracy: 0.9473 - val_loss: 0.4317 - val_accuracy: 0.9232\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1908 - accuracy: 0.9536 - val_loss: 0.3490 - val_accuracy: 0.9547\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1768 - accuracy: 0.9566 - val_loss: 0.3622 - val_accuracy: 0.9459\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 8s 5ms/step - loss: 0.1665 - accuracy: 0.9582 - val_loss: 0.4104 - val_accuracy: 0.9263\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 8s 5ms/step - loss: 0.1569 - accuracy: 0.9603 - val_loss: 0.3258 - val_accuracy: 0.9476\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1510 - accuracy: 0.9616 - val_loss: 0.3662 - val_accuracy: 0.9393\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1424 - accuracy: 0.9649 - val_loss: 0.2905 - val_accuracy: 0.9578\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1377 - accuracy: 0.9665 - val_loss: 0.3024 - val_accuracy: 0.9584\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1338 - accuracy: 0.9671 - val_loss: 0.2800 - val_accuracy: 0.9567\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1266 - accuracy: 0.9720 - val_loss: 0.3064 - val_accuracy: 0.9547\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1271 - accuracy: 0.9684 - val_loss: 0.2905 - val_accuracy: 0.9586\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1254 - accuracy: 0.9711 - val_loss: 0.3079 - val_accuracy: 0.9507\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1179 - accuracy: 0.9725 - val_loss: 0.2724 - val_accuracy: 0.9624\n",
      "Average Validation Accuracy: 0.9708455204963684\n",
      "Average Validation Loss: 0.17573139071464539\n",
      "Average Test Accuracy: 0.9698901772499084\n",
      "Final Test Accuracy for each fold: 0.9744232296943665\n",
      "Number of input features: 14\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 10s 4ms/step - loss: 3.7709 - accuracy: 0.2891 - val_loss: 2.5095 - val_accuracy: 0.5861\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.6290 - accuracy: 0.7069 - val_loss: 1.3752 - val_accuracy: 0.7703\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.8963 - accuracy: 0.8330 - val_loss: 0.9909 - val_accuracy: 0.8495\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.5701 - accuracy: 0.8865 - val_loss: 0.7625 - val_accuracy: 0.8825\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4021 - accuracy: 0.9133 - val_loss: 0.6155 - val_accuracy: 0.9072\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3175 - accuracy: 0.9267 - val_loss: 0.5351 - val_accuracy: 0.9325\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2580 - accuracy: 0.9392 - val_loss: 0.4949 - val_accuracy: 0.9278\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2245 - accuracy: 0.9469 - val_loss: 0.4536 - val_accuracy: 0.9388\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2005 - accuracy: 0.9519 - val_loss: 0.4229 - val_accuracy: 0.9468\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1823 - accuracy: 0.9556 - val_loss: 0.3983 - val_accuracy: 0.9540\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1676 - accuracy: 0.9609 - val_loss: 0.3916 - val_accuracy: 0.9496\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1575 - accuracy: 0.9640 - val_loss: 0.4139 - val_accuracy: 0.9459\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1508 - accuracy: 0.9663 - val_loss: 0.3600 - val_accuracy: 0.9531\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1380 - accuracy: 0.9670 - val_loss: 0.3989 - val_accuracy: 0.9428\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1387 - accuracy: 0.9669 - val_loss: 0.3430 - val_accuracy: 0.9619\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1283 - accuracy: 0.9699 - val_loss: 0.3350 - val_accuracy: 0.9628\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1318 - accuracy: 0.9693 - val_loss: 0.3487 - val_accuracy: 0.9663\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1176 - accuracy: 0.9745 - val_loss: 0.3512 - val_accuracy: 0.9657\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1243 - accuracy: 0.9715 - val_loss: 0.3299 - val_accuracy: 0.9681\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1164 - accuracy: 0.9744 - val_loss: 0.3328 - val_accuracy: 0.9641\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 10s 4ms/step - loss: 3.3147 - accuracy: 0.3698 - val_loss: 2.1793 - val_accuracy: 0.6260\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.4573 - accuracy: 0.7178 - val_loss: 1.4059 - val_accuracy: 0.7668\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.9079 - accuracy: 0.8136 - val_loss: 1.0425 - val_accuracy: 0.8123\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.6344 - accuracy: 0.8581 - val_loss: 0.8253 - val_accuracy: 0.8640\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4847 - accuracy: 0.8845 - val_loss: 0.7109 - val_accuracy: 0.8845\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4028 - accuracy: 0.9009 - val_loss: 0.6091 - val_accuracy: 0.8990\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.3451 - accuracy: 0.9117 - val_loss: 0.5367 - val_accuracy: 0.9091\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3019 - accuracy: 0.9213 - val_loss: 0.4990 - val_accuracy: 0.9087\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2775 - accuracy: 0.9270 - val_loss: 0.4793 - val_accuracy: 0.9232\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2464 - accuracy: 0.9338 - val_loss: 0.4495 - val_accuracy: 0.9272\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2248 - accuracy: 0.9417 - val_loss: 0.3903 - val_accuracy: 0.9366\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2093 - accuracy: 0.9464 - val_loss: 0.3701 - val_accuracy: 0.9342\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1958 - accuracy: 0.9508 - val_loss: 0.3800 - val_accuracy: 0.9397\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1929 - accuracy: 0.9501 - val_loss: 0.3674 - val_accuracy: 0.9450\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1722 - accuracy: 0.9573 - val_loss: 0.3319 - val_accuracy: 0.9406\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1723 - accuracy: 0.9561 - val_loss: 0.3531 - val_accuracy: 0.9415\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1625 - accuracy: 0.9601 - val_loss: 0.3498 - val_accuracy: 0.9419\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1571 - accuracy: 0.9606 - val_loss: 0.3741 - val_accuracy: 0.9347\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1531 - accuracy: 0.9624 - val_loss: 0.2971 - val_accuracy: 0.9523\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1433 - accuracy: 0.9651 - val_loss: 0.3954 - val_accuracy: 0.9373\n",
      "Average Validation Accuracy: 0.9590817391872406\n",
      "Average Validation Loss: 0.22276268899440765\n",
      "Average Test Accuracy: 0.9583179652690887\n",
      "Final Test Accuracy for each fold: 0.9714748859405518\n",
      "Number of input features: 15\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 10s 4ms/step - loss: 3.4785 - accuracy: 0.3421 - val_loss: 2.2216 - val_accuracy: 0.6156\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.4920 - accuracy: 0.7282 - val_loss: 1.3135 - val_accuracy: 0.7628\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.8596 - accuracy: 0.8269 - val_loss: 0.8817 - val_accuracy: 0.8515\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.5486 - accuracy: 0.8795 - val_loss: 0.6768 - val_accuracy: 0.8854\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.3994 - accuracy: 0.9034 - val_loss: 0.5983 - val_accuracy: 0.8862\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.3210 - accuracy: 0.9203 - val_loss: 0.5429 - val_accuracy: 0.8975\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2668 - accuracy: 0.9312 - val_loss: 0.4275 - val_accuracy: 0.9338\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2259 - accuracy: 0.9423 - val_loss: 0.4572 - val_accuracy: 0.9188\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2150 - accuracy: 0.9422 - val_loss: 0.3577 - val_accuracy: 0.9498\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1913 - accuracy: 0.9507 - val_loss: 0.3633 - val_accuracy: 0.9446\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1792 - accuracy: 0.9509 - val_loss: 0.3525 - val_accuracy: 0.9430\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1682 - accuracy: 0.9560 - val_loss: 0.3293 - val_accuracy: 0.9450\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1570 - accuracy: 0.9588 - val_loss: 0.3183 - val_accuracy: 0.9520\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1637 - accuracy: 0.9578 - val_loss: 0.3443 - val_accuracy: 0.9474\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1428 - accuracy: 0.9646 - val_loss: 0.3363 - val_accuracy: 0.9410\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1485 - accuracy: 0.9625 - val_loss: 0.2831 - val_accuracy: 0.9606\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1407 - accuracy: 0.9646 - val_loss: 0.3583 - val_accuracy: 0.9366\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1318 - accuracy: 0.9677 - val_loss: 0.2747 - val_accuracy: 0.9692\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1287 - accuracy: 0.9684 - val_loss: 0.2768 - val_accuracy: 0.9608\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1345 - accuracy: 0.9661 - val_loss: 0.2793 - val_accuracy: 0.9663\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 10s 4ms/step - loss: 3.4755 - accuracy: 0.3406 - val_loss: 2.1679 - val_accuracy: 0.6678\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.3867 - accuracy: 0.7448 - val_loss: 1.3047 - val_accuracy: 0.7837\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.7719 - accuracy: 0.8423 - val_loss: 0.8963 - val_accuracy: 0.8623\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.5043 - accuracy: 0.8857 - val_loss: 0.6864 - val_accuracy: 0.8915\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3750 - accuracy: 0.9099 - val_loss: 0.5449 - val_accuracy: 0.9182\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.2969 - accuracy: 0.9248 - val_loss: 0.4795 - val_accuracy: 0.9228\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2564 - accuracy: 0.9362 - val_loss: 0.4154 - val_accuracy: 0.9327\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.2180 - accuracy: 0.9448 - val_loss: 0.3834 - val_accuracy: 0.9428\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2119 - accuracy: 0.9460 - val_loss: 0.4265 - val_accuracy: 0.9206\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1915 - accuracy: 0.9490 - val_loss: 0.3435 - val_accuracy: 0.9487\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1708 - accuracy: 0.9569 - val_loss: 0.3363 - val_accuracy: 0.9503\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1686 - accuracy: 0.9570 - val_loss: 0.3577 - val_accuracy: 0.9452\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1587 - accuracy: 0.9609 - val_loss: 0.3155 - val_accuracy: 0.9558\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1540 - accuracy: 0.9615 - val_loss: 0.3257 - val_accuracy: 0.9454\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1352 - accuracy: 0.9679 - val_loss: 0.3019 - val_accuracy: 0.9547\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1471 - accuracy: 0.9629 - val_loss: 0.3010 - val_accuracy: 0.9564\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1406 - accuracy: 0.9664 - val_loss: 0.2855 - val_accuracy: 0.9540\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1324 - accuracy: 0.9680 - val_loss: 0.2897 - val_accuracy: 0.9580\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1316 - accuracy: 0.9685 - val_loss: 0.2893 - val_accuracy: 0.9545\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1272 - accuracy: 0.9681 - val_loss: 0.3169 - val_accuracy: 0.9512\n",
      "Average Validation Accuracy: 0.964636892080307\n",
      "Average Validation Loss: 0.1891816034913063\n",
      "Average Test Accuracy: 0.9687108397483826\n",
      "Final Test Accuracy for each fold: 0.976708173751831\n",
      "Number of input features: 16\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 14s 6ms/step - loss: 3.2463 - accuracy: 0.3961 - val_loss: 1.9484 - val_accuracy: 0.6702\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.2519 - accuracy: 0.7680 - val_loss: 1.0664 - val_accuracy: 0.8046\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.6604 - accuracy: 0.8665 - val_loss: 0.7673 - val_accuracy: 0.8667\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4111 - accuracy: 0.9118 - val_loss: 0.5487 - val_accuracy: 0.9078\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.3085 - accuracy: 0.9297 - val_loss: 0.4856 - val_accuracy: 0.9248\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2482 - accuracy: 0.9410 - val_loss: 0.4089 - val_accuracy: 0.9402\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2219 - accuracy: 0.9467 - val_loss: 0.3763 - val_accuracy: 0.9388\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1929 - accuracy: 0.9535 - val_loss: 0.3627 - val_accuracy: 0.9333\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1778 - accuracy: 0.9575 - val_loss: 0.3281 - val_accuracy: 0.9479\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1628 - accuracy: 0.9589 - val_loss: 0.3746 - val_accuracy: 0.9248\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1544 - accuracy: 0.9633 - val_loss: 0.3253 - val_accuracy: 0.9540\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1425 - accuracy: 0.9624 - val_loss: 0.2940 - val_accuracy: 0.9575\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1385 - accuracy: 0.9643 - val_loss: 0.3117 - val_accuracy: 0.9448\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1339 - accuracy: 0.9676 - val_loss: 0.2911 - val_accuracy: 0.9586\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1238 - accuracy: 0.9706 - val_loss: 0.2812 - val_accuracy: 0.9582\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1211 - accuracy: 0.9717 - val_loss: 0.2766 - val_accuracy: 0.9692\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1206 - accuracy: 0.9716 - val_loss: 0.2649 - val_accuracy: 0.9637\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1114 - accuracy: 0.9745 - val_loss: 0.2498 - val_accuracy: 0.9751\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1148 - accuracy: 0.9742 - val_loss: 0.2559 - val_accuracy: 0.9734\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1112 - accuracy: 0.9731 - val_loss: 0.2570 - val_accuracy: 0.9723\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 10s 4ms/step - loss: 3.3576 - accuracy: 0.3702 - val_loss: 2.0938 - val_accuracy: 0.6587\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.3104 - accuracy: 0.7694 - val_loss: 1.2150 - val_accuracy: 0.8117\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.7303 - accuracy: 0.8555 - val_loss: 0.8482 - val_accuracy: 0.8585\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4750 - accuracy: 0.8977 - val_loss: 0.6564 - val_accuracy: 0.9039\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3516 - accuracy: 0.9192 - val_loss: 0.5205 - val_accuracy: 0.9201\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2827 - accuracy: 0.9351 - val_loss: 0.4727 - val_accuracy: 0.9289\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2447 - accuracy: 0.9400 - val_loss: 0.4257 - val_accuracy: 0.9305\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2136 - accuracy: 0.9487 - val_loss: 0.4038 - val_accuracy: 0.9342\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1962 - accuracy: 0.9514 - val_loss: 0.3750 - val_accuracy: 0.9316\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1763 - accuracy: 0.9585 - val_loss: 0.3456 - val_accuracy: 0.9426\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1718 - accuracy: 0.9563 - val_loss: 0.3309 - val_accuracy: 0.9540\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1532 - accuracy: 0.9619 - val_loss: 0.3113 - val_accuracy: 0.9492\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1538 - accuracy: 0.9630 - val_loss: 0.3069 - val_accuracy: 0.9536\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1446 - accuracy: 0.9650 - val_loss: 0.2912 - val_accuracy: 0.9564\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1446 - accuracy: 0.9643 - val_loss: 0.3341 - val_accuracy: 0.9373\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1329 - accuracy: 0.9689 - val_loss: 0.2562 - val_accuracy: 0.9652\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1318 - accuracy: 0.9704 - val_loss: 0.2785 - val_accuracy: 0.9512\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1273 - accuracy: 0.9711 - val_loss: 0.2615 - val_accuracy: 0.9637\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1253 - accuracy: 0.9703 - val_loss: 0.2494 - val_accuracy: 0.9617\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1159 - accuracy: 0.9744 - val_loss: 0.2671 - val_accuracy: 0.9578\n",
      "Average Validation Accuracy: 0.9716441631317139\n",
      "Average Validation Loss: 0.162453331053257\n",
      "Average Test Accuracy: 0.9729490578174591\n",
      "Final Test Accuracy for each fold: 0.976708173751831\n",
      "Number of input features: 17\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 10s 4ms/step - loss: 3.4707 - accuracy: 0.3515 - val_loss: 2.2366 - val_accuracy: 0.5976\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.5331 - accuracy: 0.7153 - val_loss: 1.3375 - val_accuracy: 0.7725\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.8812 - accuracy: 0.8160 - val_loss: 0.9180 - val_accuracy: 0.8374\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.5666 - accuracy: 0.8726 - val_loss: 0.7535 - val_accuracy: 0.8616\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4221 - accuracy: 0.8956 - val_loss: 0.5895 - val_accuracy: 0.8964\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3339 - accuracy: 0.9157 - val_loss: 0.4858 - val_accuracy: 0.9164\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2894 - accuracy: 0.9238 - val_loss: 0.4327 - val_accuracy: 0.9276\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2548 - accuracy: 0.9323 - val_loss: 0.4236 - val_accuracy: 0.9294\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2326 - accuracy: 0.9366 - val_loss: 0.4273 - val_accuracy: 0.9226\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2121 - accuracy: 0.9422 - val_loss: 0.3882 - val_accuracy: 0.9336\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1956 - accuracy: 0.9501 - val_loss: 0.3804 - val_accuracy: 0.9391\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1898 - accuracy: 0.9504 - val_loss: 0.3458 - val_accuracy: 0.9417\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1740 - accuracy: 0.9560 - val_loss: 0.3624 - val_accuracy: 0.9369\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1676 - accuracy: 0.9574 - val_loss: 0.3963 - val_accuracy: 0.9276\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1671 - accuracy: 0.9564 - val_loss: 0.3616 - val_accuracy: 0.9415\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1618 - accuracy: 0.9600 - val_loss: 0.3401 - val_accuracy: 0.9507\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1480 - accuracy: 0.9631 - val_loss: 0.4334 - val_accuracy: 0.9210\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1448 - accuracy: 0.9648 - val_loss: 0.3718 - val_accuracy: 0.9241\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1403 - accuracy: 0.9650 - val_loss: 0.2898 - val_accuracy: 0.9547\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1427 - accuracy: 0.9642 - val_loss: 0.2850 - val_accuracy: 0.9578\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 10s 4ms/step - loss: 3.3604 - accuracy: 0.3751 - val_loss: 2.1895 - val_accuracy: 0.6376\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.3977 - accuracy: 0.7474 - val_loss: 1.3819 - val_accuracy: 0.7987\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.7987 - accuracy: 0.8472 - val_loss: 0.9582 - val_accuracy: 0.8499\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.5291 - accuracy: 0.8945 - val_loss: 0.7353 - val_accuracy: 0.8926\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3831 - accuracy: 0.9152 - val_loss: 0.6382 - val_accuracy: 0.8957\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2988 - accuracy: 0.9302 - val_loss: 0.5196 - val_accuracy: 0.9237\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2465 - accuracy: 0.9405 - val_loss: 0.4330 - val_accuracy: 0.9380\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2206 - accuracy: 0.9474 - val_loss: 0.4026 - val_accuracy: 0.9397\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1946 - accuracy: 0.9538 - val_loss: 0.6141 - val_accuracy: 0.8726\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1774 - accuracy: 0.9548 - val_loss: 0.3748 - val_accuracy: 0.9468\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1722 - accuracy: 0.9569 - val_loss: 0.3422 - val_accuracy: 0.9564\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1619 - accuracy: 0.9606 - val_loss: 0.3396 - val_accuracy: 0.9496\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1548 - accuracy: 0.9609 - val_loss: 0.3210 - val_accuracy: 0.9553\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1394 - accuracy: 0.9650 - val_loss: 0.3637 - val_accuracy: 0.9505\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1474 - accuracy: 0.9651 - val_loss: 0.3002 - val_accuracy: 0.9606\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1464 - accuracy: 0.9635 - val_loss: 0.2882 - val_accuracy: 0.9615\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1278 - accuracy: 0.9706 - val_loss: 0.3082 - val_accuracy: 0.9527\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1336 - accuracy: 0.9669 - val_loss: 0.3084 - val_accuracy: 0.9582\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1293 - accuracy: 0.9699 - val_loss: 0.2809 - val_accuracy: 0.9617\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1264 - accuracy: 0.9706 - val_loss: 0.2684 - val_accuracy: 0.9692\n",
      "Average Validation Accuracy: 0.9713178277015686\n",
      "Average Validation Loss: 0.16712485998868942\n",
      "Average Test Accuracy: 0.972764790058136\n",
      "Final Test Accuracy for each fold: 0.9778875112533569\n",
      "Number of input features: 18\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 10s 4ms/step - loss: 3.3212 - accuracy: 0.3837 - val_loss: 2.0905 - val_accuracy: 0.6466\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.3992 - accuracy: 0.7360 - val_loss: 1.2052 - val_accuracy: 0.7943\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.8164 - accuracy: 0.8323 - val_loss: 0.9534 - val_accuracy: 0.8213\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.5563 - accuracy: 0.8728 - val_loss: 0.6830 - val_accuracy: 0.8719\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4244 - accuracy: 0.8941 - val_loss: 0.5859 - val_accuracy: 0.8939\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3484 - accuracy: 0.9122 - val_loss: 0.5646 - val_accuracy: 0.8972\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2860 - accuracy: 0.9303 - val_loss: 0.5062 - val_accuracy: 0.9052\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2611 - accuracy: 0.9331 - val_loss: 0.4419 - val_accuracy: 0.9094\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2267 - accuracy: 0.9406 - val_loss: 0.3851 - val_accuracy: 0.9360\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2118 - accuracy: 0.9460 - val_loss: 0.4105 - val_accuracy: 0.9274\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1927 - accuracy: 0.9494 - val_loss: 0.3610 - val_accuracy: 0.9382\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1871 - accuracy: 0.9548 - val_loss: 0.3271 - val_accuracy: 0.9501\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 6s 4ms/step - loss: 0.1647 - accuracy: 0.9598 - val_loss: 0.3271 - val_accuracy: 0.9529\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1715 - accuracy: 0.9564 - val_loss: 0.3682 - val_accuracy: 0.9274\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1565 - accuracy: 0.9626 - val_loss: 0.3277 - val_accuracy: 0.9542\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1614 - accuracy: 0.9600 - val_loss: 0.3167 - val_accuracy: 0.9496\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1419 - accuracy: 0.9643 - val_loss: 0.2888 - val_accuracy: 0.9615\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1398 - accuracy: 0.9668 - val_loss: 0.2831 - val_accuracy: 0.9630\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1405 - accuracy: 0.9663 - val_loss: 0.2910 - val_accuracy: 0.9622\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1399 - accuracy: 0.9666 - val_loss: 0.2857 - val_accuracy: 0.9637\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 10s 4ms/step - loss: 3.3002 - accuracy: 0.3986 - val_loss: 2.0041 - val_accuracy: 0.6836\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 1.2576 - accuracy: 0.7676 - val_loss: 1.2239 - val_accuracy: 0.8044\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.6937 - accuracy: 0.8581 - val_loss: 0.8365 - val_accuracy: 0.8649\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4565 - accuracy: 0.8975 - val_loss: 0.6799 - val_accuracy: 0.8975\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3421 - accuracy: 0.9192 - val_loss: 0.5682 - val_accuracy: 0.9061\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.2841 - accuracy: 0.9319 - val_loss: 0.4791 - val_accuracy: 0.9267\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2423 - accuracy: 0.9410 - val_loss: 0.4637 - val_accuracy: 0.9142\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2186 - accuracy: 0.9462 - val_loss: 0.3805 - val_accuracy: 0.9408\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1983 - accuracy: 0.9491 - val_loss: 0.3714 - val_accuracy: 0.9316\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1771 - accuracy: 0.9561 - val_loss: 0.3452 - val_accuracy: 0.9424\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1745 - accuracy: 0.9551 - val_loss: 0.3506 - val_accuracy: 0.9443\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1644 - accuracy: 0.9614 - val_loss: 0.3165 - val_accuracy: 0.9518\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1622 - accuracy: 0.9578 - val_loss: 0.3062 - val_accuracy: 0.9564\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1541 - accuracy: 0.9611 - val_loss: 0.3114 - val_accuracy: 0.9501\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1422 - accuracy: 0.9650 - val_loss: 0.2843 - val_accuracy: 0.9573\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1476 - accuracy: 0.9636 - val_loss: 0.2943 - val_accuracy: 0.9553\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1401 - accuracy: 0.9667 - val_loss: 0.2735 - val_accuracy: 0.9639\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1297 - accuracy: 0.9688 - val_loss: 0.2876 - val_accuracy: 0.9604\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1258 - accuracy: 0.9692 - val_loss: 0.2662 - val_accuracy: 0.9635\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1326 - accuracy: 0.9690 - val_loss: 0.2730 - val_accuracy: 0.9641\n",
      "Average Validation Accuracy: 0.9717170000076294\n",
      "Average Validation Loss: 0.16940812766551971\n",
      "Average Test Accuracy: 0.970811516046524\n",
      "Final Test Accuracy for each fold: 0.9712537527084351\n",
      "Number of input features: 19\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 10s 4ms/step - loss: 3.4022 - accuracy: 0.3664 - val_loss: 2.0025 - val_accuracy: 0.6528\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.2757 - accuracy: 0.7588 - val_loss: 1.0746 - val_accuracy: 0.8106\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.6713 - accuracy: 0.8478 - val_loss: 0.7162 - val_accuracy: 0.8675\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4307 - accuracy: 0.8977 - val_loss: 0.5673 - val_accuracy: 0.8845\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3316 - accuracy: 0.9132 - val_loss: 0.4681 - val_accuracy: 0.9094\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2769 - accuracy: 0.9287 - val_loss: 0.4048 - val_accuracy: 0.9245\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2391 - accuracy: 0.9381 - val_loss: 0.3502 - val_accuracy: 0.9303\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2089 - accuracy: 0.9450 - val_loss: 0.3064 - val_accuracy: 0.9507\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1938 - accuracy: 0.9478 - val_loss: 0.2849 - val_accuracy: 0.9512\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 6s 4ms/step - loss: 0.1772 - accuracy: 0.9535 - val_loss: 0.3379 - val_accuracy: 0.9419\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1652 - accuracy: 0.9577 - val_loss: 0.2834 - val_accuracy: 0.9545\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1597 - accuracy: 0.9600 - val_loss: 0.2932 - val_accuracy: 0.9527\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1446 - accuracy: 0.9631 - val_loss: 0.2987 - val_accuracy: 0.9518\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1404 - accuracy: 0.9679 - val_loss: 0.2705 - val_accuracy: 0.9551\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1389 - accuracy: 0.9666 - val_loss: 0.3459 - val_accuracy: 0.9364\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1341 - accuracy: 0.9661 - val_loss: 0.2655 - val_accuracy: 0.9635\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1289 - accuracy: 0.9680 - val_loss: 0.2837 - val_accuracy: 0.9487\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1201 - accuracy: 0.9727 - val_loss: 0.2500 - val_accuracy: 0.9672\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1241 - accuracy: 0.9712 - val_loss: 0.2308 - val_accuracy: 0.9745\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1163 - accuracy: 0.9734 - val_loss: 0.2361 - val_accuracy: 0.9738\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 10s 4ms/step - loss: 3.3773 - accuracy: 0.3844 - val_loss: 2.1412 - val_accuracy: 0.6664\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.3945 - accuracy: 0.7498 - val_loss: 1.2914 - val_accuracy: 0.7883\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.7620 - accuracy: 0.8577 - val_loss: 0.9114 - val_accuracy: 0.8693\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4742 - accuracy: 0.9030 - val_loss: 0.6899 - val_accuracy: 0.8999\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3448 - accuracy: 0.9257 - val_loss: 0.5658 - val_accuracy: 0.9160\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2663 - accuracy: 0.9400 - val_loss: 0.5359 - val_accuracy: 0.9270\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2366 - accuracy: 0.9444 - val_loss: 0.6205 - val_accuracy: 0.8519\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2020 - accuracy: 0.9486 - val_loss: 0.4024 - val_accuracy: 0.9441\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1828 - accuracy: 0.9568 - val_loss: 0.3891 - val_accuracy: 0.9452\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1701 - accuracy: 0.9591 - val_loss: 0.3764 - val_accuracy: 0.9404\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1639 - accuracy: 0.9604 - val_loss: 0.3443 - val_accuracy: 0.9492\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1468 - accuracy: 0.9651 - val_loss: 0.3176 - val_accuracy: 0.9582\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1392 - accuracy: 0.9648 - val_loss: 0.3175 - val_accuracy: 0.9520\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1380 - accuracy: 0.9680 - val_loss: 0.3060 - val_accuracy: 0.9626\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 6s 4ms/step - loss: 0.1285 - accuracy: 0.9714 - val_loss: 0.3197 - val_accuracy: 0.9512\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1274 - accuracy: 0.9705 - val_loss: 0.3009 - val_accuracy: 0.9666\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1203 - accuracy: 0.9731 - val_loss: 0.2918 - val_accuracy: 0.9600\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1201 - accuracy: 0.9724 - val_loss: 0.3192 - val_accuracy: 0.9512\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1249 - accuracy: 0.9718 - val_loss: 0.3238 - val_accuracy: 0.9494\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1180 - accuracy: 0.9739 - val_loss: 0.2732 - val_accuracy: 0.9677\n",
      "Average Validation Accuracy: 0.9758195877075195\n",
      "Average Validation Loss: 0.15489347279071808\n",
      "Average Test Accuracy: 0.9763396382331848\n",
      "Final Test Accuracy for each fold: 0.9783297777175903\n",
      "Number of input features: 20\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 10s 4ms/step - loss: 3.3689 - accuracy: 0.3669 - val_loss: 2.1104 - val_accuracy: 0.6304\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.3359 - accuracy: 0.7462 - val_loss: 1.0893 - val_accuracy: 0.7971\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.6971 - accuracy: 0.8503 - val_loss: 0.7169 - val_accuracy: 0.8689\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4367 - accuracy: 0.9011 - val_loss: 0.5551 - val_accuracy: 0.9111\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3169 - accuracy: 0.9249 - val_loss: 0.4466 - val_accuracy: 0.9265\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.2540 - accuracy: 0.9393 - val_loss: 0.4062 - val_accuracy: 0.9287\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2115 - accuracy: 0.9498 - val_loss: 0.3988 - val_accuracy: 0.9186\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1885 - accuracy: 0.9536 - val_loss: 0.3294 - val_accuracy: 0.9430\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1806 - accuracy: 0.9540 - val_loss: 0.3412 - val_accuracy: 0.9417\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1622 - accuracy: 0.9604 - val_loss: 0.2910 - val_accuracy: 0.9505\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1565 - accuracy: 0.9623 - val_loss: 0.3255 - val_accuracy: 0.9494\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1526 - accuracy: 0.9620 - val_loss: 0.2840 - val_accuracy: 0.9573\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1406 - accuracy: 0.9669 - val_loss: 0.2749 - val_accuracy: 0.9648\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1373 - accuracy: 0.9663 - val_loss: 0.3420 - val_accuracy: 0.9355\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1352 - accuracy: 0.9665 - val_loss: 0.2747 - val_accuracy: 0.9602\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1257 - accuracy: 0.9701 - val_loss: 0.2363 - val_accuracy: 0.9714\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1273 - accuracy: 0.9693 - val_loss: 0.2945 - val_accuracy: 0.9520\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1245 - accuracy: 0.9693 - val_loss: 0.2626 - val_accuracy: 0.9613\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1140 - accuracy: 0.9751 - val_loss: 0.2276 - val_accuracy: 0.9732\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1174 - accuracy: 0.9725 - val_loss: 0.2473 - val_accuracy: 0.9611\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 11s 4ms/step - loss: 3.4625 - accuracy: 0.3450 - val_loss: 2.2425 - val_accuracy: 0.6407\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.3999 - accuracy: 0.7425 - val_loss: 1.2982 - val_accuracy: 0.8150\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.7610 - accuracy: 0.8562 - val_loss: 0.9323 - val_accuracy: 0.8671\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4891 - accuracy: 0.9006 - val_loss: 0.6900 - val_accuracy: 0.8913\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3527 - accuracy: 0.9223 - val_loss: 0.5462 - val_accuracy: 0.9186\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 6s 4ms/step - loss: 0.2818 - accuracy: 0.9355 - val_loss: 0.4619 - val_accuracy: 0.9314\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2359 - accuracy: 0.9442 - val_loss: 0.4141 - val_accuracy: 0.9358\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2056 - accuracy: 0.9523 - val_loss: 0.3585 - val_accuracy: 0.9463\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1833 - accuracy: 0.9561 - val_loss: 0.3531 - val_accuracy: 0.9276\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1724 - accuracy: 0.9588 - val_loss: 0.2991 - val_accuracy: 0.9547\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1598 - accuracy: 0.9621 - val_loss: 0.3372 - val_accuracy: 0.9430\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1553 - accuracy: 0.9634 - val_loss: 0.2712 - val_accuracy: 0.9617\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1444 - accuracy: 0.9651 - val_loss: 0.5054 - val_accuracy: 0.9061\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1397 - accuracy: 0.9662 - val_loss: 0.2607 - val_accuracy: 0.9578\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1313 - accuracy: 0.9697 - val_loss: 0.2558 - val_accuracy: 0.9652\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1255 - accuracy: 0.9701 - val_loss: 0.2723 - val_accuracy: 0.9600\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1257 - accuracy: 0.9725 - val_loss: 0.3210 - val_accuracy: 0.9529\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1187 - accuracy: 0.9730 - val_loss: 0.2670 - val_accuracy: 0.9584\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1247 - accuracy: 0.9710 - val_loss: 0.2623 - val_accuracy: 0.9595\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1178 - accuracy: 0.9734 - val_loss: 0.2770 - val_accuracy: 0.9531\n",
      "Average Validation Accuracy: 0.9653994143009186\n",
      "Average Validation Loss: 0.17240630835294724\n",
      "Average Test Accuracy: 0.966278463602066\n",
      "Final Test Accuracy for each fold: 0.9720645546913147\n"
     ]
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_fs, Y, test_size=0.33, random_state=1)\n",
    "\n",
    "# prepare output data\n",
    "y_train_enc, y_test_enc = prepare_targets(y_train, y_test)\n",
    "\n",
    "# Define the number of folds for k-fold cross-validation\n",
    "k = 2\n",
    "\n",
    "# Define the cross-validation method\n",
    "cv_method = StratifiedKFold(n_splits=k)\n",
    "\n",
    "# Initialize the list to store the history, train & validation(accuracy & loss) for each model\n",
    "models = []\n",
    "model_history = []\n",
    "model_accuracy = []\n",
    "model_train_acc = []\n",
    "model_train_loss = []\n",
    "model_val_acc = []\n",
    "model_val_loss = []\n",
    "\n",
    "\n",
    "for i in range(1,21):\n",
    "\n",
    "    models_fold = []\n",
    "    hist = []\n",
    "    train_accuracy = []\n",
    "    train_loss = []\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "    test_accuracy = []\n",
    "\n",
    "    print(\"Number of input features:\",i)\n",
    "\n",
    "    # Select the input features from the input data\n",
    "    X_train_selected = X_train[:, :i]\n",
    "    X_test_selected = X_test[:, :i]\n",
    "\n",
    "    # Loop over the folds\n",
    "    for fold, (train_index, val_index) in enumerate(cv_method.split(X_train_selected, y_train)):\n",
    "\n",
    "        print(\"Fold:\", fold+1)\n",
    "\n",
    "        # Split the data into train and validation sets using the current fold index\n",
    "        X_train_fold  = X_train_selected[train_index]\n",
    "        y_train_fold  = y_train[train_index]\n",
    "        X_val_fold = X_train_selected[val_index]\n",
    "        y_val_fold = y_train[val_index]\n",
    "\n",
    "        # Prepare the target data\n",
    "        y_train_fold_enc, y_val_fold_enc = prepare_targets(y_train_fold, y_val_fold)\n",
    "\n",
    "        # build the model\n",
    "        model = MLP_model(i)\n",
    "\n",
    "        # Fit the model to the training data for the current fold\n",
    "        history = model.fit(X_train_fold, to_categorical(y_train_fold_enc, num_classes=373), epochs=20, batch_size=5, verbose=1, validation_split = 0.33)\n",
    "    \n",
    "        # Evaluate the model on the validation data for the current fold\n",
    "        val_scores = model.evaluate(X_val_fold, to_categorical(y_val_fold_enc, num_classes=373), verbose=0)\n",
    "        val_accuracy.append(val_scores[1])\n",
    "        val_loss.append(val_scores[0])\n",
    "\n",
    "        # Evaluate the model on the test data for the current fold\n",
    "        test_scores = model.evaluate(X_test_selected, to_categorical(y_test_enc, num_classes=373), verbose=0)\n",
    "        test_accuracy.append(test_scores[1])\n",
    "\n",
    "        # add the model to the list of models\n",
    "        models_fold.append(model)\n",
    "        hist.append(history)\n",
    "\n",
    "        # store the training accuracy and loss for each fold\n",
    "        train_accuracy.append(history.history['accuracy'])\n",
    "        train_loss.append(history.history['loss'])\n",
    "        \n",
    "    # Calculate the average test and validation accuracy and loss across all folds\n",
    "    avg_test_acc = sum(test_accuracy) / len(test_accuracy)\n",
    "    avg_val_acc = sum(val_accuracy) / len(val_accuracy)\n",
    "    avg_val_loss = sum(val_loss) / len(val_loss)\n",
    "\n",
    "    # Print the average validation and test accuracy and loss\n",
    "    print(\"Average Validation Accuracy:\", avg_val_acc)\n",
    "    print(\"Average Validation Loss:\",avg_val_loss)\n",
    "    print(\"Average Test Accuracy:\", avg_test_acc)\n",
    "\n",
    "    best_fold_index = test_accuracy.index(max(test_accuracy))\n",
    "    model_accuracy.append(test_accuracy[best_fold_index])\n",
    "    models.append(models_fold[best_fold_index])\n",
    "    model_history.append(hist[best_fold_index])\n",
    "    model_train_acc.append(train_accuracy[best_fold_index])\n",
    "    model_train_loss.append(train_loss[best_fold_index])\n",
    "    model_val_acc.append(val_accuracy[best_fold_index])\n",
    "    model_val_loss.append(val_loss[best_fold_index])\n",
    "\n",
    "\n",
    "    print(\"Final Test Accuracy for each fold:\", test_accuracy[best_fold_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to show the no of input features and its corresponding model accuracy\n",
    "model_list = []\n",
    "\n",
    "#Iterate through each model's accuracy \n",
    "for i in range (len(model_accuracy)):\n",
    "    #get the number of input features for the current model\n",
    "    no_features = i + 1\n",
    "\n",
    "    #round the model accuries to 3 d.p.\n",
    "    rounded_model_acc = round(model_accuracy[i], 3)\n",
    "    \n",
    "    model_list.append([no_features, rounded_model_acc])\n",
    "\n",
    "models_df = pd.DataFrame(model_list, columns=[\"No of input features\", \"Model accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 0.9783297777175903\n"
     ]
    }
   ],
   "source": [
    "# Select the final model based on the max test accuracy across all models\n",
    "\n",
    "best_model_index = model_accuracy.index(max(model_accuracy))\n",
    "\n",
    "best_model = models[best_model_index]\n",
    "best_model_history = model_history[best_model_index]\n",
    "best_model_train_acc = model_train_acc[best_model_index]\n",
    "best_model_train_loss = model_train_loss[best_model_index]\n",
    "best_model_val_acc = model_val_acc[best_model_index]\n",
    "best_model_val_loss = model_val_loss[best_model_index]\n",
    "\n",
    "# summarize history for accuracy  \n",
    "plt.subplot(211)  \n",
    "plt.plot(best_model_history.history['accuracy'])  \n",
    "plt.plot(best_model_history.history['val_accuracy'])  \n",
    "plt.title('model accuracy')  \n",
    "plt.ylabel('accuracy')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'test'], loc='lower right')  \n",
    "\n",
    "# summarize history for loss\n",
    "plt.subplot(212)  \n",
    "plt.plot(best_model_history.history['loss'])  \n",
    "plt.plot(best_model_history.history['val_loss'])  \n",
    "plt.title('model loss')  \n",
    "plt.ylabel('loss')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'test'], loc='upper right')  \n",
    "plt.subplots_adjust(hspace=0.7)\n",
    "\n",
    "plt.show() \n",
    "\n",
    "print(\"Final Test Accuracy:\", model_accuracy[best_model_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424/424 [==============================] - 2s 2ms/step\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       591\n",
      "           1       1.00      1.00      1.00       430\n",
      "           2       1.00      1.00      1.00       419\n",
      "           3       1.00      1.00      1.00       384\n",
      "           4       1.00      1.00      1.00       339\n",
      "           5       1.00      1.00      1.00       342\n",
      "           6       1.00      1.00      1.00       310\n",
      "           7       1.00      1.00      1.00       325\n",
      "           8       1.00      1.00      1.00       294\n",
      "           9       1.00      1.00      1.00       269\n",
      "          10       1.00      1.00      1.00       296\n",
      "          11       1.00      1.00      1.00       258\n",
      "          12       1.00      1.00      1.00       247\n",
      "          13       1.00      1.00      1.00       237\n",
      "          14       0.98      1.00      0.99       239\n",
      "          15       1.00      1.00      1.00       235\n",
      "          16       1.00      1.00      1.00       213\n",
      "          17       1.00      1.00      1.00       202\n",
      "          18       1.00      1.00      1.00       196\n",
      "          19       1.00      1.00      1.00       181\n",
      "          20       0.95      1.00      0.98       177\n",
      "          21       1.00      1.00      1.00       177\n",
      "          22       1.00      1.00      1.00       155\n",
      "          23       1.00      1.00      1.00       155\n",
      "          24       1.00      1.00      1.00       144\n",
      "          25       1.00      1.00      1.00       126\n",
      "          26       1.00      1.00      1.00       108\n",
      "          27       0.93      1.00      0.96       121\n",
      "          28       1.00      1.00      1.00        95\n",
      "          29       1.00      1.00      1.00       106\n",
      "          30       1.00      1.00      1.00       102\n",
      "          31       1.00      1.00      1.00        86\n",
      "          32       1.00      1.00      1.00       108\n",
      "          33       1.00      1.00      1.00        88\n",
      "          34       1.00      1.00      1.00       102\n",
      "          35       1.00      1.00      1.00        88\n",
      "          36       1.00      1.00      1.00        83\n",
      "          37       1.00      1.00      1.00        93\n",
      "          38       1.00      1.00      1.00        76\n",
      "          39       1.00      1.00      1.00        85\n",
      "          40       1.00      1.00      1.00        86\n",
      "          41       1.00      1.00      1.00        85\n",
      "          42       1.00      1.00      1.00        68\n",
      "          43       0.74      1.00      0.85        75\n",
      "          44       1.00      1.00      1.00        71\n",
      "          45       0.73      0.79      0.76        58\n",
      "          46       1.00      1.00      1.00        71\n",
      "          47       0.51      0.98      0.67        57\n",
      "          48       1.00      1.00      1.00        67\n",
      "          49       0.89      0.89      0.89        47\n",
      "          50       1.00      1.00      1.00        48\n",
      "          51       1.00      1.00      1.00        47\n",
      "          52       1.00      1.00      1.00        43\n",
      "          53       1.00      1.00      1.00        51\n",
      "          54       1.00      1.00      1.00        44\n",
      "          55       1.00      1.00      1.00        51\n",
      "          56       1.00      1.00      1.00        45\n",
      "          57       1.00      1.00      1.00        44\n",
      "          58       0.84      1.00      0.91        41\n",
      "          59       1.00      1.00      1.00        41\n",
      "          60       1.00      1.00      1.00        52\n",
      "          61       1.00      1.00      1.00        43\n",
      "          62       1.00      1.00      1.00        37\n",
      "          63       1.00      1.00      1.00        43\n",
      "          64       0.95      1.00      0.98        42\n",
      "          65       1.00      1.00      1.00        46\n",
      "          66       1.00      1.00      1.00        43\n",
      "          67       1.00      1.00      1.00        40\n",
      "          68       1.00      1.00      1.00        44\n",
      "          69       1.00      1.00      1.00        43\n",
      "          70       0.86      1.00      0.93        38\n",
      "          71       1.00      1.00      1.00        33\n",
      "          72       1.00      1.00      1.00        45\n",
      "          73       1.00      1.00      1.00        38\n",
      "          74       1.00      1.00      1.00        42\n",
      "          75       1.00      1.00      1.00        39\n",
      "          76       0.00      0.00      0.00        30\n",
      "          77       1.00      1.00      1.00        28\n",
      "          78       1.00      0.93      0.96        28\n",
      "          79       1.00      1.00      1.00        32\n",
      "          80       1.00      1.00      1.00        33\n",
      "          81       1.00      1.00      1.00        31\n",
      "          82       1.00      1.00      1.00        35\n",
      "          83       1.00      1.00      1.00        39\n",
      "          84       1.00      1.00      1.00        27\n",
      "          85       1.00      1.00      1.00        36\n",
      "          86       1.00      1.00      1.00        31\n",
      "          87       1.00      1.00      1.00        28\n",
      "          88       1.00      1.00      1.00        20\n",
      "          89       1.00      1.00      1.00        33\n",
      "          90       1.00      1.00      1.00        24\n",
      "          91       1.00      1.00      1.00        22\n",
      "          92       1.00      1.00      1.00        26\n",
      "          93       1.00      1.00      1.00        35\n",
      "          94       1.00      0.48      0.65        27\n",
      "          95       1.00      1.00      1.00        23\n",
      "          96       1.00      1.00      1.00        27\n",
      "          97       1.00      1.00      1.00        28\n",
      "          98       0.92      0.69      0.79        16\n",
      "          99       1.00      1.00      1.00        35\n",
      "         100       1.00      1.00      1.00        28\n",
      "         101       1.00      1.00      1.00        25\n",
      "         102       1.00      1.00      1.00        26\n",
      "         103       1.00      1.00      1.00        33\n",
      "         104       0.00      0.00      0.00        26\n",
      "         105       1.00      1.00      1.00        24\n",
      "         106       0.92      1.00      0.96        22\n",
      "         107       0.87      1.00      0.93        26\n",
      "         108       1.00      1.00      1.00        25\n",
      "         109       1.00      1.00      1.00        16\n",
      "         110       1.00      1.00      1.00        20\n",
      "         111       1.00      1.00      1.00        26\n",
      "         112       1.00      1.00      1.00        18\n",
      "         113       0.68      1.00      0.81        23\n",
      "         114       1.00      1.00      1.00        25\n",
      "         115       1.00      1.00      1.00        18\n",
      "         116       1.00      1.00      1.00        19\n",
      "         117       1.00      1.00      1.00        16\n",
      "         118       1.00      1.00      1.00        26\n",
      "         119       1.00      1.00      1.00        22\n",
      "         120       0.89      1.00      0.94        17\n",
      "         121       1.00      1.00      1.00        15\n",
      "         122       1.00      1.00      1.00        18\n",
      "         123       1.00      1.00      1.00        20\n",
      "         124       1.00      1.00      1.00        14\n",
      "         125       1.00      1.00      1.00        22\n",
      "         126       1.00      1.00      1.00        19\n",
      "         127       1.00      1.00      1.00        27\n",
      "         128       1.00      1.00      1.00        26\n",
      "         129       0.00      0.00      0.00        21\n",
      "         130       1.00      1.00      1.00        18\n",
      "         131       1.00      1.00      1.00        18\n",
      "         132       1.00      1.00      1.00        20\n",
      "         133       1.00      1.00      1.00        14\n",
      "         134       1.00      1.00      1.00        19\n",
      "         135       1.00      1.00      1.00        16\n",
      "         136       1.00      1.00      1.00        23\n",
      "         137       0.00      0.00      0.00        11\n",
      "         138       1.00      1.00      1.00        14\n",
      "         139       1.00      1.00      1.00        20\n",
      "         140       1.00      1.00      1.00        23\n",
      "         141       1.00      0.86      0.92        14\n",
      "         142       1.00      1.00      1.00        13\n",
      "         143       1.00      1.00      1.00        23\n",
      "         144       1.00      0.82      0.90        17\n",
      "         145       1.00      1.00      1.00        24\n",
      "         146       1.00      0.88      0.93        16\n",
      "         147       1.00      1.00      1.00        19\n",
      "         148       1.00      1.00      1.00        22\n",
      "         149       1.00      1.00      1.00        15\n",
      "         150       1.00      1.00      1.00        11\n",
      "         151       1.00      1.00      1.00        19\n",
      "         152       1.00      1.00      1.00        20\n",
      "         153       0.86      1.00      0.92        24\n",
      "         154       0.92      1.00      0.96        11\n",
      "         155       1.00      1.00      1.00        17\n",
      "         156       1.00      1.00      1.00        18\n",
      "         157       1.00      1.00      1.00        12\n",
      "         158       1.00      1.00      1.00        18\n",
      "         159       1.00      1.00      1.00        20\n",
      "         160       1.00      1.00      1.00        20\n",
      "         161       1.00      1.00      1.00        16\n",
      "         162       1.00      1.00      1.00        15\n",
      "         163       1.00      1.00      1.00        15\n",
      "         164       1.00      1.00      1.00        13\n",
      "         165       1.00      0.74      0.85        19\n",
      "         166       1.00      1.00      1.00        11\n",
      "         167       1.00      1.00      1.00         9\n",
      "         168       1.00      1.00      1.00        11\n",
      "         169       1.00      1.00      1.00        21\n",
      "         170       1.00      1.00      1.00        15\n",
      "         171       1.00      1.00      1.00        18\n",
      "         172       1.00      1.00      1.00        11\n",
      "         173       1.00      1.00      1.00        16\n",
      "         174       1.00      1.00      1.00        10\n",
      "         175       1.00      1.00      1.00        11\n",
      "         176       1.00      1.00      1.00        10\n",
      "         177       1.00      1.00      1.00        15\n",
      "         178       1.00      1.00      1.00        11\n",
      "         179       1.00      1.00      1.00        15\n",
      "         180       1.00      1.00      1.00        13\n",
      "         181       1.00      1.00      1.00        15\n",
      "         182       1.00      1.00      1.00         9\n",
      "         183       1.00      1.00      1.00        16\n",
      "         184       1.00      1.00      1.00         8\n",
      "         185       0.36      1.00      0.53        12\n",
      "         186       1.00      1.00      1.00        15\n",
      "         187       1.00      1.00      1.00        15\n",
      "         188       0.00      0.00      0.00        13\n",
      "         189       1.00      1.00      1.00        14\n",
      "         190       1.00      1.00      1.00        11\n",
      "         191       1.00      1.00      1.00        10\n",
      "         192       1.00      1.00      1.00        17\n",
      "         193       1.00      1.00      1.00         9\n",
      "         194       1.00      1.00      1.00         9\n",
      "         195       1.00      1.00      1.00         4\n",
      "         196       1.00      1.00      1.00         7\n",
      "         197       1.00      1.00      1.00         7\n",
      "         198       1.00      1.00      1.00        12\n",
      "         199       1.00      1.00      1.00        14\n",
      "         200       1.00      1.00      1.00         6\n",
      "         201       1.00      1.00      1.00         9\n",
      "         202       1.00      1.00      1.00         7\n",
      "         203       0.27      1.00      0.43         6\n",
      "         204       1.00      1.00      1.00        11\n",
      "         205       1.00      1.00      1.00        14\n",
      "         206       1.00      1.00      1.00        12\n",
      "         207       1.00      1.00      1.00        14\n",
      "         208       1.00      1.00      1.00        12\n",
      "         209       1.00      1.00      1.00         7\n",
      "         210       1.00      1.00      1.00        19\n",
      "         211       1.00      1.00      1.00         7\n",
      "         212       1.00      1.00      1.00        11\n",
      "         213       1.00      1.00      1.00         9\n",
      "         214       1.00      1.00      1.00         7\n",
      "         215       1.00      1.00      1.00         6\n",
      "         216       1.00      1.00      1.00        12\n",
      "         217       1.00      1.00      1.00        12\n",
      "         218       0.00      0.00      0.00         9\n",
      "         219       1.00      1.00      1.00         6\n",
      "         220       1.00      1.00      1.00         8\n",
      "         221       1.00      1.00      1.00         5\n",
      "         222       0.31      1.00      0.47         4\n",
      "         223       0.56      1.00      0.72        14\n",
      "         224       1.00      1.00      1.00        13\n",
      "         225       1.00      1.00      1.00         4\n",
      "         226       1.00      1.00      1.00        10\n",
      "         227       0.75      1.00      0.86        12\n",
      "         228       1.00      1.00      1.00        13\n",
      "         229       1.00      1.00      1.00        11\n",
      "         230       1.00      0.60      0.75         5\n",
      "         231       1.00      1.00      1.00         6\n",
      "         232       0.00      0.00      0.00         8\n",
      "         233       1.00      1.00      1.00        10\n",
      "         234       1.00      1.00      1.00         4\n",
      "         235       0.53      1.00      0.70         8\n",
      "         236       1.00      1.00      1.00         9\n",
      "         237       1.00      1.00      1.00         8\n",
      "         238       1.00      1.00      1.00        10\n",
      "         239       0.35      1.00      0.51         9\n",
      "         240       1.00      1.00      1.00         7\n",
      "         241       0.00      0.00      0.00         8\n",
      "         242       1.00      1.00      1.00         6\n",
      "         243       1.00      1.00      1.00         7\n",
      "         244       1.00      1.00      1.00         9\n",
      "         245       0.00      0.00      0.00         7\n",
      "         246       1.00      0.56      0.71         9\n",
      "         247       1.00      1.00      1.00         7\n",
      "         248       1.00      1.00      1.00        10\n",
      "         249       1.00      1.00      1.00        10\n",
      "         250       1.00      1.00      1.00         7\n",
      "         251       1.00      1.00      1.00         6\n",
      "         252       1.00      1.00      1.00        11\n",
      "         253       1.00      1.00      1.00         7\n",
      "         254       1.00      1.00      1.00         8\n",
      "         255       1.00      0.80      0.89         5\n",
      "         256       1.00      1.00      1.00         7\n",
      "         257       1.00      1.00      1.00         9\n",
      "         258       1.00      1.00      1.00         6\n",
      "         259       1.00      0.67      0.80         3\n",
      "         260       1.00      1.00      1.00         4\n",
      "         261       1.00      1.00      1.00         2\n",
      "         262       1.00      1.00      1.00         5\n",
      "         263       1.00      1.00      1.00        12\n",
      "         264       1.00      1.00      1.00         5\n",
      "         265       1.00      1.00      1.00         7\n",
      "         266       1.00      1.00      1.00        10\n",
      "         267       1.00      1.00      1.00         8\n",
      "         268       0.00      0.00      0.00         9\n",
      "         269       1.00      1.00      1.00         6\n",
      "         270       1.00      1.00      1.00         4\n",
      "         271       0.88      1.00      0.93         7\n",
      "         272       1.00      1.00      1.00        10\n",
      "         273       1.00      1.00      1.00         3\n",
      "         274       1.00      1.00      1.00         9\n",
      "         275       1.00      1.00      1.00         6\n",
      "         276       1.00      1.00      1.00         5\n",
      "         277       0.00      0.00      0.00         4\n",
      "         278       0.75      1.00      0.86         3\n",
      "         279       1.00      1.00      1.00         4\n",
      "         280       0.00      0.00      0.00         5\n",
      "         281       1.00      1.00      1.00         6\n",
      "         282       1.00      1.00      1.00        11\n",
      "         283       1.00      1.00      1.00         6\n",
      "         284       1.00      1.00      1.00         2\n",
      "         285       1.00      1.00      1.00         4\n",
      "         286       1.00      1.00      1.00         7\n",
      "         287       0.00      0.00      0.00         9\n",
      "         288       1.00      0.71      0.83         7\n",
      "         289       1.00      1.00      1.00         7\n",
      "         290       1.00      1.00      1.00         6\n",
      "         291       1.00      1.00      1.00         5\n",
      "         292       0.43      1.00      0.60         6\n",
      "         293       1.00      1.00      1.00         8\n",
      "         294       1.00      1.00      1.00         7\n",
      "         295       0.00      0.00      0.00         3\n",
      "         296       1.00      0.67      0.80         6\n",
      "         297       1.00      1.00      1.00         8\n",
      "         298       1.00      1.00      1.00         4\n",
      "         299       1.00      1.00      1.00         6\n",
      "         300       1.00      1.00      1.00         5\n",
      "         301       0.00      0.00      0.00         5\n",
      "         302       0.71      1.00      0.83         5\n",
      "         303       1.00      1.00      1.00         2\n",
      "         304       1.00      1.00      1.00         8\n",
      "         305       1.00      1.00      1.00         3\n",
      "         306       1.00      1.00      1.00         6\n",
      "         307       1.00      1.00      1.00         7\n",
      "         308       1.00      1.00      1.00         4\n",
      "         309       1.00      1.00      1.00         4\n",
      "         310       0.00      0.00      0.00         9\n",
      "         311       1.00      1.00      1.00         7\n",
      "         312       1.00      1.00      1.00         6\n",
      "         313       1.00      1.00      1.00         6\n",
      "         314       1.00      0.75      0.86         8\n",
      "         315       1.00      1.00      1.00         7\n",
      "         316       1.00      1.00      1.00         3\n",
      "         317       1.00      1.00      1.00         6\n",
      "         318       1.00      1.00      1.00        10\n",
      "         319       1.00      1.00      1.00         6\n",
      "         320       1.00      1.00      1.00         2\n",
      "         321       0.80      1.00      0.89         8\n",
      "         322       1.00      1.00      1.00         4\n",
      "         323       1.00      1.00      1.00         4\n",
      "         324       0.00      0.00      0.00         8\n",
      "         325       1.00      1.00      1.00         4\n",
      "         326       1.00      1.00      1.00         7\n",
      "         327       1.00      1.00      1.00         4\n",
      "         328       1.00      1.00      1.00         6\n",
      "         329       1.00      1.00      1.00         4\n",
      "         330       0.38      1.00      0.55         3\n",
      "         331       0.00      0.00      0.00         8\n",
      "         332       1.00      1.00      1.00         1\n",
      "         333       1.00      1.00      1.00         3\n",
      "         334       1.00      1.00      1.00         4\n",
      "         335       1.00      1.00      1.00         3\n",
      "         336       0.00      0.00      0.00         6\n",
      "         337       1.00      1.00      1.00         2\n",
      "         338       1.00      1.00      1.00         7\n",
      "         339       1.00      1.00      1.00         4\n",
      "         340       1.00      1.00      1.00         6\n",
      "         341       1.00      1.00      1.00         7\n",
      "         342       1.00      1.00      1.00         2\n",
      "         343       1.00      0.80      0.89         5\n",
      "         344       0.00      0.00      0.00         4\n",
      "         345       0.00      0.00      0.00         1\n",
      "         346       1.00      1.00      1.00         2\n",
      "         347       0.16      1.00      0.28         4\n",
      "         348       1.00      1.00      1.00         7\n",
      "         349       0.00      0.00      0.00         4\n",
      "         350       1.00      1.00      1.00         6\n",
      "         351       1.00      1.00      1.00         4\n",
      "         352       1.00      1.00      1.00         5\n",
      "         353       1.00      1.00      1.00         4\n",
      "         354       1.00      1.00      1.00         3\n",
      "         355       1.00      1.00      1.00         1\n",
      "         356       1.00      1.00      1.00         4\n",
      "         357       1.00      1.00      1.00         1\n",
      "         358       0.43      1.00      0.60         3\n",
      "         359       1.00      1.00      1.00         4\n",
      "         360       0.00      0.00      0.00         3\n",
      "         361       0.00      0.00      0.00         3\n",
      "         362       1.00      1.00      1.00         3\n",
      "         363       0.00      0.00      0.00         2\n",
      "         364       0.00      0.00      0.00         3\n",
      "         365       0.00      0.00      0.00         3\n",
      "         366       0.00      0.00      0.00         2\n",
      "         367       1.00      1.00      1.00         3\n",
      "         368       1.00      1.00      1.00         1\n",
      "         369       1.00      1.00      1.00         2\n",
      "         370       1.00      1.00      1.00         2\n",
      "         372       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.98     13567\n",
      "   macro avg       0.90      0.91      0.90     13567\n",
      "weighted avg       0.97      0.98      0.97     13567\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Select only the optimal number of input features for X_test\n",
    "X_test = X_test[:,:(best_model_index+1)]\n",
    "\n",
    "# Evaluate the best model on the test data\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# np.argmax() is used to convert the one-hot encoded predictions and test labels to class labels.\n",
    "y_pred_label = np.argmax(y_pred, axis = 1)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification report:\\n\", classification_report(y_test_enc, y_pred_label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           OsID  True Class  Predicted Class  True/False\n",
      "0  Os11g0704500         328              328        True\n",
      "1  Os09g0279600         161              161        True\n",
      "2  Os03g0669100          17               17        True\n",
      "3  Os05g0542500          34               34        True\n",
      "4  Os09g0522000           7                7        True\n"
     ]
    }
   ],
   "source": [
    "# extract class labels from test data\n",
    "class_test = y_test_enc\n",
    "\n",
    "# Invert OsID_labels dictionary\n",
    "inv_OsID_labels = {v: k for k, v in OsID_labels.items()}\n",
    "\n",
    "# map OsID values to the class labels\n",
    "OsID_test = [inv_OsID_labels.get(value, 'Unknown') for value in class_test]\n",
    "\n",
    "# create dataframe with OsID, true class, predicted class, and true/false columns\n",
    "results = pd.DataFrame({\n",
    "    'OsID': OsID_test,\n",
    "    'True Class': y_test_enc,\n",
    "    'Predicted Class': y_pred_label,\n",
    "    'True/False': class_test == y_pred_label\n",
    "})\n",
    "\n",
    "# display dataframe\n",
    "print(results.head())\n",
    "\n",
    "# save results_df to a CSV file\n",
    "results.to_csv('MLP_gene classification.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54645332476aec3a1589d49135d9c8280fdb5d7db877f5b7af7a1b58b8f996bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixed random seed for reproducibility \n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of dataGene:\n",
      "        log_2FoldChange            ET  CoExpression           PCC  \\\n",
      "count     41110.000000  41110.000000  41110.000000  41110.000000   \n",
      "mean         -0.037332      1.407395      0.991997     -0.361737   \n",
      "std           0.391444      0.784327      0.089101      0.463979   \n",
      "min          -1.000000      0.000000      0.000000     -1.000000   \n",
      "25%          -0.251534      1.000000      1.000000     -0.747963   \n",
      "50%           0.030675      2.000000      1.000000     -0.449089   \n",
      "75%           0.251534      2.000000      1.000000     -0.051646   \n",
      "max           1.000000      2.000000      1.000000      1.000000   \n",
      "\n",
      "                PPI  Root10DaysSeedling  Root14DaysSeedling  \\\n",
      "count  41110.000000        41110.000000        41110.000000   \n",
      "mean       0.914668           -0.522040           -0.646982   \n",
      "std        0.279379            0.498568            0.393549   \n",
      "min        0.000000           -1.000000           -1.000000   \n",
      "25%        1.000000           -0.901371           -0.965084   \n",
      "50%        1.000000           -0.663664           -0.680003   \n",
      "75%        1.000000           -0.378497           -0.559627   \n",
      "max        1.000000            1.000000            1.000000   \n",
      "\n",
      "       Root17DaysSeedling  Root21DaysSeedling  Root24DaysSeedling  ...  \\\n",
      "count        41110.000000        41110.000000        41110.000000  ...   \n",
      "mean            -0.700869           -0.669349           -0.670048  ...   \n",
      "std              0.378219            0.405860            0.390751  ...   \n",
      "min             -1.000000           -1.000000           -1.000000  ...   \n",
      "25%             -0.980226           -1.000000           -0.982003  ...   \n",
      "50%             -0.795609           -0.726665           -0.708584  ...   \n",
      "75%             -0.601266           -0.543621           -0.482133  ...   \n",
      "max              1.000000            1.000000            1.000000  ...   \n",
      "\n",
      "       Root52DaysSeedling  Shoot3DaysSeedling  Shoot10DaysSeedling  \\\n",
      "count        41110.000000        41110.000000         41110.000000   \n",
      "mean            -0.670345           -0.590806            -0.545055   \n",
      "std              0.478222            0.443552             0.477438   \n",
      "min             -1.000000           -1.000000            -1.000000   \n",
      "25%             -1.000000           -1.000000            -0.906055   \n",
      "50%             -0.853382           -0.676286            -0.698864   \n",
      "75%             -0.542371           -0.409775            -0.250588   \n",
      "max              1.000000            0.955179             1.000000   \n",
      "\n",
      "       Shoot14DaysSeedling  Shoot17DaysSeedling  Shoot21DaysSeedling  \\\n",
      "count         41110.000000         41110.000000         41110.000000   \n",
      "mean             -0.734141            -0.680810            -0.659443   \n",
      "std               0.413716             0.478189             0.463838   \n",
      "min              -1.000000            -1.000000            -1.000000   \n",
      "25%              -1.000000            -1.000000            -1.000000   \n",
      "50%              -0.924976            -0.954040            -0.874080   \n",
      "75%              -0.513759            -0.420386            -0.440577   \n",
      "max               0.997390             1.000000             1.000000   \n",
      "\n",
      "       Shoot35DaysSeedling  Leaf21DaysSeedling  Leaf45DaysOldPlant  \\\n",
      "count         41110.000000        41110.000000        41110.000000   \n",
      "mean             -0.558906           -0.828778           -0.585144   \n",
      "std               0.506423            0.327542            0.399046   \n",
      "min              -1.000000           -1.000000           -1.000000   \n",
      "25%              -0.962199           -1.000000           -0.901444   \n",
      "50%              -0.699035           -0.951894           -0.643376   \n",
      "75%              -0.352995           -0.883755           -0.451900   \n",
      "max               0.993958            1.000000            1.000000   \n",
      "\n",
      "              class  \n",
      "count  41110.000000  \n",
      "mean      59.092703  \n",
      "std       77.624892  \n",
      "min        0.000000  \n",
      "25%        8.000000  \n",
      "50%       25.000000  \n",
      "75%       77.000000  \n",
      "max      372.000000  \n",
      "\n",
      "[8 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# load dataset (input variables = X, output variables = Y)\n",
    "df = pd.read_csv(\"2.csv\")\n",
    "\n",
    "#count the number of occurances for each osID\n",
    "OsID_counts = df['OsID'].value_counts()\n",
    "\n",
    "#filter for osIDs that have 10 or more occurances\n",
    "OsID_counts_filtered = OsID_counts[OsID_counts >= 10]\n",
    "\n",
    "#assign a label for each osID \n",
    "OsID_labels = {}\n",
    "class_no = 0\n",
    "for osID in OsID_counts_filtered.index:\n",
    "    OsID_labels[osID] = class_no\n",
    "    class_no +=1\n",
    "\n",
    "#filter the dataset with osID that contain 10 or more occurances\n",
    "dataGene = df[df['OsID'].isin(OsID_counts_filtered.index)]\n",
    "\n",
    "dataGene = dataGene.drop(['Class', 'Trait'],axis=1)\n",
    "\n",
    "# Add a new column 'class' to the filtered dataset\n",
    "dataGene['class'] = dataGene['OsID'].map(OsID_labels)\n",
    "\n",
    "print(\"Summary of dataGene:\\n\",dataGene.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X:\n",
      " (41110, 20)\n",
      "Shape of Y:\n",
      " (41110,)\n",
      "Summary of X:\n",
      "        CoExpression           PCC           PPI  Root10DaysSeedling  \\\n",
      "count  41110.000000  41110.000000  41110.000000        41110.000000   \n",
      "mean       0.991997     -0.361737      0.914668           -0.522040   \n",
      "std        0.089101      0.463979      0.279379            0.498568   \n",
      "min        0.000000     -1.000000      0.000000           -1.000000   \n",
      "25%        1.000000     -0.747963      1.000000           -0.901371   \n",
      "50%        1.000000     -0.449089      1.000000           -0.663664   \n",
      "75%        1.000000     -0.051646      1.000000           -0.378497   \n",
      "max        1.000000      1.000000      1.000000            1.000000   \n",
      "\n",
      "       Leaf21DaysSeedling  Leaf45DaysOldPlant  log_2FoldChange            ET  \\\n",
      "count        41110.000000        41110.000000     41110.000000  41110.000000   \n",
      "mean            -0.828778           -0.585144        -0.037332      1.407395   \n",
      "std              0.327542            0.399046         0.391444      0.784327   \n",
      "min             -1.000000           -1.000000        -1.000000      0.000000   \n",
      "25%             -1.000000           -0.901444        -0.251534      1.000000   \n",
      "50%             -0.951894           -0.643376         0.030675      2.000000   \n",
      "75%             -0.883755           -0.451900         0.251534      2.000000   \n",
      "max              1.000000            1.000000         1.000000      2.000000   \n",
      "\n",
      "       Shoot10DaysSeedling  Shoot3DaysSeedling  Shoot35DaysSeedling  \\\n",
      "count         41110.000000        41110.000000         41110.000000   \n",
      "mean             -0.545055           -0.590806            -0.558906   \n",
      "std               0.477438            0.443552             0.506423   \n",
      "min              -1.000000           -1.000000            -1.000000   \n",
      "25%              -0.906055           -1.000000            -0.962199   \n",
      "50%              -0.698864           -0.676286            -0.699035   \n",
      "75%              -0.250588           -0.409775            -0.352995   \n",
      "max               1.000000            0.955179             0.993958   \n",
      "\n",
      "       Shoot14DaysSeedling  Root17DaysSeedling  Shoot17DaysSeedling  \\\n",
      "count         41110.000000        41110.000000         41110.000000   \n",
      "mean             -0.734141           -0.700869            -0.680810   \n",
      "std               0.413716            0.378219             0.478189   \n",
      "min              -1.000000           -1.000000            -1.000000   \n",
      "25%              -1.000000           -0.980226            -1.000000   \n",
      "50%              -0.924976           -0.795609            -0.954040   \n",
      "75%              -0.513759           -0.601266            -0.420386   \n",
      "max               0.997390            1.000000             1.000000   \n",
      "\n",
      "       Shoot21DaysSeedling  Root24DaysSeedling  Root14DaysSeedling  \\\n",
      "count         41110.000000        41110.000000        41110.000000   \n",
      "mean             -0.659443           -0.670048           -0.646982   \n",
      "std               0.463838            0.390751            0.393549   \n",
      "min              -1.000000           -1.000000           -1.000000   \n",
      "25%              -1.000000           -0.982003           -0.965084   \n",
      "50%              -0.874080           -0.708584           -0.680003   \n",
      "75%              -0.440577           -0.482133           -0.559627   \n",
      "max               1.000000            1.000000            1.000000   \n",
      "\n",
      "       Root21DaysSeedling  Root52DaysSeedling  Root35DaysSeedling  \n",
      "count        41110.000000        41110.000000        41110.000000  \n",
      "mean            -0.669349           -0.670345           -0.596196  \n",
      "std              0.405860            0.478222            0.461679  \n",
      "min             -1.000000           -1.000000           -1.000000  \n",
      "25%             -1.000000           -1.000000           -0.937286  \n",
      "50%             -0.726665           -0.853382           -0.769184  \n",
      "75%             -0.543621           -0.542371           -0.323664  \n",
      "max              1.000000            1.000000            1.000000  \n",
      "Summary of Y:\n",
      " count    41110.000000\n",
      "mean        59.092703\n",
      "std         77.624892\n",
      "min          0.000000\n",
      "25%          8.000000\n",
      "50%         25.000000\n",
      "75%         77.000000\n",
      "max        372.000000\n",
      "Name: class, dtype: float64\n",
      "class\n",
      "0.0      1800\n",
      "1.0      1296\n",
      "2.0      1260\n",
      "3.0      1218\n",
      "4.0      1026\n",
      "         ... \n",
      "368.0      10\n",
      "369.0      10\n",
      "370.0      10\n",
      "371.0      10\n",
      "372.0      10\n",
      "Length: 373, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X = dataGene.drop(['class','OsID'], axis=1) # exclude class & OsID column\n",
    "Y = dataGene['class']\n",
    "\n",
    "#input feature names in order of descending importance scores in PCC feature selection method\n",
    "feature_names = ['CoExpression', 'PCC', 'PPI', 'Root10DaysSeedling', 'Leaf21DaysSeedling',\n",
    "                 'Leaf45DaysOldPlant', 'log_2FoldChange', 'ET', 'Shoot10DaysSeedling', 'Shoot3DaysSeedling', \n",
    "                 'Shoot35DaysSeedling', 'Shoot14DaysSeedling', 'Root17DaysSeedling', 'Shoot17DaysSeedling', 'Shoot21DaysSeedling',\n",
    "                 'Root24DaysSeedling', 'Root14DaysSeedling', 'Root21DaysSeedling', 'Root52DaysSeedling', 'Root35DaysSeedling']\n",
    "\n",
    "X_fs = X.reindex(columns=feature_names)\n",
    "\n",
    "print(\"Shape of X:\\n\",X_fs.shape)\n",
    "print(\"Shape of Y:\\n\",Y.shape)\n",
    "\n",
    "# Statistical summary of the variables\n",
    "print(\"Summary of X:\\n\",X_fs.describe())\n",
    "print(\"Summary of Y:\\n\",Y.describe())\n",
    "\n",
    "# Check for class imbalance\n",
    "print(df.groupby(Y).size())\n",
    "\n",
    "# change both input and target variables datatype to ndarray\n",
    "\n",
    "X_fs = X_fs.values # 2-D array\n",
    "\n",
    "# select target variable \n",
    "\n",
    "Y = Y.values #1-D array\n",
    "Y = Y.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class=0, n=1800 (4.378%)\n",
      "Class=1, n=1296 (3.153%)\n",
      "Class=2, n=1260 (3.065%)\n",
      "Class=3, n=1218 (2.963%)\n",
      "Class=4, n=1026 (2.496%)\n",
      "Class=5, n=1008 (2.452%)\n",
      "Class=6, n=930 (2.262%)\n",
      "Class=7, n=912 (2.218%)\n",
      "Class=8, n=880 (2.141%)\n",
      "Class=9, n=798 (1.941%)\n",
      "Class=10, n=792 (1.927%)\n",
      "Class=11, n=759 (1.846%)\n",
      "Class=12, n=729 (1.773%)\n",
      "Class=13, n=720 (1.751%)\n",
      "Class=14, n=702 (1.708%)\n",
      "Class=15, n=693 (1.686%)\n",
      "Class=16, n=672 (1.635%)\n",
      "Class=17, n=640 (1.557%)\n",
      "Class=18, n=625 (1.520%)\n",
      "Class=19, n=570 (1.387%)\n",
      "Class=20, n=546 (1.328%)\n",
      "Class=21, n=506 (1.231%)\n",
      "Class=22, n=483 (1.175%)\n",
      "Class=23, n=448 (1.090%)\n",
      "Class=24, n=432 (1.051%)\n",
      "Class=25, n=384 (0.934%)\n",
      "Class=26, n=360 (0.876%)\n",
      "Class=27, n=360 (0.876%)\n",
      "Class=28, n=320 (0.778%)\n",
      "Class=29, n=312 (0.759%)\n",
      "Class=30, n=312 (0.759%)\n",
      "Class=31, n=306 (0.744%)\n",
      "Class=32, n=304 (0.739%)\n",
      "Class=33, n=299 (0.727%)\n",
      "Class=34, n=297 (0.722%)\n",
      "Class=35, n=296 (0.720%)\n",
      "Class=36, n=280 (0.681%)\n",
      "Class=37, n=264 (0.642%)\n",
      "Class=38, n=260 (0.632%)\n",
      "Class=39, n=253 (0.615%)\n",
      "Class=40, n=252 (0.613%)\n",
      "Class=41, n=248 (0.603%)\n",
      "Class=42, n=242 (0.589%)\n",
      "Class=43, n=228 (0.555%)\n",
      "Class=44, n=216 (0.525%)\n",
      "Class=45, n=210 (0.511%)\n",
      "Class=46, n=200 (0.486%)\n",
      "Class=47, n=192 (0.467%)\n",
      "Class=48, n=180 (0.438%)\n",
      "Class=49, n=171 (0.416%)\n",
      "Class=50, n=168 (0.409%)\n",
      "Class=51, n=168 (0.409%)\n",
      "Class=52, n=162 (0.394%)\n",
      "Class=53, n=150 (0.365%)\n",
      "Class=54, n=148 (0.360%)\n",
      "Class=55, n=138 (0.336%)\n",
      "Class=56, n=135 (0.328%)\n",
      "Class=57, n=135 (0.328%)\n",
      "Class=58, n=133 (0.324%)\n",
      "Class=59, n=132 (0.321%)\n",
      "Class=60, n=132 (0.321%)\n",
      "Class=61, n=130 (0.316%)\n",
      "Class=62, n=130 (0.316%)\n",
      "Class=63, n=130 (0.316%)\n",
      "Class=64, n=128 (0.311%)\n",
      "Class=65, n=128 (0.311%)\n",
      "Class=66, n=126 (0.306%)\n",
      "Class=67, n=124 (0.302%)\n",
      "Class=68, n=124 (0.302%)\n",
      "Class=69, n=124 (0.302%)\n",
      "Class=70, n=120 (0.292%)\n",
      "Class=71, n=120 (0.292%)\n",
      "Class=72, n=118 (0.287%)\n",
      "Class=73, n=116 (0.282%)\n",
      "Class=74, n=114 (0.277%)\n",
      "Class=75, n=105 (0.255%)\n",
      "Class=76, n=104 (0.253%)\n",
      "Class=77, n=102 (0.248%)\n",
      "Class=78, n=99 (0.241%)\n",
      "Class=79, n=98 (0.238%)\n",
      "Class=80, n=98 (0.238%)\n",
      "Class=81, n=98 (0.238%)\n",
      "Class=82, n=98 (0.238%)\n",
      "Class=83, n=96 (0.234%)\n",
      "Class=84, n=96 (0.234%)\n",
      "Class=85, n=96 (0.234%)\n",
      "Class=86, n=93 (0.226%)\n",
      "Class=87, n=92 (0.224%)\n",
      "Class=88, n=92 (0.224%)\n",
      "Class=89, n=91 (0.221%)\n",
      "Class=90, n=88 (0.214%)\n",
      "Class=91, n=88 (0.214%)\n",
      "Class=92, n=86 (0.209%)\n",
      "Class=93, n=86 (0.209%)\n",
      "Class=94, n=84 (0.204%)\n",
      "Class=95, n=84 (0.204%)\n",
      "Class=96, n=84 (0.204%)\n",
      "Class=97, n=78 (0.190%)\n",
      "Class=98, n=78 (0.190%)\n",
      "Class=99, n=76 (0.185%)\n",
      "Class=100, n=75 (0.182%)\n",
      "Class=101, n=75 (0.182%)\n",
      "Class=102, n=73 (0.178%)\n",
      "Class=103, n=72 (0.175%)\n",
      "Class=104, n=72 (0.175%)\n",
      "Class=105, n=70 (0.170%)\n",
      "Class=106, n=69 (0.168%)\n",
      "Class=107, n=68 (0.165%)\n",
      "Class=108, n=67 (0.163%)\n",
      "Class=109, n=66 (0.161%)\n",
      "Class=110, n=66 (0.161%)\n",
      "Class=111, n=66 (0.161%)\n",
      "Class=112, n=66 (0.161%)\n",
      "Class=113, n=66 (0.161%)\n",
      "Class=114, n=65 (0.158%)\n",
      "Class=115, n=64 (0.156%)\n",
      "Class=116, n=63 (0.153%)\n",
      "Class=117, n=63 (0.153%)\n",
      "Class=118, n=62 (0.151%)\n",
      "Class=119, n=61 (0.148%)\n",
      "Class=120, n=60 (0.146%)\n",
      "Class=121, n=60 (0.146%)\n",
      "Class=122, n=60 (0.146%)\n",
      "Class=123, n=60 (0.146%)\n",
      "Class=124, n=60 (0.146%)\n",
      "Class=125, n=60 (0.146%)\n",
      "Class=126, n=60 (0.146%)\n",
      "Class=127, n=60 (0.146%)\n",
      "Class=128, n=60 (0.146%)\n",
      "Class=129, n=60 (0.146%)\n",
      "Class=130, n=59 (0.144%)\n",
      "Class=131, n=59 (0.144%)\n",
      "Class=132, n=58 (0.141%)\n",
      "Class=133, n=56 (0.136%)\n",
      "Class=134, n=56 (0.136%)\n",
      "Class=135, n=56 (0.136%)\n",
      "Class=136, n=56 (0.136%)\n",
      "Class=137, n=56 (0.136%)\n",
      "Class=138, n=56 (0.136%)\n",
      "Class=139, n=56 (0.136%)\n",
      "Class=140, n=56 (0.136%)\n",
      "Class=141, n=56 (0.136%)\n",
      "Class=142, n=55 (0.134%)\n",
      "Class=143, n=55 (0.134%)\n",
      "Class=144, n=54 (0.131%)\n",
      "Class=145, n=54 (0.131%)\n",
      "Class=146, n=54 (0.131%)\n",
      "Class=147, n=54 (0.131%)\n",
      "Class=148, n=54 (0.131%)\n",
      "Class=149, n=53 (0.129%)\n",
      "Class=150, n=52 (0.126%)\n",
      "Class=151, n=52 (0.126%)\n",
      "Class=152, n=52 (0.126%)\n",
      "Class=153, n=52 (0.126%)\n",
      "Class=154, n=50 (0.122%)\n",
      "Class=155, n=50 (0.122%)\n",
      "Class=156, n=49 (0.119%)\n",
      "Class=157, n=49 (0.119%)\n",
      "Class=158, n=48 (0.117%)\n",
      "Class=159, n=48 (0.117%)\n",
      "Class=160, n=48 (0.117%)\n",
      "Class=161, n=46 (0.112%)\n",
      "Class=162, n=45 (0.109%)\n",
      "Class=163, n=44 (0.107%)\n",
      "Class=164, n=44 (0.107%)\n",
      "Class=165, n=44 (0.107%)\n",
      "Class=166, n=42 (0.102%)\n",
      "Class=167, n=42 (0.102%)\n",
      "Class=168, n=42 (0.102%)\n",
      "Class=169, n=42 (0.102%)\n",
      "Class=170, n=42 (0.102%)\n",
      "Class=171, n=42 (0.102%)\n",
      "Class=172, n=42 (0.102%)\n",
      "Class=173, n=41 (0.100%)\n",
      "Class=174, n=41 (0.100%)\n",
      "Class=175, n=40 (0.097%)\n",
      "Class=176, n=40 (0.097%)\n",
      "Class=177, n=39 (0.095%)\n",
      "Class=178, n=39 (0.095%)\n",
      "Class=179, n=38 (0.092%)\n",
      "Class=180, n=37 (0.090%)\n",
      "Class=181, n=36 (0.088%)\n",
      "Class=182, n=35 (0.085%)\n",
      "Class=183, n=35 (0.085%)\n",
      "Class=184, n=35 (0.085%)\n",
      "Class=185, n=35 (0.085%)\n",
      "Class=186, n=34 (0.083%)\n",
      "Class=187, n=34 (0.083%)\n",
      "Class=188, n=34 (0.083%)\n",
      "Class=189, n=34 (0.083%)\n",
      "Class=190, n=32 (0.078%)\n",
      "Class=191, n=32 (0.078%)\n",
      "Class=192, n=32 (0.078%)\n",
      "Class=193, n=32 (0.078%)\n",
      "Class=194, n=32 (0.078%)\n",
      "Class=195, n=32 (0.078%)\n",
      "Class=196, n=31 (0.075%)\n",
      "Class=197, n=31 (0.075%)\n",
      "Class=198, n=31 (0.075%)\n",
      "Class=199, n=31 (0.075%)\n",
      "Class=200, n=30 (0.073%)\n",
      "Class=201, n=30 (0.073%)\n",
      "Class=202, n=30 (0.073%)\n",
      "Class=203, n=30 (0.073%)\n",
      "Class=204, n=30 (0.073%)\n",
      "Class=205, n=30 (0.073%)\n",
      "Class=206, n=30 (0.073%)\n",
      "Class=207, n=30 (0.073%)\n",
      "Class=208, n=30 (0.073%)\n",
      "Class=209, n=29 (0.071%)\n",
      "Class=210, n=29 (0.071%)\n",
      "Class=211, n=28 (0.068%)\n",
      "Class=212, n=28 (0.068%)\n",
      "Class=213, n=28 (0.068%)\n",
      "Class=214, n=28 (0.068%)\n",
      "Class=215, n=28 (0.068%)\n",
      "Class=216, n=28 (0.068%)\n",
      "Class=217, n=27 (0.066%)\n",
      "Class=218, n=27 (0.066%)\n",
      "Class=219, n=27 (0.066%)\n",
      "Class=220, n=27 (0.066%)\n",
      "Class=221, n=27 (0.066%)\n",
      "Class=222, n=27 (0.066%)\n",
      "Class=223, n=26 (0.063%)\n",
      "Class=224, n=26 (0.063%)\n",
      "Class=225, n=26 (0.063%)\n",
      "Class=226, n=26 (0.063%)\n",
      "Class=227, n=26 (0.063%)\n",
      "Class=228, n=25 (0.061%)\n",
      "Class=229, n=25 (0.061%)\n",
      "Class=230, n=25 (0.061%)\n",
      "Class=231, n=25 (0.061%)\n",
      "Class=232, n=24 (0.058%)\n",
      "Class=233, n=24 (0.058%)\n",
      "Class=234, n=24 (0.058%)\n",
      "Class=235, n=24 (0.058%)\n",
      "Class=236, n=24 (0.058%)\n",
      "Class=237, n=24 (0.058%)\n",
      "Class=238, n=24 (0.058%)\n",
      "Class=239, n=24 (0.058%)\n",
      "Class=240, n=24 (0.058%)\n",
      "Class=241, n=24 (0.058%)\n",
      "Class=242, n=24 (0.058%)\n",
      "Class=243, n=24 (0.058%)\n",
      "Class=244, n=23 (0.056%)\n",
      "Class=245, n=23 (0.056%)\n",
      "Class=246, n=22 (0.054%)\n",
      "Class=247, n=22 (0.054%)\n",
      "Class=248, n=22 (0.054%)\n",
      "Class=249, n=22 (0.054%)\n",
      "Class=250, n=22 (0.054%)\n",
      "Class=251, n=22 (0.054%)\n",
      "Class=252, n=22 (0.054%)\n",
      "Class=253, n=22 (0.054%)\n",
      "Class=254, n=22 (0.054%)\n",
      "Class=255, n=22 (0.054%)\n",
      "Class=256, n=22 (0.054%)\n",
      "Class=257, n=22 (0.054%)\n",
      "Class=258, n=22 (0.054%)\n",
      "Class=259, n=22 (0.054%)\n",
      "Class=260, n=22 (0.054%)\n",
      "Class=261, n=22 (0.054%)\n",
      "Class=262, n=22 (0.054%)\n",
      "Class=263, n=22 (0.054%)\n",
      "Class=264, n=21 (0.051%)\n",
      "Class=265, n=21 (0.051%)\n",
      "Class=266, n=21 (0.051%)\n",
      "Class=267, n=21 (0.051%)\n",
      "Class=268, n=21 (0.051%)\n",
      "Class=269, n=20 (0.049%)\n",
      "Class=270, n=20 (0.049%)\n",
      "Class=271, n=20 (0.049%)\n",
      "Class=272, n=20 (0.049%)\n",
      "Class=273, n=20 (0.049%)\n",
      "Class=274, n=20 (0.049%)\n",
      "Class=275, n=20 (0.049%)\n",
      "Class=276, n=20 (0.049%)\n",
      "Class=277, n=20 (0.049%)\n",
      "Class=278, n=20 (0.049%)\n",
      "Class=279, n=20 (0.049%)\n",
      "Class=280, n=19 (0.046%)\n",
      "Class=281, n=19 (0.046%)\n",
      "Class=282, n=19 (0.046%)\n",
      "Class=283, n=18 (0.044%)\n",
      "Class=284, n=18 (0.044%)\n",
      "Class=285, n=18 (0.044%)\n",
      "Class=286, n=18 (0.044%)\n",
      "Class=287, n=18 (0.044%)\n",
      "Class=288, n=18 (0.044%)\n",
      "Class=289, n=18 (0.044%)\n",
      "Class=290, n=18 (0.044%)\n",
      "Class=291, n=18 (0.044%)\n",
      "Class=292, n=17 (0.041%)\n",
      "Class=293, n=17 (0.041%)\n",
      "Class=294, n=17 (0.041%)\n",
      "Class=295, n=17 (0.041%)\n",
      "Class=296, n=17 (0.041%)\n",
      "Class=297, n=17 (0.041%)\n",
      "Class=298, n=16 (0.039%)\n",
      "Class=299, n=16 (0.039%)\n",
      "Class=300, n=16 (0.039%)\n",
      "Class=301, n=16 (0.039%)\n",
      "Class=302, n=16 (0.039%)\n",
      "Class=303, n=16 (0.039%)\n",
      "Class=304, n=16 (0.039%)\n",
      "Class=305, n=16 (0.039%)\n",
      "Class=306, n=15 (0.036%)\n",
      "Class=307, n=15 (0.036%)\n",
      "Class=308, n=15 (0.036%)\n",
      "Class=309, n=15 (0.036%)\n",
      "Class=310, n=15 (0.036%)\n",
      "Class=311, n=14 (0.034%)\n",
      "Class=312, n=14 (0.034%)\n",
      "Class=313, n=14 (0.034%)\n",
      "Class=314, n=14 (0.034%)\n",
      "Class=315, n=14 (0.034%)\n",
      "Class=316, n=14 (0.034%)\n",
      "Class=317, n=14 (0.034%)\n",
      "Class=318, n=14 (0.034%)\n",
      "Class=319, n=14 (0.034%)\n",
      "Class=320, n=14 (0.034%)\n",
      "Class=321, n=14 (0.034%)\n",
      "Class=322, n=14 (0.034%)\n",
      "Class=323, n=14 (0.034%)\n",
      "Class=324, n=14 (0.034%)\n",
      "Class=325, n=14 (0.034%)\n",
      "Class=326, n=14 (0.034%)\n",
      "Class=327, n=14 (0.034%)\n",
      "Class=328, n=13 (0.032%)\n",
      "Class=329, n=13 (0.032%)\n",
      "Class=330, n=13 (0.032%)\n",
      "Class=331, n=13 (0.032%)\n",
      "Class=332, n=13 (0.032%)\n",
      "Class=333, n=13 (0.032%)\n",
      "Class=334, n=13 (0.032%)\n",
      "Class=335, n=13 (0.032%)\n",
      "Class=336, n=13 (0.032%)\n",
      "Class=337, n=12 (0.029%)\n",
      "Class=338, n=12 (0.029%)\n",
      "Class=339, n=12 (0.029%)\n",
      "Class=340, n=12 (0.029%)\n",
      "Class=341, n=12 (0.029%)\n",
      "Class=342, n=12 (0.029%)\n",
      "Class=343, n=12 (0.029%)\n",
      "Class=344, n=12 (0.029%)\n",
      "Class=345, n=12 (0.029%)\n",
      "Class=346, n=12 (0.029%)\n",
      "Class=347, n=12 (0.029%)\n",
      "Class=348, n=12 (0.029%)\n",
      "Class=349, n=12 (0.029%)\n",
      "Class=350, n=12 (0.029%)\n",
      "Class=351, n=12 (0.029%)\n",
      "Class=352, n=12 (0.029%)\n",
      "Class=353, n=12 (0.029%)\n",
      "Class=354, n=12 (0.029%)\n",
      "Class=355, n=11 (0.027%)\n",
      "Class=356, n=11 (0.027%)\n",
      "Class=357, n=11 (0.027%)\n",
      "Class=358, n=11 (0.027%)\n",
      "Class=359, n=11 (0.027%)\n",
      "Class=360, n=11 (0.027%)\n",
      "Class=361, n=10 (0.024%)\n",
      "Class=362, n=10 (0.024%)\n",
      "Class=363, n=10 (0.024%)\n",
      "Class=364, n=10 (0.024%)\n",
      "Class=365, n=10 (0.024%)\n",
      "Class=366, n=10 (0.024%)\n",
      "Class=367, n=10 (0.024%)\n",
      "Class=368, n=10 (0.024%)\n",
      "Class=369, n=10 (0.024%)\n",
      "Class=370, n=10 (0.024%)\n",
      "Class=371, n=10 (0.024%)\n",
      "Class=372, n=10 (0.024%)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# summarize distribution\n",
    "counter = Counter(Y.flatten())\n",
    "\n",
    "# sort counter by keys\n",
    "counter = dict(sorted(counter.items()))\n",
    "\n",
    "for k,v in counter.items():\n",
    " per = v / len(Y.flatten()) * 100\n",
    " print('Class=%d, n=%d (%.3f%%)' % (k, v, per))\n",
    "# plot the distribution\n",
    "plt.bar( counter.keys(), counter.values())\n",
    "\n",
    "plt.ylabel('No of gene samples')\n",
    "plt.xlabel('Class Index of OsID')\n",
    "plt.title('Distribution of Target Classes (OsID)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare target data\n",
    "def prepare_targets(y_train, y_test):\n",
    "\tle = LabelEncoder()\n",
    "\t\n",
    "\t#fit the encoders only to the training data and then transform both train and test data\n",
    "\ty_train_enc = le.fit_transform(y_train)\n",
    "\ty_test_enc = le.transform(y_test)\n",
    "\n",
    "\treturn y_train_enc, y_test_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define baseline model (MLP)\n",
    "def MLP_model(input_dim):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_dim=input_dim,bias_initializer='normal', activation='relu',kernel_regularizer=regularizers.l2(0.0001)))\n",
    "    model.add(Dense(20,bias_initializer='normal',activation='relu',kernel_regularizer=regularizers.l2(0.0001)))\n",
    "    model.add(Dense(20,bias_initializer='normal',activation='relu',kernel_regularizer=regularizers.l2(0.0001)))\n",
    "    model.add(Dense(373,kernel_initializer='normal', activation='softmax')) #softmax for multi-class classification, num_classes = 373\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input features: 1\n",
      "Fold: 1\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 7s 3ms/step - loss: 5.0716 - accuracy: 0.0438 - val_loss: 5.0219 - val_accuracy: 0.0436\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.9500 - accuracy: 0.0444 - val_loss: 4.9978 - val_accuracy: 0.0444\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 4.9334 - accuracy: 0.0452 - val_loss: 4.9944 - val_accuracy: 0.0451\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.9295 - accuracy: 0.0453 - val_loss: 4.9979 - val_accuracy: 0.0451\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.9279 - accuracy: 0.0452 - val_loss: 5.0048 - val_accuracy: 0.0451\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.9249 - accuracy: 0.0453 - val_loss: 5.0193 - val_accuracy: 0.0451\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 4.9253 - accuracy: 0.0451 - val_loss: 5.0012 - val_accuracy: 0.0451\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.9221 - accuracy: 0.0454 - val_loss: 5.0046 - val_accuracy: 0.0451\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 4.9217 - accuracy: 0.0454 - val_loss: 5.0068 - val_accuracy: 0.0451\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.9214 - accuracy: 0.0454 - val_loss: 5.0080 - val_accuracy: 0.0451\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.9208 - accuracy: 0.0454 - val_loss: 5.0090 - val_accuracy: 0.0451\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 4.9206 - accuracy: 0.0454 - val_loss: 5.0143 - val_accuracy: 0.0451\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.9196 - accuracy: 0.0454 - val_loss: 5.0191 - val_accuracy: 0.0451\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.9196 - accuracy: 0.0454 - val_loss: 5.0116 - val_accuracy: 0.0451\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 4.9186 - accuracy: 0.0452 - val_loss: 5.0122 - val_accuracy: 0.0451\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 4.9183 - accuracy: 0.0454 - val_loss: 5.0127 - val_accuracy: 0.0451\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 4.9179 - accuracy: 0.0450 - val_loss: 5.0108 - val_accuracy: 0.0451\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 4.9175 - accuracy: 0.0453 - val_loss: 5.0170 - val_accuracy: 0.0451\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 4.9170 - accuracy: 0.0451 - val_loss: 5.0148 - val_accuracy: 0.0451\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 4.9169 - accuracy: 0.0452 - val_loss: 5.0132 - val_accuracy: 0.0451\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 7s 3ms/step - loss: 5.0723 - accuracy: 0.0407 - val_loss: 5.0293 - val_accuracy: 0.0440\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 4.9608 - accuracy: 0.0436 - val_loss: 5.0104 - val_accuracy: 0.0460\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 4.9446 - accuracy: 0.0451 - val_loss: 5.0015 - val_accuracy: 0.0460\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.9389 - accuracy: 0.0449 - val_loss: 5.0161 - val_accuracy: 0.0460\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 4.9371 - accuracy: 0.0447 - val_loss: 5.0097 - val_accuracy: 0.0460\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 4.9351 - accuracy: 0.0451 - val_loss: 5.0103 - val_accuracy: 0.0460\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.9336 - accuracy: 0.0451 - val_loss: 5.0006 - val_accuracy: 0.0460\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.9324 - accuracy: 0.0451 - val_loss: 5.0193 - val_accuracy: 0.0460\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 4.9321 - accuracy: 0.0451 - val_loss: 5.0170 - val_accuracy: 0.0460\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 4.9309 - accuracy: 0.0449 - val_loss: 5.0057 - val_accuracy: 0.0460\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 4.9292 - accuracy: 0.0451 - val_loss: 5.0070 - val_accuracy: 0.0460\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.9288 - accuracy: 0.0451 - val_loss: 5.0129 - val_accuracy: 0.0460\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 4.9276 - accuracy: 0.0451 - val_loss: 5.0094 - val_accuracy: 0.0460\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 4.9266 - accuracy: 0.0450 - val_loss: 5.0100 - val_accuracy: 0.0460\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 4.9259 - accuracy: 0.0450 - val_loss: 5.0079 - val_accuracy: 0.0460\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.9260 - accuracy: 0.0451 - val_loss: 5.0119 - val_accuracy: 0.0460\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.9257 - accuracy: 0.0451 - val_loss: 5.0075 - val_accuracy: 0.0460\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 4.9244 - accuracy: 0.0451 - val_loss: 5.0138 - val_accuracy: 0.0460\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 4.9236 - accuracy: 0.0451 - val_loss: 5.0083 - val_accuracy: 0.0460\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 4.9233 - accuracy: 0.0447 - val_loss: 5.0170 - val_accuracy: 0.0460\n",
      "Average Validation Accuracy: 0.04534727334976196\n",
      "Average Validation Loss: 4.9436726570129395\n",
      "Average Test Accuracy: 0.045035749673843384\n",
      "Final Test Accuracy for each fold: 0.045035749673843384\n",
      "Number of input features: 2\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 7s 3ms/step - loss: 5.0805 - accuracy: 0.0450 - val_loss: 4.9943 - val_accuracy: 0.0517\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 4.8937 - accuracy: 0.0534 - val_loss: 4.9187 - val_accuracy: 0.0528\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.8286 - accuracy: 0.0561 - val_loss: 4.8999 - val_accuracy: 0.0590\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 4.7994 - accuracy: 0.0568 - val_loss: 4.9058 - val_accuracy: 0.0532\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.7797 - accuracy: 0.0589 - val_loss: 4.8906 - val_accuracy: 0.0460\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 4.7655 - accuracy: 0.0560 - val_loss: 4.8885 - val_accuracy: 0.0576\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 4.7548 - accuracy: 0.0607 - val_loss: 4.9005 - val_accuracy: 0.0546\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 4.7442 - accuracy: 0.0617 - val_loss: 4.9344 - val_accuracy: 0.0570\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.7359 - accuracy: 0.0604 - val_loss: 4.9157 - val_accuracy: 0.0559\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.7271 - accuracy: 0.0648 - val_loss: 4.9200 - val_accuracy: 0.0510\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.7189 - accuracy: 0.0630 - val_loss: 4.9289 - val_accuracy: 0.0526\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 4.7096 - accuracy: 0.0647 - val_loss: 4.9913 - val_accuracy: 0.0537\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 4.7026 - accuracy: 0.0609 - val_loss: 4.9598 - val_accuracy: 0.0515\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.6961 - accuracy: 0.0625 - val_loss: 4.9334 - val_accuracy: 0.0609\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.6883 - accuracy: 0.0641 - val_loss: 4.9442 - val_accuracy: 0.0603\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.6805 - accuracy: 0.0670 - val_loss: 4.9354 - val_accuracy: 0.0574\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 4.6742 - accuracy: 0.0659 - val_loss: 4.9695 - val_accuracy: 0.0609\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 4.6680 - accuracy: 0.0666 - val_loss: 4.9848 - val_accuracy: 0.0625\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.6628 - accuracy: 0.0656 - val_loss: 4.9927 - val_accuracy: 0.0609\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.6572 - accuracy: 0.0668 - val_loss: 4.9751 - val_accuracy: 0.0614\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 7s 3ms/step - loss: 5.0418 - accuracy: 0.0474 - val_loss: 4.9556 - val_accuracy: 0.0535\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.8534 - accuracy: 0.0543 - val_loss: 4.9312 - val_accuracy: 0.0537\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 4.8239 - accuracy: 0.0603 - val_loss: 4.9068 - val_accuracy: 0.0598\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.7899 - accuracy: 0.0582 - val_loss: 4.8803 - val_accuracy: 0.0598\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.7632 - accuracy: 0.0599 - val_loss: 4.8730 - val_accuracy: 0.0570\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.7457 - accuracy: 0.0596 - val_loss: 4.8899 - val_accuracy: 0.0548\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.7319 - accuracy: 0.0599 - val_loss: 4.8788 - val_accuracy: 0.0502\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 4.7226 - accuracy: 0.0569 - val_loss: 4.8843 - val_accuracy: 0.0574\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.7116 - accuracy: 0.0596 - val_loss: 4.9012 - val_accuracy: 0.0532\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.7049 - accuracy: 0.0585 - val_loss: 4.9093 - val_accuracy: 0.0530\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 4.6969 - accuracy: 0.0572 - val_loss: 4.9357 - val_accuracy: 0.0557\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 4.6913 - accuracy: 0.0599 - val_loss: 4.9346 - val_accuracy: 0.0559\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 4.6849 - accuracy: 0.0596 - val_loss: 4.9192 - val_accuracy: 0.0519\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.6798 - accuracy: 0.0592 - val_loss: 4.9433 - val_accuracy: 0.0554\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 4.6734 - accuracy: 0.0599 - val_loss: 4.9804 - val_accuracy: 0.0543\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 4.6677 - accuracy: 0.0601 - val_loss: 4.9685 - val_accuracy: 0.0502\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.6610 - accuracy: 0.0591 - val_loss: 4.9838 - val_accuracy: 0.0576\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 4.6525 - accuracy: 0.0617 - val_loss: 4.9893 - val_accuracy: 0.0638\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 4.6461 - accuracy: 0.0676 - val_loss: 5.0213 - val_accuracy: 0.0502\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 4.6423 - accuracy: 0.0670 - val_loss: 5.0465 - val_accuracy: 0.0570\n",
      "Average Validation Accuracy: 0.06542496010661125\n",
      "Average Validation Loss: 4.879139184951782\n",
      "Average Test Accuracy: 0.06346281431615353\n",
      "Final Test Accuracy for each fold: 0.06456843763589859\n",
      "Number of input features: 3\n",
      "Fold: 1\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.9321 - accuracy: 0.0698 - val_loss: 4.7730 - val_accuracy: 0.0759\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.6373 - accuracy: 0.0845 - val_loss: 4.7012 - val_accuracy: 0.0785\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 4.5804 - accuracy: 0.0856 - val_loss: 4.6989 - val_accuracy: 0.0770\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 4.5431 - accuracy: 0.0848 - val_loss: 4.6598 - val_accuracy: 0.0750\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 8s 5ms/step - loss: 4.5135 - accuracy: 0.0843 - val_loss: 4.6437 - val_accuracy: 0.0726\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.4890 - accuracy: 0.0863 - val_loss: 4.6389 - val_accuracy: 0.0757\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.4705 - accuracy: 0.0881 - val_loss: 4.6326 - val_accuracy: 0.0849\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 4.4546 - accuracy: 0.0866 - val_loss: 4.6449 - val_accuracy: 0.0777\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 8s 5ms/step - loss: 4.4405 - accuracy: 0.0925 - val_loss: 4.6503 - val_accuracy: 0.0788\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 4.4302 - accuracy: 0.0894 - val_loss: 4.6554 - val_accuracy: 0.0812\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.4224 - accuracy: 0.0913 - val_loss: 4.7132 - val_accuracy: 0.0746\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 4.4151 - accuracy: 0.0927 - val_loss: 4.6875 - val_accuracy: 0.0803\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 4.4076 - accuracy: 0.0934 - val_loss: 4.6741 - val_accuracy: 0.0862\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.4009 - accuracy: 0.0929 - val_loss: 4.7016 - val_accuracy: 0.0845\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 4.3959 - accuracy: 0.0980 - val_loss: 4.6969 - val_accuracy: 0.0845\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 4.3912 - accuracy: 0.0959 - val_loss: 4.7321 - val_accuracy: 0.0876\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.3860 - accuracy: 0.0951 - val_loss: 4.7451 - val_accuracy: 0.0845\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.3806 - accuracy: 0.0954 - val_loss: 4.7695 - val_accuracy: 0.0887\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.3776 - accuracy: 0.0971 - val_loss: 4.7220 - val_accuracy: 0.0821\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 4.3727 - accuracy: 0.0976 - val_loss: 4.7920 - val_accuracy: 0.0889\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 4.8905 - accuracy: 0.0722 - val_loss: 4.7586 - val_accuracy: 0.0799\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 8s 5ms/step - loss: 4.6112 - accuracy: 0.0839 - val_loss: 4.6847 - val_accuracy: 0.0794\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.5527 - accuracy: 0.0856 - val_loss: 4.6916 - val_accuracy: 0.0862\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.5225 - accuracy: 0.0840 - val_loss: 4.6449 - val_accuracy: 0.0799\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 4.5019 - accuracy: 0.0843 - val_loss: 4.6358 - val_accuracy: 0.0860\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.4815 - accuracy: 0.0869 - val_loss: 4.6269 - val_accuracy: 0.0827\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.4658 - accuracy: 0.0865 - val_loss: 4.6505 - val_accuracy: 0.0823\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.4470 - accuracy: 0.0884 - val_loss: 4.6390 - val_accuracy: 0.0851\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.4333 - accuracy: 0.0898 - val_loss: 4.6378 - val_accuracy: 0.0849\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.4215 - accuracy: 0.0903 - val_loss: 4.6428 - val_accuracy: 0.0946\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 4.4108 - accuracy: 0.0905 - val_loss: 4.6925 - val_accuracy: 0.0939\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.4032 - accuracy: 0.0918 - val_loss: 4.6486 - val_accuracy: 0.0953\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 4.3982 - accuracy: 0.0897 - val_loss: 4.6897 - val_accuracy: 0.0851\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.3897 - accuracy: 0.0914 - val_loss: 4.7277 - val_accuracy: 0.0878\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.3811 - accuracy: 0.0911 - val_loss: 4.7272 - val_accuracy: 0.0944\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 4.3753 - accuracy: 0.0962 - val_loss: 4.7116 - val_accuracy: 0.0873\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 4.3701 - accuracy: 0.0928 - val_loss: 4.7614 - val_accuracy: 0.0942\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 4.3657 - accuracy: 0.0968 - val_loss: 4.7351 - val_accuracy: 0.1076\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 4.3594 - accuracy: 0.0961 - val_loss: 4.7527 - val_accuracy: 0.0860\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 4.3522 - accuracy: 0.0937 - val_loss: 4.7651 - val_accuracy: 0.0948\n",
      "Average Validation Accuracy: 0.0961405485868454\n",
      "Average Validation Loss: 4.64124870300293\n",
      "Average Test Accuracy: 0.0938674733042717\n",
      "Final Test Accuracy for each fold: 0.0952310785651207\n",
      "Number of input features: 4\n",
      "Fold: 1\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1846/1846 [==============================] - 9s 4ms/step - loss: 4.4656 - accuracy: 0.1161 - val_loss: 4.0140 - val_accuracy: 0.1602\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 6s 4ms/step - loss: 3.6142 - accuracy: 0.2119 - val_loss: 3.5459 - val_accuracy: 0.2440\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 3.1945 - accuracy: 0.2637 - val_loss: 3.2340 - val_accuracy: 0.2946\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 2.9094 - accuracy: 0.3042 - val_loss: 3.0406 - val_accuracy: 0.3058\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 2.7075 - accuracy: 0.3272 - val_loss: 2.8869 - val_accuracy: 0.3351\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 2.5469 - accuracy: 0.3500 - val_loss: 2.7701 - val_accuracy: 0.3382\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 2.4286 - accuracy: 0.3653 - val_loss: 2.6815 - val_accuracy: 0.3615\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 2.3323 - accuracy: 0.3814 - val_loss: 2.5934 - val_accuracy: 0.4004\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 2.2770 - accuracy: 0.3931 - val_loss: 2.5735 - val_accuracy: 0.3877\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 8s 5ms/step - loss: 2.2277 - accuracy: 0.3987 - val_loss: 2.5395 - val_accuracy: 0.3817\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 2.1865 - accuracy: 0.4073 - val_loss: 2.4979 - val_accuracy: 0.3846\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 2.1582 - accuracy: 0.4126 - val_loss: 2.4763 - val_accuracy: 0.3822\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 2.1266 - accuracy: 0.4162 - val_loss: 2.4040 - val_accuracy: 0.4095\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 6s 4ms/step - loss: 2.1113 - accuracy: 0.4125 - val_loss: 2.3942 - val_accuracy: 0.4084\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 8s 5ms/step - loss: 2.0786 - accuracy: 0.4263 - val_loss: 2.3547 - val_accuracy: 0.4013\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 2.0591 - accuracy: 0.4254 - val_loss: 2.3067 - val_accuracy: 0.4444\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 2.0497 - accuracy: 0.4274 - val_loss: 2.4402 - val_accuracy: 0.3987\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 2.0287 - accuracy: 0.4267 - val_loss: 2.2873 - val_accuracy: 0.4185\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 2.0213 - accuracy: 0.4359 - val_loss: 2.3662 - val_accuracy: 0.3941\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 2.0104 - accuracy: 0.4334 - val_loss: 2.3726 - val_accuracy: 0.4031\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 7s 3ms/step - loss: 4.3722 - accuracy: 0.1262 - val_loss: 3.9007 - val_accuracy: 0.1705\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 3.5261 - accuracy: 0.2097 - val_loss: 3.4649 - val_accuracy: 0.2832\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 3.0756 - accuracy: 0.2866 - val_loss: 3.2300 - val_accuracy: 0.2869\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 2.7551 - accuracy: 0.3313 - val_loss: 2.8978 - val_accuracy: 0.3560\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 2.5623 - accuracy: 0.3524 - val_loss: 2.7905 - val_accuracy: 0.3560\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 2.4292 - accuracy: 0.3754 - val_loss: 2.7412 - val_accuracy: 0.3802\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 2.3360 - accuracy: 0.3917 - val_loss: 2.6305 - val_accuracy: 0.3905\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 2.2715 - accuracy: 0.3985 - val_loss: 2.5674 - val_accuracy: 0.4075\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 2.2087 - accuracy: 0.4129 - val_loss: 2.5197 - val_accuracy: 0.3824\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 2.1558 - accuracy: 0.4216 - val_loss: 2.5394 - val_accuracy: 0.3965\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 2.1197 - accuracy: 0.4360 - val_loss: 2.4196 - val_accuracy: 0.4583\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 2.0862 - accuracy: 0.4401 - val_loss: 2.4499 - val_accuracy: 0.4359\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 2.0598 - accuracy: 0.4425 - val_loss: 2.4469 - val_accuracy: 0.4156\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 2.0424 - accuracy: 0.4443 - val_loss: 2.3613 - val_accuracy: 0.4616\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 2.0141 - accuracy: 0.4537 - val_loss: 2.3283 - val_accuracy: 0.4546\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 1.9964 - accuracy: 0.4593 - val_loss: 2.4028 - val_accuracy: 0.4295\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 1.9790 - accuracy: 0.4623 - val_loss: 2.2971 - val_accuracy: 0.4519\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 1.9649 - accuracy: 0.4619 - val_loss: 2.2650 - val_accuracy: 0.4825\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 1.9420 - accuracy: 0.4687 - val_loss: 2.2728 - val_accuracy: 0.4849\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 1.9395 - accuracy: 0.4646 - val_loss: 2.3536 - val_accuracy: 0.4363\n",
      "Average Validation Accuracy: 0.43168905377388\n",
      "Average Validation Loss: 2.128999948501587\n",
      "Average Test Accuracy: 0.433220311999321\n",
      "Final Test Accuracy for each fold: 0.4483673572540283\n",
      "Number of input features: 5\n",
      "Fold: 1\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1846/1846 [==============================] - 7s 3ms/step - loss: 4.3775 - accuracy: 0.1435 - val_loss: 3.8478 - val_accuracy: 0.1890\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 3.4200 - accuracy: 0.2553 - val_loss: 3.2893 - val_accuracy: 0.2860\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 2.8818 - accuracy: 0.3311 - val_loss: 2.8283 - val_accuracy: 0.3498\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 2.4479 - accuracy: 0.4010 - val_loss: 2.5215 - val_accuracy: 0.3707\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 2.1576 - accuracy: 0.4455 - val_loss: 2.2766 - val_accuracy: 0.4770\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 1.9446 - accuracy: 0.4764 - val_loss: 2.1822 - val_accuracy: 0.4499\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 1.7998 - accuracy: 0.5083 - val_loss: 2.0367 - val_accuracy: 0.5373\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 1.6853 - accuracy: 0.5336 - val_loss: 1.9181 - val_accuracy: 0.5091\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 1.5823 - accuracy: 0.5512 - val_loss: 1.8895 - val_accuracy: 0.5278\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 6s 4ms/step - loss: 1.5161 - accuracy: 0.5644 - val_loss: 1.7777 - val_accuracy: 0.5454\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 1.4664 - accuracy: 0.5732 - val_loss: 1.7733 - val_accuracy: 0.5399\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 1.4071 - accuracy: 0.5907 - val_loss: 1.7294 - val_accuracy: 0.5778\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 8s 5ms/step - loss: 1.3660 - accuracy: 0.6025 - val_loss: 1.6716 - val_accuracy: 0.5897\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 1.3198 - accuracy: 0.6082 - val_loss: 1.6348 - val_accuracy: 0.6172\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 1.2921 - accuracy: 0.6149 - val_loss: 1.5801 - val_accuracy: 0.5525\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 1.2590 - accuracy: 0.6191 - val_loss: 1.6685 - val_accuracy: 0.5842\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 1.2386 - accuracy: 0.6330 - val_loss: 1.4694 - val_accuracy: 0.6447\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.2181 - accuracy: 0.6284 - val_loss: 1.5034 - val_accuracy: 0.6442\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 1.1974 - accuracy: 0.6390 - val_loss: 1.4566 - val_accuracy: 0.6480\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 1.1782 - accuracy: 0.6461 - val_loss: 1.4454 - val_accuracy: 0.6315\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 4.3489 - accuracy: 0.1459 - val_loss: 3.8542 - val_accuracy: 0.2240\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 3.3050 - accuracy: 0.2938 - val_loss: 3.2197 - val_accuracy: 0.3021\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 2.7326 - accuracy: 0.3792 - val_loss: 2.7491 - val_accuracy: 0.4044\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 2.3385 - accuracy: 0.4313 - val_loss: 2.4256 - val_accuracy: 0.4594\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 2.0360 - accuracy: 0.4789 - val_loss: 2.1593 - val_accuracy: 0.4882\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 1.8195 - accuracy: 0.5095 - val_loss: 1.9798 - val_accuracy: 0.5270\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 1.6569 - accuracy: 0.5412 - val_loss: 1.8495 - val_accuracy: 0.5250\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 1.5424 - accuracy: 0.5606 - val_loss: 1.7364 - val_accuracy: 0.5534\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 1.4445 - accuracy: 0.5816 - val_loss: 1.7014 - val_accuracy: 0.5391\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 1.3827 - accuracy: 0.5928 - val_loss: 1.5640 - val_accuracy: 0.5637\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 1.3151 - accuracy: 0.6132 - val_loss: 1.4444 - val_accuracy: 0.6367\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 1.2604 - accuracy: 0.6209 - val_loss: 1.4277 - val_accuracy: 0.6207\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 1.2124 - accuracy: 0.6377 - val_loss: 1.4959 - val_accuracy: 0.5947\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 1.1778 - accuracy: 0.6486 - val_loss: 1.3320 - val_accuracy: 0.6284\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 1.1414 - accuracy: 0.6544 - val_loss: 1.3342 - val_accuracy: 0.6416\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 1.1173 - accuracy: 0.6603 - val_loss: 1.4185 - val_accuracy: 0.5927\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 1.0951 - accuracy: 0.6685 - val_loss: 1.2659 - val_accuracy: 0.6535\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 1.0626 - accuracy: 0.6754 - val_loss: 1.3189 - val_accuracy: 0.6156\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 1.0545 - accuracy: 0.6717 - val_loss: 1.1849 - val_accuracy: 0.6728\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 1.0269 - accuracy: 0.6830 - val_loss: 1.2355 - val_accuracy: 0.6196\n",
      "Average Validation Accuracy: 0.6440835893154144\n",
      "Average Validation Loss: 1.1642199754714966\n",
      "Average Test Accuracy: 0.6397508382797241\n",
      "Final Test Accuracy for each fold: 0.6470111012458801\n",
      "Number of input features: 6\n",
      "Fold: 1\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.1943 - accuracy: 0.1852 - val_loss: 3.3664 - val_accuracy: 0.3076\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 2.7234 - accuracy: 0.4162 - val_loss: 2.5603 - val_accuracy: 0.4975\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 2.0696 - accuracy: 0.5337 - val_loss: 2.1129 - val_accuracy: 0.5646\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.6593 - accuracy: 0.6179 - val_loss: 1.8310 - val_accuracy: 0.6222\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 1.3783 - accuracy: 0.6705 - val_loss: 1.6281 - val_accuracy: 0.6719\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 1.1882 - accuracy: 0.7063 - val_loss: 1.4566 - val_accuracy: 0.7230\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 1.0528 - accuracy: 0.7338 - val_loss: 1.3366 - val_accuracy: 0.7153\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.9616 - accuracy: 0.7463 - val_loss: 1.2722 - val_accuracy: 0.7107\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.8829 - accuracy: 0.7582 - val_loss: 1.1311 - val_accuracy: 0.7540\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.8322 - accuracy: 0.7659 - val_loss: 1.1112 - val_accuracy: 0.7556\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.7801 - accuracy: 0.7789 - val_loss: 1.0913 - val_accuracy: 0.7692\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.7446 - accuracy: 0.7876 - val_loss: 1.0283 - val_accuracy: 0.7716\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.7148 - accuracy: 0.7930 - val_loss: 0.9750 - val_accuracy: 0.7782\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.6907 - accuracy: 0.7971 - val_loss: 0.9584 - val_accuracy: 0.7749\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.6662 - accuracy: 0.7990 - val_loss: 0.9483 - val_accuracy: 0.7842\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.6485 - accuracy: 0.8057 - val_loss: 0.9064 - val_accuracy: 0.7910\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.6243 - accuracy: 0.8121 - val_loss: 1.0838 - val_accuracy: 0.7410\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.6040 - accuracy: 0.8161 - val_loss: 0.8554 - val_accuracy: 0.8066\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.5957 - accuracy: 0.8158 - val_loss: 0.8841 - val_accuracy: 0.7784\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.5743 - accuracy: 0.8239 - val_loss: 0.8300 - val_accuracy: 0.8301\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 4.1498 - accuracy: 0.1863 - val_loss: 3.2882 - val_accuracy: 0.3406\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 2.4969 - accuracy: 0.4681 - val_loss: 2.2688 - val_accuracy: 0.5765\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 1.6523 - accuracy: 0.6208 - val_loss: 1.7582 - val_accuracy: 0.6464\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 1.2248 - accuracy: 0.6988 - val_loss: 1.4837 - val_accuracy: 0.6854\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.9968 - accuracy: 0.7408 - val_loss: 1.2966 - val_accuracy: 0.7358\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.8646 - accuracy: 0.7642 - val_loss: 1.2375 - val_accuracy: 0.7646\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 8s 5ms/step - loss: 0.7771 - accuracy: 0.7815 - val_loss: 1.1330 - val_accuracy: 0.7800\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.7083 - accuracy: 0.7997 - val_loss: 1.0874 - val_accuracy: 0.7793\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.6708 - accuracy: 0.8056 - val_loss: 0.9828 - val_accuracy: 0.7800\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.6297 - accuracy: 0.8180 - val_loss: 0.8762 - val_accuracy: 0.8356\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.5979 - accuracy: 0.8209 - val_loss: 0.9126 - val_accuracy: 0.8132\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.5785 - accuracy: 0.8318 - val_loss: 0.8672 - val_accuracy: 0.8321\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.5462 - accuracy: 0.8354 - val_loss: 0.8047 - val_accuracy: 0.8436\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.5233 - accuracy: 0.8414 - val_loss: 0.7965 - val_accuracy: 0.8306\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.5116 - accuracy: 0.8496 - val_loss: 0.7628 - val_accuracy: 0.8462\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.5026 - accuracy: 0.8448 - val_loss: 0.7530 - val_accuracy: 0.8462\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.4785 - accuracy: 0.8559 - val_loss: 0.7845 - val_accuracy: 0.8348\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.4828 - accuracy: 0.8535 - val_loss: 0.7155 - val_accuracy: 0.8491\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.4544 - accuracy: 0.8603 - val_loss: 0.8702 - val_accuracy: 0.7969\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.4655 - accuracy: 0.8569 - val_loss: 0.8245 - val_accuracy: 0.8128\n",
      "Average Validation Accuracy: 0.8356384634971619\n",
      "Average Validation Loss: 0.6365662217140198\n",
      "Average Test Accuracy: 0.8326454162597656\n",
      "Final Test Accuracy for each fold: 0.8430014252662659\n",
      "Number of input features: 7\n",
      "Fold: 1\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1846/1846 [==============================] - 6s 3ms/step - loss: 4.0935 - accuracy: 0.2065 - val_loss: 3.2397 - val_accuracy: 0.3677\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 2.5178 - accuracy: 0.4769 - val_loss: 2.2595 - val_accuracy: 0.5529\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 16s 9ms/step - loss: 1.7553 - accuracy: 0.6046 - val_loss: 1.7750 - val_accuracy: 0.6293\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 15s 8ms/step - loss: 1.3341 - accuracy: 0.6761 - val_loss: 1.4779 - val_accuracy: 0.6900\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.0813 - accuracy: 0.7246 - val_loss: 1.2532 - val_accuracy: 0.7580\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.9200 - accuracy: 0.7527 - val_loss: 1.1580 - val_accuracy: 0.7591\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.8063 - accuracy: 0.7781 - val_loss: 1.0564 - val_accuracy: 0.7864\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.7297 - accuracy: 0.7943 - val_loss: 0.9545 - val_accuracy: 0.7978\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.6620 - accuracy: 0.8096 - val_loss: 0.9192 - val_accuracy: 0.8165\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.6337 - accuracy: 0.8157 - val_loss: 0.8986 - val_accuracy: 0.8007\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.5765 - accuracy: 0.8296 - val_loss: 0.8442 - val_accuracy: 0.8075\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.5515 - accuracy: 0.8312 - val_loss: 0.7970 - val_accuracy: 0.8420\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.5269 - accuracy: 0.8426 - val_loss: 0.7521 - val_accuracy: 0.8477\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4928 - accuracy: 0.8507 - val_loss: 0.8023 - val_accuracy: 0.8185\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.4794 - accuracy: 0.8565 - val_loss: 0.7477 - val_accuracy: 0.8449\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4597 - accuracy: 0.8586 - val_loss: 0.6941 - val_accuracy: 0.8612\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4434 - accuracy: 0.8638 - val_loss: 0.7329 - val_accuracy: 0.8614\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4418 - accuracy: 0.8641 - val_loss: 0.6523 - val_accuracy: 0.8810\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4206 - accuracy: 0.8745 - val_loss: 0.7021 - val_accuracy: 0.8576\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4120 - accuracy: 0.8768 - val_loss: 0.6463 - val_accuracy: 0.8759\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 10s 4ms/step - loss: 4.1734 - accuracy: 0.1978 - val_loss: 3.4058 - val_accuracy: 0.3272\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 2.7000 - accuracy: 0.4347 - val_loss: 2.5031 - val_accuracy: 0.5124\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.8900 - accuracy: 0.5734 - val_loss: 1.9726 - val_accuracy: 0.6099\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 1.4133 - accuracy: 0.6545 - val_loss: 1.6301 - val_accuracy: 0.7003\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 8s 5ms/step - loss: 1.1267 - accuracy: 0.7128 - val_loss: 1.4455 - val_accuracy: 0.7406\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.9453 - accuracy: 0.7494 - val_loss: 1.3141 - val_accuracy: 0.7415\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.8229 - accuracy: 0.7749 - val_loss: 1.1855 - val_accuracy: 0.7732\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.7385 - accuracy: 0.7914 - val_loss: 1.1304 - val_accuracy: 0.7622\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.6772 - accuracy: 0.8033 - val_loss: 1.0707 - val_accuracy: 0.7996\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.6193 - accuracy: 0.8152 - val_loss: 0.9583 - val_accuracy: 0.8275\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.5707 - accuracy: 0.8294 - val_loss: 1.0145 - val_accuracy: 0.7905\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.5395 - accuracy: 0.8342 - val_loss: 0.9112 - val_accuracy: 0.8284\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.5100 - accuracy: 0.8472 - val_loss: 0.9035 - val_accuracy: 0.8416\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.4909 - accuracy: 0.8504 - val_loss: 0.9594 - val_accuracy: 0.8112\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 13s 7ms/step - loss: 0.4670 - accuracy: 0.8567 - val_loss: 0.8644 - val_accuracy: 0.8495\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.4651 - accuracy: 0.8569 - val_loss: 0.9164 - val_accuracy: 0.8370\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.4287 - accuracy: 0.8694 - val_loss: 0.8650 - val_accuracy: 0.8354\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.4286 - accuracy: 0.8680 - val_loss: 0.8742 - val_accuracy: 0.8513\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.4162 - accuracy: 0.8728 - val_loss: 0.8726 - val_accuracy: 0.8453\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.4052 - accuracy: 0.8737 - val_loss: 0.8126 - val_accuracy: 0.8662\n",
      "Average Validation Accuracy: 0.8822929561138153\n",
      "Average Validation Loss: 0.49539928138256073\n",
      "Average Test Accuracy: 0.8774231374263763\n",
      "Final Test Accuracy for each fold: 0.8798555135726929\n",
      "Number of input features: 8\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 11s 5ms/step - loss: 3.9636 - accuracy: 0.2104 - val_loss: 3.0281 - val_accuracy: 0.3945\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 2.2995 - accuracy: 0.4988 - val_loss: 2.0843 - val_accuracy: 0.5309\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.5974 - accuracy: 0.6197 - val_loss: 1.5935 - val_accuracy: 0.6451\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 1.2143 - accuracy: 0.6905 - val_loss: 1.3341 - val_accuracy: 0.7065\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.9831 - accuracy: 0.7308 - val_loss: 1.1470 - val_accuracy: 0.7461\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.8367 - accuracy: 0.7666 - val_loss: 0.9897 - val_accuracy: 0.7789\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.7473 - accuracy: 0.7842 - val_loss: 0.9487 - val_accuracy: 0.7842\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.6721 - accuracy: 0.7987 - val_loss: 0.8613 - val_accuracy: 0.7888\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.6155 - accuracy: 0.8106 - val_loss: 0.7971 - val_accuracy: 0.8110\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.5804 - accuracy: 0.8143 - val_loss: 0.7865 - val_accuracy: 0.7989\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.5429 - accuracy: 0.8261 - val_loss: 0.7345 - val_accuracy: 0.8246\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.5125 - accuracy: 0.8329 - val_loss: 0.7727 - val_accuracy: 0.8084\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4903 - accuracy: 0.8406 - val_loss: 0.6802 - val_accuracy: 0.8376\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4666 - accuracy: 0.8509 - val_loss: 0.6605 - val_accuracy: 0.8510\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4570 - accuracy: 0.8503 - val_loss: 0.6884 - val_accuracy: 0.8363\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.4441 - accuracy: 0.8595 - val_loss: 0.6559 - val_accuracy: 0.8466\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.4264 - accuracy: 0.8625 - val_loss: 0.6430 - val_accuracy: 0.8563\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.4177 - accuracy: 0.8649 - val_loss: 0.6019 - val_accuracy: 0.8592\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3999 - accuracy: 0.8680 - val_loss: 0.5956 - val_accuracy: 0.8662\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.3885 - accuracy: 0.8722 - val_loss: 0.6219 - val_accuracy: 0.8495\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 10s 4ms/step - loss: 3.8146 - accuracy: 0.2326 - val_loss: 3.0061 - val_accuracy: 0.3936\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 2.2948 - accuracy: 0.4899 - val_loss: 2.2636 - val_accuracy: 0.5476\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 1.6733 - accuracy: 0.6036 - val_loss: 1.8448 - val_accuracy: 0.6385\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 1.3077 - accuracy: 0.6709 - val_loss: 1.5215 - val_accuracy: 0.6981\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 1.0759 - accuracy: 0.7193 - val_loss: 1.4079 - val_accuracy: 0.6891\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.9267 - accuracy: 0.7399 - val_loss: 1.2159 - val_accuracy: 0.7408\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.8187 - accuracy: 0.7656 - val_loss: 1.1644 - val_accuracy: 0.7481\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.7506 - accuracy: 0.7793 - val_loss: 1.0816 - val_accuracy: 0.7749\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.6914 - accuracy: 0.7900 - val_loss: 0.9781 - val_accuracy: 0.7987\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.6423 - accuracy: 0.8039 - val_loss: 0.9351 - val_accuracy: 0.8053\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.6060 - accuracy: 0.8168 - val_loss: 0.8801 - val_accuracy: 0.8180\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.5669 - accuracy: 0.8258 - val_loss: 0.8245 - val_accuracy: 0.8279\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.5496 - accuracy: 0.8290 - val_loss: 0.9175 - val_accuracy: 0.8123\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.5131 - accuracy: 0.8424 - val_loss: 0.9486 - val_accuracy: 0.7991\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.5002 - accuracy: 0.8458 - val_loss: 0.8648 - val_accuracy: 0.7870\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.4840 - accuracy: 0.8483 - val_loss: 0.8023 - val_accuracy: 0.8257\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.4667 - accuracy: 0.8526 - val_loss: 0.7845 - val_accuracy: 0.8194\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.4461 - accuracy: 0.8591 - val_loss: 0.7135 - val_accuracy: 0.8539\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.4459 - accuracy: 0.8656 - val_loss: 0.7362 - val_accuracy: 0.8471\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.4197 - accuracy: 0.8701 - val_loss: 0.6767 - val_accuracy: 0.8647\n",
      "Average Validation Accuracy: 0.8701671957969666\n",
      "Average Validation Loss: 0.47086454927921295\n",
      "Average Test Accuracy: 0.8671408593654633\n",
      "Final Test Accuracy for each fold: 0.8816245198249817\n",
      "Number of input features: 9\n",
      "Fold: 1\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1846/1846 [==============================] - 5s 2ms/step - loss: 3.8908 - accuracy: 0.2353 - val_loss: 2.8544 - val_accuracy: 0.4400\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 2.0497 - accuracy: 0.5605 - val_loss: 1.7940 - val_accuracy: 0.6442\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 1.2930 - accuracy: 0.7066 - val_loss: 1.3146 - val_accuracy: 0.7342\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.9332 - accuracy: 0.7713 - val_loss: 1.1167 - val_accuracy: 0.7813\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.7460 - accuracy: 0.8047 - val_loss: 0.9514 - val_accuracy: 0.8002\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.6311 - accuracy: 0.8290 - val_loss: 0.8301 - val_accuracy: 0.8191\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.5607 - accuracy: 0.8434 - val_loss: 0.7769 - val_accuracy: 0.8295\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.5114 - accuracy: 0.8532 - val_loss: 0.7138 - val_accuracy: 0.8418\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.4643 - accuracy: 0.8646 - val_loss: 0.6264 - val_accuracy: 0.8689\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.4198 - accuracy: 0.8775 - val_loss: 0.6508 - val_accuracy: 0.8634\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.4058 - accuracy: 0.8794 - val_loss: 0.6543 - val_accuracy: 0.8609\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3801 - accuracy: 0.8869 - val_loss: 0.5775 - val_accuracy: 0.8733\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3525 - accuracy: 0.8948 - val_loss: 0.5548 - val_accuracy: 0.8876\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3426 - accuracy: 0.8979 - val_loss: 0.5059 - val_accuracy: 0.8979\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3281 - accuracy: 0.9016 - val_loss: 0.5231 - val_accuracy: 0.8966\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3080 - accuracy: 0.9061 - val_loss: 0.5041 - val_accuracy: 0.9063\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2976 - accuracy: 0.9098 - val_loss: 0.4540 - val_accuracy: 0.9215\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2901 - accuracy: 0.9152 - val_loss: 0.4905 - val_accuracy: 0.9058\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2700 - accuracy: 0.9208 - val_loss: 0.4665 - val_accuracy: 0.9094\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2789 - accuracy: 0.9165 - val_loss: 0.4940 - val_accuracy: 0.8997\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 3.9716 - accuracy: 0.1937 - val_loss: 3.1280 - val_accuracy: 0.3952\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 2.2272 - accuracy: 0.5620 - val_loss: 1.9245 - val_accuracy: 0.6548\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 1.3251 - accuracy: 0.7275 - val_loss: 1.3982 - val_accuracy: 0.7481\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 0.9285 - accuracy: 0.7876 - val_loss: 1.0933 - val_accuracy: 0.8145\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.7032 - accuracy: 0.8297 - val_loss: 0.9110 - val_accuracy: 0.8460\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.5687 - accuracy: 0.8564 - val_loss: 0.8163 - val_accuracy: 0.8612\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.4917 - accuracy: 0.8734 - val_loss: 0.7677 - val_accuracy: 0.8667\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.4360 - accuracy: 0.8836 - val_loss: 0.7126 - val_accuracy: 0.8684\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3886 - accuracy: 0.8981 - val_loss: 0.6238 - val_accuracy: 0.8977\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3632 - accuracy: 0.9011 - val_loss: 0.5617 - val_accuracy: 0.8975\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3346 - accuracy: 0.9093 - val_loss: 0.5857 - val_accuracy: 0.8891\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3185 - accuracy: 0.9136 - val_loss: 0.5231 - val_accuracy: 0.8961\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 0.3002 - accuracy: 0.9147 - val_loss: 0.5110 - val_accuracy: 0.9113\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2858 - accuracy: 0.9238 - val_loss: 0.4623 - val_accuracy: 0.9201\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2714 - accuracy: 0.9239 - val_loss: 0.5318 - val_accuracy: 0.8845\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2565 - accuracy: 0.9268 - val_loss: 0.4583 - val_accuracy: 0.9230\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2495 - accuracy: 0.9289 - val_loss: 0.4399 - val_accuracy: 0.9267\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2462 - accuracy: 0.9326 - val_loss: 0.4692 - val_accuracy: 0.9193\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2390 - accuracy: 0.9324 - val_loss: 0.4345 - val_accuracy: 0.9342\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2235 - accuracy: 0.9406 - val_loss: 0.4857 - val_accuracy: 0.9144\n",
      "Average Validation Accuracy: 0.9227756261825562\n",
      "Average Validation Loss: 0.3325955867767334\n",
      "Average Test Accuracy: 0.9206530451774597\n",
      "Final Test Accuracy for each fold: 0.9321146607398987\n",
      "Number of input features: 10\n",
      "Fold: 1\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1846/1846 [==============================] - 7s 3ms/step - loss: 3.7894 - accuracy: 0.2680 - val_loss: 2.6982 - val_accuracy: 0.5054\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 1.9026 - accuracy: 0.6212 - val_loss: 1.6341 - val_accuracy: 0.7039\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 1.0993 - accuracy: 0.7744 - val_loss: 1.1167 - val_accuracy: 0.8154\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.7288 - accuracy: 0.8387 - val_loss: 0.8449 - val_accuracy: 0.8526\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.5439 - accuracy: 0.8703 - val_loss: 0.7104 - val_accuracy: 0.8737\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.4453 - accuracy: 0.8885 - val_loss: 0.6317 - val_accuracy: 0.8766\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3857 - accuracy: 0.8991 - val_loss: 0.5839 - val_accuracy: 0.8933\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3411 - accuracy: 0.9080 - val_loss: 0.5280 - val_accuracy: 0.9120\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3018 - accuracy: 0.9192 - val_loss: 0.5010 - val_accuracy: 0.9164\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2838 - accuracy: 0.9206 - val_loss: 0.4973 - val_accuracy: 0.9102\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2644 - accuracy: 0.9258 - val_loss: 0.4791 - val_accuracy: 0.9089\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2527 - accuracy: 0.9311 - val_loss: 0.4789 - val_accuracy: 0.9201\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 0.2417 - accuracy: 0.9341 - val_loss: 0.4856 - val_accuracy: 0.9023\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2226 - accuracy: 0.9387 - val_loss: 0.4222 - val_accuracy: 0.9333\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2158 - accuracy: 0.9410 - val_loss: 0.4582 - val_accuracy: 0.9237\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2071 - accuracy: 0.9436 - val_loss: 0.4174 - val_accuracy: 0.9375\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1951 - accuracy: 0.9463 - val_loss: 0.4101 - val_accuracy: 0.9360\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1925 - accuracy: 0.9471 - val_loss: 0.3937 - val_accuracy: 0.9421\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1860 - accuracy: 0.9487 - val_loss: 0.3863 - val_accuracy: 0.9439\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1840 - accuracy: 0.9495 - val_loss: 0.4559 - val_accuracy: 0.9254\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 3.5047 - accuracy: 0.3369 - val_loss: 2.3271 - val_accuracy: 0.5707\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 1.5641 - accuracy: 0.6911 - val_loss: 1.4158 - val_accuracy: 0.7580\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.9249 - accuracy: 0.7967 - val_loss: 1.0119 - val_accuracy: 0.8143\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.6253 - accuracy: 0.8516 - val_loss: 0.8478 - val_accuracy: 0.8431\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.4849 - accuracy: 0.8772 - val_loss: 0.7082 - val_accuracy: 0.8717\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3924 - accuracy: 0.8953 - val_loss: 0.5960 - val_accuracy: 0.8968\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3467 - accuracy: 0.9047 - val_loss: 0.5583 - val_accuracy: 0.8981\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3095 - accuracy: 0.9156 - val_loss: 0.5058 - val_accuracy: 0.9058\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2844 - accuracy: 0.9189 - val_loss: 0.4879 - val_accuracy: 0.9056\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2597 - accuracy: 0.9275 - val_loss: 0.5313 - val_accuracy: 0.9012\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2538 - accuracy: 0.9307 - val_loss: 0.4600 - val_accuracy: 0.9237\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2370 - accuracy: 0.9357 - val_loss: 0.4789 - val_accuracy: 0.9076\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2206 - accuracy: 0.9380 - val_loss: 0.4509 - val_accuracy: 0.9248\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2263 - accuracy: 0.9377 - val_loss: 0.4321 - val_accuracy: 0.9228\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2039 - accuracy: 0.9441 - val_loss: 0.4064 - val_accuracy: 0.9351\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1976 - accuracy: 0.9468 - val_loss: 0.4028 - val_accuracy: 0.9353\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1942 - accuracy: 0.9481 - val_loss: 0.4165 - val_accuracy: 0.9285\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1876 - accuracy: 0.9507 - val_loss: 0.3914 - val_accuracy: 0.9448\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1774 - accuracy: 0.9539 - val_loss: 0.3691 - val_accuracy: 0.9406\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1796 - accuracy: 0.9507 - val_loss: 0.4141 - val_accuracy: 0.9285\n",
      "Average Validation Accuracy: 0.9388957023620605\n",
      "Average Validation Loss: 0.2835354208946228\n",
      "Average Test Accuracy: 0.9409228265285492\n",
      "Final Test Accuracy for each fold: 0.9436131715774536\n",
      "Number of input features: 11\n",
      "Fold: 1\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1846/1846 [==============================] - 5s 2ms/step - loss: 3.5721 - accuracy: 0.3086 - val_loss: 2.3528 - val_accuracy: 0.5364\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 1.5383 - accuracy: 0.6973 - val_loss: 1.3586 - val_accuracy: 0.7668\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.8947 - accuracy: 0.8054 - val_loss: 0.9558 - val_accuracy: 0.8156\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.6273 - accuracy: 0.8544 - val_loss: 0.7535 - val_accuracy: 0.8548\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.4829 - accuracy: 0.8810 - val_loss: 0.7530 - val_accuracy: 0.8488\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3969 - accuracy: 0.8992 - val_loss: 0.6024 - val_accuracy: 0.8869\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3413 - accuracy: 0.9107 - val_loss: 0.5594 - val_accuracy: 0.8944\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3103 - accuracy: 0.9168 - val_loss: 0.5169 - val_accuracy: 0.9010\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2757 - accuracy: 0.9242 - val_loss: 0.4972 - val_accuracy: 0.9171\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2589 - accuracy: 0.9262 - val_loss: 0.4395 - val_accuracy: 0.9177\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2346 - accuracy: 0.9370 - val_loss: 0.4553 - val_accuracy: 0.9347\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2331 - accuracy: 0.9330 - val_loss: 0.4127 - val_accuracy: 0.9364\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2140 - accuracy: 0.9407 - val_loss: 0.3926 - val_accuracy: 0.9358\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2038 - accuracy: 0.9436 - val_loss: 0.4148 - val_accuracy: 0.9336\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1892 - accuracy: 0.9475 - val_loss: 0.3700 - val_accuracy: 0.9461\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1875 - accuracy: 0.9483 - val_loss: 0.4022 - val_accuracy: 0.9259\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1794 - accuracy: 0.9526 - val_loss: 0.3334 - val_accuracy: 0.9569\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1711 - accuracy: 0.9533 - val_loss: 0.3943 - val_accuracy: 0.9338\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1633 - accuracy: 0.9565 - val_loss: 0.3876 - val_accuracy: 0.9415\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1599 - accuracy: 0.9572 - val_loss: 0.4530 - val_accuracy: 0.9223\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 3.5440 - accuracy: 0.3364 - val_loss: 2.4245 - val_accuracy: 0.6000\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 1.6351 - accuracy: 0.6923 - val_loss: 1.6043 - val_accuracy: 0.7384\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 1.0072 - accuracy: 0.8013 - val_loss: 1.1663 - val_accuracy: 0.8139\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.6894 - accuracy: 0.8594 - val_loss: 0.9608 - val_accuracy: 0.8631\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.5019 - accuracy: 0.8966 - val_loss: 0.7949 - val_accuracy: 0.8935\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3952 - accuracy: 0.9129 - val_loss: 0.6957 - val_accuracy: 0.8986\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3260 - accuracy: 0.9252 - val_loss: 0.5859 - val_accuracy: 0.9217\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2721 - accuracy: 0.9362 - val_loss: 0.5915 - val_accuracy: 0.9067\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2480 - accuracy: 0.9382 - val_loss: 0.5169 - val_accuracy: 0.9276\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2193 - accuracy: 0.9461 - val_loss: 0.4582 - val_accuracy: 0.9472\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2009 - accuracy: 0.9510 - val_loss: 0.4358 - val_accuracy: 0.9476\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1917 - accuracy: 0.9534 - val_loss: 0.4233 - val_accuracy: 0.9406\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1732 - accuracy: 0.9563 - val_loss: 0.4393 - val_accuracy: 0.9303\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1689 - accuracy: 0.9594 - val_loss: 0.4176 - val_accuracy: 0.9406\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1562 - accuracy: 0.9627 - val_loss: 0.4012 - val_accuracy: 0.9501\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1520 - accuracy: 0.9619 - val_loss: 0.3831 - val_accuracy: 0.9485\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1458 - accuracy: 0.9654 - val_loss: 0.3751 - val_accuracy: 0.9525\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1422 - accuracy: 0.9666 - val_loss: 0.3857 - val_accuracy: 0.9483\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1372 - accuracy: 0.9668 - val_loss: 0.3693 - val_accuracy: 0.9553\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1356 - accuracy: 0.9689 - val_loss: 0.3906 - val_accuracy: 0.9441\n",
      "Average Validation Accuracy: 0.9437973499298096\n",
      "Average Validation Loss: 0.26664894819259644\n",
      "Average Test Accuracy: 0.9436500370502472\n",
      "Final Test Accuracy for each fold: 0.954964280128479\n",
      "Number of input features: 12\n",
      "Fold: 1\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1846/1846 [==============================] - 6s 2ms/step - loss: 3.7413 - accuracy: 0.2917 - val_loss: 2.4122 - val_accuracy: 0.5758\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 1.6221 - accuracy: 0.6902 - val_loss: 1.3767 - val_accuracy: 0.7518\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.9497 - accuracy: 0.8026 - val_loss: 0.9852 - val_accuracy: 0.8125\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.6373 - accuracy: 0.8566 - val_loss: 0.7673 - val_accuracy: 0.8561\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.4736 - accuracy: 0.8832 - val_loss: 0.6485 - val_accuracy: 0.8746\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3860 - accuracy: 0.8988 - val_loss: 0.5631 - val_accuracy: 0.9032\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3206 - accuracy: 0.9143 - val_loss: 0.5380 - val_accuracy: 0.9017\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2808 - accuracy: 0.9226 - val_loss: 0.4794 - val_accuracy: 0.9142\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2548 - accuracy: 0.9314 - val_loss: 0.5028 - val_accuracy: 0.9105\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2383 - accuracy: 0.9337 - val_loss: 0.4339 - val_accuracy: 0.9349\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2176 - accuracy: 0.9392 - val_loss: 0.3945 - val_accuracy: 0.9424\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2053 - accuracy: 0.9455 - val_loss: 0.3826 - val_accuracy: 0.9468\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2046 - accuracy: 0.9439 - val_loss: 0.3696 - val_accuracy: 0.9415\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1816 - accuracy: 0.9521 - val_loss: 0.3748 - val_accuracy: 0.9536\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1815 - accuracy: 0.9519 - val_loss: 0.3703 - val_accuracy: 0.9509\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1711 - accuracy: 0.9564 - val_loss: 0.4097 - val_accuracy: 0.9331\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1761 - accuracy: 0.9527 - val_loss: 0.4126 - val_accuracy: 0.9336\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1606 - accuracy: 0.9579 - val_loss: 0.3830 - val_accuracy: 0.9362\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1611 - accuracy: 0.9595 - val_loss: 0.3379 - val_accuracy: 0.9507\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1555 - accuracy: 0.9599 - val_loss: 0.3560 - val_accuracy: 0.9490\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 3.5045 - accuracy: 0.3287 - val_loss: 2.3104 - val_accuracy: 0.6007\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 1.4912 - accuracy: 0.7257 - val_loss: 1.3282 - val_accuracy: 0.7758\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.8199 - accuracy: 0.8343 - val_loss: 0.9259 - val_accuracy: 0.8662\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.5249 - accuracy: 0.8838 - val_loss: 0.7612 - val_accuracy: 0.8845\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3898 - accuracy: 0.9091 - val_loss: 0.6066 - val_accuracy: 0.9118\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3150 - accuracy: 0.9253 - val_loss: 0.5012 - val_accuracy: 0.9234\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2649 - accuracy: 0.9363 - val_loss: 0.4811 - val_accuracy: 0.9186\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2288 - accuracy: 0.9446 - val_loss: 0.4283 - val_accuracy: 0.9336\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2065 - accuracy: 0.9491 - val_loss: 0.3876 - val_accuracy: 0.9371\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1893 - accuracy: 0.9514 - val_loss: 0.3594 - val_accuracy: 0.9446\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1789 - accuracy: 0.9563 - val_loss: 0.3740 - val_accuracy: 0.9397\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1681 - accuracy: 0.9581 - val_loss: 0.3693 - val_accuracy: 0.9377\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1608 - accuracy: 0.9603 - val_loss: 0.3431 - val_accuracy: 0.9545\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1563 - accuracy: 0.9619 - val_loss: 0.3118 - val_accuracy: 0.9571\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1526 - accuracy: 0.9628 - val_loss: 0.3444 - val_accuracy: 0.9417\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1502 - accuracy: 0.9632 - val_loss: 0.3465 - val_accuracy: 0.9472\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1371 - accuracy: 0.9686 - val_loss: 0.3246 - val_accuracy: 0.9490\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1360 - accuracy: 0.9675 - val_loss: 0.2979 - val_accuracy: 0.9578\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1328 - accuracy: 0.9699 - val_loss: 0.2940 - val_accuracy: 0.9650\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1335 - accuracy: 0.9681 - val_loss: 0.2742 - val_accuracy: 0.9712\n",
      "Average Validation Accuracy: 0.967070072889328\n",
      "Average Validation Loss: 0.19360718876123428\n",
      "Average Test Accuracy: 0.9674578011035919\n",
      "Final Test Accuracy for each fold: 0.9778875112533569\n",
      "Number of input features: 13\n",
      "Fold: 1\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1846/1846 [==============================] - 6s 3ms/step - loss: 3.3971 - accuracy: 0.3622 - val_loss: 2.1803 - val_accuracy: 0.6491\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 1.4314 - accuracy: 0.7365 - val_loss: 1.2387 - val_accuracy: 0.7872\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.8051 - accuracy: 0.8351 - val_loss: 0.8750 - val_accuracy: 0.8612\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.5291 - accuracy: 0.8830 - val_loss: 0.7026 - val_accuracy: 0.8739\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3918 - accuracy: 0.9059 - val_loss: 0.5792 - val_accuracy: 0.9023\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3134 - accuracy: 0.9226 - val_loss: 0.5864 - val_accuracy: 0.8790\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2673 - accuracy: 0.9304 - val_loss: 0.4767 - val_accuracy: 0.9111\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2340 - accuracy: 0.9389 - val_loss: 0.4594 - val_accuracy: 0.9133\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2141 - accuracy: 0.9408 - val_loss: 0.4269 - val_accuracy: 0.9212\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2005 - accuracy: 0.9462 - val_loss: 0.3472 - val_accuracy: 0.9498\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1894 - accuracy: 0.9511 - val_loss: 0.3980 - val_accuracy: 0.9320\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1666 - accuracy: 0.9571 - val_loss: 0.3621 - val_accuracy: 0.9406\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1704 - accuracy: 0.9548 - val_loss: 0.3435 - val_accuracy: 0.9545\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1520 - accuracy: 0.9627 - val_loss: 0.3655 - val_accuracy: 0.9452\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1473 - accuracy: 0.9642 - val_loss: 0.3637 - val_accuracy: 0.9397\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1480 - accuracy: 0.9627 - val_loss: 0.3068 - val_accuracy: 0.9635\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1382 - accuracy: 0.9686 - val_loss: 0.3005 - val_accuracy: 0.9666\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1329 - accuracy: 0.9676 - val_loss: 0.3356 - val_accuracy: 0.9512\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1306 - accuracy: 0.9706 - val_loss: 0.3036 - val_accuracy: 0.9624\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1268 - accuracy: 0.9723 - val_loss: 0.2772 - val_accuracy: 0.9745\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 3.6630 - accuracy: 0.2979 - val_loss: 2.6039 - val_accuracy: 0.5160\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 1.7255 - accuracy: 0.6840 - val_loss: 1.5771 - val_accuracy: 0.7696\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.9967 - accuracy: 0.8133 - val_loss: 1.1611 - val_accuracy: 0.8282\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.6521 - accuracy: 0.8701 - val_loss: 0.8703 - val_accuracy: 0.8667\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.4559 - accuracy: 0.9028 - val_loss: 0.6942 - val_accuracy: 0.9047\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3451 - accuracy: 0.9201 - val_loss: 0.6089 - val_accuracy: 0.9177\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2763 - accuracy: 0.9362 - val_loss: 0.5103 - val_accuracy: 0.9248\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2378 - accuracy: 0.9436 - val_loss: 0.4498 - val_accuracy: 0.9402\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2097 - accuracy: 0.9511 - val_loss: 0.4103 - val_accuracy: 0.9397\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1864 - accuracy: 0.9552 - val_loss: 0.4363 - val_accuracy: 0.9417\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1779 - accuracy: 0.9588 - val_loss: 0.3723 - val_accuracy: 0.9388\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1634 - accuracy: 0.9599 - val_loss: 0.3503 - val_accuracy: 0.9507\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1553 - accuracy: 0.9629 - val_loss: 0.3264 - val_accuracy: 0.9575\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1535 - accuracy: 0.9623 - val_loss: 0.3498 - val_accuracy: 0.9512\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1465 - accuracy: 0.9638 - val_loss: 0.3568 - val_accuracy: 0.9441\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1426 - accuracy: 0.9654 - val_loss: 0.3235 - val_accuracy: 0.9529\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1371 - accuracy: 0.9680 - val_loss: 0.3134 - val_accuracy: 0.9540\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1311 - accuracy: 0.9702 - val_loss: 0.2960 - val_accuracy: 0.9586\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1337 - accuracy: 0.9664 - val_loss: 0.2886 - val_accuracy: 0.9657\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1223 - accuracy: 0.9701 - val_loss: 0.3002 - val_accuracy: 0.9602\n",
      "Average Validation Accuracy: 0.9738589525222778\n",
      "Average Validation Loss: 0.17162197083234787\n",
      "Average Test Accuracy: 0.9734650254249573\n",
      "Final Test Accuracy for each fold: 0.9778875112533569\n",
      "Number of input features: 14\n",
      "Fold: 1\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1846/1846 [==============================] - 5s 2ms/step - loss: 3.5515 - accuracy: 0.3216 - val_loss: 2.3424 - val_accuracy: 0.5448\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 1.5432 - accuracy: 0.7092 - val_loss: 1.3201 - val_accuracy: 0.7626\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.8694 - accuracy: 0.8279 - val_loss: 0.8420 - val_accuracy: 0.8572\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.5491 - accuracy: 0.8780 - val_loss: 0.6348 - val_accuracy: 0.8884\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3827 - accuracy: 0.9094 - val_loss: 0.5416 - val_accuracy: 0.9107\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3003 - accuracy: 0.9266 - val_loss: 0.4491 - val_accuracy: 0.9217\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2489 - accuracy: 0.9367 - val_loss: 0.4069 - val_accuracy: 0.9355\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2215 - accuracy: 0.9442 - val_loss: 0.3758 - val_accuracy: 0.9439\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1966 - accuracy: 0.9504 - val_loss: 0.3469 - val_accuracy: 0.9485\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1805 - accuracy: 0.9531 - val_loss: 0.3406 - val_accuracy: 0.9474\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1710 - accuracy: 0.9565 - val_loss: 0.3470 - val_accuracy: 0.9468\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1552 - accuracy: 0.9599 - val_loss: 0.3130 - val_accuracy: 0.9551\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1580 - accuracy: 0.9590 - val_loss: 0.3055 - val_accuracy: 0.9551\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1401 - accuracy: 0.9652 - val_loss: 0.2997 - val_accuracy: 0.9606\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1432 - accuracy: 0.9638 - val_loss: 0.2997 - val_accuracy: 0.9602\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1309 - accuracy: 0.9698 - val_loss: 0.2889 - val_accuracy: 0.9622\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1278 - accuracy: 0.9681 - val_loss: 0.2799 - val_accuracy: 0.9666\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1341 - accuracy: 0.9670 - val_loss: 0.3588 - val_accuracy: 0.9419\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1236 - accuracy: 0.9697 - val_loss: 0.2727 - val_accuracy: 0.9661\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1229 - accuracy: 0.9688 - val_loss: 0.3056 - val_accuracy: 0.9562\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 3.5265 - accuracy: 0.3378 - val_loss: 2.2931 - val_accuracy: 0.6070\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 1.5358 - accuracy: 0.7107 - val_loss: 1.4631 - val_accuracy: 0.7765\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.8967 - accuracy: 0.8333 - val_loss: 1.0647 - val_accuracy: 0.8367\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.5968 - accuracy: 0.8793 - val_loss: 0.8971 - val_accuracy: 0.8779\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.4289 - accuracy: 0.9106 - val_loss: 0.7660 - val_accuracy: 0.8957\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3381 - accuracy: 0.9275 - val_loss: 0.6851 - val_accuracy: 0.9208\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2824 - accuracy: 0.9383 - val_loss: 0.5687 - val_accuracy: 0.9364\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2425 - accuracy: 0.9464 - val_loss: 0.5195 - val_accuracy: 0.9461\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2111 - accuracy: 0.9521 - val_loss: 0.4542 - val_accuracy: 0.9536\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1950 - accuracy: 0.9555 - val_loss: 0.4648 - val_accuracy: 0.9443\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1737 - accuracy: 0.9588 - val_loss: 0.4281 - val_accuracy: 0.9435\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1639 - accuracy: 0.9611 - val_loss: 0.3851 - val_accuracy: 0.9597\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1571 - accuracy: 0.9625 - val_loss: 0.4068 - val_accuracy: 0.9448\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1491 - accuracy: 0.9662 - val_loss: 0.4372 - val_accuracy: 0.9404\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1413 - accuracy: 0.9672 - val_loss: 0.3722 - val_accuracy: 0.9542\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1328 - accuracy: 0.9697 - val_loss: 0.3655 - val_accuracy: 0.9531\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1291 - accuracy: 0.9698 - val_loss: 0.3830 - val_accuracy: 0.9468\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1277 - accuracy: 0.9703 - val_loss: 0.3280 - val_accuracy: 0.9690\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1207 - accuracy: 0.9741 - val_loss: 0.3481 - val_accuracy: 0.9633\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1269 - accuracy: 0.9714 - val_loss: 0.3828 - val_accuracy: 0.9523\n",
      "Average Validation Accuracy: 0.9636568129062653\n",
      "Average Validation Loss: 0.21241207420825958\n",
      "Average Test Accuracy: 0.96550452709198\n",
      "Final Test Accuracy for each fold: 0.9662415981292725\n",
      "Number of input features: 15\n",
      "Fold: 1\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1846/1846 [==============================] - 5s 2ms/step - loss: 3.5850 - accuracy: 0.3257 - val_loss: 2.4589 - val_accuracy: 0.5608\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 1.6688 - accuracy: 0.6977 - val_loss: 1.4512 - val_accuracy: 0.7476\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.9592 - accuracy: 0.8180 - val_loss: 0.9963 - val_accuracy: 0.8405\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.6402 - accuracy: 0.8720 - val_loss: 0.7861 - val_accuracy: 0.8829\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.4670 - accuracy: 0.9008 - val_loss: 0.6867 - val_accuracy: 0.8953\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3630 - accuracy: 0.9220 - val_loss: 0.5596 - val_accuracy: 0.9298\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2876 - accuracy: 0.9372 - val_loss: 0.4853 - val_accuracy: 0.9342\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2452 - accuracy: 0.9423 - val_loss: 0.4324 - val_accuracy: 0.9413\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2065 - accuracy: 0.9517 - val_loss: 0.4196 - val_accuracy: 0.9424\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1859 - accuracy: 0.9550 - val_loss: 0.4464 - val_accuracy: 0.9371\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1676 - accuracy: 0.9609 - val_loss: 0.3738 - val_accuracy: 0.9531\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1606 - accuracy: 0.9613 - val_loss: 0.3616 - val_accuracy: 0.9556\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1485 - accuracy: 0.9660 - val_loss: 0.3426 - val_accuracy: 0.9624\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1406 - accuracy: 0.9684 - val_loss: 0.3581 - val_accuracy: 0.9611\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1451 - accuracy: 0.9668 - val_loss: 0.3229 - val_accuracy: 0.9690\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1357 - accuracy: 0.9697 - val_loss: 0.3269 - val_accuracy: 0.9666\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1229 - accuracy: 0.9734 - val_loss: 0.3551 - val_accuracy: 0.9492\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1264 - accuracy: 0.9700 - val_loss: 0.3187 - val_accuracy: 0.9604\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1315 - accuracy: 0.9681 - val_loss: 0.3182 - val_accuracy: 0.9626\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1166 - accuracy: 0.9733 - val_loss: 0.3172 - val_accuracy: 0.9652\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 3.3545 - accuracy: 0.3622 - val_loss: 2.1256 - val_accuracy: 0.6436\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 1.3939 - accuracy: 0.7385 - val_loss: 1.3138 - val_accuracy: 0.7941\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.8242 - accuracy: 0.8347 - val_loss: 0.9709 - val_accuracy: 0.8513\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.5650 - accuracy: 0.8800 - val_loss: 0.8831 - val_accuracy: 0.8572\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.4183 - accuracy: 0.9052 - val_loss: 0.6366 - val_accuracy: 0.9135\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3288 - accuracy: 0.9227 - val_loss: 0.5511 - val_accuracy: 0.9190\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2740 - accuracy: 0.9377 - val_loss: 0.4742 - val_accuracy: 0.9333\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2355 - accuracy: 0.9440 - val_loss: 0.4067 - val_accuracy: 0.9417\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2169 - accuracy: 0.9462 - val_loss: 0.4071 - val_accuracy: 0.9358\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1913 - accuracy: 0.9550 - val_loss: 0.3848 - val_accuracy: 0.9435\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1825 - accuracy: 0.9565 - val_loss: 0.3798 - val_accuracy: 0.9340\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1656 - accuracy: 0.9584 - val_loss: 0.3292 - val_accuracy: 0.9529\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1576 - accuracy: 0.9611 - val_loss: 0.3605 - val_accuracy: 0.9344\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1503 - accuracy: 0.9654 - val_loss: 0.3731 - val_accuracy: 0.9468\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 0.1456 - accuracy: 0.9647 - val_loss: 0.3068 - val_accuracy: 0.9595\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1394 - accuracy: 0.9669 - val_loss: 0.3497 - val_accuracy: 0.9457\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1393 - accuracy: 0.9661 - val_loss: 0.3388 - val_accuracy: 0.9408\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1321 - accuracy: 0.9699 - val_loss: 0.3006 - val_accuracy: 0.9553\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1311 - accuracy: 0.9703 - val_loss: 0.3372 - val_accuracy: 0.9441\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1201 - accuracy: 0.9719 - val_loss: 0.3168 - val_accuracy: 0.9534\n",
      "Average Validation Accuracy: 0.9713901281356812\n",
      "Average Validation Loss: 0.18049903213977814\n",
      "Average Test Accuracy: 0.9718802869319916\n",
      "Final Test Accuracy for each fold: 0.9736124277114868\n",
      "Number of input features: 16\n",
      "Fold: 1\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1846/1846 [==============================] - 5s 2ms/step - loss: 3.2629 - accuracy: 0.3758 - val_loss: 1.9366 - val_accuracy: 0.6733\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 1.2719 - accuracy: 0.7607 - val_loss: 1.0980 - val_accuracy: 0.8136\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.7322 - accuracy: 0.8525 - val_loss: 0.7762 - val_accuracy: 0.8559\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.4928 - accuracy: 0.8893 - val_loss: 0.6148 - val_accuracy: 0.8790\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3697 - accuracy: 0.9082 - val_loss: 0.5535 - val_accuracy: 0.8741\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2996 - accuracy: 0.9228 - val_loss: 0.5077 - val_accuracy: 0.8939\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2586 - accuracy: 0.9321 - val_loss: 0.3928 - val_accuracy: 0.9320\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2289 - accuracy: 0.9381 - val_loss: 0.3915 - val_accuracy: 0.9263\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2128 - accuracy: 0.9422 - val_loss: 0.3683 - val_accuracy: 0.9316\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1911 - accuracy: 0.9510 - val_loss: 0.3205 - val_accuracy: 0.9538\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1842 - accuracy: 0.9511 - val_loss: 0.3241 - val_accuracy: 0.9452\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1758 - accuracy: 0.9526 - val_loss: 0.3593 - val_accuracy: 0.9384\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1616 - accuracy: 0.9587 - val_loss: 0.3326 - val_accuracy: 0.9384\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1558 - accuracy: 0.9591 - val_loss: 0.2941 - val_accuracy: 0.9545\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1548 - accuracy: 0.9595 - val_loss: 0.2812 - val_accuracy: 0.9569\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1400 - accuracy: 0.9674 - val_loss: 0.2876 - val_accuracy: 0.9527\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1462 - accuracy: 0.9627 - val_loss: 0.2718 - val_accuracy: 0.9648\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1360 - accuracy: 0.9651 - val_loss: 0.2657 - val_accuracy: 0.9655\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1332 - accuracy: 0.9675 - val_loss: 0.2921 - val_accuracy: 0.9461\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1373 - accuracy: 0.9651 - val_loss: 0.3270 - val_accuracy: 0.9501\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 3.4671 - accuracy: 0.3535 - val_loss: 2.3798 - val_accuracy: 0.5916\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 1.4628 - accuracy: 0.7346 - val_loss: 1.3345 - val_accuracy: 0.7978\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.7968 - accuracy: 0.8534 - val_loss: 0.9667 - val_accuracy: 0.8645\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.5280 - accuracy: 0.8924 - val_loss: 0.8060 - val_accuracy: 0.8869\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 0.3925 - accuracy: 0.9143 - val_loss: 0.6446 - val_accuracy: 0.8999\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.3084 - accuracy: 0.9265 - val_loss: 0.5395 - val_accuracy: 0.9267\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2639 - accuracy: 0.9342 - val_loss: 0.4911 - val_accuracy: 0.9283\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2251 - accuracy: 0.9447 - val_loss: 0.4784 - val_accuracy: 0.9245\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2063 - accuracy: 0.9501 - val_loss: 0.3985 - val_accuracy: 0.9344\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1846 - accuracy: 0.9558 - val_loss: 0.4268 - val_accuracy: 0.9366\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1738 - accuracy: 0.9581 - val_loss: 0.4538 - val_accuracy: 0.9360\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1629 - accuracy: 0.9624 - val_loss: 0.4134 - val_accuracy: 0.9344\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1672 - accuracy: 0.9622 - val_loss: 0.3511 - val_accuracy: 0.9637\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1466 - accuracy: 0.9651 - val_loss: 0.3813 - val_accuracy: 0.9474\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1490 - accuracy: 0.9660 - val_loss: 0.3574 - val_accuracy: 0.9527\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1380 - accuracy: 0.9690 - val_loss: 0.3412 - val_accuracy: 0.9512\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1347 - accuracy: 0.9691 - val_loss: 0.3188 - val_accuracy: 0.9575\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1302 - accuracy: 0.9720 - val_loss: 0.3264 - val_accuracy: 0.9534\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1254 - accuracy: 0.9737 - val_loss: 0.3094 - val_accuracy: 0.9569\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1275 - accuracy: 0.9708 - val_loss: 0.2849 - val_accuracy: 0.9721\n",
      "Average Validation Accuracy: 0.967070072889328\n",
      "Average Validation Loss: 0.18852975219488144\n",
      "Average Test Accuracy: 0.9698533415794373\n",
      "Final Test Accuracy for each fold: 0.9795091152191162\n",
      "Number of input features: 17\n",
      "Fold: 1\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1846/1846 [==============================] - 6s 2ms/step - loss: 3.4158 - accuracy: 0.3502 - val_loss: 2.0248 - val_accuracy: 0.6601\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 1.2628 - accuracy: 0.7639 - val_loss: 1.0359 - val_accuracy: 0.8205\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.6601 - accuracy: 0.8620 - val_loss: 0.7264 - val_accuracy: 0.8704\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.4370 - accuracy: 0.8992 - val_loss: 0.5418 - val_accuracy: 0.9041\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3293 - accuracy: 0.9200 - val_loss: 0.4768 - val_accuracy: 0.9122\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2727 - accuracy: 0.9285 - val_loss: 0.4388 - val_accuracy: 0.9212\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2341 - accuracy: 0.9398 - val_loss: 0.4002 - val_accuracy: 0.9234\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2057 - accuracy: 0.9445 - val_loss: 0.3778 - val_accuracy: 0.9296\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1933 - accuracy: 0.9501 - val_loss: 0.3815 - val_accuracy: 0.9307\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1847 - accuracy: 0.9515 - val_loss: 0.3347 - val_accuracy: 0.9426\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1641 - accuracy: 0.9586 - val_loss: 0.3159 - val_accuracy: 0.9551\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1611 - accuracy: 0.9573 - val_loss: 0.2999 - val_accuracy: 0.9549\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1490 - accuracy: 0.9611 - val_loss: 0.3025 - val_accuracy: 0.9536\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1449 - accuracy: 0.9621 - val_loss: 0.3224 - val_accuracy: 0.9419\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1374 - accuracy: 0.9653 - val_loss: 0.2907 - val_accuracy: 0.9558\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1382 - accuracy: 0.9662 - val_loss: 0.2633 - val_accuracy: 0.9650\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1236 - accuracy: 0.9707 - val_loss: 0.2717 - val_accuracy: 0.9606\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1331 - accuracy: 0.9664 - val_loss: 0.2803 - val_accuracy: 0.9635\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1266 - accuracy: 0.9701 - val_loss: 0.2759 - val_accuracy: 0.9593\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1189 - accuracy: 0.9718 - val_loss: 0.3403 - val_accuracy: 0.9531\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 7s 3ms/step - loss: 3.4478 - accuracy: 0.3411 - val_loss: 2.2385 - val_accuracy: 0.6013\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.4658 - accuracy: 0.7231 - val_loss: 1.3802 - val_accuracy: 0.7857\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.8280 - accuracy: 0.8397 - val_loss: 0.9787 - val_accuracy: 0.8618\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.5316 - accuracy: 0.8917 - val_loss: 0.7472 - val_accuracy: 0.9034\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.3805 - accuracy: 0.9198 - val_loss: 0.6456 - val_accuracy: 0.9098\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2993 - accuracy: 0.9333 - val_loss: 0.5316 - val_accuracy: 0.9263\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2468 - accuracy: 0.9446 - val_loss: 0.5023 - val_accuracy: 0.9190\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2219 - accuracy: 0.9459 - val_loss: 0.4325 - val_accuracy: 0.9386\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1989 - accuracy: 0.9522 - val_loss: 0.3794 - val_accuracy: 0.9507\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1847 - accuracy: 0.9535 - val_loss: 0.3594 - val_accuracy: 0.9492\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1672 - accuracy: 0.9579 - val_loss: 0.3337 - val_accuracy: 0.9481\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1613 - accuracy: 0.9610 - val_loss: 0.3713 - val_accuracy: 0.9465\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1514 - accuracy: 0.9626 - val_loss: 0.3514 - val_accuracy: 0.9417\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1501 - accuracy: 0.9629 - val_loss: 0.3484 - val_accuracy: 0.9408\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1373 - accuracy: 0.9663 - val_loss: 0.2951 - val_accuracy: 0.9580\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1347 - accuracy: 0.9682 - val_loss: 0.2852 - val_accuracy: 0.9628\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1361 - accuracy: 0.9684 - val_loss: 0.3515 - val_accuracy: 0.9459\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1293 - accuracy: 0.9675 - val_loss: 0.2853 - val_accuracy: 0.9633\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1207 - accuracy: 0.9721 - val_loss: 0.2680 - val_accuracy: 0.9604\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1243 - accuracy: 0.9706 - val_loss: 0.3160 - val_accuracy: 0.9586\n",
      "Average Validation Accuracy: 0.9639837443828583\n",
      "Average Validation Loss: 0.2104901671409607\n",
      "Average Test Accuracy: 0.9643989205360413\n",
      "Final Test Accuracy for each fold: 0.9690425395965576\n",
      "Number of input features: 18\n",
      "Fold: 1\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1846/1846 [==============================] - 6s 3ms/step - loss: 3.3482 - accuracy: 0.3861 - val_loss: 2.0832 - val_accuracy: 0.6189\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.3748 - accuracy: 0.7470 - val_loss: 1.1701 - val_accuracy: 0.8097\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.7664 - accuracy: 0.8500 - val_loss: 0.8185 - val_accuracy: 0.8543\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.4930 - accuracy: 0.8903 - val_loss: 0.6017 - val_accuracy: 0.9008\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.3661 - accuracy: 0.9134 - val_loss: 0.5635 - val_accuracy: 0.8911\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2936 - accuracy: 0.9286 - val_loss: 0.4497 - val_accuracy: 0.9063\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2449 - accuracy: 0.9389 - val_loss: 0.4619 - val_accuracy: 0.9162\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.2200 - accuracy: 0.9454 - val_loss: 0.3926 - val_accuracy: 0.9210\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.2108 - accuracy: 0.9446 - val_loss: 0.3624 - val_accuracy: 0.9320\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1894 - accuracy: 0.9511 - val_loss: 0.3918 - val_accuracy: 0.9241\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 8s 5ms/step - loss: 0.1708 - accuracy: 0.9566 - val_loss: 0.3453 - val_accuracy: 0.9287\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1688 - accuracy: 0.9562 - val_loss: 0.2788 - val_accuracy: 0.9564\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1587 - accuracy: 0.9604 - val_loss: 0.3331 - val_accuracy: 0.9439\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1468 - accuracy: 0.9624 - val_loss: 0.2670 - val_accuracy: 0.9657\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 8s 5ms/step - loss: 0.1460 - accuracy: 0.9640 - val_loss: 0.2717 - val_accuracy: 0.9571\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1356 - accuracy: 0.9655 - val_loss: 0.2622 - val_accuracy: 0.9591\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1325 - accuracy: 0.9655 - val_loss: 0.4096 - val_accuracy: 0.9212\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1333 - accuracy: 0.9686 - val_loss: 0.2655 - val_accuracy: 0.9608\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1254 - accuracy: 0.9688 - val_loss: 0.3019 - val_accuracy: 0.9479\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1255 - accuracy: 0.9703 - val_loss: 0.2429 - val_accuracy: 0.9692\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 13s 5ms/step - loss: 3.0486 - accuracy: 0.4411 - val_loss: 1.9054 - val_accuracy: 0.6594\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 1.2232 - accuracy: 0.7645 - val_loss: 1.1434 - val_accuracy: 0.7833\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.7013 - accuracy: 0.8536 - val_loss: 0.8465 - val_accuracy: 0.8587\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.4761 - accuracy: 0.8891 - val_loss: 0.6431 - val_accuracy: 0.8704\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 0.3633 - accuracy: 0.9108 - val_loss: 0.5281 - val_accuracy: 0.9047\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 0.3060 - accuracy: 0.9209 - val_loss: 0.4671 - val_accuracy: 0.9144\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2560 - accuracy: 0.9331 - val_loss: 0.3774 - val_accuracy: 0.9369\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2355 - accuracy: 0.9377 - val_loss: 0.3893 - val_accuracy: 0.9254\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2152 - accuracy: 0.9434 - val_loss: 0.3347 - val_accuracy: 0.9316\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1906 - accuracy: 0.9506 - val_loss: 0.2953 - val_accuracy: 0.9468\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1778 - accuracy: 0.9522 - val_loss: 0.2764 - val_accuracy: 0.9516\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1712 - accuracy: 0.9540 - val_loss: 0.2902 - val_accuracy: 0.9472\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1655 - accuracy: 0.9543 - val_loss: 0.3162 - val_accuracy: 0.9338\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1563 - accuracy: 0.9582 - val_loss: 0.2548 - val_accuracy: 0.9584\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1438 - accuracy: 0.9629 - val_loss: 0.2607 - val_accuracy: 0.9534\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1407 - accuracy: 0.9634 - val_loss: 0.3677 - val_accuracy: 0.9204\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1319 - accuracy: 0.9680 - val_loss: 0.2696 - val_accuracy: 0.9452\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1411 - accuracy: 0.9642 - val_loss: 0.2956 - val_accuracy: 0.9441\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1219 - accuracy: 0.9711 - val_loss: 0.2614 - val_accuracy: 0.9441\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1258 - accuracy: 0.9715 - val_loss: 0.2732 - val_accuracy: 0.9525\n",
      "Average Validation Accuracy: 0.9671057462692261\n",
      "Average Validation Loss: 0.1693868190050125\n",
      "Average Test Accuracy: 0.9689688086509705\n",
      "Final Test Accuracy for each fold: 0.9750128984451294\n",
      "Number of input features: 19\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 3.3713 - accuracy: 0.3656 - val_loss: 2.1615 - val_accuracy: 0.6026\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 1.4091 - accuracy: 0.7356 - val_loss: 1.1659 - val_accuracy: 0.8081\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.7600 - accuracy: 0.8477 - val_loss: 0.7832 - val_accuracy: 0.8528\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.4776 - accuracy: 0.8901 - val_loss: 0.6299 - val_accuracy: 0.8909\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3408 - accuracy: 0.9202 - val_loss: 0.4882 - val_accuracy: 0.9129\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2680 - accuracy: 0.9357 - val_loss: 0.4662 - val_accuracy: 0.9173\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2354 - accuracy: 0.9424 - val_loss: 0.4077 - val_accuracy: 0.9369\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2115 - accuracy: 0.9475 - val_loss: 0.3800 - val_accuracy: 0.9428\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1867 - accuracy: 0.9533 - val_loss: 0.3395 - val_accuracy: 0.9490\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1700 - accuracy: 0.9570 - val_loss: 0.3533 - val_accuracy: 0.9481\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1641 - accuracy: 0.9598 - val_loss: 0.3277 - val_accuracy: 0.9476\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1544 - accuracy: 0.9614 - val_loss: 0.3059 - val_accuracy: 0.9529\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1454 - accuracy: 0.9636 - val_loss: 0.3409 - val_accuracy: 0.9509\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1376 - accuracy: 0.9678 - val_loss: 0.3071 - val_accuracy: 0.9542\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 0.1389 - accuracy: 0.9667 - val_loss: 0.2816 - val_accuracy: 0.9608\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1301 - accuracy: 0.9695 - val_loss: 0.2840 - val_accuracy: 0.9633\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1226 - accuracy: 0.9713 - val_loss: 0.2699 - val_accuracy: 0.9672\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1207 - accuracy: 0.9711 - val_loss: 0.2712 - val_accuracy: 0.9633\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1197 - accuracy: 0.9715 - val_loss: 0.3399 - val_accuracy: 0.9487\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1198 - accuracy: 0.9714 - val_loss: 0.2665 - val_accuracy: 0.9694\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 3.5386 - accuracy: 0.3431 - val_loss: 2.2696 - val_accuracy: 0.6251\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 1.4319 - accuracy: 0.7402 - val_loss: 1.3245 - val_accuracy: 0.8117\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.7862 - accuracy: 0.8522 - val_loss: 0.9544 - val_accuracy: 0.8554\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.5141 - accuracy: 0.8903 - val_loss: 0.6968 - val_accuracy: 0.8900\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3864 - accuracy: 0.9135 - val_loss: 0.5876 - val_accuracy: 0.9006\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3088 - accuracy: 0.9266 - val_loss: 0.4990 - val_accuracy: 0.9212\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2540 - accuracy: 0.9363 - val_loss: 0.5717 - val_accuracy: 0.8788\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2303 - accuracy: 0.9416 - val_loss: 0.3836 - val_accuracy: 0.9349\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 0.2031 - accuracy: 0.9498 - val_loss: 0.4142 - val_accuracy: 0.9267\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 0.1867 - accuracy: 0.9520 - val_loss: 0.3532 - val_accuracy: 0.9373\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1695 - accuracy: 0.9578 - val_loss: 0.3858 - val_accuracy: 0.9316\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1690 - accuracy: 0.9572 - val_loss: 0.3887 - val_accuracy: 0.9314\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 0.1630 - accuracy: 0.9591 - val_loss: 0.3175 - val_accuracy: 0.9437\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 0.1447 - accuracy: 0.9660 - val_loss: 0.3006 - val_accuracy: 0.9562\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 0.1523 - accuracy: 0.9625 - val_loss: 0.2769 - val_accuracy: 0.9648\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1366 - accuracy: 0.9662 - val_loss: 0.2946 - val_accuracy: 0.9547\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1354 - accuracy: 0.9676 - val_loss: 0.2921 - val_accuracy: 0.9593\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1316 - accuracy: 0.9694 - val_loss: 0.2785 - val_accuracy: 0.9633\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1235 - accuracy: 0.9725 - val_loss: 0.2896 - val_accuracy: 0.9597\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1333 - accuracy: 0.9677 - val_loss: 0.2630 - val_accuracy: 0.9595\n",
      "Average Validation Accuracy: 0.9724792540073395\n",
      "Average Validation Loss: 0.1617509350180626\n",
      "Average Test Accuracy: 0.9732438921928406\n",
      "Final Test Accuracy for each fold: 0.976708173751831\n",
      "Number of input features: 20\n",
      "Fold: 1\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1846/1846 [==============================] - 6s 3ms/step - loss: 3.2045 - accuracy: 0.3993 - val_loss: 1.8952 - val_accuracy: 0.6530\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 1.1824 - accuracy: 0.7790 - val_loss: 1.0888 - val_accuracy: 0.8246\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.6260 - accuracy: 0.8698 - val_loss: 0.7385 - val_accuracy: 0.8748\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.4182 - accuracy: 0.9024 - val_loss: 0.5495 - val_accuracy: 0.9017\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3249 - accuracy: 0.9210 - val_loss: 0.5100 - val_accuracy: 0.9127\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2662 - accuracy: 0.9330 - val_loss: 0.4158 - val_accuracy: 0.9296\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2441 - accuracy: 0.9362 - val_loss: 0.4025 - val_accuracy: 0.9300\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2147 - accuracy: 0.9428 - val_loss: 0.4056 - val_accuracy: 0.9300\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2024 - accuracy: 0.9471 - val_loss: 0.3578 - val_accuracy: 0.9393\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1814 - accuracy: 0.9540 - val_loss: 0.3430 - val_accuracy: 0.9457\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1803 - accuracy: 0.9534 - val_loss: 0.3592 - val_accuracy: 0.9366\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1649 - accuracy: 0.9574 - val_loss: 0.3297 - val_accuracy: 0.9523\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1630 - accuracy: 0.9600 - val_loss: 0.3054 - val_accuracy: 0.9635\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1455 - accuracy: 0.9649 - val_loss: 0.2970 - val_accuracy: 0.9602\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1465 - accuracy: 0.9650 - val_loss: 0.3420 - val_accuracy: 0.9512\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1401 - accuracy: 0.9654 - val_loss: 0.3220 - val_accuracy: 0.9531\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1418 - accuracy: 0.9661 - val_loss: 0.2658 - val_accuracy: 0.9677\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1333 - accuracy: 0.9680 - val_loss: 0.2782 - val_accuracy: 0.9657\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1237 - accuracy: 0.9702 - val_loss: 0.4074 - val_accuracy: 0.9285\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1294 - accuracy: 0.9687 - val_loss: 0.3106 - val_accuracy: 0.9562\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 7s 3ms/step - loss: 3.5771 - accuracy: 0.3170 - val_loss: 2.3379 - val_accuracy: 0.6150\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.4866 - accuracy: 0.7271 - val_loss: 1.3869 - val_accuracy: 0.7769\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.8061 - accuracy: 0.8448 - val_loss: 0.9252 - val_accuracy: 0.8653\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.4977 - accuracy: 0.8947 - val_loss: 0.7019 - val_accuracy: 0.9050\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.3468 - accuracy: 0.9241 - val_loss: 0.5991 - val_accuracy: 0.9157\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2749 - accuracy: 0.9356 - val_loss: 0.4750 - val_accuracy: 0.9377\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2269 - accuracy: 0.9478 - val_loss: 0.4286 - val_accuracy: 0.9366\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2000 - accuracy: 0.9509 - val_loss: 0.3810 - val_accuracy: 0.9479\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1891 - accuracy: 0.9523 - val_loss: 0.3714 - val_accuracy: 0.9494\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1715 - accuracy: 0.9591 - val_loss: 0.3611 - val_accuracy: 0.9492\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1612 - accuracy: 0.9607 - val_loss: 0.3264 - val_accuracy: 0.9512\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1567 - accuracy: 0.9601 - val_loss: 0.3697 - val_accuracy: 0.9377\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1465 - accuracy: 0.9629 - val_loss: 0.3272 - val_accuracy: 0.9514\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1393 - accuracy: 0.9678 - val_loss: 0.3153 - val_accuracy: 0.9428\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1374 - accuracy: 0.9660 - val_loss: 0.2829 - val_accuracy: 0.9613\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1324 - accuracy: 0.9688 - val_loss: 0.2781 - val_accuracy: 0.9637\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1318 - accuracy: 0.9688 - val_loss: 0.2981 - val_accuracy: 0.9564\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1324 - accuracy: 0.9685 - val_loss: 0.2630 - val_accuracy: 0.9659\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1190 - accuracy: 0.9730 - val_loss: 0.2762 - val_accuracy: 0.9615\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1243 - accuracy: 0.9703 - val_loss: 0.2875 - val_accuracy: 0.9624\n",
      "Average Validation Accuracy: 0.9655449092388153\n",
      "Average Validation Loss: 0.18963957577943802\n",
      "Average Test Accuracy: 0.96731036901474\n",
      "Final Test Accuracy for each fold: 0.9693373441696167\n"
     ]
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_fs, Y, test_size=0.33, random_state=1)\n",
    "\n",
    "# prepare output data\n",
    "y_train_enc, y_test_enc = prepare_targets(y_train, y_test)\n",
    "\n",
    "# Define the number of folds for k-fold cross-validation\n",
    "k = 2\n",
    "\n",
    "# Define the cross-validation method\n",
    "cv_method = StratifiedKFold(n_splits=k)\n",
    "\n",
    "# Initialize the list to store the history, train & validation(accuracy & loss) for each model\n",
    "models = []\n",
    "model_history = []\n",
    "model_accuracy = []\n",
    "model_train_acc = []\n",
    "model_train_loss = []\n",
    "model_val_acc = []\n",
    "model_val_loss = []\n",
    "\n",
    "\n",
    "for i in range(1,21):\n",
    "\n",
    "    models_fold = []\n",
    "    hist = []\n",
    "    train_accuracy = []\n",
    "    train_loss = []\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "    test_accuracy = []\n",
    "\n",
    "    print(\"Number of input features:\",i)\n",
    "\n",
    "    # Select the input features from the input data\n",
    "    X_train_selected = X_train[:, :i]\n",
    "    X_test_selected = X_test[:, :i]\n",
    "\n",
    "    # Loop over the folds\n",
    "    for fold, (train_index, val_index) in enumerate(cv_method.split(X_train_selected, y_train)):\n",
    "\n",
    "        print(\"Fold:\", fold+1)\n",
    "\n",
    "        # Split the data into train and validation sets using the current fold index\n",
    "        X_train_fold  = X_train_selected[train_index]\n",
    "        y_train_fold  = y_train[train_index]\n",
    "        X_val_fold = X_train_selected[val_index]\n",
    "        y_val_fold = y_train[val_index]\n",
    "\n",
    "        # Prepare the target data\n",
    "        y_train_fold_enc, y_val_fold_enc = prepare_targets(y_train_fold, y_val_fold)\n",
    "\n",
    "        # build the model\n",
    "        model = MLP_model(i)\n",
    "\n",
    "        # Fit the model to the training data for the current fold\n",
    "        history = model.fit(X_train_fold, to_categorical(y_train_fold_enc, num_classes=373), epochs=20, batch_size=5, verbose=1, validation_split = 0.33)\n",
    "    \n",
    "        # Evaluate the model on the validation data for the current fold\n",
    "        val_scores = model.evaluate(X_val_fold, to_categorical(y_val_fold_enc, num_classes=373), verbose=0)\n",
    "        val_accuracy.append(val_scores[1])\n",
    "        val_loss.append(val_scores[0])\n",
    "\n",
    "        # Evaluate the model on the test data for the current fold\n",
    "        test_scores = model.evaluate(X_test_selected, to_categorical(y_test_enc, num_classes=373), verbose=0)\n",
    "        test_accuracy.append(test_scores[1])\n",
    "\n",
    "        # add the model to the list of models\n",
    "        models_fold.append(model)\n",
    "        hist.append(history)\n",
    "\n",
    "        # store the training accuracy and loss for each fold\n",
    "        train_accuracy.append(history.history['accuracy'])\n",
    "        train_loss.append(history.history['loss'])\n",
    "        \n",
    "    # Calculate the average test and validation accuracy and loss across all folds\n",
    "    avg_test_acc = sum(test_accuracy) / len(test_accuracy)\n",
    "    avg_val_acc = sum(val_accuracy) / len(val_accuracy)\n",
    "    avg_val_loss = sum(val_loss) / len(val_loss)\n",
    "\n",
    "    # Print the average validation and test accuracy and loss\n",
    "    print(\"Average Validation Accuracy:\", avg_val_acc)\n",
    "    print(\"Average Validation Loss:\",avg_val_loss)\n",
    "    print(\"Average Test Accuracy:\", avg_test_acc)\n",
    "\n",
    "    best_fold_index = test_accuracy.index(max(test_accuracy))\n",
    "    model_accuracy.append(test_accuracy[best_fold_index])\n",
    "    models.append(models_fold[best_fold_index])\n",
    "    model_history.append(hist[best_fold_index])\n",
    "    model_train_acc.append(train_accuracy[best_fold_index])\n",
    "    model_train_loss.append(train_loss[best_fold_index])\n",
    "    model_val_acc.append(val_accuracy[best_fold_index])\n",
    "    model_val_loss.append(val_loss[best_fold_index])\n",
    "\n",
    "\n",
    "    print(\"Final Test Accuracy for each fold:\", test_accuracy[best_fold_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to show the no of input features and its corresponding model accuracy\n",
    "model_list = []\n",
    "\n",
    "#Iterate through each model's accuracy \n",
    "for i in range (len(model_accuracy)):\n",
    "    #get the number of input features for the current model\n",
    "    no_features = i + 1\n",
    "\n",
    "    #round the model accuries to 3 d.p.\n",
    "    rounded_model_acc = round(model_accuracy[i], 3)\n",
    "    \n",
    "    model_list.append([no_features, rounded_model_acc])\n",
    "\n",
    "models_df = pd.DataFrame(model_list, columns=[\"No of input features\", \"Model accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 0.9795091152191162\n"
     ]
    }
   ],
   "source": [
    "# Select the final model based on the max test accuracy across all models\n",
    "\n",
    "best_model_index = model_accuracy.index(max(model_accuracy))\n",
    "\n",
    "best_model = models[best_model_index]\n",
    "best_model_history = model_history[best_model_index]\n",
    "best_model_train_acc = model_train_acc[best_model_index]\n",
    "best_model_train_loss = model_train_loss[best_model_index]\n",
    "best_model_val_acc = model_val_acc[best_model_index]\n",
    "best_model_val_loss = model_val_loss[best_model_index]\n",
    "\n",
    "# summarize history for accuracy  \n",
    "plt.subplot(211)  \n",
    "plt.plot(best_model_history.history['accuracy'])  \n",
    "plt.plot(best_model_history.history['val_accuracy'])  \n",
    "plt.title('model accuracy')  \n",
    "plt.ylabel('accuracy')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'test'], loc='lower right')  \n",
    "\n",
    "# summarize history for loss\n",
    "plt.subplot(212)  \n",
    "plt.plot(best_model_history.history['loss'])  \n",
    "plt.plot(best_model_history.history['val_loss'])  \n",
    "plt.title('model loss')  \n",
    "plt.ylabel('loss')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'test'], loc='upper right')  \n",
    "plt.subplots_adjust(hspace=0.7)\n",
    "\n",
    "plt.show() \n",
    "\n",
    "print(\"Final Test Accuracy:\", model_accuracy[best_model_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424/424 [==============================] - 1s 1ms/step\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       591\n",
      "           1       1.00      1.00      1.00       430\n",
      "           2       1.00      1.00      1.00       419\n",
      "           3       1.00      1.00      1.00       384\n",
      "           4       1.00      1.00      1.00       339\n",
      "           5       1.00      1.00      1.00       342\n",
      "           6       1.00      1.00      1.00       310\n",
      "           7       1.00      1.00      1.00       325\n",
      "           8       0.97      1.00      0.98       294\n",
      "           9       1.00      1.00      1.00       269\n",
      "          10       1.00      1.00      1.00       296\n",
      "          11       1.00      1.00      1.00       258\n",
      "          12       1.00      1.00      1.00       247\n",
      "          13       1.00      1.00      1.00       237\n",
      "          14       1.00      1.00      1.00       239\n",
      "          15       1.00      1.00      1.00       235\n",
      "          16       1.00      1.00      1.00       213\n",
      "          17       1.00      1.00      1.00       202\n",
      "          18       1.00      1.00      1.00       196\n",
      "          19       1.00      1.00      1.00       181\n",
      "          20       1.00      1.00      1.00       177\n",
      "          21       1.00      1.00      1.00       177\n",
      "          22       1.00      1.00      1.00       155\n",
      "          23       1.00      1.00      1.00       155\n",
      "          24       1.00      1.00      1.00       144\n",
      "          25       1.00      1.00      1.00       126\n",
      "          26       1.00      1.00      1.00       108\n",
      "          27       1.00      1.00      1.00       121\n",
      "          28       1.00      1.00      1.00        95\n",
      "          29       1.00      1.00      1.00       106\n",
      "          30       1.00      1.00      1.00       102\n",
      "          31       1.00      1.00      1.00        86\n",
      "          32       1.00      1.00      1.00       108\n",
      "          33       1.00      1.00      1.00        88\n",
      "          34       1.00      1.00      1.00       102\n",
      "          35       1.00      1.00      1.00        88\n",
      "          36       1.00      1.00      1.00        83\n",
      "          37       1.00      1.00      1.00        93\n",
      "          38       1.00      1.00      1.00        76\n",
      "          39       1.00      1.00      1.00        85\n",
      "          40       1.00      1.00      1.00        86\n",
      "          41       1.00      1.00      1.00        85\n",
      "          42       1.00      1.00      1.00        68\n",
      "          43       1.00      1.00      1.00        75\n",
      "          44       0.97      1.00      0.99        71\n",
      "          45       0.50      0.91      0.65        58\n",
      "          46       1.00      1.00      1.00        71\n",
      "          47       0.66      0.37      0.47        57\n",
      "          48       1.00      1.00      1.00        67\n",
      "          49       0.64      1.00      0.78        47\n",
      "          50       1.00      1.00      1.00        48\n",
      "          51       1.00      1.00      1.00        47\n",
      "          52       1.00      1.00      1.00        43\n",
      "          53       1.00      1.00      1.00        51\n",
      "          54       1.00      1.00      1.00        44\n",
      "          55       1.00      1.00      1.00        51\n",
      "          56       1.00      1.00      1.00        45\n",
      "          57       1.00      1.00      1.00        44\n",
      "          58       0.91      1.00      0.95        41\n",
      "          59       1.00      1.00      1.00        41\n",
      "          60       1.00      1.00      1.00        52\n",
      "          61       1.00      1.00      1.00        43\n",
      "          62       1.00      1.00      1.00        37\n",
      "          63       1.00      1.00      1.00        43\n",
      "          64       1.00      1.00      1.00        42\n",
      "          65       1.00      1.00      1.00        46\n",
      "          66       1.00      1.00      1.00        43\n",
      "          67       1.00      1.00      1.00        40\n",
      "          68       1.00      1.00      1.00        44\n",
      "          69       1.00      1.00      1.00        43\n",
      "          70       1.00      1.00      1.00        38\n",
      "          71       1.00      1.00      1.00        33\n",
      "          72       1.00      1.00      1.00        45\n",
      "          73       1.00      1.00      1.00        38\n",
      "          74       1.00      1.00      1.00        42\n",
      "          75       1.00      1.00      1.00        39\n",
      "          76       0.44      0.13      0.21        30\n",
      "          77       1.00      1.00      1.00        28\n",
      "          78       0.96      0.96      0.96        28\n",
      "          79       0.78      1.00      0.88        32\n",
      "          80       1.00      1.00      1.00        33\n",
      "          81       1.00      1.00      1.00        31\n",
      "          82       1.00      1.00      1.00        35\n",
      "          83       1.00      1.00      1.00        39\n",
      "          84       1.00      1.00      1.00        27\n",
      "          85       1.00      1.00      1.00        36\n",
      "          86       1.00      1.00      1.00        31\n",
      "          87       1.00      1.00      1.00        28\n",
      "          88       1.00      1.00      1.00        20\n",
      "          89       1.00      1.00      1.00        33\n",
      "          90       1.00      1.00      1.00        24\n",
      "          91       1.00      1.00      1.00        22\n",
      "          92       1.00      1.00      1.00        26\n",
      "          93       1.00      1.00      1.00        35\n",
      "          94       0.82      1.00      0.90        27\n",
      "          95       1.00      1.00      1.00        23\n",
      "          96       1.00      1.00      1.00        27\n",
      "          97       1.00      1.00      1.00        28\n",
      "          98       0.58      0.44      0.50        16\n",
      "          99       1.00      1.00      1.00        35\n",
      "         100       1.00      1.00      1.00        28\n",
      "         101       1.00      1.00      1.00        25\n",
      "         102       1.00      1.00      1.00        26\n",
      "         103       1.00      1.00      1.00        33\n",
      "         104       1.00      1.00      1.00        26\n",
      "         105       1.00      1.00      1.00        24\n",
      "         106       0.61      1.00      0.76        22\n",
      "         107       1.00      1.00      1.00        26\n",
      "         108       1.00      1.00      1.00        25\n",
      "         109       1.00      1.00      1.00        16\n",
      "         110       1.00      1.00      1.00        20\n",
      "         111       1.00      1.00      1.00        26\n",
      "         112       0.82      1.00      0.90        18\n",
      "         113       0.57      1.00      0.73        23\n",
      "         114       1.00      1.00      1.00        25\n",
      "         115       1.00      1.00      1.00        18\n",
      "         116       1.00      0.79      0.88        19\n",
      "         117       1.00      1.00      1.00        16\n",
      "         118       1.00      1.00      1.00        26\n",
      "         119       1.00      1.00      1.00        22\n",
      "         120       0.94      1.00      0.97        17\n",
      "         121       1.00      1.00      1.00        15\n",
      "         122       1.00      1.00      1.00        18\n",
      "         123       1.00      1.00      1.00        20\n",
      "         124       1.00      1.00      1.00        14\n",
      "         125       1.00      1.00      1.00        22\n",
      "         126       1.00      1.00      1.00        19\n",
      "         127       1.00      1.00      1.00        27\n",
      "         128       1.00      1.00      1.00        26\n",
      "         129       0.84      1.00      0.91        21\n",
      "         130       1.00      1.00      1.00        18\n",
      "         131       1.00      1.00      1.00        18\n",
      "         132       1.00      1.00      1.00        20\n",
      "         133       1.00      1.00      1.00        14\n",
      "         134       1.00      1.00      1.00        19\n",
      "         135       1.00      1.00      1.00        16\n",
      "         136       1.00      1.00      1.00        23\n",
      "         137       0.44      1.00      0.61        11\n",
      "         138       1.00      1.00      1.00        14\n",
      "         139       1.00      1.00      1.00        20\n",
      "         140       1.00      1.00      1.00        23\n",
      "         141       0.00      0.00      0.00        14\n",
      "         142       1.00      0.46      0.63        13\n",
      "         143       1.00      1.00      1.00        23\n",
      "         144       1.00      0.94      0.97        17\n",
      "         145       1.00      1.00      1.00        24\n",
      "         146       1.00      0.94      0.97        16\n",
      "         147       1.00      1.00      1.00        19\n",
      "         148       1.00      1.00      1.00        22\n",
      "         149       1.00      1.00      1.00        15\n",
      "         150       1.00      1.00      1.00        11\n",
      "         151       1.00      1.00      1.00        19\n",
      "         152       1.00      1.00      1.00        20\n",
      "         153       1.00      1.00      1.00        24\n",
      "         154       0.85      1.00      0.92        11\n",
      "         155       1.00      1.00      1.00        17\n",
      "         156       1.00      1.00      1.00        18\n",
      "         157       1.00      1.00      1.00        12\n",
      "         158       1.00      1.00      1.00        18\n",
      "         159       1.00      1.00      1.00        20\n",
      "         160       1.00      1.00      1.00        20\n",
      "         161       1.00      1.00      1.00        16\n",
      "         162       1.00      1.00      1.00        15\n",
      "         163       1.00      1.00      1.00        15\n",
      "         164       1.00      1.00      1.00        13\n",
      "         165       1.00      0.68      0.81        19\n",
      "         166       1.00      0.91      0.95        11\n",
      "         167       1.00      1.00      1.00         9\n",
      "         168       1.00      1.00      1.00        11\n",
      "         169       1.00      1.00      1.00        21\n",
      "         170       1.00      1.00      1.00        15\n",
      "         171       1.00      1.00      1.00        18\n",
      "         172       1.00      1.00      1.00        11\n",
      "         173       1.00      1.00      1.00        16\n",
      "         174       1.00      1.00      1.00        10\n",
      "         175       1.00      1.00      1.00        11\n",
      "         176       1.00      1.00      1.00        10\n",
      "         177       1.00      1.00      1.00        15\n",
      "         178       1.00      1.00      1.00        11\n",
      "         179       1.00      1.00      1.00        15\n",
      "         180       1.00      1.00      1.00        13\n",
      "         181       1.00      1.00      1.00        15\n",
      "         182       1.00      1.00      1.00         9\n",
      "         183       1.00      1.00      1.00        16\n",
      "         184       1.00      1.00      1.00         8\n",
      "         185       0.39      0.92      0.55        12\n",
      "         186       1.00      1.00      1.00        15\n",
      "         187       0.68      1.00      0.81        15\n",
      "         188       0.75      0.23      0.35        13\n",
      "         189       1.00      1.00      1.00        14\n",
      "         190       1.00      1.00      1.00        11\n",
      "         191       0.83      1.00      0.91        10\n",
      "         192       1.00      1.00      1.00        17\n",
      "         193       0.00      0.00      0.00         9\n",
      "         194       1.00      1.00      1.00         9\n",
      "         195       1.00      1.00      1.00         4\n",
      "         196       1.00      1.00      1.00         7\n",
      "         197       0.00      0.00      0.00         7\n",
      "         198       1.00      1.00      1.00        12\n",
      "         199       1.00      1.00      1.00        14\n",
      "         200       1.00      1.00      1.00         6\n",
      "         201       1.00      1.00      1.00         9\n",
      "         202       1.00      1.00      1.00         7\n",
      "         203       0.00      0.00      0.00         6\n",
      "         204       1.00      1.00      1.00        11\n",
      "         205       1.00      1.00      1.00        14\n",
      "         206       1.00      1.00      1.00        12\n",
      "         207       1.00      1.00      1.00        14\n",
      "         208       1.00      1.00      1.00        12\n",
      "         209       0.54      1.00      0.70         7\n",
      "         210       1.00      1.00      1.00        19\n",
      "         211       1.00      1.00      1.00         7\n",
      "         212       1.00      1.00      1.00        11\n",
      "         213       1.00      1.00      1.00         9\n",
      "         214       1.00      1.00      1.00         7\n",
      "         215       1.00      1.00      1.00         6\n",
      "         216       1.00      1.00      1.00        12\n",
      "         217       1.00      1.00      1.00        12\n",
      "         218       0.00      0.00      0.00         9\n",
      "         219       1.00      1.00      1.00         6\n",
      "         220       1.00      1.00      1.00         8\n",
      "         221       1.00      1.00      1.00         5\n",
      "         222       0.31      1.00      0.47         4\n",
      "         223       0.00      0.00      0.00        14\n",
      "         224       1.00      1.00      1.00        13\n",
      "         225       1.00      1.00      1.00         4\n",
      "         226       1.00      1.00      1.00        10\n",
      "         227       1.00      1.00      1.00        12\n",
      "         228       1.00      1.00      1.00        13\n",
      "         229       0.92      1.00      0.96        11\n",
      "         230       1.00      1.00      1.00         5\n",
      "         231       1.00      1.00      1.00         6\n",
      "         232       0.00      0.00      0.00         8\n",
      "         233       1.00      1.00      1.00        10\n",
      "         234       1.00      1.00      1.00         4\n",
      "         235       1.00      1.00      1.00         8\n",
      "         236       1.00      1.00      1.00         9\n",
      "         237       1.00      1.00      1.00         8\n",
      "         238       1.00      1.00      1.00        10\n",
      "         239       0.00      0.00      0.00         9\n",
      "         240       1.00      1.00      1.00         7\n",
      "         241       1.00      1.00      1.00         8\n",
      "         242       1.00      1.00      1.00         6\n",
      "         243       0.44      1.00      0.61         7\n",
      "         244       1.00      1.00      1.00         9\n",
      "         245       1.00      1.00      1.00         7\n",
      "         246       1.00      1.00      1.00         9\n",
      "         247       1.00      1.00      1.00         7\n",
      "         248       1.00      0.30      0.46        10\n",
      "         249       1.00      1.00      1.00        10\n",
      "         250       1.00      1.00      1.00         7\n",
      "         251       1.00      1.00      1.00         6\n",
      "         252       1.00      1.00      1.00        11\n",
      "         253       1.00      1.00      1.00         7\n",
      "         254       1.00      1.00      1.00         8\n",
      "         255       1.00      1.00      1.00         5\n",
      "         256       1.00      1.00      1.00         7\n",
      "         257       1.00      1.00      1.00         9\n",
      "         258       1.00      1.00      1.00         6\n",
      "         259       0.00      0.00      0.00         3\n",
      "         260       1.00      1.00      1.00         4\n",
      "         261       1.00      1.00      1.00         2\n",
      "         262       1.00      1.00      1.00         5\n",
      "         263       1.00      1.00      1.00        12\n",
      "         264       1.00      1.00      1.00         5\n",
      "         265       1.00      1.00      1.00         7\n",
      "         266       1.00      1.00      1.00        10\n",
      "         267       1.00      1.00      1.00         8\n",
      "         268       0.00      0.00      0.00         9\n",
      "         269       1.00      1.00      1.00         6\n",
      "         270       1.00      1.00      1.00         4\n",
      "         271       1.00      1.00      1.00         7\n",
      "         272       1.00      0.70      0.82        10\n",
      "         273       1.00      1.00      1.00         3\n",
      "         274       1.00      1.00      1.00         9\n",
      "         275       1.00      1.00      1.00         6\n",
      "         276       1.00      1.00      1.00         5\n",
      "         277       0.17      1.00      0.29         4\n",
      "         278       0.75      1.00      0.86         3\n",
      "         279       1.00      1.00      1.00         4\n",
      "         280       0.00      0.00      0.00         5\n",
      "         281       1.00      1.00      1.00         6\n",
      "         282       1.00      1.00      1.00        11\n",
      "         283       1.00      1.00      1.00         6\n",
      "         284       1.00      1.00      1.00         2\n",
      "         285       1.00      1.00      1.00         4\n",
      "         286       1.00      1.00      1.00         7\n",
      "         287       1.00      1.00      1.00         9\n",
      "         288       1.00      1.00      1.00         7\n",
      "         289       1.00      1.00      1.00         7\n",
      "         290       1.00      1.00      1.00         6\n",
      "         291       1.00      1.00      1.00         5\n",
      "         292       0.67      1.00      0.80         6\n",
      "         293       1.00      1.00      1.00         8\n",
      "         294       1.00      1.00      1.00         7\n",
      "         295       0.00      0.00      0.00         3\n",
      "         296       0.55      1.00      0.71         6\n",
      "         297       1.00      1.00      1.00         8\n",
      "         298       1.00      1.00      1.00         4\n",
      "         299       1.00      1.00      1.00         6\n",
      "         300       1.00      1.00      1.00         5\n",
      "         301       0.00      0.00      0.00         5\n",
      "         302       1.00      1.00      1.00         5\n",
      "         303       1.00      1.00      1.00         2\n",
      "         304       1.00      1.00      1.00         8\n",
      "         305       1.00      1.00      1.00         3\n",
      "         306       1.00      1.00      1.00         6\n",
      "         307       1.00      1.00      1.00         7\n",
      "         308       1.00      1.00      1.00         4\n",
      "         309       1.00      1.00      1.00         4\n",
      "         310       1.00      0.33      0.50         9\n",
      "         311       1.00      1.00      1.00         7\n",
      "         312       1.00      1.00      1.00         6\n",
      "         313       1.00      1.00      1.00         6\n",
      "         314       1.00      1.00      1.00         8\n",
      "         315       1.00      1.00      1.00         7\n",
      "         316       1.00      1.00      1.00         3\n",
      "         317       1.00      1.00      1.00         6\n",
      "         318       1.00      1.00      1.00        10\n",
      "         319       1.00      1.00      1.00         6\n",
      "         320       1.00      1.00      1.00         2\n",
      "         321       0.00      0.00      0.00         8\n",
      "         322       1.00      1.00      1.00         4\n",
      "         323       1.00      1.00      1.00         4\n",
      "         324       0.00      0.00      0.00         8\n",
      "         325       1.00      1.00      1.00         4\n",
      "         326       1.00      1.00      1.00         7\n",
      "         327       1.00      1.00      1.00         4\n",
      "         328       1.00      1.00      1.00         6\n",
      "         329       1.00      1.00      1.00         4\n",
      "         330       0.00      0.00      0.00         3\n",
      "         331       1.00      0.62      0.77         8\n",
      "         332       1.00      1.00      1.00         1\n",
      "         333       1.00      1.00      1.00         3\n",
      "         334       1.00      1.00      1.00         4\n",
      "         335       1.00      0.33      0.50         3\n",
      "         336       0.00      0.00      0.00         6\n",
      "         337       1.00      1.00      1.00         2\n",
      "         338       1.00      1.00      1.00         7\n",
      "         339       1.00      1.00      1.00         4\n",
      "         340       1.00      1.00      1.00         6\n",
      "         341       1.00      1.00      1.00         7\n",
      "         342       1.00      1.00      1.00         2\n",
      "         343       1.00      1.00      1.00         5\n",
      "         344       1.00      1.00      1.00         4\n",
      "         345       0.00      0.00      0.00         1\n",
      "         346       1.00      0.50      0.67         2\n",
      "         347       0.00      0.00      0.00         4\n",
      "         348       1.00      1.00      1.00         7\n",
      "         349       1.00      1.00      1.00         4\n",
      "         350       0.50      0.17      0.25         6\n",
      "         351       1.00      1.00      1.00         4\n",
      "         352       1.00      1.00      1.00         5\n",
      "         353       1.00      1.00      1.00         4\n",
      "         354       1.00      1.00      1.00         3\n",
      "         355       1.00      1.00      1.00         1\n",
      "         356       1.00      1.00      1.00         4\n",
      "         357       1.00      1.00      1.00         1\n",
      "         358       1.00      1.00      1.00         3\n",
      "         359       1.00      1.00      1.00         4\n",
      "         360       1.00      1.00      1.00         3\n",
      "         361       0.75      1.00      0.86         3\n",
      "         362       1.00      1.00      1.00         3\n",
      "         363       0.50      1.00      0.67         2\n",
      "         364       0.00      0.00      0.00         3\n",
      "         365       0.00      0.00      0.00         3\n",
      "         366       0.00      0.00      0.00         2\n",
      "         367       1.00      1.00      1.00         3\n",
      "         368       1.00      1.00      1.00         1\n",
      "         369       1.00      1.00      1.00         2\n",
      "         370       1.00      1.00      1.00         2\n",
      "         372       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.98     13567\n",
      "   macro avg       0.91      0.92      0.91     13567\n",
      "weighted avg       0.98      0.98      0.98     13567\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Select only the optimal number of input features for X_test\n",
    "X_test = X_test[:,:(best_model_index+1)]\n",
    "\n",
    "# Evaluate the best model on the test data\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# np.argmax() is used to convert the one-hot encoded predictions and test labels to class labels.\n",
    "y_pred_label = np.argmax(y_pred, axis = 1)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification report:\\n\", classification_report(y_test_enc, y_pred_label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           OsID  True Class  Predicted Class  True/False\n",
      "0  Os11g0704500         328              328        True\n",
      "1  Os09g0279600         161              161        True\n",
      "2  Os03g0669100          17               17        True\n",
      "3  Os05g0542500          34               34        True\n",
      "4  Os09g0522000           7                7        True\n"
     ]
    }
   ],
   "source": [
    "# extract class labels from test data\n",
    "class_test = y_test_enc\n",
    "\n",
    "# Invert OsID_labels dictionary\n",
    "inv_OsID_labels = {v: k for k, v in OsID_labels.items()}\n",
    "\n",
    "# map OsID values to the class labels\n",
    "OsID_test = [inv_OsID_labels.get(value, 'Unknown') for value in class_test]\n",
    "\n",
    "# create dataframe with OsID, true class, predicted class, and true/false columns\n",
    "results = pd.DataFrame({\n",
    "    'OsID': OsID_test,\n",
    "    'True Class': y_test_enc,\n",
    "    'Predicted Class': y_pred_label,\n",
    "    'True/False': class_test == y_pred_label\n",
    "})\n",
    "\n",
    "# display dataframe\n",
    "print(results.head())\n",
    "\n",
    "# save results_df to a CSV file\n",
    "results.to_csv('MLP_gene classification.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54645332476aec3a1589d49135d9c8280fdb5d7db877f5b7af7a1b58b8f996bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixed random seed for reproducibility \n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of dataGene:\n",
      "        log_2FoldChange            ET  CoExpression           PCC  \\\n",
      "count     41110.000000  41110.000000  41110.000000  41110.000000   \n",
      "mean         -0.037332      1.407395      0.991997     -0.361737   \n",
      "std           0.391444      0.784327      0.089101      0.463979   \n",
      "min          -1.000000      0.000000      0.000000     -1.000000   \n",
      "25%          -0.251534      1.000000      1.000000     -0.747963   \n",
      "50%           0.030675      2.000000      1.000000     -0.449089   \n",
      "75%           0.251534      2.000000      1.000000     -0.051646   \n",
      "max           1.000000      2.000000      1.000000      1.000000   \n",
      "\n",
      "                PPI  Root10DaysSeedling  Root14DaysSeedling  \\\n",
      "count  41110.000000        41110.000000        41110.000000   \n",
      "mean       0.914668           -0.522040           -0.646982   \n",
      "std        0.279379            0.498568            0.393549   \n",
      "min        0.000000           -1.000000           -1.000000   \n",
      "25%        1.000000           -0.901371           -0.965084   \n",
      "50%        1.000000           -0.663664           -0.680003   \n",
      "75%        1.000000           -0.378497           -0.559627   \n",
      "max        1.000000            1.000000            1.000000   \n",
      "\n",
      "       Root17DaysSeedling  Root21DaysSeedling  Root24DaysSeedling  ...  \\\n",
      "count        41110.000000        41110.000000        41110.000000  ...   \n",
      "mean            -0.700869           -0.669349           -0.670048  ...   \n",
      "std              0.378219            0.405860            0.390751  ...   \n",
      "min             -1.000000           -1.000000           -1.000000  ...   \n",
      "25%             -0.980226           -1.000000           -0.982003  ...   \n",
      "50%             -0.795609           -0.726665           -0.708584  ...   \n",
      "75%             -0.601266           -0.543621           -0.482133  ...   \n",
      "max              1.000000            1.000000            1.000000  ...   \n",
      "\n",
      "       Root52DaysSeedling  Shoot3DaysSeedling  Shoot10DaysSeedling  \\\n",
      "count        41110.000000        41110.000000         41110.000000   \n",
      "mean            -0.670345           -0.590806            -0.545055   \n",
      "std              0.478222            0.443552             0.477438   \n",
      "min             -1.000000           -1.000000            -1.000000   \n",
      "25%             -1.000000           -1.000000            -0.906055   \n",
      "50%             -0.853382           -0.676286            -0.698864   \n",
      "75%             -0.542371           -0.409775            -0.250588   \n",
      "max              1.000000            0.955179             1.000000   \n",
      "\n",
      "       Shoot14DaysSeedling  Shoot17DaysSeedling  Shoot21DaysSeedling  \\\n",
      "count         41110.000000         41110.000000         41110.000000   \n",
      "mean             -0.734141            -0.680810            -0.659443   \n",
      "std               0.413716             0.478189             0.463838   \n",
      "min              -1.000000            -1.000000            -1.000000   \n",
      "25%              -1.000000            -1.000000            -1.000000   \n",
      "50%              -0.924976            -0.954040            -0.874080   \n",
      "75%              -0.513759            -0.420386            -0.440577   \n",
      "max               0.997390             1.000000             1.000000   \n",
      "\n",
      "       Shoot35DaysSeedling  Leaf21DaysSeedling  Leaf45DaysOldPlant  \\\n",
      "count         41110.000000        41110.000000        41110.000000   \n",
      "mean             -0.558906           -0.828778           -0.585144   \n",
      "std               0.506423            0.327542            0.399046   \n",
      "min              -1.000000           -1.000000           -1.000000   \n",
      "25%              -0.962199           -1.000000           -0.901444   \n",
      "50%              -0.699035           -0.951894           -0.643376   \n",
      "75%              -0.352995           -0.883755           -0.451900   \n",
      "max               0.993958            1.000000            1.000000   \n",
      "\n",
      "              class  \n",
      "count  41110.000000  \n",
      "mean      59.092703  \n",
      "std       77.624892  \n",
      "min        0.000000  \n",
      "25%        8.000000  \n",
      "50%       25.000000  \n",
      "75%       77.000000  \n",
      "max      372.000000  \n",
      "\n",
      "[8 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# load dataset (input variables = X, output variables = Y)\n",
    "df = pd.read_csv(\"2.csv\")\n",
    "\n",
    "#count the number of occurances for each osID\n",
    "OsID_counts = df['OsID'].value_counts()\n",
    "\n",
    "#filter for osIDs that have 10 or more occurances\n",
    "OsID_counts_filtered = OsID_counts[OsID_counts >= 10]\n",
    "\n",
    "#assign a label for each osID \n",
    "OsID_labels = {}\n",
    "class_no = 0\n",
    "for osID in OsID_counts_filtered.index:\n",
    "    OsID_labels[osID] = class_no\n",
    "    class_no +=1\n",
    "\n",
    "#filter the dataset with osID that contain 10 or more occurances\n",
    "dataGene = df[df['OsID'].isin(OsID_counts_filtered.index)]\n",
    "\n",
    "dataGene = dataGene.drop(['Class', 'Trait'],axis=1)\n",
    "\n",
    "# Add a new column 'class' to the filtered dataset\n",
    "dataGene['class'] = dataGene['OsID'].map(OsID_labels)\n",
    "\n",
    "print(\"Summary of dataGene:\\n\",dataGene.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X:\n",
      " (41110, 20)\n",
      "Shape of Y:\n",
      " (41110,)\n",
      "Summary of X:\n",
      "        Root10DaysSeedling  Leaf45DaysOldPlant  Shoot10DaysSeedling  \\\n",
      "count        41110.000000        41110.000000         41110.000000   \n",
      "mean            -0.522040           -0.585144            -0.545055   \n",
      "std              0.498568            0.399046             0.477438   \n",
      "min             -1.000000           -1.000000            -1.000000   \n",
      "25%             -0.901371           -0.901444            -0.906055   \n",
      "50%             -0.663664           -0.643376            -0.698864   \n",
      "75%             -0.378497           -0.451900            -0.250588   \n",
      "max              1.000000            1.000000             1.000000   \n",
      "\n",
      "       Shoot35DaysSeedling  Root35DaysSeedling  Leaf21DaysSeedling  \\\n",
      "count         41110.000000        41110.000000        41110.000000   \n",
      "mean             -0.558906           -0.596196           -0.828778   \n",
      "std               0.506423            0.461679            0.327542   \n",
      "min              -1.000000           -1.000000           -1.000000   \n",
      "25%              -0.962199           -0.937286           -1.000000   \n",
      "50%              -0.699035           -0.769184           -0.951894   \n",
      "75%              -0.352995           -0.323664           -0.883755   \n",
      "max               0.993958            1.000000            1.000000   \n",
      "\n",
      "       Root14DaysSeedling  Shoot3DaysSeedling  Root24DaysSeedling  \\\n",
      "count        41110.000000        41110.000000        41110.000000   \n",
      "mean            -0.646982           -0.590806           -0.670048   \n",
      "std              0.393549            0.443552            0.390751   \n",
      "min             -1.000000           -1.000000           -1.000000   \n",
      "25%             -0.965084           -1.000000           -0.982003   \n",
      "50%             -0.680003           -0.676286           -0.708584   \n",
      "75%             -0.559627           -0.409775           -0.482133   \n",
      "max              1.000000            0.955179            1.000000   \n",
      "\n",
      "       Root52DaysSeedling  Root17DaysSeedling  Root21DaysSeedling  \\\n",
      "count        41110.000000        41110.000000        41110.000000   \n",
      "mean            -0.670345           -0.700869           -0.669349   \n",
      "std              0.478222            0.378219            0.405860   \n",
      "min             -1.000000           -1.000000           -1.000000   \n",
      "25%             -1.000000           -0.980226           -1.000000   \n",
      "50%             -0.853382           -0.795609           -0.726665   \n",
      "75%             -0.542371           -0.601266           -0.543621   \n",
      "max              1.000000            1.000000            1.000000   \n",
      "\n",
      "       Shoot14DaysSeedling  Shoot21DaysSeedling  Shoot17DaysSeedling  \\\n",
      "count         41110.000000         41110.000000         41110.000000   \n",
      "mean             -0.734141            -0.659443            -0.680810   \n",
      "std               0.413716             0.463838             0.478189   \n",
      "min              -1.000000            -1.000000            -1.000000   \n",
      "25%              -1.000000            -1.000000            -1.000000   \n",
      "50%              -0.924976            -0.874080            -0.954040   \n",
      "75%              -0.513759            -0.440577            -0.420386   \n",
      "max               0.997390             1.000000             1.000000   \n",
      "\n",
      "                 ET           PCC  log_2FoldChange           PPI  CoExpression  \n",
      "count  41110.000000  41110.000000     41110.000000  41110.000000  41110.000000  \n",
      "mean       1.407395     -0.361737        -0.037332      0.914668      0.991997  \n",
      "std        0.784327      0.463979         0.391444      0.279379      0.089101  \n",
      "min        0.000000     -1.000000        -1.000000      0.000000      0.000000  \n",
      "25%        1.000000     -0.747963        -0.251534      1.000000      1.000000  \n",
      "50%        2.000000     -0.449089         0.030675      1.000000      1.000000  \n",
      "75%        2.000000     -0.051646         0.251534      1.000000      1.000000  \n",
      "max        2.000000      1.000000         1.000000      1.000000      1.000000  \n",
      "Summary of Y:\n",
      " count    41110.000000\n",
      "mean        59.092703\n",
      "std         77.624892\n",
      "min          0.000000\n",
      "25%          8.000000\n",
      "50%         25.000000\n",
      "75%         77.000000\n",
      "max        372.000000\n",
      "Name: class, dtype: float64\n",
      "class\n",
      "0.0      1800\n",
      "1.0      1296\n",
      "2.0      1260\n",
      "3.0      1218\n",
      "4.0      1026\n",
      "         ... \n",
      "368.0      10\n",
      "369.0      10\n",
      "370.0      10\n",
      "371.0      10\n",
      "372.0      10\n",
      "Length: 373, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X = dataGene.drop(['class','OsID'], axis=1) # exclude class & OsID column\n",
    "Y = dataGene['class']\n",
    "\n",
    "#input feature names in order of descending importance scores in PCC feature selection method\n",
    "feature_names = ['Root10DaysSeedling', 'Leaf45DaysOldPlant', 'Shoot10DaysSeedling', 'Shoot35DaysSeedling', 'Root35DaysSeedling', \n",
    "                 'Leaf21DaysSeedling', 'Root14DaysSeedling', 'Shoot3DaysSeedling', 'Root24DaysSeedling', 'Root52DaysSeedling', \n",
    "                 'Root17DaysSeedling', 'Root21DaysSeedling', 'Shoot14DaysSeedling', 'Shoot21DaysSeedling', 'Shoot17DaysSeedling',\n",
    "                  'ET', 'PCC', 'log_2FoldChange', 'PPI', 'CoExpression']\n",
    "\n",
    "X_fs = X.reindex(columns=feature_names)\n",
    "\n",
    "print(\"Shape of X:\\n\",X_fs.shape)\n",
    "print(\"Shape of Y:\\n\",Y.shape)\n",
    "\n",
    "# Statistical summary of the variables\n",
    "print(\"Summary of X:\\n\",X_fs.describe())\n",
    "print(\"Summary of Y:\\n\",Y.describe())\n",
    "\n",
    "# Check for class imbalance\n",
    "print(df.groupby(Y).size())\n",
    "\n",
    "# change both input and target variables datatype to ndarray\n",
    "\n",
    "X_fs = X_fs.values # 2-D array\n",
    "\n",
    "# select target variable \n",
    "\n",
    "Y = Y.values #1-D array\n",
    "Y = Y.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class=0, n=1800 (4.378%)\n",
      "Class=1, n=1296 (3.153%)\n",
      "Class=2, n=1260 (3.065%)\n",
      "Class=3, n=1218 (2.963%)\n",
      "Class=4, n=1026 (2.496%)\n",
      "Class=5, n=1008 (2.452%)\n",
      "Class=6, n=930 (2.262%)\n",
      "Class=7, n=912 (2.218%)\n",
      "Class=8, n=880 (2.141%)\n",
      "Class=9, n=798 (1.941%)\n",
      "Class=10, n=792 (1.927%)\n",
      "Class=11, n=759 (1.846%)\n",
      "Class=12, n=729 (1.773%)\n",
      "Class=13, n=720 (1.751%)\n",
      "Class=14, n=702 (1.708%)\n",
      "Class=15, n=693 (1.686%)\n",
      "Class=16, n=672 (1.635%)\n",
      "Class=17, n=640 (1.557%)\n",
      "Class=18, n=625 (1.520%)\n",
      "Class=19, n=570 (1.387%)\n",
      "Class=20, n=546 (1.328%)\n",
      "Class=21, n=506 (1.231%)\n",
      "Class=22, n=483 (1.175%)\n",
      "Class=23, n=448 (1.090%)\n",
      "Class=24, n=432 (1.051%)\n",
      "Class=25, n=384 (0.934%)\n",
      "Class=26, n=360 (0.876%)\n",
      "Class=27, n=360 (0.876%)\n",
      "Class=28, n=320 (0.778%)\n",
      "Class=29, n=312 (0.759%)\n",
      "Class=30, n=312 (0.759%)\n",
      "Class=31, n=306 (0.744%)\n",
      "Class=32, n=304 (0.739%)\n",
      "Class=33, n=299 (0.727%)\n",
      "Class=34, n=297 (0.722%)\n",
      "Class=35, n=296 (0.720%)\n",
      "Class=36, n=280 (0.681%)\n",
      "Class=37, n=264 (0.642%)\n",
      "Class=38, n=260 (0.632%)\n",
      "Class=39, n=253 (0.615%)\n",
      "Class=40, n=252 (0.613%)\n",
      "Class=41, n=248 (0.603%)\n",
      "Class=42, n=242 (0.589%)\n",
      "Class=43, n=228 (0.555%)\n",
      "Class=44, n=216 (0.525%)\n",
      "Class=45, n=210 (0.511%)\n",
      "Class=46, n=200 (0.486%)\n",
      "Class=47, n=192 (0.467%)\n",
      "Class=48, n=180 (0.438%)\n",
      "Class=49, n=171 (0.416%)\n",
      "Class=50, n=168 (0.409%)\n",
      "Class=51, n=168 (0.409%)\n",
      "Class=52, n=162 (0.394%)\n",
      "Class=53, n=150 (0.365%)\n",
      "Class=54, n=148 (0.360%)\n",
      "Class=55, n=138 (0.336%)\n",
      "Class=56, n=135 (0.328%)\n",
      "Class=57, n=135 (0.328%)\n",
      "Class=58, n=133 (0.324%)\n",
      "Class=59, n=132 (0.321%)\n",
      "Class=60, n=132 (0.321%)\n",
      "Class=61, n=130 (0.316%)\n",
      "Class=62, n=130 (0.316%)\n",
      "Class=63, n=130 (0.316%)\n",
      "Class=64, n=128 (0.311%)\n",
      "Class=65, n=128 (0.311%)\n",
      "Class=66, n=126 (0.306%)\n",
      "Class=67, n=124 (0.302%)\n",
      "Class=68, n=124 (0.302%)\n",
      "Class=69, n=124 (0.302%)\n",
      "Class=70, n=120 (0.292%)\n",
      "Class=71, n=120 (0.292%)\n",
      "Class=72, n=118 (0.287%)\n",
      "Class=73, n=116 (0.282%)\n",
      "Class=74, n=114 (0.277%)\n",
      "Class=75, n=105 (0.255%)\n",
      "Class=76, n=104 (0.253%)\n",
      "Class=77, n=102 (0.248%)\n",
      "Class=78, n=99 (0.241%)\n",
      "Class=79, n=98 (0.238%)\n",
      "Class=80, n=98 (0.238%)\n",
      "Class=81, n=98 (0.238%)\n",
      "Class=82, n=98 (0.238%)\n",
      "Class=83, n=96 (0.234%)\n",
      "Class=84, n=96 (0.234%)\n",
      "Class=85, n=96 (0.234%)\n",
      "Class=86, n=93 (0.226%)\n",
      "Class=87, n=92 (0.224%)\n",
      "Class=88, n=92 (0.224%)\n",
      "Class=89, n=91 (0.221%)\n",
      "Class=90, n=88 (0.214%)\n",
      "Class=91, n=88 (0.214%)\n",
      "Class=92, n=86 (0.209%)\n",
      "Class=93, n=86 (0.209%)\n",
      "Class=94, n=84 (0.204%)\n",
      "Class=95, n=84 (0.204%)\n",
      "Class=96, n=84 (0.204%)\n",
      "Class=97, n=78 (0.190%)\n",
      "Class=98, n=78 (0.190%)\n",
      "Class=99, n=76 (0.185%)\n",
      "Class=100, n=75 (0.182%)\n",
      "Class=101, n=75 (0.182%)\n",
      "Class=102, n=73 (0.178%)\n",
      "Class=103, n=72 (0.175%)\n",
      "Class=104, n=72 (0.175%)\n",
      "Class=105, n=70 (0.170%)\n",
      "Class=106, n=69 (0.168%)\n",
      "Class=107, n=68 (0.165%)\n",
      "Class=108, n=67 (0.163%)\n",
      "Class=109, n=66 (0.161%)\n",
      "Class=110, n=66 (0.161%)\n",
      "Class=111, n=66 (0.161%)\n",
      "Class=112, n=66 (0.161%)\n",
      "Class=113, n=66 (0.161%)\n",
      "Class=114, n=65 (0.158%)\n",
      "Class=115, n=64 (0.156%)\n",
      "Class=116, n=63 (0.153%)\n",
      "Class=117, n=63 (0.153%)\n",
      "Class=118, n=62 (0.151%)\n",
      "Class=119, n=61 (0.148%)\n",
      "Class=120, n=60 (0.146%)\n",
      "Class=121, n=60 (0.146%)\n",
      "Class=122, n=60 (0.146%)\n",
      "Class=123, n=60 (0.146%)\n",
      "Class=124, n=60 (0.146%)\n",
      "Class=125, n=60 (0.146%)\n",
      "Class=126, n=60 (0.146%)\n",
      "Class=127, n=60 (0.146%)\n",
      "Class=128, n=60 (0.146%)\n",
      "Class=129, n=60 (0.146%)\n",
      "Class=130, n=59 (0.144%)\n",
      "Class=131, n=59 (0.144%)\n",
      "Class=132, n=58 (0.141%)\n",
      "Class=133, n=56 (0.136%)\n",
      "Class=134, n=56 (0.136%)\n",
      "Class=135, n=56 (0.136%)\n",
      "Class=136, n=56 (0.136%)\n",
      "Class=137, n=56 (0.136%)\n",
      "Class=138, n=56 (0.136%)\n",
      "Class=139, n=56 (0.136%)\n",
      "Class=140, n=56 (0.136%)\n",
      "Class=141, n=56 (0.136%)\n",
      "Class=142, n=55 (0.134%)\n",
      "Class=143, n=55 (0.134%)\n",
      "Class=144, n=54 (0.131%)\n",
      "Class=145, n=54 (0.131%)\n",
      "Class=146, n=54 (0.131%)\n",
      "Class=147, n=54 (0.131%)\n",
      "Class=148, n=54 (0.131%)\n",
      "Class=149, n=53 (0.129%)\n",
      "Class=150, n=52 (0.126%)\n",
      "Class=151, n=52 (0.126%)\n",
      "Class=152, n=52 (0.126%)\n",
      "Class=153, n=52 (0.126%)\n",
      "Class=154, n=50 (0.122%)\n",
      "Class=155, n=50 (0.122%)\n",
      "Class=156, n=49 (0.119%)\n",
      "Class=157, n=49 (0.119%)\n",
      "Class=158, n=48 (0.117%)\n",
      "Class=159, n=48 (0.117%)\n",
      "Class=160, n=48 (0.117%)\n",
      "Class=161, n=46 (0.112%)\n",
      "Class=162, n=45 (0.109%)\n",
      "Class=163, n=44 (0.107%)\n",
      "Class=164, n=44 (0.107%)\n",
      "Class=165, n=44 (0.107%)\n",
      "Class=166, n=42 (0.102%)\n",
      "Class=167, n=42 (0.102%)\n",
      "Class=168, n=42 (0.102%)\n",
      "Class=169, n=42 (0.102%)\n",
      "Class=170, n=42 (0.102%)\n",
      "Class=171, n=42 (0.102%)\n",
      "Class=172, n=42 (0.102%)\n",
      "Class=173, n=41 (0.100%)\n",
      "Class=174, n=41 (0.100%)\n",
      "Class=175, n=40 (0.097%)\n",
      "Class=176, n=40 (0.097%)\n",
      "Class=177, n=39 (0.095%)\n",
      "Class=178, n=39 (0.095%)\n",
      "Class=179, n=38 (0.092%)\n",
      "Class=180, n=37 (0.090%)\n",
      "Class=181, n=36 (0.088%)\n",
      "Class=182, n=35 (0.085%)\n",
      "Class=183, n=35 (0.085%)\n",
      "Class=184, n=35 (0.085%)\n",
      "Class=185, n=35 (0.085%)\n",
      "Class=186, n=34 (0.083%)\n",
      "Class=187, n=34 (0.083%)\n",
      "Class=188, n=34 (0.083%)\n",
      "Class=189, n=34 (0.083%)\n",
      "Class=190, n=32 (0.078%)\n",
      "Class=191, n=32 (0.078%)\n",
      "Class=192, n=32 (0.078%)\n",
      "Class=193, n=32 (0.078%)\n",
      "Class=194, n=32 (0.078%)\n",
      "Class=195, n=32 (0.078%)\n",
      "Class=196, n=31 (0.075%)\n",
      "Class=197, n=31 (0.075%)\n",
      "Class=198, n=31 (0.075%)\n",
      "Class=199, n=31 (0.075%)\n",
      "Class=200, n=30 (0.073%)\n",
      "Class=201, n=30 (0.073%)\n",
      "Class=202, n=30 (0.073%)\n",
      "Class=203, n=30 (0.073%)\n",
      "Class=204, n=30 (0.073%)\n",
      "Class=205, n=30 (0.073%)\n",
      "Class=206, n=30 (0.073%)\n",
      "Class=207, n=30 (0.073%)\n",
      "Class=208, n=30 (0.073%)\n",
      "Class=209, n=29 (0.071%)\n",
      "Class=210, n=29 (0.071%)\n",
      "Class=211, n=28 (0.068%)\n",
      "Class=212, n=28 (0.068%)\n",
      "Class=213, n=28 (0.068%)\n",
      "Class=214, n=28 (0.068%)\n",
      "Class=215, n=28 (0.068%)\n",
      "Class=216, n=28 (0.068%)\n",
      "Class=217, n=27 (0.066%)\n",
      "Class=218, n=27 (0.066%)\n",
      "Class=219, n=27 (0.066%)\n",
      "Class=220, n=27 (0.066%)\n",
      "Class=221, n=27 (0.066%)\n",
      "Class=222, n=27 (0.066%)\n",
      "Class=223, n=26 (0.063%)\n",
      "Class=224, n=26 (0.063%)\n",
      "Class=225, n=26 (0.063%)\n",
      "Class=226, n=26 (0.063%)\n",
      "Class=227, n=26 (0.063%)\n",
      "Class=228, n=25 (0.061%)\n",
      "Class=229, n=25 (0.061%)\n",
      "Class=230, n=25 (0.061%)\n",
      "Class=231, n=25 (0.061%)\n",
      "Class=232, n=24 (0.058%)\n",
      "Class=233, n=24 (0.058%)\n",
      "Class=234, n=24 (0.058%)\n",
      "Class=235, n=24 (0.058%)\n",
      "Class=236, n=24 (0.058%)\n",
      "Class=237, n=24 (0.058%)\n",
      "Class=238, n=24 (0.058%)\n",
      "Class=239, n=24 (0.058%)\n",
      "Class=240, n=24 (0.058%)\n",
      "Class=241, n=24 (0.058%)\n",
      "Class=242, n=24 (0.058%)\n",
      "Class=243, n=24 (0.058%)\n",
      "Class=244, n=23 (0.056%)\n",
      "Class=245, n=23 (0.056%)\n",
      "Class=246, n=22 (0.054%)\n",
      "Class=247, n=22 (0.054%)\n",
      "Class=248, n=22 (0.054%)\n",
      "Class=249, n=22 (0.054%)\n",
      "Class=250, n=22 (0.054%)\n",
      "Class=251, n=22 (0.054%)\n",
      "Class=252, n=22 (0.054%)\n",
      "Class=253, n=22 (0.054%)\n",
      "Class=254, n=22 (0.054%)\n",
      "Class=255, n=22 (0.054%)\n",
      "Class=256, n=22 (0.054%)\n",
      "Class=257, n=22 (0.054%)\n",
      "Class=258, n=22 (0.054%)\n",
      "Class=259, n=22 (0.054%)\n",
      "Class=260, n=22 (0.054%)\n",
      "Class=261, n=22 (0.054%)\n",
      "Class=262, n=22 (0.054%)\n",
      "Class=263, n=22 (0.054%)\n",
      "Class=264, n=21 (0.051%)\n",
      "Class=265, n=21 (0.051%)\n",
      "Class=266, n=21 (0.051%)\n",
      "Class=267, n=21 (0.051%)\n",
      "Class=268, n=21 (0.051%)\n",
      "Class=269, n=20 (0.049%)\n",
      "Class=270, n=20 (0.049%)\n",
      "Class=271, n=20 (0.049%)\n",
      "Class=272, n=20 (0.049%)\n",
      "Class=273, n=20 (0.049%)\n",
      "Class=274, n=20 (0.049%)\n",
      "Class=275, n=20 (0.049%)\n",
      "Class=276, n=20 (0.049%)\n",
      "Class=277, n=20 (0.049%)\n",
      "Class=278, n=20 (0.049%)\n",
      "Class=279, n=20 (0.049%)\n",
      "Class=280, n=19 (0.046%)\n",
      "Class=281, n=19 (0.046%)\n",
      "Class=282, n=19 (0.046%)\n",
      "Class=283, n=18 (0.044%)\n",
      "Class=284, n=18 (0.044%)\n",
      "Class=285, n=18 (0.044%)\n",
      "Class=286, n=18 (0.044%)\n",
      "Class=287, n=18 (0.044%)\n",
      "Class=288, n=18 (0.044%)\n",
      "Class=289, n=18 (0.044%)\n",
      "Class=290, n=18 (0.044%)\n",
      "Class=291, n=18 (0.044%)\n",
      "Class=292, n=17 (0.041%)\n",
      "Class=293, n=17 (0.041%)\n",
      "Class=294, n=17 (0.041%)\n",
      "Class=295, n=17 (0.041%)\n",
      "Class=296, n=17 (0.041%)\n",
      "Class=297, n=17 (0.041%)\n",
      "Class=298, n=16 (0.039%)\n",
      "Class=299, n=16 (0.039%)\n",
      "Class=300, n=16 (0.039%)\n",
      "Class=301, n=16 (0.039%)\n",
      "Class=302, n=16 (0.039%)\n",
      "Class=303, n=16 (0.039%)\n",
      "Class=304, n=16 (0.039%)\n",
      "Class=305, n=16 (0.039%)\n",
      "Class=306, n=15 (0.036%)\n",
      "Class=307, n=15 (0.036%)\n",
      "Class=308, n=15 (0.036%)\n",
      "Class=309, n=15 (0.036%)\n",
      "Class=310, n=15 (0.036%)\n",
      "Class=311, n=14 (0.034%)\n",
      "Class=312, n=14 (0.034%)\n",
      "Class=313, n=14 (0.034%)\n",
      "Class=314, n=14 (0.034%)\n",
      "Class=315, n=14 (0.034%)\n",
      "Class=316, n=14 (0.034%)\n",
      "Class=317, n=14 (0.034%)\n",
      "Class=318, n=14 (0.034%)\n",
      "Class=319, n=14 (0.034%)\n",
      "Class=320, n=14 (0.034%)\n",
      "Class=321, n=14 (0.034%)\n",
      "Class=322, n=14 (0.034%)\n",
      "Class=323, n=14 (0.034%)\n",
      "Class=324, n=14 (0.034%)\n",
      "Class=325, n=14 (0.034%)\n",
      "Class=326, n=14 (0.034%)\n",
      "Class=327, n=14 (0.034%)\n",
      "Class=328, n=13 (0.032%)\n",
      "Class=329, n=13 (0.032%)\n",
      "Class=330, n=13 (0.032%)\n",
      "Class=331, n=13 (0.032%)\n",
      "Class=332, n=13 (0.032%)\n",
      "Class=333, n=13 (0.032%)\n",
      "Class=334, n=13 (0.032%)\n",
      "Class=335, n=13 (0.032%)\n",
      "Class=336, n=13 (0.032%)\n",
      "Class=337, n=12 (0.029%)\n",
      "Class=338, n=12 (0.029%)\n",
      "Class=339, n=12 (0.029%)\n",
      "Class=340, n=12 (0.029%)\n",
      "Class=341, n=12 (0.029%)\n",
      "Class=342, n=12 (0.029%)\n",
      "Class=343, n=12 (0.029%)\n",
      "Class=344, n=12 (0.029%)\n",
      "Class=345, n=12 (0.029%)\n",
      "Class=346, n=12 (0.029%)\n",
      "Class=347, n=12 (0.029%)\n",
      "Class=348, n=12 (0.029%)\n",
      "Class=349, n=12 (0.029%)\n",
      "Class=350, n=12 (0.029%)\n",
      "Class=351, n=12 (0.029%)\n",
      "Class=352, n=12 (0.029%)\n",
      "Class=353, n=12 (0.029%)\n",
      "Class=354, n=12 (0.029%)\n",
      "Class=355, n=11 (0.027%)\n",
      "Class=356, n=11 (0.027%)\n",
      "Class=357, n=11 (0.027%)\n",
      "Class=358, n=11 (0.027%)\n",
      "Class=359, n=11 (0.027%)\n",
      "Class=360, n=11 (0.027%)\n",
      "Class=361, n=10 (0.024%)\n",
      "Class=362, n=10 (0.024%)\n",
      "Class=363, n=10 (0.024%)\n",
      "Class=364, n=10 (0.024%)\n",
      "Class=365, n=10 (0.024%)\n",
      "Class=366, n=10 (0.024%)\n",
      "Class=367, n=10 (0.024%)\n",
      "Class=368, n=10 (0.024%)\n",
      "Class=369, n=10 (0.024%)\n",
      "Class=370, n=10 (0.024%)\n",
      "Class=371, n=10 (0.024%)\n",
      "Class=372, n=10 (0.024%)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# summarize distribution\n",
    "counter = Counter(Y.flatten())\n",
    "\n",
    "# sort counter by keys\n",
    "counter = dict(sorted(counter.items()))\n",
    "\n",
    "for k,v in counter.items():\n",
    " per = v / len(Y.flatten()) * 100\n",
    " print('Class=%d, n=%d (%.3f%%)' % (k, v, per))\n",
    "# plot the distribution\n",
    "plt.bar( counter.keys(), counter.values())\n",
    "\n",
    "plt.ylabel('No of gene samples')\n",
    "plt.xlabel('Class Index of OsID')\n",
    "plt.title('Distribution of Target Classes (OsID)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_confusion_matrix(cm):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    sns.heatmap(cm,annot=True,fmt=\"d\", center=0, cmap='autumn') \n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare target data\n",
    "def prepare_targets(y_train, y_test):\n",
    "\tle = LabelEncoder()\n",
    "\t\n",
    "\t#fit the encoders only to the training data and then transform both train and test data\n",
    "\ty_train_enc = le.fit_transform(y_train)\n",
    "\ty_test_enc = le.transform(y_test)\n",
    "\n",
    "\treturn y_train_enc, y_test_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define baseline model (MLP)\n",
    "def MLP_model(input_dim):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_dim=input_dim,bias_initializer='normal', activation='relu',kernel_regularizer=regularizers.l2(0.0001)))\n",
    "    model.add(Dense(20,bias_initializer='normal',activation='relu',kernel_regularizer=regularizers.l2(0.0001)))\n",
    "    model.add(Dense(20,bias_initializer='normal',activation='relu',kernel_regularizer=regularizers.l2(0.0001)))\n",
    "    model.add(Dense(373,kernel_initializer='normal', activation='softmax')) #softmax for multi-class classification, num_classes = 373\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input features: 1\n",
      "Fold: 1\n",
      "Epoch 1/40\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 4.3508 - accuracy: 0.1139 - val_loss: 3.9500 - val_accuracy: 0.1778\n",
      "Epoch 2/40\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 3.6223 - accuracy: 0.2063 - val_loss: 3.5462 - val_accuracy: 0.2367\n",
      "Epoch 3/40\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 3.2654 - accuracy: 0.2621 - val_loss: 3.2614 - val_accuracy: 0.2937\n",
      "Epoch 4/40\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 2.9896 - accuracy: 0.2974 - val_loss: 3.0512 - val_accuracy: 0.2942\n",
      "Epoch 5/40\n",
      "1846/1846 [==============================] - 3s 1ms/step - loss: 2.7921 - accuracy: 0.3292 - val_loss: 2.8675 - val_accuracy: 0.3270\n",
      "Epoch 6/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 2.6497 - accuracy: 0.3519 - val_loss: 2.7447 - val_accuracy: 0.3540\n",
      "Epoch 7/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 2.5377 - accuracy: 0.3691 - val_loss: 2.6519 - val_accuracy: 0.3305\n",
      "Epoch 8/40\n",
      "1846/1846 [==============================] - 3s 1ms/step - loss: 2.4542 - accuracy: 0.3789 - val_loss: 2.5497 - val_accuracy: 0.3877\n",
      "Epoch 9/40\n",
      "1846/1846 [==============================] - 3s 1ms/step - loss: 2.3874 - accuracy: 0.3910 - val_loss: 2.5361 - val_accuracy: 0.4189\n",
      "Epoch 10/40\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 2.3292 - accuracy: 0.4078 - val_loss: 2.5250 - val_accuracy: 0.3503\n",
      "Epoch 11/40\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 2.2853 - accuracy: 0.4158 - val_loss: 2.4435 - val_accuracy: 0.4075\n",
      "Epoch 12/40\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 2.2457 - accuracy: 0.4227 - val_loss: 2.5960 - val_accuracy: 0.3894\n",
      "Epoch 13/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 2.2114 - accuracy: 0.4276 - val_loss: 2.4062 - val_accuracy: 0.3936\n",
      "Epoch 14/40\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 2.1870 - accuracy: 0.4300 - val_loss: 2.3919 - val_accuracy: 0.4125\n",
      "Epoch 15/40\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 2.1534 - accuracy: 0.4410 - val_loss: 2.3277 - val_accuracy: 0.4334\n",
      "Epoch 16/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 2.1309 - accuracy: 0.4417 - val_loss: 2.4708 - val_accuracy: 0.3934\n",
      "Epoch 17/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 2.1121 - accuracy: 0.4471 - val_loss: 2.2814 - val_accuracy: 0.4785\n",
      "Epoch 18/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 2.0905 - accuracy: 0.4563 - val_loss: 2.2941 - val_accuracy: 0.4594\n",
      "Epoch 19/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 2.0750 - accuracy: 0.4521 - val_loss: 2.2638 - val_accuracy: 0.4304\n",
      "Epoch 20/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 2.0522 - accuracy: 0.4620 - val_loss: 2.2724 - val_accuracy: 0.4176\n",
      "Epoch 21/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 2.0392 - accuracy: 0.4610 - val_loss: 2.2609 - val_accuracy: 0.4702\n",
      "Epoch 22/40\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 2.0233 - accuracy: 0.4663 - val_loss: 2.2339 - val_accuracy: 0.4684\n",
      "Epoch 23/40\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 2.0152 - accuracy: 0.4688 - val_loss: 2.1739 - val_accuracy: 0.4957\n",
      "Epoch 24/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 2.0076 - accuracy: 0.4711 - val_loss: 2.1767 - val_accuracy: 0.4429\n",
      "Epoch 25/40\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 1.9795 - accuracy: 0.4764 - val_loss: 2.1719 - val_accuracy: 0.4524\n",
      "Epoch 26/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 1.9721 - accuracy: 0.4731 - val_loss: 2.1342 - val_accuracy: 0.5085\n",
      "Epoch 27/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 1.9723 - accuracy: 0.4754 - val_loss: 2.2238 - val_accuracy: 0.4389\n",
      "Epoch 28/40\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 1.9558 - accuracy: 0.4757 - val_loss: 2.1327 - val_accuracy: 0.4669\n",
      "Epoch 29/40\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 1.9434 - accuracy: 0.4794 - val_loss: 2.1631 - val_accuracy: 0.4354\n",
      "Epoch 30/40\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 1.9392 - accuracy: 0.4767 - val_loss: 2.1177 - val_accuracy: 0.4763\n",
      "Epoch 31/40\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 1.9322 - accuracy: 0.4823 - val_loss: 2.0878 - val_accuracy: 0.5076\n",
      "Epoch 32/40\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 1.9241 - accuracy: 0.4819 - val_loss: 2.0818 - val_accuracy: 0.5113\n",
      "Epoch 33/40\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 1.9157 - accuracy: 0.4828 - val_loss: 2.0744 - val_accuracy: 0.4882\n",
      "Epoch 34/40\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 1.9176 - accuracy: 0.4804 - val_loss: 2.1123 - val_accuracy: 0.4792\n",
      "Epoch 35/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 1.9051 - accuracy: 0.4836 - val_loss: 2.0259 - val_accuracy: 0.4757\n",
      "Epoch 36/40\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 1.9004 - accuracy: 0.4900 - val_loss: 2.1208 - val_accuracy: 0.4561\n",
      "Epoch 37/40\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 1.8827 - accuracy: 0.4927 - val_loss: 2.0901 - val_accuracy: 0.4517\n",
      "Epoch 38/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 1.8872 - accuracy: 0.4870 - val_loss: 2.1092 - val_accuracy: 0.4684\n",
      "Epoch 39/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 1.8778 - accuracy: 0.4892 - val_loss: 2.0463 - val_accuracy: 0.4889\n",
      "Epoch 40/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 1.8645 - accuracy: 0.4912 - val_loss: 2.0702 - val_accuracy: 0.5067\n",
      "Fold: 2\n",
      "Epoch 1/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 4.4403 - accuracy: 0.1040 - val_loss: 4.0821 - val_accuracy: 0.1617\n",
      "Epoch 2/40\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 3.7583 - accuracy: 0.1761 - val_loss: 3.7929 - val_accuracy: 0.1707\n",
      "Epoch 3/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 3.4419 - accuracy: 0.2168 - val_loss: 3.5286 - val_accuracy: 0.2134\n",
      "Epoch 4/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 3.1908 - accuracy: 0.2630 - val_loss: 3.3349 - val_accuracy: 0.2785\n",
      "Epoch 5/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 2.9802 - accuracy: 0.2828 - val_loss: 3.1629 - val_accuracy: 0.3204\n",
      "Epoch 6/40\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 2.8104 - accuracy: 0.3135 - val_loss: 3.0692 - val_accuracy: 0.3369\n",
      "Epoch 7/40\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 2.6708 - accuracy: 0.3445 - val_loss: 2.9610 - val_accuracy: 0.3171\n",
      "Epoch 8/40\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 2.5748 - accuracy: 0.3624 - val_loss: 2.8309 - val_accuracy: 0.4024\n",
      "Epoch 9/40\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 2.4932 - accuracy: 0.3762 - val_loss: 2.8119 - val_accuracy: 0.3628\n",
      "Epoch 10/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 2.4287 - accuracy: 0.3846 - val_loss: 2.7158 - val_accuracy: 0.3943\n",
      "Epoch 11/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 2.3805 - accuracy: 0.3955 - val_loss: 2.6093 - val_accuracy: 0.4411\n",
      "Epoch 12/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 2.3347 - accuracy: 0.4048 - val_loss: 2.5460 - val_accuracy: 0.4156\n",
      "Epoch 13/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 2.2892 - accuracy: 0.4188 - val_loss: 2.5160 - val_accuracy: 0.3569\n",
      "Epoch 14/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 2.2587 - accuracy: 0.4269 - val_loss: 2.4641 - val_accuracy: 0.4398\n",
      "Epoch 15/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 2.2285 - accuracy: 0.4293 - val_loss: 2.4507 - val_accuracy: 0.3954\n",
      "Epoch 16/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 2.2001 - accuracy: 0.4401 - val_loss: 2.4294 - val_accuracy: 0.4427\n",
      "Epoch 17/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 2.1751 - accuracy: 0.4396 - val_loss: 2.3640 - val_accuracy: 0.4684\n",
      "Epoch 18/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 2.1546 - accuracy: 0.4487 - val_loss: 2.3753 - val_accuracy: 0.4315\n",
      "Epoch 19/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 2.1431 - accuracy: 0.4519 - val_loss: 2.3133 - val_accuracy: 0.4392\n",
      "Epoch 20/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 2.1190 - accuracy: 0.4561 - val_loss: 2.3541 - val_accuracy: 0.4572\n",
      "Epoch 21/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 2.1129 - accuracy: 0.4546 - val_loss: 2.3032 - val_accuracy: 0.4812\n",
      "Epoch 22/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 2.0920 - accuracy: 0.4607 - val_loss: 2.3426 - val_accuracy: 0.4286\n",
      "Epoch 23/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 2.0815 - accuracy: 0.4642 - val_loss: 2.2987 - val_accuracy: 0.4282\n",
      "Epoch 24/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 2.0745 - accuracy: 0.4655 - val_loss: 2.2476 - val_accuracy: 0.4790\n",
      "Epoch 25/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 2.0613 - accuracy: 0.4637 - val_loss: 2.3011 - val_accuracy: 0.4361\n",
      "Epoch 26/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 2.0533 - accuracy: 0.4682 - val_loss: 2.3330 - val_accuracy: 0.4354\n",
      "Epoch 27/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 2.0393 - accuracy: 0.4685 - val_loss: 2.2236 - val_accuracy: 0.4779\n",
      "Epoch 28/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 2.0329 - accuracy: 0.4787 - val_loss: 2.2773 - val_accuracy: 0.4114\n",
      "Epoch 29/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 2.0237 - accuracy: 0.4727 - val_loss: 2.1917 - val_accuracy: 0.4462\n",
      "Epoch 30/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 2.0066 - accuracy: 0.4755 - val_loss: 2.2026 - val_accuracy: 0.3996\n",
      "Epoch 31/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 2.0050 - accuracy: 0.4732 - val_loss: 2.1808 - val_accuracy: 0.4539\n",
      "Epoch 32/40\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 1.9841 - accuracy: 0.4827 - val_loss: 2.1825 - val_accuracy: 0.4722\n",
      "Epoch 33/40\n",
      "1846/1846 [==============================] - 12s 6ms/step - loss: 1.9989 - accuracy: 0.4773 - val_loss: 2.1648 - val_accuracy: 0.4840\n",
      "Epoch 34/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 1.9847 - accuracy: 0.4814 - val_loss: 2.1905 - val_accuracy: 0.4662\n",
      "Epoch 35/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 1.9784 - accuracy: 0.4799 - val_loss: 2.1813 - val_accuracy: 0.4605\n",
      "Epoch 36/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 1.9655 - accuracy: 0.4877 - val_loss: 2.2081 - val_accuracy: 0.4552\n",
      "Epoch 37/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 1.9655 - accuracy: 0.4813 - val_loss: 2.2012 - val_accuracy: 0.4471\n",
      "Epoch 38/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 1.9582 - accuracy: 0.4831 - val_loss: 2.1569 - val_accuracy: 0.4873\n",
      "Epoch 39/40\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 1.9513 - accuracy: 0.4891 - val_loss: 2.1964 - val_accuracy: 0.4796\n",
      "Epoch 40/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 1.9394 - accuracy: 0.4898 - val_loss: 2.1546 - val_accuracy: 0.5065\n",
      "Average Validation Accuracy: 0.5154849886894226\n",
      "Average Validation Loss: 1.9493159651756287\n",
      "Average Test Accuracy: 0.5140414237976074\n",
      "Final Test Accuracy for each fold: 0.5169897675514221\n",
      "Number of input features: 2\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "1846/1846 [==============================] - 14s 6ms/step - loss: 4.2204 - accuracy: 0.1663 - val_loss: 3.5564 - val_accuracy: 0.2966\n",
      "Epoch 2/40\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 2.9250 - accuracy: 0.3859 - val_loss: 2.6973 - val_accuracy: 0.4625\n",
      "Epoch 3/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 2.1608 - accuracy: 0.5158 - val_loss: 2.1310 - val_accuracy: 0.5272\n",
      "Epoch 4/40\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 1.6973 - accuracy: 0.5860 - val_loss: 1.8444 - val_accuracy: 0.6114\n",
      "Epoch 5/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 1.4421 - accuracy: 0.6380 - val_loss: 1.6303 - val_accuracy: 0.6627\n",
      "Epoch 6/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 1.2796 - accuracy: 0.6715 - val_loss: 1.5341 - val_accuracy: 0.6686\n",
      "Epoch 7/40\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 1.1706 - accuracy: 0.6915 - val_loss: 1.3702 - val_accuracy: 0.7052\n",
      "Epoch 8/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 1.0973 - accuracy: 0.7092 - val_loss: 1.3156 - val_accuracy: 0.7028\n",
      "Epoch 9/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 1.0291 - accuracy: 0.7238 - val_loss: 1.2769 - val_accuracy: 0.7118\n",
      "Epoch 10/40\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.9920 - accuracy: 0.7263 - val_loss: 1.1526 - val_accuracy: 0.7380\n",
      "Epoch 11/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.9502 - accuracy: 0.7416 - val_loss: 1.1533 - val_accuracy: 0.7452\n",
      "Epoch 12/40\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.9197 - accuracy: 0.7463 - val_loss: 1.1033 - val_accuracy: 0.7567\n",
      "Epoch 13/40\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.8936 - accuracy: 0.7515 - val_loss: 1.1430 - val_accuracy: 0.7316\n",
      "Epoch 14/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.8720 - accuracy: 0.7557 - val_loss: 1.0681 - val_accuracy: 0.7608\n",
      "Epoch 15/40\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.8457 - accuracy: 0.7622 - val_loss: 1.0446 - val_accuracy: 0.7666\n",
      "Epoch 16/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.8347 - accuracy: 0.7650 - val_loss: 1.0258 - val_accuracy: 0.7509\n",
      "Epoch 17/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.8241 - accuracy: 0.7695 - val_loss: 1.0781 - val_accuracy: 0.7364\n",
      "Epoch 18/40\n",
      "1846/1846 [==============================] - 12s 7ms/step - loss: 0.8069 - accuracy: 0.7686 - val_loss: 1.0083 - val_accuracy: 0.7626\n",
      "Epoch 19/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.7975 - accuracy: 0.7683 - val_loss: 0.9489 - val_accuracy: 0.7894\n",
      "Epoch 20/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.7844 - accuracy: 0.7753 - val_loss: 0.9522 - val_accuracy: 0.8007\n",
      "Epoch 21/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.7732 - accuracy: 0.7773 - val_loss: 0.9475 - val_accuracy: 0.7756\n",
      "Epoch 22/40\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.7506 - accuracy: 0.7837 - val_loss: 0.9812 - val_accuracy: 0.7721\n",
      "Epoch 23/40\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.7494 - accuracy: 0.7821 - val_loss: 0.9442 - val_accuracy: 0.7747\n",
      "Epoch 24/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.7390 - accuracy: 0.7845 - val_loss: 0.8785 - val_accuracy: 0.8084\n",
      "Epoch 25/40\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.7348 - accuracy: 0.7859 - val_loss: 0.9453 - val_accuracy: 0.7833\n",
      "Epoch 26/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.7300 - accuracy: 0.7851 - val_loss: 0.9110 - val_accuracy: 0.7923\n",
      "Epoch 27/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.7212 - accuracy: 0.7916 - val_loss: 0.8531 - val_accuracy: 0.7987\n",
      "Epoch 28/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.7001 - accuracy: 0.7957 - val_loss: 0.9104 - val_accuracy: 0.7789\n",
      "Epoch 29/40\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.7056 - accuracy: 0.7914 - val_loss: 0.8845 - val_accuracy: 0.7899\n",
      "Epoch 30/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.6982 - accuracy: 0.7918 - val_loss: 0.8655 - val_accuracy: 0.7890\n",
      "Epoch 31/40\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.6853 - accuracy: 0.7969 - val_loss: 0.8330 - val_accuracy: 0.7778\n",
      "Epoch 32/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.6800 - accuracy: 0.8013 - val_loss: 0.8380 - val_accuracy: 0.8062\n",
      "Epoch 33/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.6652 - accuracy: 0.8058 - val_loss: 0.8397 - val_accuracy: 0.7908\n",
      "Epoch 34/40\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.6610 - accuracy: 0.8060 - val_loss: 0.8237 - val_accuracy: 0.7993\n",
      "Epoch 35/40\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.6693 - accuracy: 0.8015 - val_loss: 0.8279 - val_accuracy: 0.8059\n",
      "Epoch 36/40\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.6560 - accuracy: 0.8062 - val_loss: 0.8350 - val_accuracy: 0.7916\n",
      "Epoch 37/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.6429 - accuracy: 0.8100 - val_loss: 0.7864 - val_accuracy: 0.8029\n",
      "Epoch 38/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.6401 - accuracy: 0.8072 - val_loss: 0.8682 - val_accuracy: 0.7784\n",
      "Epoch 39/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.6321 - accuracy: 0.8137 - val_loss: 0.8229 - val_accuracy: 0.7989\n",
      "Epoch 40/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.6354 - accuracy: 0.8086 - val_loss: 0.8661 - val_accuracy: 0.7718\n",
      "Fold: 2\n",
      "Epoch 1/40\n",
      "1846/1846 [==============================] - 13s 6ms/step - loss: 4.3174 - accuracy: 0.1406 - val_loss: 3.7618 - val_accuracy: 0.2671\n",
      "Epoch 2/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 3.1596 - accuracy: 0.3457 - val_loss: 3.1360 - val_accuracy: 0.3448\n",
      "Epoch 3/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 2.5733 - accuracy: 0.4425 - val_loss: 2.7133 - val_accuracy: 0.4878\n",
      "Epoch 4/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 2.1226 - accuracy: 0.5361 - val_loss: 2.4001 - val_accuracy: 0.6000\n",
      "Epoch 5/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 1.7881 - accuracy: 0.5995 - val_loss: 2.1421 - val_accuracy: 0.6268\n",
      "Epoch 6/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 1.5617 - accuracy: 0.6329 - val_loss: 1.9744 - val_accuracy: 0.6680\n",
      "Epoch 7/40\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 1.3971 - accuracy: 0.6613 - val_loss: 1.8123 - val_accuracy: 0.6816\n",
      "Epoch 8/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 1.2780 - accuracy: 0.6864 - val_loss: 1.7170 - val_accuracy: 0.6711\n",
      "Epoch 9/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 1.1857 - accuracy: 0.6977 - val_loss: 1.6324 - val_accuracy: 0.7149\n",
      "Epoch 10/40\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 1.1125 - accuracy: 0.7072 - val_loss: 1.5090 - val_accuracy: 0.7373\n",
      "Epoch 11/40\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 1.0588 - accuracy: 0.7213 - val_loss: 1.5002 - val_accuracy: 0.7230\n",
      "Epoch 12/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 1.0140 - accuracy: 0.7312 - val_loss: 1.4073 - val_accuracy: 0.7276\n",
      "Epoch 13/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.9758 - accuracy: 0.7351 - val_loss: 1.3523 - val_accuracy: 0.7527\n",
      "Epoch 14/40\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.9402 - accuracy: 0.7461 - val_loss: 1.4296 - val_accuracy: 0.7190\n",
      "Epoch 15/40\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.9219 - accuracy: 0.7480 - val_loss: 1.3015 - val_accuracy: 0.7435\n",
      "Epoch 16/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.8907 - accuracy: 0.7580 - val_loss: 1.2310 - val_accuracy: 0.7705\n",
      "Epoch 17/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.8659 - accuracy: 0.7629 - val_loss: 1.2559 - val_accuracy: 0.7395\n",
      "Epoch 18/40\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.8522 - accuracy: 0.7650 - val_loss: 1.2434 - val_accuracy: 0.7703\n",
      "Epoch 19/40\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.8400 - accuracy: 0.7653 - val_loss: 1.2025 - val_accuracy: 0.7483\n",
      "Epoch 20/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.8211 - accuracy: 0.7721 - val_loss: 1.2190 - val_accuracy: 0.7787\n",
      "Epoch 21/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.8032 - accuracy: 0.7802 - val_loss: 1.1767 - val_accuracy: 0.7877\n",
      "Epoch 22/40\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.7886 - accuracy: 0.7800 - val_loss: 1.3080 - val_accuracy: 0.6926\n",
      "Epoch 23/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.7703 - accuracy: 0.7888 - val_loss: 1.1750 - val_accuracy: 0.7784\n",
      "Epoch 24/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.7781 - accuracy: 0.7777 - val_loss: 1.1822 - val_accuracy: 0.7809\n",
      "Epoch 25/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.7606 - accuracy: 0.7861 - val_loss: 1.1034 - val_accuracy: 0.7905\n",
      "Epoch 26/40\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.7468 - accuracy: 0.7899 - val_loss: 1.1507 - val_accuracy: 0.7923\n",
      "Epoch 27/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.7380 - accuracy: 0.7901 - val_loss: 1.0891 - val_accuracy: 0.7833\n",
      "Epoch 28/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.7280 - accuracy: 0.7942 - val_loss: 1.0889 - val_accuracy: 0.7996\n",
      "Epoch 29/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.7185 - accuracy: 0.7969 - val_loss: 1.1342 - val_accuracy: 0.7674\n",
      "Epoch 30/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.7057 - accuracy: 0.8022 - val_loss: 1.0666 - val_accuracy: 0.7987\n",
      "Epoch 31/40\n",
      "1846/1846 [==============================] - 12s 6ms/step - loss: 0.7015 - accuracy: 0.8021 - val_loss: 1.0526 - val_accuracy: 0.8139\n",
      "Epoch 32/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.6950 - accuracy: 0.8061 - val_loss: 1.0309 - val_accuracy: 0.8114\n",
      "Epoch 33/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.6877 - accuracy: 0.8071 - val_loss: 1.0854 - val_accuracy: 0.7870\n",
      "Epoch 34/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.6707 - accuracy: 0.8111 - val_loss: 1.0427 - val_accuracy: 0.8163\n",
      "Epoch 35/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.6745 - accuracy: 0.8110 - val_loss: 1.0222 - val_accuracy: 0.7831\n",
      "Epoch 36/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.6594 - accuracy: 0.8156 - val_loss: 1.0056 - val_accuracy: 0.8086\n",
      "Epoch 37/40\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.6576 - accuracy: 0.8136 - val_loss: 1.0196 - val_accuracy: 0.8057\n",
      "Epoch 38/40\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.6510 - accuracy: 0.8171 - val_loss: 0.9988 - val_accuracy: 0.8035\n",
      "Epoch 39/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.6482 - accuracy: 0.8165 - val_loss: 0.9829 - val_accuracy: 0.8286\n",
      "Epoch 40/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.6375 - accuracy: 0.8213 - val_loss: 0.9844 - val_accuracy: 0.8282\n",
      "Average Validation Accuracy: 0.8099346458911896\n",
      "Average Validation Loss: 0.7198795676231384\n",
      "Average Test Accuracy: 0.8046362400054932\n",
      "Final Test Accuracy for each fold: 0.8328296542167664\n",
      "Number of input features: 3\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "1846/1846 [==============================] - 13s 6ms/step - loss: 3.9418 - accuracy: 0.2481 - val_loss: 3.0771 - val_accuracy: 0.4323\n",
      "Epoch 2/40\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 2.3514 - accuracy: 0.5435 - val_loss: 2.0738 - val_accuracy: 0.5903\n",
      "Epoch 3/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 1.5663 - accuracy: 0.6615 - val_loss: 1.5873 - val_accuracy: 0.6616\n",
      "Epoch 4/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 1.1557 - accuracy: 0.7246 - val_loss: 1.2518 - val_accuracy: 0.7639\n",
      "Epoch 5/40\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.9304 - accuracy: 0.7661 - val_loss: 1.1241 - val_accuracy: 0.7553\n",
      "Epoch 6/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.8059 - accuracy: 0.7918 - val_loss: 0.9740 - val_accuracy: 0.8007\n",
      "Epoch 7/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.7154 - accuracy: 0.8110 - val_loss: 0.9137 - val_accuracy: 0.8143\n",
      "Epoch 8/40\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.6579 - accuracy: 0.8256 - val_loss: 0.8525 - val_accuracy: 0.8139\n",
      "Epoch 9/40\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.6215 - accuracy: 0.8299 - val_loss: 0.7863 - val_accuracy: 0.8389\n",
      "Epoch 10/40\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.5831 - accuracy: 0.8380 - val_loss: 0.7627 - val_accuracy: 0.8451\n",
      "Epoch 11/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.5504 - accuracy: 0.8487 - val_loss: 0.7434 - val_accuracy: 0.8433\n",
      "Epoch 12/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.5268 - accuracy: 0.8518 - val_loss: 0.7224 - val_accuracy: 0.8396\n",
      "Epoch 13/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.5090 - accuracy: 0.8581 - val_loss: 0.6747 - val_accuracy: 0.8528\n",
      "Epoch 14/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.4913 - accuracy: 0.8613 - val_loss: 0.6763 - val_accuracy: 0.8392\n",
      "Epoch 15/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.4748 - accuracy: 0.8633 - val_loss: 0.6331 - val_accuracy: 0.8761\n",
      "Epoch 16/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.4634 - accuracy: 0.8704 - val_loss: 0.6142 - val_accuracy: 0.8777\n",
      "Epoch 17/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.4569 - accuracy: 0.8715 - val_loss: 0.5990 - val_accuracy: 0.8871\n",
      "Epoch 18/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.4378 - accuracy: 0.8780 - val_loss: 0.6066 - val_accuracy: 0.8783\n",
      "Epoch 19/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.4339 - accuracy: 0.8777 - val_loss: 0.5838 - val_accuracy: 0.8865\n",
      "Epoch 20/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.4191 - accuracy: 0.8823 - val_loss: 0.5450 - val_accuracy: 0.8939\n",
      "Epoch 21/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.4168 - accuracy: 0.8809 - val_loss: 0.5657 - val_accuracy: 0.8823\n",
      "Epoch 22/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.4034 - accuracy: 0.8828 - val_loss: 0.5805 - val_accuracy: 0.8770\n",
      "Epoch 23/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.4092 - accuracy: 0.8837 - val_loss: 0.5495 - val_accuracy: 0.8966\n",
      "Epoch 24/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.3970 - accuracy: 0.8874 - val_loss: 0.5676 - val_accuracy: 0.8843\n",
      "Epoch 25/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.3826 - accuracy: 0.8943 - val_loss: 0.5144 - val_accuracy: 0.9072\n",
      "Epoch 26/40\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.3896 - accuracy: 0.8930 - val_loss: 0.5523 - val_accuracy: 0.8812\n",
      "Epoch 27/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.3725 - accuracy: 0.8988 - val_loss: 0.5321 - val_accuracy: 0.9006\n",
      "Epoch 28/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.3712 - accuracy: 0.8985 - val_loss: 0.5159 - val_accuracy: 0.9094\n",
      "Epoch 29/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.3661 - accuracy: 0.8982 - val_loss: 0.5020 - val_accuracy: 0.9195\n",
      "Epoch 30/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.3555 - accuracy: 0.9003 - val_loss: 0.6055 - val_accuracy: 0.8814\n",
      "Epoch 31/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.3553 - accuracy: 0.8997 - val_loss: 0.4845 - val_accuracy: 0.9177\n",
      "Epoch 32/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.3504 - accuracy: 0.9015 - val_loss: 0.4900 - val_accuracy: 0.9232\n",
      "Epoch 33/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.3452 - accuracy: 0.9041 - val_loss: 0.6192 - val_accuracy: 0.8706\n",
      "Epoch 34/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.3359 - accuracy: 0.9061 - val_loss: 0.4914 - val_accuracy: 0.9138\n",
      "Epoch 35/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.3321 - accuracy: 0.9094 - val_loss: 0.4751 - val_accuracy: 0.9171\n",
      "Epoch 36/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.3413 - accuracy: 0.9056 - val_loss: 0.4843 - val_accuracy: 0.9127\n",
      "Epoch 37/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.3314 - accuracy: 0.9100 - val_loss: 0.4753 - val_accuracy: 0.9232\n",
      "Epoch 38/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.3238 - accuracy: 0.9135 - val_loss: 0.4926 - val_accuracy: 0.9091\n",
      "Epoch 39/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.3171 - accuracy: 0.9125 - val_loss: 0.5139 - val_accuracy: 0.8997\n",
      "Epoch 40/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.3253 - accuracy: 0.9105 - val_loss: 0.4742 - val_accuracy: 0.9241\n",
      "Fold: 2\n",
      "Epoch 1/40\n",
      "1846/1846 [==============================] - 13s 6ms/step - loss: 3.8393 - accuracy: 0.2803 - val_loss: 2.9408 - val_accuracy: 0.4836\n",
      "Epoch 2/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 2.2341 - accuracy: 0.5497 - val_loss: 2.1393 - val_accuracy: 0.5782\n",
      "Epoch 3/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 1.5752 - accuracy: 0.6519 - val_loss: 1.7383 - val_accuracy: 0.6891\n",
      "Epoch 4/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 1.2257 - accuracy: 0.7115 - val_loss: 1.4755 - val_accuracy: 0.7190\n",
      "Epoch 5/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 1.0224 - accuracy: 0.7501 - val_loss: 1.3216 - val_accuracy: 0.7637\n",
      "Epoch 6/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.8962 - accuracy: 0.7737 - val_loss: 1.1790 - val_accuracy: 0.7806\n",
      "Epoch 7/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.8007 - accuracy: 0.7946 - val_loss: 1.0993 - val_accuracy: 0.8066\n",
      "Epoch 8/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.7328 - accuracy: 0.8089 - val_loss: 1.1098 - val_accuracy: 0.7760\n",
      "Epoch 9/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.6836 - accuracy: 0.8145 - val_loss: 0.8993 - val_accuracy: 0.8271\n",
      "Epoch 10/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.6393 - accuracy: 0.8235 - val_loss: 0.8864 - val_accuracy: 0.8220\n",
      "Epoch 11/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.6027 - accuracy: 0.8339 - val_loss: 0.8535 - val_accuracy: 0.8185\n",
      "Epoch 12/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.5756 - accuracy: 0.8427 - val_loss: 0.7889 - val_accuracy: 0.8365\n",
      "Epoch 13/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.5449 - accuracy: 0.8491 - val_loss: 0.8691 - val_accuracy: 0.8429\n",
      "Epoch 14/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.5225 - accuracy: 0.8578 - val_loss: 0.7387 - val_accuracy: 0.8706\n",
      "Epoch 15/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.5062 - accuracy: 0.8586 - val_loss: 0.7271 - val_accuracy: 0.8508\n",
      "Epoch 16/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.4854 - accuracy: 0.8639 - val_loss: 0.7285 - val_accuracy: 0.8513\n",
      "Epoch 17/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.4754 - accuracy: 0.8675 - val_loss: 0.7128 - val_accuracy: 0.8649\n",
      "Epoch 18/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.4577 - accuracy: 0.8719 - val_loss: 0.7359 - val_accuracy: 0.8625\n",
      "Epoch 19/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.4488 - accuracy: 0.8754 - val_loss: 0.7008 - val_accuracy: 0.8708\n",
      "Epoch 20/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.4349 - accuracy: 0.8792 - val_loss: 0.7049 - val_accuracy: 0.8497\n",
      "Epoch 21/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.4366 - accuracy: 0.8786 - val_loss: 0.7618 - val_accuracy: 0.8405\n",
      "Epoch 22/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.4184 - accuracy: 0.8853 - val_loss: 0.6788 - val_accuracy: 0.8858\n",
      "Epoch 23/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.4063 - accuracy: 0.8900 - val_loss: 0.6560 - val_accuracy: 0.8810\n",
      "Epoch 24/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.4028 - accuracy: 0.8879 - val_loss: 0.6396 - val_accuracy: 0.8884\n",
      "Epoch 25/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.3931 - accuracy: 0.8903 - val_loss: 0.6311 - val_accuracy: 0.8653\n",
      "Epoch 26/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.3931 - accuracy: 0.8911 - val_loss: 0.6292 - val_accuracy: 0.8893\n",
      "Epoch 27/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.3904 - accuracy: 0.8925 - val_loss: 0.7195 - val_accuracy: 0.8772\n",
      "Epoch 28/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.3724 - accuracy: 0.8987 - val_loss: 0.6333 - val_accuracy: 0.8777\n",
      "Epoch 29/40\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.3783 - accuracy: 0.8953 - val_loss: 0.6420 - val_accuracy: 0.8708\n",
      "Epoch 30/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.3676 - accuracy: 0.9014 - val_loss: 0.5761 - val_accuracy: 0.8906\n",
      "Epoch 31/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.3536 - accuracy: 0.9053 - val_loss: 0.5859 - val_accuracy: 0.9034\n",
      "Epoch 32/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.3655 - accuracy: 0.9006 - val_loss: 0.6007 - val_accuracy: 0.8970\n",
      "Epoch 33/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.3440 - accuracy: 0.9069 - val_loss: 0.6000 - val_accuracy: 0.8882\n",
      "Epoch 34/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.3490 - accuracy: 0.9050 - val_loss: 0.5614 - val_accuracy: 0.9074\n",
      "Epoch 35/40\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.3503 - accuracy: 0.9056 - val_loss: 0.5587 - val_accuracy: 0.8953\n",
      "Epoch 36/40\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.3386 - accuracy: 0.9095 - val_loss: 0.6158 - val_accuracy: 0.8878\n",
      "Epoch 37/40\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.3360 - accuracy: 0.9130 - val_loss: 0.5439 - val_accuracy: 0.9182\n",
      "Epoch 38/40\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.3371 - accuracy: 0.9099 - val_loss: 0.5505 - val_accuracy: 0.9080\n",
      "Epoch 39/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.3272 - accuracy: 0.9093 - val_loss: 0.5703 - val_accuracy: 0.9061\n",
      "Epoch 40/40\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.3257 - accuracy: 0.9121 - val_loss: 0.5275 - val_accuracy: 0.9085\n",
      "Average Validation Accuracy: 0.9205967783927917\n",
      "Average Validation Loss: 0.3562300205230713\n",
      "Average Test Accuracy: 0.9160462915897369\n",
      "Final Test Accuracy for each fold: 0.9211321473121643\n",
      "Number of input features: 4\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "1846/1846 [==============================] - 13s 6ms/step - loss: 3.6316 - accuracy: 0.3175 - val_loss: 2.5917 - val_accuracy: 0.5424\n",
      "Epoch 2/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 1.8464 - accuracy: 0.6543 - val_loss: 1.7849 - val_accuracy: 0.7045\n",
      "Epoch 3/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 1.1563 - accuracy: 0.7647 - val_loss: 1.2744 - val_accuracy: 0.7714\n",
      "Epoch 4/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.8351 - accuracy: 0.8160 - val_loss: 0.9919 - val_accuracy: 0.8152\n",
      "Epoch 5/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.6650 - accuracy: 0.8431 - val_loss: 0.8573 - val_accuracy: 0.8332\n",
      "Epoch 6/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.5715 - accuracy: 0.8586 - val_loss: 0.7759 - val_accuracy: 0.8590\n",
      "Epoch 7/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.5113 - accuracy: 0.8684 - val_loss: 0.7073 - val_accuracy: 0.8625\n",
      "Epoch 8/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.4684 - accuracy: 0.8731 - val_loss: 0.7027 - val_accuracy: 0.8473\n",
      "Epoch 9/40\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.4462 - accuracy: 0.8811 - val_loss: 0.6358 - val_accuracy: 0.8609\n",
      "Epoch 10/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.4154 - accuracy: 0.8868 - val_loss: 0.5950 - val_accuracy: 0.8724\n",
      "Epoch 11/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.3978 - accuracy: 0.8897 - val_loss: 0.5708 - val_accuracy: 0.9058\n",
      "Epoch 12/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.3769 - accuracy: 0.8956 - val_loss: 0.6196 - val_accuracy: 0.8704\n",
      "Epoch 13/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.3586 - accuracy: 0.9044 - val_loss: 0.5691 - val_accuracy: 0.8880\n",
      "Epoch 14/40\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.3525 - accuracy: 0.9033 - val_loss: 0.6366 - val_accuracy: 0.8724\n",
      "Epoch 15/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.3410 - accuracy: 0.9059 - val_loss: 0.5454 - val_accuracy: 0.8926\n",
      "Epoch 16/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.3327 - accuracy: 0.9107 - val_loss: 0.4854 - val_accuracy: 0.9131\n",
      "Epoch 17/40\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 0.3231 - accuracy: 0.9131 - val_loss: 0.5009 - val_accuracy: 0.9034\n",
      "Epoch 18/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.3171 - accuracy: 0.9148 - val_loss: 0.5035 - val_accuracy: 0.9094\n",
      "Epoch 19/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.3058 - accuracy: 0.9187 - val_loss: 0.4750 - val_accuracy: 0.9076\n",
      "Epoch 20/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2954 - accuracy: 0.9213 - val_loss: 0.6117 - val_accuracy: 0.8843\n",
      "Epoch 21/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2992 - accuracy: 0.9208 - val_loss: 0.4499 - val_accuracy: 0.9184\n",
      "Epoch 22/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2911 - accuracy: 0.9226 - val_loss: 0.4472 - val_accuracy: 0.9168\n",
      "Epoch 23/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2819 - accuracy: 0.9245 - val_loss: 0.4174 - val_accuracy: 0.9289\n",
      "Epoch 24/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2772 - accuracy: 0.9274 - val_loss: 0.4731 - val_accuracy: 0.9047\n",
      "Epoch 25/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2735 - accuracy: 0.9280 - val_loss: 0.4383 - val_accuracy: 0.9217\n",
      "Epoch 26/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2654 - accuracy: 0.9301 - val_loss: 0.4020 - val_accuracy: 0.9347\n",
      "Epoch 27/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2633 - accuracy: 0.9300 - val_loss: 0.4032 - val_accuracy: 0.9369\n",
      "Epoch 28/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2567 - accuracy: 0.9343 - val_loss: 0.4188 - val_accuracy: 0.9232\n",
      "Epoch 29/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2599 - accuracy: 0.9310 - val_loss: 0.4061 - val_accuracy: 0.9168\n",
      "Epoch 30/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2556 - accuracy: 0.9332 - val_loss: 0.4352 - val_accuracy: 0.9190\n",
      "Epoch 31/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2413 - accuracy: 0.9379 - val_loss: 0.3976 - val_accuracy: 0.9234\n",
      "Epoch 32/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2418 - accuracy: 0.9370 - val_loss: 0.3673 - val_accuracy: 0.9494\n",
      "Epoch 33/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2442 - accuracy: 0.9352 - val_loss: 0.3852 - val_accuracy: 0.9298\n",
      "Epoch 34/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2327 - accuracy: 0.9392 - val_loss: 0.3890 - val_accuracy: 0.9351\n",
      "Epoch 35/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2306 - accuracy: 0.9415 - val_loss: 0.3942 - val_accuracy: 0.9404\n",
      "Epoch 36/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2293 - accuracy: 0.9397 - val_loss: 0.3584 - val_accuracy: 0.9573\n",
      "Epoch 37/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2351 - accuracy: 0.9384 - val_loss: 0.3916 - val_accuracy: 0.9437\n",
      "Epoch 38/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2312 - accuracy: 0.9392 - val_loss: 0.4159 - val_accuracy: 0.9289\n",
      "Epoch 39/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2224 - accuracy: 0.9452 - val_loss: 0.3788 - val_accuracy: 0.9395\n",
      "Epoch 40/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2226 - accuracy: 0.9437 - val_loss: 0.3526 - val_accuracy: 0.9529\n",
      "Fold: 2\n",
      "Epoch 1/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 3.7072 - accuracy: 0.2912 - val_loss: 2.6367 - val_accuracy: 0.5677\n",
      "Epoch 2/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 1.7936 - accuracy: 0.6529 - val_loss: 1.6049 - val_accuracy: 0.7212\n",
      "Epoch 3/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 1.0979 - accuracy: 0.7644 - val_loss: 1.1995 - val_accuracy: 0.7815\n",
      "Epoch 4/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.8084 - accuracy: 0.8145 - val_loss: 1.0464 - val_accuracy: 0.7872\n",
      "Epoch 5/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.6639 - accuracy: 0.8378 - val_loss: 0.8285 - val_accuracy: 0.8596\n",
      "Epoch 6/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.5737 - accuracy: 0.8555 - val_loss: 0.7683 - val_accuracy: 0.8631\n",
      "Epoch 7/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.5166 - accuracy: 0.8657 - val_loss: 0.7235 - val_accuracy: 0.8638\n",
      "Epoch 8/40\n",
      "1846/1846 [==============================] - 6s 4ms/step - loss: 0.4775 - accuracy: 0.8735 - val_loss: 0.6536 - val_accuracy: 0.8554\n",
      "Epoch 9/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.4391 - accuracy: 0.8849 - val_loss: 0.6648 - val_accuracy: 0.8664\n",
      "Epoch 10/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.4184 - accuracy: 0.8870 - val_loss: 0.6346 - val_accuracy: 0.8871\n",
      "Epoch 11/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.3999 - accuracy: 0.8934 - val_loss: 0.6256 - val_accuracy: 0.8788\n",
      "Epoch 12/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.3808 - accuracy: 0.8977 - val_loss: 0.5820 - val_accuracy: 0.8878\n",
      "Epoch 13/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.3597 - accuracy: 0.9022 - val_loss: 0.6310 - val_accuracy: 0.8768\n",
      "Epoch 14/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.3636 - accuracy: 0.9041 - val_loss: 0.5533 - val_accuracy: 0.8961\n",
      "Epoch 15/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3410 - accuracy: 0.9087 - val_loss: 0.5348 - val_accuracy: 0.9047\n",
      "Epoch 16/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.3399 - accuracy: 0.9098 - val_loss: 0.5312 - val_accuracy: 0.8906\n",
      "Epoch 17/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.3224 - accuracy: 0.9129 - val_loss: 0.5066 - val_accuracy: 0.9078\n",
      "Epoch 18/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.3126 - accuracy: 0.9173 - val_loss: 0.4957 - val_accuracy: 0.9166\n",
      "Epoch 19/40\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 0.3121 - accuracy: 0.9184 - val_loss: 0.5065 - val_accuracy: 0.9034\n",
      "Epoch 20/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2994 - accuracy: 0.9205 - val_loss: 0.5454 - val_accuracy: 0.8889\n",
      "Epoch 21/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2955 - accuracy: 0.9205 - val_loss: 0.5666 - val_accuracy: 0.8968\n",
      "Epoch 22/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2786 - accuracy: 0.9277 - val_loss: 0.5222 - val_accuracy: 0.9096\n",
      "Epoch 23/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2799 - accuracy: 0.9292 - val_loss: 0.5326 - val_accuracy: 0.8913\n",
      "Epoch 24/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2802 - accuracy: 0.9253 - val_loss: 0.4744 - val_accuracy: 0.9149\n",
      "Epoch 25/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2745 - accuracy: 0.9267 - val_loss: 0.4713 - val_accuracy: 0.9206\n",
      "Epoch 26/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2654 - accuracy: 0.9307 - val_loss: 0.4748 - val_accuracy: 0.9252\n",
      "Epoch 27/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2582 - accuracy: 0.9328 - val_loss: 0.5235 - val_accuracy: 0.9050\n",
      "Epoch 28/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2651 - accuracy: 0.9317 - val_loss: 0.5045 - val_accuracy: 0.9047\n",
      "Epoch 29/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2539 - accuracy: 0.9350 - val_loss: 0.5047 - val_accuracy: 0.9223\n",
      "Epoch 30/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2442 - accuracy: 0.9385 - val_loss: 0.4662 - val_accuracy: 0.9344\n",
      "Epoch 31/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2551 - accuracy: 0.9350 - val_loss: 0.5647 - val_accuracy: 0.8981\n",
      "Epoch 32/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2448 - accuracy: 0.9374 - val_loss: 0.4310 - val_accuracy: 0.9417\n",
      "Epoch 33/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2374 - accuracy: 0.9404 - val_loss: 0.4972 - val_accuracy: 0.9120\n",
      "Epoch 34/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2491 - accuracy: 0.9348 - val_loss: 0.4766 - val_accuracy: 0.9270\n",
      "Epoch 35/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2339 - accuracy: 0.9399 - val_loss: 0.4960 - val_accuracy: 0.9270\n",
      "Epoch 36/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2417 - accuracy: 0.9363 - val_loss: 0.4456 - val_accuracy: 0.9382\n",
      "Epoch 37/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2361 - accuracy: 0.9387 - val_loss: 0.6403 - val_accuracy: 0.9006\n",
      "Epoch 38/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2291 - accuracy: 0.9432 - val_loss: 0.4654 - val_accuracy: 0.9265\n",
      "Epoch 39/40\n",
      "1846/1846 [==============================] - 6s 4ms/step - loss: 0.2314 - accuracy: 0.9423 - val_loss: 0.4407 - val_accuracy: 0.9329\n",
      "Epoch 40/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2264 - accuracy: 0.9418 - val_loss: 0.5141 - val_accuracy: 0.9003\n",
      "Average Validation Accuracy: 0.9308708012104034\n",
      "Average Validation Loss: 0.2987059950828552\n",
      "Average Test Accuracy: 0.9274710714817047\n",
      "Final Test Accuracy for each fold: 0.9500257968902588\n",
      "Number of input features: 5\n",
      "Fold: 1\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1846/1846 [==============================] - 6s 3ms/step - loss: 3.5496 - accuracy: 0.3386 - val_loss: 2.3777 - val_accuracy: 0.5802\n",
      "Epoch 2/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 1.6102 - accuracy: 0.6954 - val_loss: 1.3585 - val_accuracy: 0.7481\n",
      "Epoch 3/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.9198 - accuracy: 0.8097 - val_loss: 1.2013 - val_accuracy: 0.7930\n",
      "Epoch 4/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.6350 - accuracy: 0.8552 - val_loss: 0.8217 - val_accuracy: 0.8486\n",
      "Epoch 5/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.4973 - accuracy: 0.8806 - val_loss: 0.6994 - val_accuracy: 0.8647\n",
      "Epoch 6/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.4222 - accuracy: 0.8939 - val_loss: 0.6137 - val_accuracy: 0.8728\n",
      "Epoch 7/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.3762 - accuracy: 0.9030 - val_loss: 0.5388 - val_accuracy: 0.8990\n",
      "Epoch 8/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.3436 - accuracy: 0.9098 - val_loss: 0.5524 - val_accuracy: 0.8939\n",
      "Epoch 9/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.3176 - accuracy: 0.9160 - val_loss: 0.5128 - val_accuracy: 0.9076\n",
      "Epoch 10/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2966 - accuracy: 0.9210 - val_loss: 0.5742 - val_accuracy: 0.8845\n",
      "Epoch 11/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2815 - accuracy: 0.9255 - val_loss: 0.4540 - val_accuracy: 0.9245\n",
      "Epoch 12/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2742 - accuracy: 0.9273 - val_loss: 0.4952 - val_accuracy: 0.8983\n",
      "Epoch 13/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2602 - accuracy: 0.9325 - val_loss: 0.4267 - val_accuracy: 0.9380\n",
      "Epoch 14/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2503 - accuracy: 0.9368 - val_loss: 0.4019 - val_accuracy: 0.9358\n",
      "Epoch 15/40\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 0.2404 - accuracy: 0.9371 - val_loss: 0.4040 - val_accuracy: 0.9318\n",
      "Epoch 16/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2411 - accuracy: 0.9351 - val_loss: 0.3904 - val_accuracy: 0.9399\n",
      "Epoch 17/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2322 - accuracy: 0.9410 - val_loss: 0.3774 - val_accuracy: 0.9417\n",
      "Epoch 18/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2296 - accuracy: 0.9435 - val_loss: 0.3866 - val_accuracy: 0.9300\n",
      "Epoch 19/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2196 - accuracy: 0.9432 - val_loss: 0.3962 - val_accuracy: 0.9322\n",
      "Epoch 20/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2131 - accuracy: 0.9429 - val_loss: 0.3398 - val_accuracy: 0.9531\n",
      "Epoch 21/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2105 - accuracy: 0.9455 - val_loss: 0.3698 - val_accuracy: 0.9355\n",
      "Epoch 22/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2044 - accuracy: 0.9480 - val_loss: 0.3556 - val_accuracy: 0.9443\n",
      "Epoch 23/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2066 - accuracy: 0.9469 - val_loss: 0.3839 - val_accuracy: 0.9402\n",
      "Epoch 24/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1993 - accuracy: 0.9491 - val_loss: 0.3985 - val_accuracy: 0.9294\n",
      "Epoch 25/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1993 - accuracy: 0.9502 - val_loss: 0.3360 - val_accuracy: 0.9503\n",
      "Epoch 26/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1941 - accuracy: 0.9502 - val_loss: 0.3629 - val_accuracy: 0.9402\n",
      "Epoch 27/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1863 - accuracy: 0.9539 - val_loss: 0.3510 - val_accuracy: 0.9399\n",
      "Epoch 28/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1852 - accuracy: 0.9548 - val_loss: 0.3332 - val_accuracy: 0.9483\n",
      "Epoch 29/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1879 - accuracy: 0.9517 - val_loss: 0.3370 - val_accuracy: 0.9435\n",
      "Epoch 30/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1865 - accuracy: 0.9523 - val_loss: 0.3225 - val_accuracy: 0.9536\n",
      "Epoch 31/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1839 - accuracy: 0.9543 - val_loss: 0.3647 - val_accuracy: 0.9421\n",
      "Epoch 32/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1729 - accuracy: 0.9574 - val_loss: 0.3416 - val_accuracy: 0.9562\n",
      "Epoch 33/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1824 - accuracy: 0.9524 - val_loss: 0.3295 - val_accuracy: 0.9553\n",
      "Epoch 34/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1783 - accuracy: 0.9542 - val_loss: 0.3265 - val_accuracy: 0.9443\n",
      "Epoch 35/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1739 - accuracy: 0.9561 - val_loss: 0.3342 - val_accuracy: 0.9516\n",
      "Epoch 36/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1694 - accuracy: 0.9570 - val_loss: 0.3395 - val_accuracy: 0.9375\n",
      "Epoch 37/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1716 - accuracy: 0.9594 - val_loss: 0.3245 - val_accuracy: 0.9406\n",
      "Epoch 38/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1710 - accuracy: 0.9564 - val_loss: 0.3017 - val_accuracy: 0.9549\n",
      "Epoch 39/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1680 - accuracy: 0.9600 - val_loss: 0.2876 - val_accuracy: 0.9672\n",
      "Epoch 40/40\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 0.1690 - accuracy: 0.9579 - val_loss: 0.2913 - val_accuracy: 0.9538\n",
      "Fold: 2\n",
      "Epoch 1/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 3.5549 - accuracy: 0.3399 - val_loss: 2.4400 - val_accuracy: 0.5795\n",
      "Epoch 2/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 1.6553 - accuracy: 0.6970 - val_loss: 1.4618 - val_accuracy: 0.7514\n",
      "Epoch 3/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.9500 - accuracy: 0.8033 - val_loss: 1.0278 - val_accuracy: 0.8253\n",
      "Epoch 4/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.6365 - accuracy: 0.8523 - val_loss: 0.8375 - val_accuracy: 0.8464\n",
      "Epoch 5/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.4830 - accuracy: 0.8795 - val_loss: 0.7023 - val_accuracy: 0.8680\n",
      "Epoch 6/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.4079 - accuracy: 0.8975 - val_loss: 0.6775 - val_accuracy: 0.8858\n",
      "Epoch 7/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.3576 - accuracy: 0.9072 - val_loss: 0.5522 - val_accuracy: 0.8975\n",
      "Epoch 8/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.3279 - accuracy: 0.9150 - val_loss: 0.5630 - val_accuracy: 0.8840\n",
      "Epoch 9/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.3014 - accuracy: 0.9220 - val_loss: 0.5256 - val_accuracy: 0.8926\n",
      "Epoch 10/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2799 - accuracy: 0.9267 - val_loss: 0.4633 - val_accuracy: 0.9063\n",
      "Epoch 11/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2686 - accuracy: 0.9320 - val_loss: 0.4530 - val_accuracy: 0.9151\n",
      "Epoch 12/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2537 - accuracy: 0.9357 - val_loss: 0.4236 - val_accuracy: 0.9173\n",
      "Epoch 13/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2564 - accuracy: 0.9345 - val_loss: 0.4278 - val_accuracy: 0.9245\n",
      "Epoch 14/40\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 0.2342 - accuracy: 0.9417 - val_loss: 0.4005 - val_accuracy: 0.9364\n",
      "Epoch 15/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2338 - accuracy: 0.9376 - val_loss: 0.3677 - val_accuracy: 0.9498\n",
      "Epoch 16/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2242 - accuracy: 0.9448 - val_loss: 0.3861 - val_accuracy: 0.9340\n",
      "Epoch 17/40\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 0.2247 - accuracy: 0.9419 - val_loss: 0.3946 - val_accuracy: 0.9254\n",
      "Epoch 18/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2145 - accuracy: 0.9461 - val_loss: 0.4039 - val_accuracy: 0.9245\n",
      "Epoch 19/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2111 - accuracy: 0.9466 - val_loss: 0.3683 - val_accuracy: 0.9386\n",
      "Epoch 20/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2041 - accuracy: 0.9499 - val_loss: 0.3565 - val_accuracy: 0.9439\n",
      "Epoch 21/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2004 - accuracy: 0.9511 - val_loss: 0.5201 - val_accuracy: 0.9149\n",
      "Epoch 22/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2057 - accuracy: 0.9485 - val_loss: 0.4002 - val_accuracy: 0.9353\n",
      "Epoch 23/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1966 - accuracy: 0.9508 - val_loss: 0.3426 - val_accuracy: 0.9448\n",
      "Epoch 24/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1916 - accuracy: 0.9514 - val_loss: 0.3263 - val_accuracy: 0.9446\n",
      "Epoch 25/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1947 - accuracy: 0.9527 - val_loss: 0.3246 - val_accuracy: 0.9492\n",
      "Epoch 26/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1800 - accuracy: 0.9563 - val_loss: 0.3758 - val_accuracy: 0.9296\n",
      "Epoch 27/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1856 - accuracy: 0.9523 - val_loss: 0.3243 - val_accuracy: 0.9505\n",
      "Epoch 28/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1819 - accuracy: 0.9559 - val_loss: 0.3655 - val_accuracy: 0.9364\n",
      "Epoch 29/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1762 - accuracy: 0.9555 - val_loss: 0.4636 - val_accuracy: 0.9085\n",
      "Epoch 30/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1836 - accuracy: 0.9542 - val_loss: 0.3181 - val_accuracy: 0.9538\n",
      "Epoch 31/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1748 - accuracy: 0.9565 - val_loss: 0.3492 - val_accuracy: 0.9250\n",
      "Epoch 32/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1723 - accuracy: 0.9562 - val_loss: 0.3444 - val_accuracy: 0.9329\n",
      "Epoch 33/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1777 - accuracy: 0.9543 - val_loss: 0.3232 - val_accuracy: 0.9545\n",
      "Epoch 34/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1721 - accuracy: 0.9566 - val_loss: 0.3308 - val_accuracy: 0.9388\n",
      "Epoch 35/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1689 - accuracy: 0.9569 - val_loss: 0.3216 - val_accuracy: 0.9494\n",
      "Epoch 36/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1659 - accuracy: 0.9589 - val_loss: 0.3054 - val_accuracy: 0.9580\n",
      "Epoch 37/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1686 - accuracy: 0.9591 - val_loss: 0.3391 - val_accuracy: 0.9349\n",
      "Epoch 38/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1638 - accuracy: 0.9577 - val_loss: 0.3317 - val_accuracy: 0.9553\n",
      "Epoch 39/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1654 - accuracy: 0.9579 - val_loss: 0.2928 - val_accuracy: 0.9597\n",
      "Epoch 40/40\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 0.1638 - accuracy: 0.9597 - val_loss: 0.3060 - val_accuracy: 0.9564\n",
      "Average Validation Accuracy: 0.9627129137516022\n",
      "Average Validation Loss: 0.19198614358901978\n",
      "Average Test Accuracy: 0.9636249840259552\n",
      "Final Test Accuracy for each fold: 0.9649885892868042\n",
      "Number of input features: 6\n",
      "Fold: 1\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1846/1846 [==============================] - 5s 2ms/step - loss: 3.6881 - accuracy: 0.3322 - val_loss: 2.5620 - val_accuracy: 0.5842\n",
      "Epoch 2/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 1.7325 - accuracy: 0.6960 - val_loss: 1.4817 - val_accuracy: 0.7408\n",
      "Epoch 3/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.9597 - accuracy: 0.8072 - val_loss: 0.9861 - val_accuracy: 0.8108\n",
      "Epoch 4/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.6233 - accuracy: 0.8651 - val_loss: 0.7389 - val_accuracy: 0.8739\n",
      "Epoch 5/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.4635 - accuracy: 0.8958 - val_loss: 0.6322 - val_accuracy: 0.8911\n",
      "Epoch 6/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.3770 - accuracy: 0.9138 - val_loss: 0.5426 - val_accuracy: 0.9215\n",
      "Epoch 7/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.3228 - accuracy: 0.9262 - val_loss: 0.5047 - val_accuracy: 0.9256\n",
      "Epoch 8/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2814 - accuracy: 0.9351 - val_loss: 0.4624 - val_accuracy: 0.9250\n",
      "Epoch 9/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2552 - accuracy: 0.9431 - val_loss: 0.4317 - val_accuracy: 0.9190\n",
      "Epoch 10/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2377 - accuracy: 0.9422 - val_loss: 0.4149 - val_accuracy: 0.9404\n",
      "Epoch 11/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2241 - accuracy: 0.9439 - val_loss: 0.3929 - val_accuracy: 0.9424\n",
      "Epoch 12/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2059 - accuracy: 0.9497 - val_loss: 0.3929 - val_accuracy: 0.9338\n",
      "Epoch 13/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1991 - accuracy: 0.9511 - val_loss: 0.3537 - val_accuracy: 0.9534\n",
      "Epoch 14/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1878 - accuracy: 0.9571 - val_loss: 0.3497 - val_accuracy: 0.9424\n",
      "Epoch 15/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1907 - accuracy: 0.9508 - val_loss: 0.3784 - val_accuracy: 0.9417\n",
      "Epoch 16/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1814 - accuracy: 0.9553 - val_loss: 0.3263 - val_accuracy: 0.9512\n",
      "Epoch 17/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1714 - accuracy: 0.9583 - val_loss: 0.3212 - val_accuracy: 0.9558\n",
      "Epoch 18/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1729 - accuracy: 0.9595 - val_loss: 0.3024 - val_accuracy: 0.9586\n",
      "Epoch 19/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1736 - accuracy: 0.9583 - val_loss: 0.2909 - val_accuracy: 0.9699\n",
      "Epoch 20/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1555 - accuracy: 0.9643 - val_loss: 0.3383 - val_accuracy: 0.9474\n",
      "Epoch 21/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1591 - accuracy: 0.9615 - val_loss: 0.2910 - val_accuracy: 0.9600\n",
      "Epoch 22/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1558 - accuracy: 0.9637 - val_loss: 0.2966 - val_accuracy: 0.9619\n",
      "Epoch 23/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1481 - accuracy: 0.9659 - val_loss: 0.2952 - val_accuracy: 0.9589\n",
      "Epoch 24/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1501 - accuracy: 0.9650 - val_loss: 0.2964 - val_accuracy: 0.9556\n",
      "Epoch 25/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1469 - accuracy: 0.9642 - val_loss: 0.2766 - val_accuracy: 0.9630\n",
      "Epoch 26/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1450 - accuracy: 0.9672 - val_loss: 0.2958 - val_accuracy: 0.9578\n",
      "Epoch 27/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1389 - accuracy: 0.9679 - val_loss: 0.3046 - val_accuracy: 0.9589\n",
      "Epoch 28/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1392 - accuracy: 0.9676 - val_loss: 0.2953 - val_accuracy: 0.9593\n",
      "Epoch 29/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1427 - accuracy: 0.9661 - val_loss: 0.2737 - val_accuracy: 0.9659\n",
      "Epoch 30/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1316 - accuracy: 0.9692 - val_loss: 0.2754 - val_accuracy: 0.9639\n",
      "Epoch 31/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1335 - accuracy: 0.9697 - val_loss: 0.2763 - val_accuracy: 0.9716\n",
      "Epoch 32/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1382 - accuracy: 0.9662 - val_loss: 0.2881 - val_accuracy: 0.9657\n",
      "Epoch 33/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1298 - accuracy: 0.9704 - val_loss: 0.3191 - val_accuracy: 0.9503\n",
      "Epoch 34/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1293 - accuracy: 0.9702 - val_loss: 0.2677 - val_accuracy: 0.9712\n",
      "Epoch 35/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1274 - accuracy: 0.9714 - val_loss: 0.2932 - val_accuracy: 0.9655\n",
      "Epoch 36/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1272 - accuracy: 0.9715 - val_loss: 0.3107 - val_accuracy: 0.9569\n",
      "Epoch 37/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1220 - accuracy: 0.9717 - val_loss: 0.2707 - val_accuracy: 0.9659\n",
      "Epoch 38/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1251 - accuracy: 0.9702 - val_loss: 0.2903 - val_accuracy: 0.9652\n",
      "Epoch 39/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1268 - accuracy: 0.9718 - val_loss: 0.2953 - val_accuracy: 0.9604\n",
      "Epoch 40/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1252 - accuracy: 0.9713 - val_loss: 0.2761 - val_accuracy: 0.9646\n",
      "Fold: 2\n",
      "Epoch 1/40\n",
      "1846/1846 [==============================] - 7s 3ms/step - loss: 3.7733 - accuracy: 0.2992 - val_loss: 2.6343 - val_accuracy: 0.5263\n",
      "Epoch 2/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 1.7148 - accuracy: 0.6933 - val_loss: 1.5727 - val_accuracy: 0.7562\n",
      "Epoch 3/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.9537 - accuracy: 0.8048 - val_loss: 1.0675 - val_accuracy: 0.8363\n",
      "Epoch 4/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.6316 - accuracy: 0.8572 - val_loss: 0.8599 - val_accuracy: 0.8647\n",
      "Epoch 5/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.4762 - accuracy: 0.8899 - val_loss: 0.7062 - val_accuracy: 0.8917\n",
      "Epoch 6/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.3946 - accuracy: 0.9031 - val_loss: 0.5990 - val_accuracy: 0.8999\n",
      "Epoch 7/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.3275 - accuracy: 0.9206 - val_loss: 0.5352 - val_accuracy: 0.9017\n",
      "Epoch 8/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2916 - accuracy: 0.9268 - val_loss: 0.4830 - val_accuracy: 0.9230\n",
      "Epoch 9/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2684 - accuracy: 0.9315 - val_loss: 0.4580 - val_accuracy: 0.9201\n",
      "Epoch 10/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2477 - accuracy: 0.9388 - val_loss: 0.4109 - val_accuracy: 0.9347\n",
      "Epoch 11/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2286 - accuracy: 0.9435 - val_loss: 0.4108 - val_accuracy: 0.9272\n",
      "Epoch 12/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2164 - accuracy: 0.9446 - val_loss: 0.3518 - val_accuracy: 0.9452\n",
      "Epoch 13/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2028 - accuracy: 0.9507 - val_loss: 0.3880 - val_accuracy: 0.9395\n",
      "Epoch 14/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2059 - accuracy: 0.9492 - val_loss: 0.3537 - val_accuracy: 0.9366\n",
      "Epoch 15/40\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 0.1867 - accuracy: 0.9551 - val_loss: 0.3221 - val_accuracy: 0.9481\n",
      "Epoch 16/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1862 - accuracy: 0.9571 - val_loss: 0.3192 - val_accuracy: 0.9446\n",
      "Epoch 17/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1815 - accuracy: 0.9557 - val_loss: 0.3190 - val_accuracy: 0.9481\n",
      "Epoch 18/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1723 - accuracy: 0.9584 - val_loss: 0.3500 - val_accuracy: 0.9397\n",
      "Epoch 19/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1678 - accuracy: 0.9591 - val_loss: 0.3054 - val_accuracy: 0.9479\n",
      "Epoch 20/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1697 - accuracy: 0.9596 - val_loss: 0.3144 - val_accuracy: 0.9505\n",
      "Epoch 21/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1610 - accuracy: 0.9613 - val_loss: 0.3282 - val_accuracy: 0.9384\n",
      "Epoch 22/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1537 - accuracy: 0.9635 - val_loss: 0.3040 - val_accuracy: 0.9421\n",
      "Epoch 23/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1626 - accuracy: 0.9614 - val_loss: 0.2841 - val_accuracy: 0.9551\n",
      "Epoch 24/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1446 - accuracy: 0.9652 - val_loss: 0.2899 - val_accuracy: 0.9534\n",
      "Epoch 25/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1503 - accuracy: 0.9638 - val_loss: 0.2565 - val_accuracy: 0.9666\n",
      "Epoch 26/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1420 - accuracy: 0.9677 - val_loss: 0.2613 - val_accuracy: 0.9604\n",
      "Epoch 27/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1415 - accuracy: 0.9671 - val_loss: 0.2661 - val_accuracy: 0.9611\n",
      "Epoch 28/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1382 - accuracy: 0.9678 - val_loss: 0.2737 - val_accuracy: 0.9597\n",
      "Epoch 29/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1374 - accuracy: 0.9674 - val_loss: 0.2762 - val_accuracy: 0.9652\n",
      "Epoch 30/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1408 - accuracy: 0.9669 - val_loss: 0.2666 - val_accuracy: 0.9611\n",
      "Epoch 31/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1431 - accuracy: 0.9675 - val_loss: 0.2808 - val_accuracy: 0.9498\n",
      "Epoch 32/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1254 - accuracy: 0.9716 - val_loss: 0.2677 - val_accuracy: 0.9611\n",
      "Epoch 33/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1302 - accuracy: 0.9693 - val_loss: 0.2503 - val_accuracy: 0.9657\n",
      "Epoch 34/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1325 - accuracy: 0.9698 - val_loss: 0.2562 - val_accuracy: 0.9652\n",
      "Epoch 35/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1278 - accuracy: 0.9690 - val_loss: 0.2505 - val_accuracy: 0.9648\n",
      "Epoch 36/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1317 - accuracy: 0.9697 - val_loss: 0.2828 - val_accuracy: 0.9571\n",
      "Epoch 37/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1264 - accuracy: 0.9707 - val_loss: 0.2954 - val_accuracy: 0.9514\n",
      "Epoch 38/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1188 - accuracy: 0.9725 - val_loss: 0.2697 - val_accuracy: 0.9582\n",
      "Epoch 39/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1246 - accuracy: 0.9708 - val_loss: 0.2357 - val_accuracy: 0.9646\n",
      "Epoch 40/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1179 - accuracy: 0.9727 - val_loss: 0.2549 - val_accuracy: 0.9626\n",
      "Average Validation Accuracy: 0.9711360335350037\n",
      "Average Validation Loss: 0.16492432355880737\n",
      "Average Test Accuracy: 0.9728753864765167\n",
      "Final Test Accuracy for each fold: 0.9741284251213074\n",
      "Number of input features: 7\n",
      "Fold: 1\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1846/1846 [==============================] - 5s 2ms/step - loss: 3.6531 - accuracy: 0.3151 - val_loss: 2.4177 - val_accuracy: 0.5734\n",
      "Epoch 2/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 1.5938 - accuracy: 0.7031 - val_loss: 1.3016 - val_accuracy: 0.7624\n",
      "Epoch 3/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.8490 - accuracy: 0.8226 - val_loss: 0.8799 - val_accuracy: 0.8537\n",
      "Epoch 4/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.5366 - accuracy: 0.8762 - val_loss: 0.6766 - val_accuracy: 0.8891\n",
      "Epoch 5/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.4062 - accuracy: 0.9010 - val_loss: 0.5670 - val_accuracy: 0.9052\n",
      "Epoch 6/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.3384 - accuracy: 0.9176 - val_loss: 0.4964 - val_accuracy: 0.9052\n",
      "Epoch 7/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2891 - accuracy: 0.9277 - val_loss: 0.4405 - val_accuracy: 0.9250\n",
      "Epoch 8/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2664 - accuracy: 0.9320 - val_loss: 0.4231 - val_accuracy: 0.9164\n",
      "Epoch 9/40\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 0.2348 - accuracy: 0.9426 - val_loss: 0.4039 - val_accuracy: 0.9338\n",
      "Epoch 10/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2228 - accuracy: 0.9416 - val_loss: 0.3790 - val_accuracy: 0.9426\n",
      "Epoch 11/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2059 - accuracy: 0.9497 - val_loss: 0.3379 - val_accuracy: 0.9404\n",
      "Epoch 12/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1926 - accuracy: 0.9510 - val_loss: 0.3069 - val_accuracy: 0.9560\n",
      "Epoch 13/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1909 - accuracy: 0.9518 - val_loss: 0.3707 - val_accuracy: 0.9252\n",
      "Epoch 14/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1829 - accuracy: 0.9573 - val_loss: 0.3031 - val_accuracy: 0.9496\n",
      "Epoch 15/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1742 - accuracy: 0.9573 - val_loss: 0.3107 - val_accuracy: 0.9476\n",
      "Epoch 16/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1710 - accuracy: 0.9589 - val_loss: 0.3129 - val_accuracy: 0.9474\n",
      "Epoch 17/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1632 - accuracy: 0.9621 - val_loss: 0.2975 - val_accuracy: 0.9586\n",
      "Epoch 18/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1642 - accuracy: 0.9613 - val_loss: 0.3811 - val_accuracy: 0.9274\n",
      "Epoch 19/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1553 - accuracy: 0.9636 - val_loss: 0.2613 - val_accuracy: 0.9615\n",
      "Epoch 20/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1529 - accuracy: 0.9641 - val_loss: 0.2601 - val_accuracy: 0.9600\n",
      "Epoch 21/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1562 - accuracy: 0.9625 - val_loss: 0.2765 - val_accuracy: 0.9575\n",
      "Epoch 22/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1567 - accuracy: 0.9622 - val_loss: 0.3164 - val_accuracy: 0.9421\n",
      "Epoch 23/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1446 - accuracy: 0.9670 - val_loss: 0.2459 - val_accuracy: 0.9637\n",
      "Epoch 24/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1380 - accuracy: 0.9698 - val_loss: 0.2618 - val_accuracy: 0.9580\n",
      "Epoch 25/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1394 - accuracy: 0.9664 - val_loss: 0.2998 - val_accuracy: 0.9496\n",
      "Epoch 26/40\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 0.1386 - accuracy: 0.9693 - val_loss: 0.2591 - val_accuracy: 0.9710\n",
      "Epoch 27/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1346 - accuracy: 0.9697 - val_loss: 0.2611 - val_accuracy: 0.9534\n",
      "Epoch 28/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1335 - accuracy: 0.9686 - val_loss: 0.2363 - val_accuracy: 0.9712\n",
      "Epoch 29/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1317 - accuracy: 0.9702 - val_loss: 0.5776 - val_accuracy: 0.8950\n",
      "Epoch 30/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1283 - accuracy: 0.9717 - val_loss: 0.3464 - val_accuracy: 0.9388\n",
      "Epoch 31/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1301 - accuracy: 0.9708 - val_loss: 0.2339 - val_accuracy: 0.9626\n",
      "Epoch 32/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1284 - accuracy: 0.9697 - val_loss: 0.2554 - val_accuracy: 0.9712\n",
      "Epoch 33/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1257 - accuracy: 0.9726 - val_loss: 0.2414 - val_accuracy: 0.9683\n",
      "Epoch 34/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1243 - accuracy: 0.9717 - val_loss: 0.2405 - val_accuracy: 0.9758\n",
      "Epoch 35/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1200 - accuracy: 0.9736 - val_loss: 0.2436 - val_accuracy: 0.9646\n",
      "Epoch 36/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1170 - accuracy: 0.9733 - val_loss: 0.2355 - val_accuracy: 0.9714\n",
      "Epoch 37/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1267 - accuracy: 0.9711 - val_loss: 0.2531 - val_accuracy: 0.9751\n",
      "Epoch 38/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1162 - accuracy: 0.9752 - val_loss: 0.2439 - val_accuracy: 0.9663\n",
      "Epoch 39/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1244 - accuracy: 0.9717 - val_loss: 0.2748 - val_accuracy: 0.9564\n",
      "Epoch 40/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1191 - accuracy: 0.9738 - val_loss: 0.2293 - val_accuracy: 0.9705\n",
      "Fold: 2\n",
      "Epoch 1/40\n",
      "1846/1846 [==============================] - 7s 3ms/step - loss: 3.4958 - accuracy: 0.3592 - val_loss: 2.2337 - val_accuracy: 0.5978\n",
      "Epoch 2/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.4366 - accuracy: 0.7319 - val_loss: 1.2631 - val_accuracy: 0.8026\n",
      "Epoch 3/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.8079 - accuracy: 0.8367 - val_loss: 0.9047 - val_accuracy: 0.8411\n",
      "Epoch 4/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.5553 - accuracy: 0.8709 - val_loss: 0.7172 - val_accuracy: 0.8832\n",
      "Epoch 5/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4348 - accuracy: 0.8960 - val_loss: 0.6410 - val_accuracy: 0.8799\n",
      "Epoch 6/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.3640 - accuracy: 0.9124 - val_loss: 0.5531 - val_accuracy: 0.8990\n",
      "Epoch 7/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.3181 - accuracy: 0.9205 - val_loss: 0.5234 - val_accuracy: 0.8944\n",
      "Epoch 8/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2886 - accuracy: 0.9288 - val_loss: 0.4651 - val_accuracy: 0.9047\n",
      "Epoch 9/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2583 - accuracy: 0.9363 - val_loss: 0.4146 - val_accuracy: 0.9230\n",
      "Epoch 10/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2448 - accuracy: 0.9382 - val_loss: 0.3896 - val_accuracy: 0.9239\n",
      "Epoch 11/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2252 - accuracy: 0.9429 - val_loss: 0.3567 - val_accuracy: 0.9404\n",
      "Epoch 12/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2213 - accuracy: 0.9455 - val_loss: 0.4096 - val_accuracy: 0.9228\n",
      "Epoch 13/40\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 0.2121 - accuracy: 0.9464 - val_loss: 0.3349 - val_accuracy: 0.9487\n",
      "Epoch 14/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1916 - accuracy: 0.9525 - val_loss: 0.3280 - val_accuracy: 0.9408\n",
      "Epoch 15/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1976 - accuracy: 0.9512 - val_loss: 0.3286 - val_accuracy: 0.9472\n",
      "Epoch 16/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1866 - accuracy: 0.9561 - val_loss: 0.3064 - val_accuracy: 0.9551\n",
      "Epoch 17/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1758 - accuracy: 0.9579 - val_loss: 0.3226 - val_accuracy: 0.9417\n",
      "Epoch 18/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1819 - accuracy: 0.9571 - val_loss: 0.3235 - val_accuracy: 0.9311\n",
      "Epoch 19/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1675 - accuracy: 0.9602 - val_loss: 0.2924 - val_accuracy: 0.9481\n",
      "Epoch 20/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1674 - accuracy: 0.9602 - val_loss: 0.2877 - val_accuracy: 0.9468\n",
      "Epoch 21/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1591 - accuracy: 0.9627 - val_loss: 0.2824 - val_accuracy: 0.9549\n",
      "Epoch 22/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1533 - accuracy: 0.9645 - val_loss: 0.2880 - val_accuracy: 0.9527\n",
      "Epoch 23/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1544 - accuracy: 0.9640 - val_loss: 0.3072 - val_accuracy: 0.9494\n",
      "Epoch 24/40\n",
      "1846/1846 [==============================] - 6s 4ms/step - loss: 0.1500 - accuracy: 0.9650 - val_loss: 0.2773 - val_accuracy: 0.9509\n",
      "Epoch 25/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1525 - accuracy: 0.9630 - val_loss: 0.2708 - val_accuracy: 0.9582\n",
      "Epoch 26/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1440 - accuracy: 0.9656 - val_loss: 0.3140 - val_accuracy: 0.9384\n",
      "Epoch 27/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1405 - accuracy: 0.9679 - val_loss: 0.2524 - val_accuracy: 0.9564\n",
      "Epoch 28/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1367 - accuracy: 0.9690 - val_loss: 0.2487 - val_accuracy: 0.9529\n",
      "Epoch 29/40\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 0.1394 - accuracy: 0.9667 - val_loss: 0.2404 - val_accuracy: 0.9652\n",
      "Epoch 30/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1290 - accuracy: 0.9726 - val_loss: 0.2603 - val_accuracy: 0.9604\n",
      "Epoch 31/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1335 - accuracy: 0.9693 - val_loss: 0.2546 - val_accuracy: 0.9644\n",
      "Epoch 32/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1288 - accuracy: 0.9718 - val_loss: 0.2412 - val_accuracy: 0.9644\n",
      "Epoch 33/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1295 - accuracy: 0.9701 - val_loss: 0.2379 - val_accuracy: 0.9699\n",
      "Epoch 34/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1284 - accuracy: 0.9716 - val_loss: 0.2289 - val_accuracy: 0.9714\n",
      "Epoch 35/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1221 - accuracy: 0.9737 - val_loss: 0.2744 - val_accuracy: 0.9494\n",
      "Epoch 36/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1265 - accuracy: 0.9706 - val_loss: 0.2353 - val_accuracy: 0.9668\n",
      "Epoch 37/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1293 - accuracy: 0.9689 - val_loss: 0.2346 - val_accuracy: 0.9613\n",
      "Epoch 38/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1176 - accuracy: 0.9738 - val_loss: 0.2267 - val_accuracy: 0.9683\n",
      "Epoch 39/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1119 - accuracy: 0.9754 - val_loss: 0.2218 - val_accuracy: 0.9729\n",
      "Epoch 40/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1222 - accuracy: 0.9719 - val_loss: 0.2589 - val_accuracy: 0.9564\n",
      "Average Validation Accuracy: 0.9712811410427094\n",
      "Average Validation Loss: 0.15672780573368073\n",
      "Average Test Accuracy: 0.9715486168861389\n",
      "Final Test Accuracy for each fold: 0.9759711027145386\n",
      "Number of input features: 8\n",
      "Fold: 1\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1846/1846 [==============================] - 7s 3ms/step - loss: 3.4399 - accuracy: 0.3750 - val_loss: 2.1890 - val_accuracy: 0.6587\n",
      "Epoch 2/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 1.4686 - accuracy: 0.7392 - val_loss: 1.3174 - val_accuracy: 0.7822\n",
      "Epoch 3/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.8592 - accuracy: 0.8282 - val_loss: 0.9611 - val_accuracy: 0.8337\n",
      "Epoch 4/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.5782 - accuracy: 0.8769 - val_loss: 0.7408 - val_accuracy: 0.8592\n",
      "Epoch 5/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.4359 - accuracy: 0.9001 - val_loss: 0.6463 - val_accuracy: 0.8766\n",
      "Epoch 6/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.3496 - accuracy: 0.9188 - val_loss: 0.5520 - val_accuracy: 0.8986\n",
      "Epoch 7/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3027 - accuracy: 0.9268 - val_loss: 0.5100 - val_accuracy: 0.9171\n",
      "Epoch 8/40\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 0.2591 - accuracy: 0.9382 - val_loss: 0.4292 - val_accuracy: 0.9314\n",
      "Epoch 9/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2361 - accuracy: 0.9434 - val_loss: 0.4405 - val_accuracy: 0.9300\n",
      "Epoch 10/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2158 - accuracy: 0.9489 - val_loss: 0.4104 - val_accuracy: 0.9305\n",
      "Epoch 11/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2027 - accuracy: 0.9507 - val_loss: 0.3749 - val_accuracy: 0.9450\n",
      "Epoch 12/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1890 - accuracy: 0.9540 - val_loss: 0.3620 - val_accuracy: 0.9564\n",
      "Epoch 13/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1853 - accuracy: 0.9549 - val_loss: 0.3424 - val_accuracy: 0.9483\n",
      "Epoch 14/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1766 - accuracy: 0.9583 - val_loss: 0.3557 - val_accuracy: 0.9428\n",
      "Epoch 15/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1641 - accuracy: 0.9604 - val_loss: 0.3513 - val_accuracy: 0.9437\n",
      "Epoch 16/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1645 - accuracy: 0.9616 - val_loss: 0.3486 - val_accuracy: 0.9468\n",
      "Epoch 17/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1546 - accuracy: 0.9654 - val_loss: 0.3002 - val_accuracy: 0.9694\n",
      "Epoch 18/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1521 - accuracy: 0.9643 - val_loss: 0.3057 - val_accuracy: 0.9597\n",
      "Epoch 19/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1525 - accuracy: 0.9640 - val_loss: 0.2922 - val_accuracy: 0.9661\n",
      "Epoch 20/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1498 - accuracy: 0.9657 - val_loss: 0.3050 - val_accuracy: 0.9584\n",
      "Epoch 21/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1391 - accuracy: 0.9682 - val_loss: 0.3562 - val_accuracy: 0.9452\n",
      "Epoch 22/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1441 - accuracy: 0.9675 - val_loss: 0.2745 - val_accuracy: 0.9655\n",
      "Epoch 23/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1467 - accuracy: 0.9660 - val_loss: 0.2702 - val_accuracy: 0.9712\n",
      "Epoch 24/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1320 - accuracy: 0.9688 - val_loss: 0.2757 - val_accuracy: 0.9668\n",
      "Epoch 25/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1337 - accuracy: 0.9702 - val_loss: 0.2866 - val_accuracy: 0.9624\n",
      "Epoch 26/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1348 - accuracy: 0.9679 - val_loss: 0.3063 - val_accuracy: 0.9518\n",
      "Epoch 27/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1273 - accuracy: 0.9706 - val_loss: 0.3415 - val_accuracy: 0.9479\n",
      "Epoch 28/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1261 - accuracy: 0.9717 - val_loss: 0.2522 - val_accuracy: 0.9745\n",
      "Epoch 29/40\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 0.1290 - accuracy: 0.9699 - val_loss: 0.5648 - val_accuracy: 0.8834\n",
      "Epoch 30/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1269 - accuracy: 0.9710 - val_loss: 0.2642 - val_accuracy: 0.9650\n",
      "Epoch 31/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1235 - accuracy: 0.9719 - val_loss: 0.3247 - val_accuracy: 0.9470\n",
      "Epoch 32/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1269 - accuracy: 0.9715 - val_loss: 0.2581 - val_accuracy: 0.9760\n",
      "Epoch 33/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1194 - accuracy: 0.9724 - val_loss: 0.2950 - val_accuracy: 0.9633\n",
      "Epoch 34/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1202 - accuracy: 0.9719 - val_loss: 0.2578 - val_accuracy: 0.9751\n",
      "Epoch 35/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1150 - accuracy: 0.9737 - val_loss: 0.2772 - val_accuracy: 0.9626\n",
      "Epoch 36/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1243 - accuracy: 0.9708 - val_loss: 0.2676 - val_accuracy: 0.9703\n",
      "Epoch 37/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1114 - accuracy: 0.9750 - val_loss: 0.2624 - val_accuracy: 0.9734\n",
      "Epoch 38/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1135 - accuracy: 0.9753 - val_loss: 0.2660 - val_accuracy: 0.9670\n",
      "Epoch 39/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1129 - accuracy: 0.9753 - val_loss: 0.2564 - val_accuracy: 0.9723\n",
      "Epoch 40/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1156 - accuracy: 0.9729 - val_loss: 0.2583 - val_accuracy: 0.9688\n",
      "Fold: 2\n",
      "Epoch 1/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 3.4615 - accuracy: 0.3759 - val_loss: 2.2630 - val_accuracy: 0.6244\n",
      "Epoch 2/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 1.4628 - accuracy: 0.7196 - val_loss: 1.3810 - val_accuracy: 0.7674\n",
      "Epoch 3/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.8474 - accuracy: 0.8241 - val_loss: 1.0140 - val_accuracy: 0.8398\n",
      "Epoch 4/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.5917 - accuracy: 0.8691 - val_loss: 0.8320 - val_accuracy: 0.8669\n",
      "Epoch 5/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.4578 - accuracy: 0.8931 - val_loss: 0.6632 - val_accuracy: 0.8840\n",
      "Epoch 6/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.3780 - accuracy: 0.9099 - val_loss: 0.5924 - val_accuracy: 0.9144\n",
      "Epoch 7/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.3221 - accuracy: 0.9233 - val_loss: 0.5478 - val_accuracy: 0.8999\n",
      "Epoch 8/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2917 - accuracy: 0.9277 - val_loss: 0.5092 - val_accuracy: 0.9113\n",
      "Epoch 9/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2553 - accuracy: 0.9370 - val_loss: 0.4373 - val_accuracy: 0.9232\n",
      "Epoch 10/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2359 - accuracy: 0.9401 - val_loss: 0.4181 - val_accuracy: 0.9217\n",
      "Epoch 11/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2248 - accuracy: 0.9447 - val_loss: 0.3961 - val_accuracy: 0.9369\n",
      "Epoch 12/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2068 - accuracy: 0.9510 - val_loss: 0.3961 - val_accuracy: 0.9415\n",
      "Epoch 13/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2009 - accuracy: 0.9525 - val_loss: 0.3479 - val_accuracy: 0.9498\n",
      "Epoch 14/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1819 - accuracy: 0.9588 - val_loss: 0.3374 - val_accuracy: 0.9525\n",
      "Epoch 15/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1871 - accuracy: 0.9574 - val_loss: 0.3579 - val_accuracy: 0.9413\n",
      "Epoch 16/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1725 - accuracy: 0.9594 - val_loss: 0.3192 - val_accuracy: 0.9556\n",
      "Epoch 17/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1668 - accuracy: 0.9610 - val_loss: 0.3192 - val_accuracy: 0.9494\n",
      "Epoch 18/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1666 - accuracy: 0.9620 - val_loss: 0.3275 - val_accuracy: 0.9448\n",
      "Epoch 19/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1565 - accuracy: 0.9633 - val_loss: 0.3001 - val_accuracy: 0.9571\n",
      "Epoch 20/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1612 - accuracy: 0.9619 - val_loss: 0.3111 - val_accuracy: 0.9494\n",
      "Epoch 21/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1414 - accuracy: 0.9680 - val_loss: 0.2882 - val_accuracy: 0.9481\n",
      "Epoch 22/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1561 - accuracy: 0.9652 - val_loss: 0.2832 - val_accuracy: 0.9584\n",
      "Epoch 23/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1377 - accuracy: 0.9692 - val_loss: 0.3031 - val_accuracy: 0.9512\n",
      "Epoch 24/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1460 - accuracy: 0.9666 - val_loss: 0.2508 - val_accuracy: 0.9646\n",
      "Epoch 25/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1309 - accuracy: 0.9707 - val_loss: 0.2722 - val_accuracy: 0.9624\n",
      "Epoch 26/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1377 - accuracy: 0.9686 - val_loss: 0.2880 - val_accuracy: 0.9571\n",
      "Epoch 27/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1330 - accuracy: 0.9682 - val_loss: 0.2986 - val_accuracy: 0.9589\n",
      "Epoch 28/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1315 - accuracy: 0.9690 - val_loss: 0.2424 - val_accuracy: 0.9650\n",
      "Epoch 29/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1284 - accuracy: 0.9714 - val_loss: 0.2623 - val_accuracy: 0.9564\n",
      "Epoch 30/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1247 - accuracy: 0.9716 - val_loss: 0.2390 - val_accuracy: 0.9681\n",
      "Epoch 31/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1224 - accuracy: 0.9740 - val_loss: 0.2776 - val_accuracy: 0.9617\n",
      "Epoch 32/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1252 - accuracy: 0.9725 - val_loss: 0.2786 - val_accuracy: 0.9516\n",
      "Epoch 33/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1311 - accuracy: 0.9686 - val_loss: 0.2402 - val_accuracy: 0.9622\n",
      "Epoch 34/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1124 - accuracy: 0.9745 - val_loss: 0.2689 - val_accuracy: 0.9578\n",
      "Epoch 35/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1226 - accuracy: 0.9728 - val_loss: 0.2475 - val_accuracy: 0.9679\n",
      "Epoch 36/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1195 - accuracy: 0.9730 - val_loss: 0.2634 - val_accuracy: 0.9589\n",
      "Epoch 37/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1218 - accuracy: 0.9713 - val_loss: 0.2461 - val_accuracy: 0.9600\n",
      "Epoch 38/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1111 - accuracy: 0.9756 - val_loss: 0.2426 - val_accuracy: 0.9661\n",
      "Epoch 39/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1172 - accuracy: 0.9737 - val_loss: 0.2497 - val_accuracy: 0.9672\n",
      "Epoch 40/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1135 - accuracy: 0.9754 - val_loss: 0.2346 - val_accuracy: 0.9714\n",
      "Average Validation Accuracy: 0.9745127558708191\n",
      "Average Validation Loss: 0.15025607496500015\n",
      "Average Test Accuracy: 0.9756762683391571\n",
      "Final Test Accuracy for each fold: 0.9805409908294678\n",
      "Number of input features: 9\n",
      "Fold: 1\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1846/1846 [==============================] - 8s 4ms/step - loss: 3.6271 - accuracy: 0.3233 - val_loss: 2.4119 - val_accuracy: 0.6119\n",
      "Epoch 2/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 1.6004 - accuracy: 0.7141 - val_loss: 1.3923 - val_accuracy: 0.7633\n",
      "Epoch 3/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.9064 - accuracy: 0.8204 - val_loss: 0.9763 - val_accuracy: 0.8400\n",
      "Epoch 4/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.6149 - accuracy: 0.8664 - val_loss: 0.8066 - val_accuracy: 0.8605\n",
      "Epoch 5/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.4592 - accuracy: 0.8920 - val_loss: 0.6641 - val_accuracy: 0.8920\n",
      "Epoch 6/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.3765 - accuracy: 0.9118 - val_loss: 0.5701 - val_accuracy: 0.9138\n",
      "Epoch 7/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3131 - accuracy: 0.9247 - val_loss: 0.5412 - val_accuracy: 0.9072\n",
      "Epoch 8/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2721 - accuracy: 0.9346 - val_loss: 0.4816 - val_accuracy: 0.9340\n",
      "Epoch 9/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2549 - accuracy: 0.9371 - val_loss: 0.4547 - val_accuracy: 0.9311\n",
      "Epoch 10/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2238 - accuracy: 0.9448 - val_loss: 0.4860 - val_accuracy: 0.9248\n",
      "Epoch 11/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2111 - accuracy: 0.9478 - val_loss: 0.4260 - val_accuracy: 0.9206\n",
      "Epoch 12/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2026 - accuracy: 0.9522 - val_loss: 0.4122 - val_accuracy: 0.9380\n",
      "Epoch 13/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1856 - accuracy: 0.9559 - val_loss: 0.3904 - val_accuracy: 0.9492\n",
      "Epoch 14/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1847 - accuracy: 0.9573 - val_loss: 0.4183 - val_accuracy: 0.9329\n",
      "Epoch 15/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1772 - accuracy: 0.9571 - val_loss: 0.4168 - val_accuracy: 0.9523\n",
      "Epoch 16/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1649 - accuracy: 0.9620 - val_loss: 0.4263 - val_accuracy: 0.9393\n",
      "Epoch 17/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1630 - accuracy: 0.9638 - val_loss: 0.3580 - val_accuracy: 0.9549\n",
      "Epoch 18/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1581 - accuracy: 0.9629 - val_loss: 0.3569 - val_accuracy: 0.9688\n",
      "Epoch 19/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1497 - accuracy: 0.9656 - val_loss: 0.3670 - val_accuracy: 0.9545\n",
      "Epoch 20/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1542 - accuracy: 0.9638 - val_loss: 0.3547 - val_accuracy: 0.9569\n",
      "Epoch 21/40\n",
      "1846/1846 [==============================] - 6s 4ms/step - loss: 0.1493 - accuracy: 0.9641 - val_loss: 0.3370 - val_accuracy: 0.9628\n",
      "Epoch 22/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1491 - accuracy: 0.9656 - val_loss: 0.3492 - val_accuracy: 0.9479\n",
      "Epoch 23/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1368 - accuracy: 0.9698 - val_loss: 0.3257 - val_accuracy: 0.9655\n",
      "Epoch 24/40\n",
      "1846/1846 [==============================] - 6s 4ms/step - loss: 0.1438 - accuracy: 0.9680 - val_loss: 0.3315 - val_accuracy: 0.9573\n",
      "Epoch 25/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1344 - accuracy: 0.9690 - val_loss: 0.3294 - val_accuracy: 0.9690\n",
      "Epoch 26/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1351 - accuracy: 0.9686 - val_loss: 0.3255 - val_accuracy: 0.9674\n",
      "Epoch 27/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1290 - accuracy: 0.9708 - val_loss: 0.3123 - val_accuracy: 0.9677\n",
      "Epoch 28/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1340 - accuracy: 0.9700 - val_loss: 0.3154 - val_accuracy: 0.9714\n",
      "Epoch 29/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1290 - accuracy: 0.9715 - val_loss: 0.3110 - val_accuracy: 0.9743\n",
      "Epoch 30/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1241 - accuracy: 0.9730 - val_loss: 0.3042 - val_accuracy: 0.9749\n",
      "Epoch 31/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1235 - accuracy: 0.9736 - val_loss: 0.2863 - val_accuracy: 0.9822\n",
      "Epoch 32/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1362 - accuracy: 0.9680 - val_loss: 0.3024 - val_accuracy: 0.9714\n",
      "Epoch 33/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1204 - accuracy: 0.9727 - val_loss: 0.2908 - val_accuracy: 0.9677\n",
      "Epoch 34/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1175 - accuracy: 0.9741 - val_loss: 0.3011 - val_accuracy: 0.9608\n",
      "Epoch 35/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1163 - accuracy: 0.9746 - val_loss: 0.3051 - val_accuracy: 0.9608\n",
      "Epoch 36/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1272 - accuracy: 0.9715 - val_loss: 0.3027 - val_accuracy: 0.9674\n",
      "Epoch 37/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1122 - accuracy: 0.9746 - val_loss: 0.2758 - val_accuracy: 0.9822\n",
      "Epoch 38/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1202 - accuracy: 0.9728 - val_loss: 0.2703 - val_accuracy: 0.9809\n",
      "Epoch 39/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1152 - accuracy: 0.9741 - val_loss: 0.3024 - val_accuracy: 0.9650\n",
      "Epoch 40/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1116 - accuracy: 0.9741 - val_loss: 0.2824 - val_accuracy: 0.9705\n",
      "Fold: 2\n",
      "Epoch 1/40\n",
      "1846/1846 [==============================] - 7s 3ms/step - loss: 3.4874 - accuracy: 0.3721 - val_loss: 2.3267 - val_accuracy: 0.5998\n",
      "Epoch 2/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 1.4779 - accuracy: 0.7300 - val_loss: 1.2819 - val_accuracy: 0.8099\n",
      "Epoch 3/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.7781 - accuracy: 0.8436 - val_loss: 0.8753 - val_accuracy: 0.8565\n",
      "Epoch 4/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.5056 - accuracy: 0.8880 - val_loss: 0.6804 - val_accuracy: 0.8638\n",
      "Epoch 5/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.3827 - accuracy: 0.9089 - val_loss: 0.5498 - val_accuracy: 0.8953\n",
      "Epoch 6/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.3193 - accuracy: 0.9216 - val_loss: 0.4851 - val_accuracy: 0.9197\n",
      "Epoch 7/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2815 - accuracy: 0.9275 - val_loss: 0.4665 - val_accuracy: 0.9201\n",
      "Epoch 8/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2504 - accuracy: 0.9365 - val_loss: 0.4097 - val_accuracy: 0.9241\n",
      "Epoch 9/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2304 - accuracy: 0.9422 - val_loss: 0.4018 - val_accuracy: 0.9248\n",
      "Epoch 10/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2134 - accuracy: 0.9479 - val_loss: 0.3283 - val_accuracy: 0.9507\n",
      "Epoch 11/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1980 - accuracy: 0.9512 - val_loss: 0.3651 - val_accuracy: 0.9371\n",
      "Epoch 12/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1895 - accuracy: 0.9514 - val_loss: 0.3575 - val_accuracy: 0.9432\n",
      "Epoch 13/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1831 - accuracy: 0.9551 - val_loss: 0.3586 - val_accuracy: 0.9309\n",
      "Epoch 14/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1710 - accuracy: 0.9579 - val_loss: 0.3201 - val_accuracy: 0.9417\n",
      "Epoch 15/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1674 - accuracy: 0.9590 - val_loss: 0.3625 - val_accuracy: 0.9428\n",
      "Epoch 16/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1636 - accuracy: 0.9593 - val_loss: 0.3439 - val_accuracy: 0.9472\n",
      "Epoch 17/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1526 - accuracy: 0.9641 - val_loss: 0.2857 - val_accuracy: 0.9613\n",
      "Epoch 18/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1523 - accuracy: 0.9640 - val_loss: 0.3205 - val_accuracy: 0.9663\n",
      "Epoch 19/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1502 - accuracy: 0.9648 - val_loss: 0.2801 - val_accuracy: 0.9558\n",
      "Epoch 20/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1403 - accuracy: 0.9678 - val_loss: 0.2779 - val_accuracy: 0.9657\n",
      "Epoch 21/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1366 - accuracy: 0.9694 - val_loss: 0.2670 - val_accuracy: 0.9597\n",
      "Epoch 22/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1338 - accuracy: 0.9668 - val_loss: 0.3129 - val_accuracy: 0.9463\n",
      "Epoch 23/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1405 - accuracy: 0.9659 - val_loss: 0.2698 - val_accuracy: 0.9663\n",
      "Epoch 24/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1275 - accuracy: 0.9718 - val_loss: 0.2866 - val_accuracy: 0.9628\n",
      "Epoch 25/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1300 - accuracy: 0.9701 - val_loss: 0.2661 - val_accuracy: 0.9694\n",
      "Epoch 26/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1241 - accuracy: 0.9718 - val_loss: 0.2647 - val_accuracy: 0.9751\n",
      "Epoch 27/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1279 - accuracy: 0.9714 - val_loss: 0.2754 - val_accuracy: 0.9650\n",
      "Epoch 28/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1181 - accuracy: 0.9747 - val_loss: 0.2703 - val_accuracy: 0.9701\n",
      "Epoch 29/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1240 - accuracy: 0.9718 - val_loss: 0.2753 - val_accuracy: 0.9595\n",
      "Epoch 30/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1202 - accuracy: 0.9718 - val_loss: 0.2692 - val_accuracy: 0.9641\n",
      "Epoch 31/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1178 - accuracy: 0.9740 - val_loss: 0.2566 - val_accuracy: 0.9743\n",
      "Epoch 32/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1154 - accuracy: 0.9746 - val_loss: 0.2592 - val_accuracy: 0.9694\n",
      "Epoch 33/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1126 - accuracy: 0.9756 - val_loss: 0.2911 - val_accuracy: 0.9589\n",
      "Epoch 34/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1145 - accuracy: 0.9741 - val_loss: 0.2638 - val_accuracy: 0.9672\n",
      "Epoch 35/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1130 - accuracy: 0.9744 - val_loss: 0.2630 - val_accuracy: 0.9606\n",
      "Epoch 36/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1129 - accuracy: 0.9756 - val_loss: 0.2799 - val_accuracy: 0.9633\n",
      "Epoch 37/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1085 - accuracy: 0.9764 - val_loss: 0.2641 - val_accuracy: 0.9637\n",
      "Epoch 38/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1198 - accuracy: 0.9720 - val_loss: 0.2665 - val_accuracy: 0.9690\n",
      "Epoch 39/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1108 - accuracy: 0.9743 - val_loss: 0.2811 - val_accuracy: 0.9633\n",
      "Epoch 40/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1091 - accuracy: 0.9764 - val_loss: 0.2631 - val_accuracy: 0.9683\n",
      "Average Validation Accuracy: 0.9747304320335388\n",
      "Average Validation Loss: 0.15937981009483337\n",
      "Average Test Accuracy: 0.9755657017230988\n",
      "Final Test Accuracy for each fold: 0.976708173751831\n",
      "Number of input features: 10\n",
      "Fold: 1\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1846/1846 [==============================] - 8s 4ms/step - loss: 3.4379 - accuracy: 0.3713 - val_loss: 2.1059 - val_accuracy: 0.6761\n",
      "Epoch 2/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 1.3285 - accuracy: 0.7641 - val_loss: 1.1515 - val_accuracy: 0.8020\n",
      "Epoch 3/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.7192 - accuracy: 0.8537 - val_loss: 0.8007 - val_accuracy: 0.8548\n",
      "Epoch 4/40\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 0.4728 - accuracy: 0.8932 - val_loss: 0.6385 - val_accuracy: 0.8849\n",
      "Epoch 5/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.3637 - accuracy: 0.9132 - val_loss: 0.5134 - val_accuracy: 0.9144\n",
      "Epoch 6/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2952 - accuracy: 0.9285 - val_loss: 0.4902 - val_accuracy: 0.9118\n",
      "Epoch 7/40\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 0.2622 - accuracy: 0.9339 - val_loss: 0.4309 - val_accuracy: 0.9248\n",
      "Epoch 8/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2335 - accuracy: 0.9411 - val_loss: 0.4292 - val_accuracy: 0.9171\n",
      "Epoch 9/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2174 - accuracy: 0.9462 - val_loss: 0.3592 - val_accuracy: 0.9432\n",
      "Epoch 10/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2024 - accuracy: 0.9498 - val_loss: 0.3827 - val_accuracy: 0.9373\n",
      "Epoch 11/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1906 - accuracy: 0.9507 - val_loss: 0.3726 - val_accuracy: 0.9386\n",
      "Epoch 12/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1758 - accuracy: 0.9566 - val_loss: 0.3742 - val_accuracy: 0.9399\n",
      "Epoch 13/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1789 - accuracy: 0.9572 - val_loss: 0.3262 - val_accuracy: 0.9485\n",
      "Epoch 14/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1624 - accuracy: 0.9614 - val_loss: 0.3395 - val_accuracy: 0.9303\n",
      "Epoch 15/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1625 - accuracy: 0.9603 - val_loss: 0.3378 - val_accuracy: 0.9452\n",
      "Epoch 16/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1564 - accuracy: 0.9623 - val_loss: 0.3088 - val_accuracy: 0.9542\n",
      "Epoch 17/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1515 - accuracy: 0.9648 - val_loss: 0.3086 - val_accuracy: 0.9582\n",
      "Epoch 18/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1497 - accuracy: 0.9657 - val_loss: 0.2966 - val_accuracy: 0.9573\n",
      "Epoch 19/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1458 - accuracy: 0.9665 - val_loss: 0.2836 - val_accuracy: 0.9666\n",
      "Epoch 20/40\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 0.1448 - accuracy: 0.9646 - val_loss: 0.3192 - val_accuracy: 0.9529\n",
      "Epoch 21/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1356 - accuracy: 0.9692 - val_loss: 0.2974 - val_accuracy: 0.9611\n",
      "Epoch 22/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1393 - accuracy: 0.9672 - val_loss: 0.3034 - val_accuracy: 0.9600\n",
      "Epoch 23/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1333 - accuracy: 0.9693 - val_loss: 0.3184 - val_accuracy: 0.9553\n",
      "Epoch 24/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1389 - accuracy: 0.9692 - val_loss: 0.2774 - val_accuracy: 0.9729\n",
      "Epoch 25/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1299 - accuracy: 0.9691 - val_loss: 0.3026 - val_accuracy: 0.9496\n",
      "Epoch 26/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1263 - accuracy: 0.9712 - val_loss: 0.2669 - val_accuracy: 0.9725\n",
      "Epoch 27/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1266 - accuracy: 0.9707 - val_loss: 0.2956 - val_accuracy: 0.9602\n",
      "Epoch 28/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1256 - accuracy: 0.9700 - val_loss: 0.2911 - val_accuracy: 0.9628\n",
      "Epoch 29/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1192 - accuracy: 0.9733 - val_loss: 0.2634 - val_accuracy: 0.9685\n",
      "Epoch 30/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1157 - accuracy: 0.9744 - val_loss: 0.2772 - val_accuracy: 0.9688\n",
      "Epoch 31/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1206 - accuracy: 0.9716 - val_loss: 0.2568 - val_accuracy: 0.9745\n",
      "Epoch 32/40\n",
      "1846/1846 [==============================] - 6s 4ms/step - loss: 0.1161 - accuracy: 0.9727 - val_loss: 0.3216 - val_accuracy: 0.9589\n",
      "Epoch 33/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1146 - accuracy: 0.9738 - val_loss: 0.2843 - val_accuracy: 0.9639\n",
      "Epoch 34/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1162 - accuracy: 0.9730 - val_loss: 0.2736 - val_accuracy: 0.9712\n",
      "Epoch 35/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1191 - accuracy: 0.9719 - val_loss: 0.3224 - val_accuracy: 0.9569\n",
      "Epoch 36/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1104 - accuracy: 0.9753 - val_loss: 0.2713 - val_accuracy: 0.9745\n",
      "Epoch 37/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1120 - accuracy: 0.9757 - val_loss: 0.2725 - val_accuracy: 0.9721\n",
      "Epoch 38/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1152 - accuracy: 0.9740 - val_loss: 0.3002 - val_accuracy: 0.9595\n",
      "Epoch 39/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1076 - accuracy: 0.9766 - val_loss: 0.2716 - val_accuracy: 0.9780\n",
      "Epoch 40/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1104 - accuracy: 0.9742 - val_loss: 0.2751 - val_accuracy: 0.9751\n",
      "Fold: 2\n",
      "Epoch 1/40\n",
      "1846/1846 [==============================] - 7s 3ms/step - loss: 3.5917 - accuracy: 0.3519 - val_loss: 2.3418 - val_accuracy: 0.5927\n",
      "Epoch 2/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 1.4487 - accuracy: 0.7338 - val_loss: 1.2610 - val_accuracy: 0.8013\n",
      "Epoch 3/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.7740 - accuracy: 0.8393 - val_loss: 0.8970 - val_accuracy: 0.8205\n",
      "Epoch 4/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.5070 - accuracy: 0.8888 - val_loss: 0.6842 - val_accuracy: 0.8843\n",
      "Epoch 5/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3748 - accuracy: 0.9147 - val_loss: 0.5570 - val_accuracy: 0.9043\n",
      "Epoch 6/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.3070 - accuracy: 0.9258 - val_loss: 0.4879 - val_accuracy: 0.9197\n",
      "Epoch 7/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2749 - accuracy: 0.9346 - val_loss: 0.4242 - val_accuracy: 0.9267\n",
      "Epoch 8/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2381 - accuracy: 0.9414 - val_loss: 0.4206 - val_accuracy: 0.9256\n",
      "Epoch 9/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2208 - accuracy: 0.9430 - val_loss: 0.4285 - val_accuracy: 0.9300\n",
      "Epoch 10/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1993 - accuracy: 0.9523 - val_loss: 0.3608 - val_accuracy: 0.9421\n",
      "Epoch 11/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1898 - accuracy: 0.9534 - val_loss: 0.3439 - val_accuracy: 0.9366\n",
      "Epoch 12/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1854 - accuracy: 0.9531 - val_loss: 0.3428 - val_accuracy: 0.9415\n",
      "Epoch 13/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1741 - accuracy: 0.9600 - val_loss: 0.3325 - val_accuracy: 0.9553\n",
      "Epoch 14/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1690 - accuracy: 0.9597 - val_loss: 0.3264 - val_accuracy: 0.9538\n",
      "Epoch 15/40\n",
      "1846/1846 [==============================] - 6s 4ms/step - loss: 0.1607 - accuracy: 0.9619 - val_loss: 0.3100 - val_accuracy: 0.9507\n",
      "Epoch 16/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1568 - accuracy: 0.9640 - val_loss: 0.3032 - val_accuracy: 0.9485\n",
      "Epoch 17/40\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 0.1513 - accuracy: 0.9640 - val_loss: 0.2960 - val_accuracy: 0.9545\n",
      "Epoch 18/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1441 - accuracy: 0.9678 - val_loss: 0.3127 - val_accuracy: 0.9459\n",
      "Epoch 19/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1454 - accuracy: 0.9667 - val_loss: 0.2721 - val_accuracy: 0.9652\n",
      "Epoch 20/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1376 - accuracy: 0.9692 - val_loss: 0.3301 - val_accuracy: 0.9457\n",
      "Epoch 21/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1404 - accuracy: 0.9686 - val_loss: 0.2817 - val_accuracy: 0.9595\n",
      "Epoch 22/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1355 - accuracy: 0.9681 - val_loss: 0.2867 - val_accuracy: 0.9540\n",
      "Epoch 23/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1294 - accuracy: 0.9705 - val_loss: 0.2918 - val_accuracy: 0.9562\n",
      "Epoch 24/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1272 - accuracy: 0.9701 - val_loss: 0.2938 - val_accuracy: 0.9569\n",
      "Epoch 25/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1298 - accuracy: 0.9717 - val_loss: 0.2650 - val_accuracy: 0.9591\n",
      "Epoch 26/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1264 - accuracy: 0.9701 - val_loss: 0.2782 - val_accuracy: 0.9584\n",
      "Epoch 27/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1219 - accuracy: 0.9720 - val_loss: 0.2883 - val_accuracy: 0.9527\n",
      "Epoch 28/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1255 - accuracy: 0.9708 - val_loss: 0.2846 - val_accuracy: 0.9586\n",
      "Epoch 29/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1207 - accuracy: 0.9742 - val_loss: 0.2793 - val_accuracy: 0.9677\n",
      "Epoch 30/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1152 - accuracy: 0.9736 - val_loss: 0.2645 - val_accuracy: 0.9674\n",
      "Epoch 31/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1234 - accuracy: 0.9706 - val_loss: 0.3268 - val_accuracy: 0.9483\n",
      "Epoch 32/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1188 - accuracy: 0.9717 - val_loss: 0.2931 - val_accuracy: 0.9626\n",
      "Epoch 33/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1132 - accuracy: 0.9737 - val_loss: 0.2693 - val_accuracy: 0.9661\n",
      "Epoch 34/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1154 - accuracy: 0.9737 - val_loss: 0.2683 - val_accuracy: 0.9674\n",
      "Epoch 35/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1130 - accuracy: 0.9740 - val_loss: 0.2726 - val_accuracy: 0.9661\n",
      "Epoch 36/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1130 - accuracy: 0.9742 - val_loss: 0.2796 - val_accuracy: 0.9648\n",
      "Epoch 37/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1027 - accuracy: 0.9770 - val_loss: 0.2580 - val_accuracy: 0.9683\n",
      "Epoch 38/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1158 - accuracy: 0.9726 - val_loss: 0.2411 - val_accuracy: 0.9806\n",
      "Epoch 39/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1045 - accuracy: 0.9768 - val_loss: 0.2541 - val_accuracy: 0.9738\n",
      "Epoch 40/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1181 - accuracy: 0.9717 - val_loss: 0.2531 - val_accuracy: 0.9648\n",
      "Average Validation Accuracy: 0.9759285151958466\n",
      "Average Validation Loss: 0.15183531492948532\n",
      "Average Test Accuracy: 0.9764133393764496\n",
      "Final Test Accuracy for each fold: 0.9774452447891235\n",
      "Number of input features: 11\n",
      "Fold: 1\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1846/1846 [==============================] - 8s 4ms/step - loss: 3.4883 - accuracy: 0.3618 - val_loss: 2.2157 - val_accuracy: 0.6475\n",
      "Epoch 2/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.3905 - accuracy: 0.7407 - val_loss: 1.2308 - val_accuracy: 0.8084\n",
      "Epoch 3/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.7535 - accuracy: 0.8449 - val_loss: 0.9196 - val_accuracy: 0.8293\n",
      "Epoch 4/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.5209 - accuracy: 0.8834 - val_loss: 0.7291 - val_accuracy: 0.8803\n",
      "Epoch 5/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4090 - accuracy: 0.9047 - val_loss: 0.6098 - val_accuracy: 0.9030\n",
      "Epoch 6/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3372 - accuracy: 0.9187 - val_loss: 0.5410 - val_accuracy: 0.9221\n",
      "Epoch 7/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2990 - accuracy: 0.9258 - val_loss: 0.5019 - val_accuracy: 0.9309\n",
      "Epoch 8/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2699 - accuracy: 0.9362 - val_loss: 0.4888 - val_accuracy: 0.9265\n",
      "Epoch 9/40\n",
      "1846/1846 [==============================] - 6s 4ms/step - loss: 0.2497 - accuracy: 0.9406 - val_loss: 0.4499 - val_accuracy: 0.9309\n",
      "Epoch 10/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2364 - accuracy: 0.9416 - val_loss: 0.4303 - val_accuracy: 0.9408\n",
      "Epoch 11/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2218 - accuracy: 0.9445 - val_loss: 0.4248 - val_accuracy: 0.9490\n",
      "Epoch 12/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2060 - accuracy: 0.9519 - val_loss: 0.3865 - val_accuracy: 0.9584\n",
      "Epoch 13/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1940 - accuracy: 0.9520 - val_loss: 0.3698 - val_accuracy: 0.9549\n",
      "Epoch 14/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1964 - accuracy: 0.9506 - val_loss: 0.3832 - val_accuracy: 0.9578\n",
      "Epoch 15/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1805 - accuracy: 0.9570 - val_loss: 0.3683 - val_accuracy: 0.9413\n",
      "Epoch 16/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1790 - accuracy: 0.9563 - val_loss: 0.3979 - val_accuracy: 0.9369\n",
      "Epoch 17/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1704 - accuracy: 0.9587 - val_loss: 0.3490 - val_accuracy: 0.9633\n",
      "Epoch 18/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1694 - accuracy: 0.9595 - val_loss: 0.3723 - val_accuracy: 0.9481\n",
      "Epoch 19/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1574 - accuracy: 0.9634 - val_loss: 0.3615 - val_accuracy: 0.9523\n",
      "Epoch 20/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1579 - accuracy: 0.9610 - val_loss: 0.3850 - val_accuracy: 0.9399\n",
      "Epoch 21/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1523 - accuracy: 0.9662 - val_loss: 0.3649 - val_accuracy: 0.9483\n",
      "Epoch 22/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1587 - accuracy: 0.9627 - val_loss: 0.3303 - val_accuracy: 0.9571\n",
      "Epoch 23/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1460 - accuracy: 0.9651 - val_loss: 0.3506 - val_accuracy: 0.9446\n",
      "Epoch 24/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1529 - accuracy: 0.9621 - val_loss: 0.3345 - val_accuracy: 0.9525\n",
      "Epoch 25/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1452 - accuracy: 0.9672 - val_loss: 0.3497 - val_accuracy: 0.9529\n",
      "Epoch 26/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1453 - accuracy: 0.9661 - val_loss: 0.3315 - val_accuracy: 0.9494\n",
      "Epoch 27/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1410 - accuracy: 0.9672 - val_loss: 0.3508 - val_accuracy: 0.9534\n",
      "Epoch 28/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1418 - accuracy: 0.9666 - val_loss: 0.3287 - val_accuracy: 0.9626\n",
      "Epoch 29/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1330 - accuracy: 0.9716 - val_loss: 0.3554 - val_accuracy: 0.9476\n",
      "Epoch 30/40\n",
      "1846/1846 [==============================] - 8s 5ms/step - loss: 0.1303 - accuracy: 0.9710 - val_loss: 0.3149 - val_accuracy: 0.9685\n",
      "Epoch 31/40\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1384 - accuracy: 0.9682 - val_loss: 0.3152 - val_accuracy: 0.9560\n",
      "Epoch 32/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1320 - accuracy: 0.9704 - val_loss: 0.3117 - val_accuracy: 0.9707\n",
      "Epoch 33/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1321 - accuracy: 0.9693 - val_loss: 0.3243 - val_accuracy: 0.9600\n",
      "Epoch 34/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1275 - accuracy: 0.9710 - val_loss: 0.3128 - val_accuracy: 0.9644\n",
      "Epoch 35/40\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1237 - accuracy: 0.9723 - val_loss: 0.3152 - val_accuracy: 0.9644\n",
      "Epoch 36/40\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1276 - accuracy: 0.9706 - val_loss: 0.3153 - val_accuracy: 0.9615\n",
      "Epoch 37/40\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1224 - accuracy: 0.9715 - val_loss: 0.3068 - val_accuracy: 0.9707\n",
      "Epoch 38/40\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1219 - accuracy: 0.9711 - val_loss: 0.3238 - val_accuracy: 0.9619\n",
      "Epoch 39/40\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1225 - accuracy: 0.9720 - val_loss: 0.3248 - val_accuracy: 0.9558\n",
      "Epoch 40/40\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1237 - accuracy: 0.9713 - val_loss: 0.3165 - val_accuracy: 0.9600\n",
      "Fold: 2\n",
      "Epoch 1/40\n",
      "1846/1846 [==============================] - 12s 5ms/step - loss: 3.5449 - accuracy: 0.3509 - val_loss: 2.3209 - val_accuracy: 0.5969\n",
      "Epoch 2/40\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 1.4131 - accuracy: 0.7409 - val_loss: 1.2275 - val_accuracy: 0.8130\n",
      "Epoch 3/40\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.7027 - accuracy: 0.8542 - val_loss: 0.8191 - val_accuracy: 0.8840\n",
      "Epoch 4/40\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.4550 - accuracy: 0.8977 - val_loss: 0.7542 - val_accuracy: 0.8662\n",
      "Epoch 5/40\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.3444 - accuracy: 0.9192 - val_loss: 0.5955 - val_accuracy: 0.9050\n",
      "Epoch 6/40\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.2831 - accuracy: 0.9326 - val_loss: 0.4991 - val_accuracy: 0.9175\n",
      "Epoch 7/40\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.2575 - accuracy: 0.9389 - val_loss: 0.4426 - val_accuracy: 0.9300\n",
      "Epoch 8/40\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.2227 - accuracy: 0.9461 - val_loss: 0.3752 - val_accuracy: 0.9417\n",
      "Epoch 9/40\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.2080 - accuracy: 0.9462 - val_loss: 0.3694 - val_accuracy: 0.9384\n",
      "Epoch 10/40\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1944 - accuracy: 0.9530 - val_loss: 0.3937 - val_accuracy: 0.9426\n",
      "Epoch 11/40\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1834 - accuracy: 0.9570 - val_loss: 0.3629 - val_accuracy: 0.9395\n",
      "Epoch 12/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1756 - accuracy: 0.9582 - val_loss: 0.3404 - val_accuracy: 0.9507\n",
      "Epoch 13/40\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1688 - accuracy: 0.9615 - val_loss: 0.3451 - val_accuracy: 0.9454\n",
      "Epoch 14/40\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1618 - accuracy: 0.9608 - val_loss: 0.3541 - val_accuracy: 0.9413\n",
      "Epoch 15/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1576 - accuracy: 0.9622 - val_loss: 0.3025 - val_accuracy: 0.9505\n",
      "Epoch 16/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1490 - accuracy: 0.9659 - val_loss: 0.3220 - val_accuracy: 0.9597\n",
      "Epoch 17/40\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1486 - accuracy: 0.9637 - val_loss: 0.3347 - val_accuracy: 0.9509\n",
      "Epoch 18/40\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1444 - accuracy: 0.9672 - val_loss: 0.3062 - val_accuracy: 0.9646\n",
      "Epoch 19/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1394 - accuracy: 0.9681 - val_loss: 0.3240 - val_accuracy: 0.9509\n",
      "Epoch 20/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1334 - accuracy: 0.9721 - val_loss: 0.3035 - val_accuracy: 0.9613\n",
      "Epoch 21/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1370 - accuracy: 0.9690 - val_loss: 0.3020 - val_accuracy: 0.9650\n",
      "Epoch 22/40\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1293 - accuracy: 0.9701 - val_loss: 0.2757 - val_accuracy: 0.9712\n",
      "Epoch 23/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1301 - accuracy: 0.9694 - val_loss: 0.2896 - val_accuracy: 0.9657\n",
      "Epoch 24/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1239 - accuracy: 0.9721 - val_loss: 0.2844 - val_accuracy: 0.9655\n",
      "Epoch 25/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1286 - accuracy: 0.9716 - val_loss: 0.2964 - val_accuracy: 0.9619\n",
      "Epoch 26/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1216 - accuracy: 0.9726 - val_loss: 0.3039 - val_accuracy: 0.9604\n",
      "Epoch 27/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1205 - accuracy: 0.9719 - val_loss: 0.2963 - val_accuracy: 0.9690\n",
      "Epoch 28/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1273 - accuracy: 0.9701 - val_loss: 0.2889 - val_accuracy: 0.9604\n",
      "Epoch 29/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1165 - accuracy: 0.9729 - val_loss: 0.3540 - val_accuracy: 0.9270\n",
      "Epoch 30/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1198 - accuracy: 0.9741 - val_loss: 0.2796 - val_accuracy: 0.9646\n",
      "Epoch 31/40\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1154 - accuracy: 0.9730 - val_loss: 0.2740 - val_accuracy: 0.9663\n",
      "Epoch 32/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1178 - accuracy: 0.9738 - val_loss: 0.2983 - val_accuracy: 0.9657\n",
      "Epoch 33/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1147 - accuracy: 0.9730 - val_loss: 0.3210 - val_accuracy: 0.9536\n",
      "Epoch 34/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1104 - accuracy: 0.9744 - val_loss: 0.2779 - val_accuracy: 0.9602\n",
      "Epoch 35/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1099 - accuracy: 0.9743 - val_loss: 0.2580 - val_accuracy: 0.9668\n",
      "Epoch 36/40\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1096 - accuracy: 0.9750 - val_loss: 0.2799 - val_accuracy: 0.9611\n",
      "Epoch 37/40\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1066 - accuracy: 0.9763 - val_loss: 0.2880 - val_accuracy: 0.9659\n",
      "Epoch 38/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1089 - accuracy: 0.9746 - val_loss: 0.2648 - val_accuracy: 0.9740\n",
      "Epoch 39/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1103 - accuracy: 0.9744 - val_loss: 0.2871 - val_accuracy: 0.9549\n",
      "Epoch 40/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1116 - accuracy: 0.9740 - val_loss: 0.2639 - val_accuracy: 0.9681\n",
      "Average Validation Accuracy: 0.9718259871006012\n",
      "Average Validation Loss: 0.16621894389390945\n",
      "Average Test Accuracy: 0.9733913242816925\n",
      "Final Test Accuracy for each fold: 0.976413369178772\n",
      "Number of input features: 12\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "1846/1846 [==============================] - 14s 6ms/step - loss: 3.4783 - accuracy: 0.3660 - val_loss: 2.3768 - val_accuracy: 0.5421\n",
      "Epoch 2/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 1.5414 - accuracy: 0.7072 - val_loss: 1.3054 - val_accuracy: 0.7630\n",
      "Epoch 3/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.8449 - accuracy: 0.8248 - val_loss: 0.8951 - val_accuracy: 0.8385\n",
      "Epoch 4/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.5609 - accuracy: 0.8735 - val_loss: 0.6952 - val_accuracy: 0.8871\n",
      "Epoch 5/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.4212 - accuracy: 0.9008 - val_loss: 0.6176 - val_accuracy: 0.8739\n",
      "Epoch 6/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.3447 - accuracy: 0.9160 - val_loss: 0.5184 - val_accuracy: 0.9131\n",
      "Epoch 7/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2903 - accuracy: 0.9276 - val_loss: 0.4887 - val_accuracy: 0.9221\n",
      "Epoch 8/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2598 - accuracy: 0.9340 - val_loss: 0.4871 - val_accuracy: 0.9089\n",
      "Epoch 9/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2409 - accuracy: 0.9419 - val_loss: 0.4020 - val_accuracy: 0.9437\n",
      "Epoch 10/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2135 - accuracy: 0.9504 - val_loss: 0.5475 - val_accuracy: 0.9054\n",
      "Epoch 11/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2038 - accuracy: 0.9529 - val_loss: 0.3761 - val_accuracy: 0.9430\n",
      "Epoch 12/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1979 - accuracy: 0.9518 - val_loss: 0.3562 - val_accuracy: 0.9408\n",
      "Epoch 13/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1783 - accuracy: 0.9582 - val_loss: 0.3754 - val_accuracy: 0.9395\n",
      "Epoch 14/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1804 - accuracy: 0.9563 - val_loss: 0.3605 - val_accuracy: 0.9296\n",
      "Epoch 15/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1711 - accuracy: 0.9602 - val_loss: 0.3263 - val_accuracy: 0.9569\n",
      "Epoch 16/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1650 - accuracy: 0.9628 - val_loss: 0.3276 - val_accuracy: 0.9573\n",
      "Epoch 17/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1594 - accuracy: 0.9623 - val_loss: 0.3210 - val_accuracy: 0.9556\n",
      "Epoch 18/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1536 - accuracy: 0.9633 - val_loss: 0.3174 - val_accuracy: 0.9551\n",
      "Epoch 19/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1498 - accuracy: 0.9651 - val_loss: 0.3264 - val_accuracy: 0.9600\n",
      "Epoch 20/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1451 - accuracy: 0.9652 - val_loss: 0.3091 - val_accuracy: 0.9538\n",
      "Epoch 21/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1463 - accuracy: 0.9653 - val_loss: 0.3881 - val_accuracy: 0.9239\n",
      "Epoch 22/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1374 - accuracy: 0.9675 - val_loss: 0.2945 - val_accuracy: 0.9547\n",
      "Epoch 23/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1335 - accuracy: 0.9676 - val_loss: 0.3051 - val_accuracy: 0.9606\n",
      "Epoch 24/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1373 - accuracy: 0.9670 - val_loss: 0.3788 - val_accuracy: 0.9523\n",
      "Epoch 25/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1382 - accuracy: 0.9689 - val_loss: 0.2741 - val_accuracy: 0.9736\n",
      "Epoch 26/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1348 - accuracy: 0.9694 - val_loss: 0.2729 - val_accuracy: 0.9793\n",
      "Epoch 27/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1272 - accuracy: 0.9716 - val_loss: 0.3048 - val_accuracy: 0.9635\n",
      "Epoch 28/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1244 - accuracy: 0.9706 - val_loss: 0.2751 - val_accuracy: 0.9705\n",
      "Epoch 29/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1283 - accuracy: 0.9686 - val_loss: 0.2842 - val_accuracy: 0.9703\n",
      "Epoch 30/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1175 - accuracy: 0.9745 - val_loss: 0.2782 - val_accuracy: 0.9710\n",
      "Epoch 31/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1267 - accuracy: 0.9700 - val_loss: 0.2909 - val_accuracy: 0.9630\n",
      "Epoch 32/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1184 - accuracy: 0.9729 - val_loss: 0.2721 - val_accuracy: 0.9690\n",
      "Epoch 33/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1176 - accuracy: 0.9742 - val_loss: 0.2800 - val_accuracy: 0.9688\n",
      "Epoch 34/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1176 - accuracy: 0.9725 - val_loss: 0.2754 - val_accuracy: 0.9663\n",
      "Epoch 35/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1185 - accuracy: 0.9712 - val_loss: 0.2996 - val_accuracy: 0.9600\n",
      "Epoch 36/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1084 - accuracy: 0.9756 - val_loss: 0.3025 - val_accuracy: 0.9582\n",
      "Epoch 37/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1161 - accuracy: 0.9736 - val_loss: 0.2715 - val_accuracy: 0.9690\n",
      "Epoch 38/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1181 - accuracy: 0.9727 - val_loss: 0.2994 - val_accuracy: 0.9562\n",
      "Epoch 39/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1090 - accuracy: 0.9744 - val_loss: 0.3511 - val_accuracy: 0.9560\n",
      "Epoch 40/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1211 - accuracy: 0.9710 - val_loss: 0.2697 - val_accuracy: 0.9736\n",
      "Fold: 2\n",
      "Epoch 1/40\n",
      "1846/1846 [==============================] - 14s 6ms/step - loss: 3.4523 - accuracy: 0.3776 - val_loss: 2.2573 - val_accuracy: 0.6607\n",
      "Epoch 2/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 1.4275 - accuracy: 0.7362 - val_loss: 1.2987 - val_accuracy: 0.7903\n",
      "Epoch 3/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.7588 - accuracy: 0.8400 - val_loss: 0.9107 - val_accuracy: 0.8680\n",
      "Epoch 4/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.5012 - accuracy: 0.8862 - val_loss: 0.7092 - val_accuracy: 0.8964\n",
      "Epoch 5/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.3774 - accuracy: 0.9113 - val_loss: 0.5725 - val_accuracy: 0.9175\n",
      "Epoch 6/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.3136 - accuracy: 0.9227 - val_loss: 0.5131 - val_accuracy: 0.9100\n",
      "Epoch 7/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2620 - accuracy: 0.9372 - val_loss: 0.5051 - val_accuracy: 0.9265\n",
      "Epoch 8/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2383 - accuracy: 0.9414 - val_loss: 0.3959 - val_accuracy: 0.9388\n",
      "Epoch 9/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2073 - accuracy: 0.9516 - val_loss: 0.3761 - val_accuracy: 0.9432\n",
      "Epoch 10/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2029 - accuracy: 0.9514 - val_loss: 0.3929 - val_accuracy: 0.9318\n",
      "Epoch 11/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1895 - accuracy: 0.9553 - val_loss: 0.3647 - val_accuracy: 0.9397\n",
      "Epoch 12/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1721 - accuracy: 0.9575 - val_loss: 0.3395 - val_accuracy: 0.9410\n",
      "Epoch 13/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1758 - accuracy: 0.9577 - val_loss: 0.3036 - val_accuracy: 0.9468\n",
      "Epoch 14/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1648 - accuracy: 0.9601 - val_loss: 0.3211 - val_accuracy: 0.9408\n",
      "Epoch 15/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1657 - accuracy: 0.9593 - val_loss: 0.2780 - val_accuracy: 0.9606\n",
      "Epoch 16/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1530 - accuracy: 0.9629 - val_loss: 0.2592 - val_accuracy: 0.9626\n",
      "Epoch 17/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1479 - accuracy: 0.9641 - val_loss: 0.3078 - val_accuracy: 0.9465\n",
      "Epoch 18/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1507 - accuracy: 0.9638 - val_loss: 0.3055 - val_accuracy: 0.9514\n",
      "Epoch 19/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1513 - accuracy: 0.9622 - val_loss: 0.2799 - val_accuracy: 0.9450\n",
      "Epoch 20/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1352 - accuracy: 0.9682 - val_loss: 0.2765 - val_accuracy: 0.9443\n",
      "Epoch 21/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1428 - accuracy: 0.9659 - val_loss: 0.2565 - val_accuracy: 0.9641\n",
      "Epoch 22/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1359 - accuracy: 0.9675 - val_loss: 0.2868 - val_accuracy: 0.9468\n",
      "Epoch 23/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1365 - accuracy: 0.9676 - val_loss: 0.2686 - val_accuracy: 0.9586\n",
      "Epoch 24/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1338 - accuracy: 0.9672 - val_loss: 0.2727 - val_accuracy: 0.9527\n",
      "Epoch 25/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1282 - accuracy: 0.9690 - val_loss: 0.2955 - val_accuracy: 0.9551\n",
      "Epoch 26/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1279 - accuracy: 0.9699 - val_loss: 0.2439 - val_accuracy: 0.9655\n",
      "Epoch 27/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1265 - accuracy: 0.9690 - val_loss: 0.2507 - val_accuracy: 0.9630\n",
      "Epoch 28/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1230 - accuracy: 0.9711 - val_loss: 0.2651 - val_accuracy: 0.9567\n",
      "Epoch 29/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1200 - accuracy: 0.9734 - val_loss: 0.2558 - val_accuracy: 0.9701\n",
      "Epoch 30/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1286 - accuracy: 0.9686 - val_loss: 0.3103 - val_accuracy: 0.9404\n",
      "Epoch 31/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1182 - accuracy: 0.9716 - val_loss: 0.2747 - val_accuracy: 0.9496\n",
      "Epoch 32/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1170 - accuracy: 0.9714 - val_loss: 0.2606 - val_accuracy: 0.9637\n",
      "Epoch 33/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1217 - accuracy: 0.9712 - val_loss: 0.2597 - val_accuracy: 0.9604\n",
      "Epoch 34/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1169 - accuracy: 0.9730 - val_loss: 0.2374 - val_accuracy: 0.9723\n",
      "Epoch 35/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1160 - accuracy: 0.9731 - val_loss: 0.2399 - val_accuracy: 0.9639\n",
      "Epoch 36/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1141 - accuracy: 0.9731 - val_loss: 0.2519 - val_accuracy: 0.9668\n",
      "Epoch 37/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1065 - accuracy: 0.9758 - val_loss: 0.2469 - val_accuracy: 0.9648\n",
      "Epoch 38/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1181 - accuracy: 0.9721 - val_loss: 0.2290 - val_accuracy: 0.9659\n",
      "Epoch 39/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1088 - accuracy: 0.9736 - val_loss: 0.2280 - val_accuracy: 0.9758\n",
      "Epoch 40/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1043 - accuracy: 0.9767 - val_loss: 0.2344 - val_accuracy: 0.9663\n",
      "Average Validation Accuracy: 0.9777801632881165\n",
      "Average Validation Loss: 0.1421298012137413\n",
      "Average Test Accuracy: 0.9779980778694153\n",
      "Final Test Accuracy for each fold: 0.9786245822906494\n",
      "Number of input features: 13\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "1846/1846 [==============================] - 14s 7ms/step - loss: 3.5580 - accuracy: 0.3424 - val_loss: 2.3371 - val_accuracy: 0.6183\n",
      "Epoch 2/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 1.5347 - accuracy: 0.7076 - val_loss: 1.3010 - val_accuracy: 0.7650\n",
      "Epoch 3/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.8481 - accuracy: 0.8246 - val_loss: 0.9139 - val_accuracy: 0.8251\n",
      "Epoch 4/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.5658 - accuracy: 0.8729 - val_loss: 0.7056 - val_accuracy: 0.8785\n",
      "Epoch 5/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.4319 - accuracy: 0.8949 - val_loss: 0.6109 - val_accuracy: 0.9036\n",
      "Epoch 6/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.3577 - accuracy: 0.9134 - val_loss: 0.5760 - val_accuracy: 0.8997\n",
      "Epoch 7/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.3028 - accuracy: 0.9252 - val_loss: 0.5600 - val_accuracy: 0.9094\n",
      "Epoch 8/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2775 - accuracy: 0.9299 - val_loss: 0.4728 - val_accuracy: 0.9195\n",
      "Epoch 9/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2511 - accuracy: 0.9365 - val_loss: 0.5743 - val_accuracy: 0.9003\n",
      "Epoch 10/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2357 - accuracy: 0.9384 - val_loss: 0.4514 - val_accuracy: 0.9380\n",
      "Epoch 11/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2176 - accuracy: 0.9452 - val_loss: 0.4329 - val_accuracy: 0.9316\n",
      "Epoch 12/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2072 - accuracy: 0.9469 - val_loss: 0.4245 - val_accuracy: 0.9285\n",
      "Epoch 13/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2011 - accuracy: 0.9497 - val_loss: 0.3967 - val_accuracy: 0.9439\n",
      "Epoch 14/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1897 - accuracy: 0.9540 - val_loss: 0.4728 - val_accuracy: 0.9193\n",
      "Epoch 15/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1831 - accuracy: 0.9556 - val_loss: 0.3806 - val_accuracy: 0.9520\n",
      "Epoch 16/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1761 - accuracy: 0.9564 - val_loss: 0.3820 - val_accuracy: 0.9483\n",
      "Epoch 17/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1740 - accuracy: 0.9572 - val_loss: 0.3799 - val_accuracy: 0.9516\n",
      "Epoch 18/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1640 - accuracy: 0.9603 - val_loss: 0.3901 - val_accuracy: 0.9382\n",
      "Epoch 19/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1708 - accuracy: 0.9600 - val_loss: 0.3520 - val_accuracy: 0.9529\n",
      "Epoch 20/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1517 - accuracy: 0.9631 - val_loss: 0.3761 - val_accuracy: 0.9481\n",
      "Epoch 21/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1507 - accuracy: 0.9648 - val_loss: 0.3517 - val_accuracy: 0.9487\n",
      "Epoch 22/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1531 - accuracy: 0.9634 - val_loss: 0.3419 - val_accuracy: 0.9582\n",
      "Epoch 23/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1471 - accuracy: 0.9676 - val_loss: 0.3753 - val_accuracy: 0.9496\n",
      "Epoch 24/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1477 - accuracy: 0.9638 - val_loss: 0.3281 - val_accuracy: 0.9564\n",
      "Epoch 25/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1418 - accuracy: 0.9673 - val_loss: 0.3734 - val_accuracy: 0.9496\n",
      "Epoch 26/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1416 - accuracy: 0.9664 - val_loss: 0.3178 - val_accuracy: 0.9668\n",
      "Epoch 27/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1433 - accuracy: 0.9668 - val_loss: 0.3348 - val_accuracy: 0.9602\n",
      "Epoch 28/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1318 - accuracy: 0.9699 - val_loss: 0.3118 - val_accuracy: 0.9703\n",
      "Epoch 29/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1341 - accuracy: 0.9698 - val_loss: 0.2985 - val_accuracy: 0.9674\n",
      "Epoch 30/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1311 - accuracy: 0.9702 - val_loss: 0.3310 - val_accuracy: 0.9534\n",
      "Epoch 31/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1320 - accuracy: 0.9685 - val_loss: 0.3163 - val_accuracy: 0.9626\n",
      "Epoch 32/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1323 - accuracy: 0.9674 - val_loss: 0.3105 - val_accuracy: 0.9712\n",
      "Epoch 33/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1241 - accuracy: 0.9718 - val_loss: 0.2938 - val_accuracy: 0.9628\n",
      "Epoch 34/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1384 - accuracy: 0.9668 - val_loss: 0.3141 - val_accuracy: 0.9597\n",
      "Epoch 35/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1218 - accuracy: 0.9717 - val_loss: 0.3237 - val_accuracy: 0.9558\n",
      "Epoch 36/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1246 - accuracy: 0.9717 - val_loss: 0.2919 - val_accuracy: 0.9736\n",
      "Epoch 37/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1201 - accuracy: 0.9727 - val_loss: 0.3314 - val_accuracy: 0.9564\n",
      "Epoch 38/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1280 - accuracy: 0.9707 - val_loss: 0.3383 - val_accuracy: 0.9549\n",
      "Epoch 39/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1154 - accuracy: 0.9736 - val_loss: 0.3037 - val_accuracy: 0.9670\n",
      "Epoch 40/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1244 - accuracy: 0.9715 - val_loss: 0.3117 - val_accuracy: 0.9659\n",
      "Fold: 2\n",
      "Epoch 1/40\n",
      "1846/1846 [==============================] - 13s 6ms/step - loss: 3.4118 - accuracy: 0.3796 - val_loss: 2.1802 - val_accuracy: 0.6504\n",
      "Epoch 2/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 1.4192 - accuracy: 0.7348 - val_loss: 1.3156 - val_accuracy: 0.7798\n",
      "Epoch 3/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.8020 - accuracy: 0.8331 - val_loss: 0.9178 - val_accuracy: 0.8323\n",
      "Epoch 4/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.5380 - accuracy: 0.8764 - val_loss: 0.6803 - val_accuracy: 0.8878\n",
      "Epoch 5/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.4068 - accuracy: 0.9033 - val_loss: 0.5709 - val_accuracy: 0.9131\n",
      "Epoch 6/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.3267 - accuracy: 0.9218 - val_loss: 0.5156 - val_accuracy: 0.9184\n",
      "Epoch 7/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2780 - accuracy: 0.9297 - val_loss: 0.4317 - val_accuracy: 0.9281\n",
      "Epoch 8/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2466 - accuracy: 0.9393 - val_loss: 0.4232 - val_accuracy: 0.9364\n",
      "Epoch 9/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2293 - accuracy: 0.9427 - val_loss: 0.3787 - val_accuracy: 0.9419\n",
      "Epoch 10/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2107 - accuracy: 0.9481 - val_loss: 0.3657 - val_accuracy: 0.9353\n",
      "Epoch 11/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2072 - accuracy: 0.9507 - val_loss: 0.3781 - val_accuracy: 0.9470\n",
      "Epoch 12/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1878 - accuracy: 0.9564 - val_loss: 0.3785 - val_accuracy: 0.9474\n",
      "Epoch 13/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1857 - accuracy: 0.9552 - val_loss: 0.3444 - val_accuracy: 0.9439\n",
      "Epoch 14/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1703 - accuracy: 0.9611 - val_loss: 0.3532 - val_accuracy: 0.9560\n",
      "Epoch 15/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1727 - accuracy: 0.9596 - val_loss: 0.3266 - val_accuracy: 0.9580\n",
      "Epoch 16/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1670 - accuracy: 0.9623 - val_loss: 0.3334 - val_accuracy: 0.9600\n",
      "Epoch 17/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1601 - accuracy: 0.9612 - val_loss: 0.3311 - val_accuracy: 0.9560\n",
      "Epoch 18/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1537 - accuracy: 0.9648 - val_loss: 0.3076 - val_accuracy: 0.9582\n",
      "Epoch 19/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1568 - accuracy: 0.9637 - val_loss: 0.3311 - val_accuracy: 0.9527\n",
      "Epoch 20/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1434 - accuracy: 0.9656 - val_loss: 0.3196 - val_accuracy: 0.9567\n",
      "Epoch 21/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1465 - accuracy: 0.9662 - val_loss: 0.2997 - val_accuracy: 0.9567\n",
      "Epoch 22/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1455 - accuracy: 0.9692 - val_loss: 0.3241 - val_accuracy: 0.9498\n",
      "Epoch 23/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1467 - accuracy: 0.9656 - val_loss: 0.2969 - val_accuracy: 0.9624\n",
      "Epoch 24/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1362 - accuracy: 0.9692 - val_loss: 0.2919 - val_accuracy: 0.9560\n",
      "Epoch 25/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1337 - accuracy: 0.9693 - val_loss: 0.2821 - val_accuracy: 0.9626\n",
      "Epoch 26/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1365 - accuracy: 0.9673 - val_loss: 0.2843 - val_accuracy: 0.9608\n",
      "Epoch 27/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1239 - accuracy: 0.9720 - val_loss: 0.3502 - val_accuracy: 0.9402\n",
      "Epoch 28/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1302 - accuracy: 0.9705 - val_loss: 0.3093 - val_accuracy: 0.9575\n",
      "Epoch 29/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1257 - accuracy: 0.9707 - val_loss: 0.3161 - val_accuracy: 0.9443\n",
      "Epoch 30/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1283 - accuracy: 0.9691 - val_loss: 0.3155 - val_accuracy: 0.9538\n",
      "Epoch 31/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1255 - accuracy: 0.9707 - val_loss: 0.2917 - val_accuracy: 0.9639\n",
      "Epoch 32/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1227 - accuracy: 0.9707 - val_loss: 0.2949 - val_accuracy: 0.9657\n",
      "Epoch 33/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1193 - accuracy: 0.9727 - val_loss: 0.2865 - val_accuracy: 0.9663\n",
      "Epoch 34/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1206 - accuracy: 0.9723 - val_loss: 0.3292 - val_accuracy: 0.9386\n",
      "Epoch 35/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1162 - accuracy: 0.9724 - val_loss: 0.3566 - val_accuracy: 0.9448\n",
      "Epoch 36/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1227 - accuracy: 0.9699 - val_loss: 0.3032 - val_accuracy: 0.9553\n",
      "Epoch 37/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1140 - accuracy: 0.9733 - val_loss: 0.2925 - val_accuracy: 0.9641\n",
      "Epoch 38/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1141 - accuracy: 0.9727 - val_loss: 0.2777 - val_accuracy: 0.9630\n",
      "Epoch 39/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1155 - accuracy: 0.9721 - val_loss: 0.2766 - val_accuracy: 0.9712\n",
      "Epoch 40/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1147 - accuracy: 0.9737 - val_loss: 0.2960 - val_accuracy: 0.9644\n",
      "Average Validation Accuracy: 0.9731329083442688\n",
      "Average Validation Loss: 0.17354832589626312\n",
      "Average Test Accuracy: 0.9736124277114868\n",
      "Final Test Accuracy for each fold: 0.9753077030181885\n",
      "Number of input features: 14\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "1846/1846 [==============================] - 13s 6ms/step - loss: 3.4744 - accuracy: 0.3759 - val_loss: 2.2526 - val_accuracy: 0.6341\n",
      "Epoch 2/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 1.4608 - accuracy: 0.7276 - val_loss: 1.2536 - val_accuracy: 0.7806\n",
      "Epoch 3/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.7885 - accuracy: 0.8420 - val_loss: 0.8574 - val_accuracy: 0.8508\n",
      "Epoch 4/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.5069 - accuracy: 0.8858 - val_loss: 0.6765 - val_accuracy: 0.8812\n",
      "Epoch 5/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.3773 - accuracy: 0.9117 - val_loss: 0.5782 - val_accuracy: 0.9120\n",
      "Epoch 6/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.3144 - accuracy: 0.9229 - val_loss: 0.5074 - val_accuracy: 0.9230\n",
      "Epoch 7/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2750 - accuracy: 0.9331 - val_loss: 0.4796 - val_accuracy: 0.9283\n",
      "Epoch 8/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2439 - accuracy: 0.9389 - val_loss: 0.4599 - val_accuracy: 0.9303\n",
      "Epoch 9/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2181 - accuracy: 0.9471 - val_loss: 0.4659 - val_accuracy: 0.9221\n",
      "Epoch 10/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2154 - accuracy: 0.9440 - val_loss: 0.3926 - val_accuracy: 0.9490\n",
      "Epoch 11/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1970 - accuracy: 0.9511 - val_loss: 0.4208 - val_accuracy: 0.9270\n",
      "Epoch 12/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1857 - accuracy: 0.9533 - val_loss: 0.3934 - val_accuracy: 0.9408\n",
      "Epoch 13/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1872 - accuracy: 0.9538 - val_loss: 0.3640 - val_accuracy: 0.9468\n",
      "Epoch 14/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1741 - accuracy: 0.9583 - val_loss: 0.3841 - val_accuracy: 0.9461\n",
      "Epoch 15/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1690 - accuracy: 0.9598 - val_loss: 0.3635 - val_accuracy: 0.9496\n",
      "Epoch 16/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1629 - accuracy: 0.9611 - val_loss: 0.3526 - val_accuracy: 0.9549\n",
      "Epoch 17/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1625 - accuracy: 0.9620 - val_loss: 0.3339 - val_accuracy: 0.9569\n",
      "Epoch 18/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1534 - accuracy: 0.9628 - val_loss: 0.3328 - val_accuracy: 0.9569\n",
      "Epoch 19/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1515 - accuracy: 0.9641 - val_loss: 0.3232 - val_accuracy: 0.9624\n",
      "Epoch 20/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1513 - accuracy: 0.9641 - val_loss: 0.3367 - val_accuracy: 0.9545\n",
      "Epoch 21/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1450 - accuracy: 0.9663 - val_loss: 0.3390 - val_accuracy: 0.9556\n",
      "Epoch 22/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1414 - accuracy: 0.9672 - val_loss: 0.3019 - val_accuracy: 0.9666\n",
      "Epoch 23/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1383 - accuracy: 0.9686 - val_loss: 0.3470 - val_accuracy: 0.9586\n",
      "Epoch 24/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1371 - accuracy: 0.9674 - val_loss: 0.3197 - val_accuracy: 0.9633\n",
      "Epoch 25/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1407 - accuracy: 0.9675 - val_loss: 0.3162 - val_accuracy: 0.9674\n",
      "Epoch 26/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1336 - accuracy: 0.9682 - val_loss: 0.3245 - val_accuracy: 0.9582\n",
      "Epoch 27/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1285 - accuracy: 0.9699 - val_loss: 0.3158 - val_accuracy: 0.9630\n",
      "Epoch 28/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1305 - accuracy: 0.9701 - val_loss: 0.3329 - val_accuracy: 0.9593\n",
      "Epoch 29/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1250 - accuracy: 0.9707 - val_loss: 0.3030 - val_accuracy: 0.9747\n",
      "Epoch 30/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1333 - accuracy: 0.9680 - val_loss: 0.2967 - val_accuracy: 0.9650\n",
      "Epoch 31/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1194 - accuracy: 0.9723 - val_loss: 0.2958 - val_accuracy: 0.9666\n",
      "Epoch 32/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1237 - accuracy: 0.9703 - val_loss: 0.3211 - val_accuracy: 0.9591\n",
      "Epoch 33/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1199 - accuracy: 0.9734 - val_loss: 0.3311 - val_accuracy: 0.9509\n",
      "Epoch 34/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1302 - accuracy: 0.9710 - val_loss: 0.3122 - val_accuracy: 0.9600\n",
      "Epoch 35/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1122 - accuracy: 0.9756 - val_loss: 0.3008 - val_accuracy: 0.9635\n",
      "Epoch 36/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1232 - accuracy: 0.9710 - val_loss: 0.3418 - val_accuracy: 0.9534\n",
      "Epoch 37/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1184 - accuracy: 0.9718 - val_loss: 0.2963 - val_accuracy: 0.9677\n",
      "Epoch 38/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1169 - accuracy: 0.9733 - val_loss: 0.3201 - val_accuracy: 0.9626\n",
      "Epoch 39/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1187 - accuracy: 0.9728 - val_loss: 0.2904 - val_accuracy: 0.9721\n",
      "Epoch 40/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1166 - accuracy: 0.9730 - val_loss: 0.2982 - val_accuracy: 0.9641\n",
      "Fold: 2\n",
      "Epoch 1/40\n",
      "1846/1846 [==============================] - 14s 6ms/step - loss: 3.4253 - accuracy: 0.3835 - val_loss: 2.2546 - val_accuracy: 0.6266\n",
      "Epoch 2/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 1.4455 - accuracy: 0.7302 - val_loss: 1.3470 - val_accuracy: 0.7677\n",
      "Epoch 3/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.8043 - accuracy: 0.8408 - val_loss: 0.9382 - val_accuracy: 0.8508\n",
      "Epoch 4/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.5218 - accuracy: 0.8879 - val_loss: 0.7279 - val_accuracy: 0.8708\n",
      "Epoch 5/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.3865 - accuracy: 0.9108 - val_loss: 0.6343 - val_accuracy: 0.8946\n",
      "Epoch 6/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.3282 - accuracy: 0.9199 - val_loss: 0.5665 - val_accuracy: 0.9067\n",
      "Epoch 7/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2802 - accuracy: 0.9288 - val_loss: 0.4568 - val_accuracy: 0.9278\n",
      "Epoch 8/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2518 - accuracy: 0.9358 - val_loss: 0.4143 - val_accuracy: 0.9347\n",
      "Epoch 9/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2273 - accuracy: 0.9434 - val_loss: 0.3997 - val_accuracy: 0.9217\n",
      "Epoch 10/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2098 - accuracy: 0.9483 - val_loss: 0.3657 - val_accuracy: 0.9360\n",
      "Epoch 11/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1966 - accuracy: 0.9519 - val_loss: 0.3617 - val_accuracy: 0.9452\n",
      "Epoch 12/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1911 - accuracy: 0.9517 - val_loss: 0.3447 - val_accuracy: 0.9362\n",
      "Epoch 13/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1808 - accuracy: 0.9562 - val_loss: 0.3673 - val_accuracy: 0.9380\n",
      "Epoch 14/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1701 - accuracy: 0.9590 - val_loss: 0.3162 - val_accuracy: 0.9560\n",
      "Epoch 15/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1642 - accuracy: 0.9603 - val_loss: 0.3001 - val_accuracy: 0.9558\n",
      "Epoch 16/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1608 - accuracy: 0.9609 - val_loss: 0.3192 - val_accuracy: 0.9448\n",
      "Epoch 17/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1608 - accuracy: 0.9620 - val_loss: 0.2966 - val_accuracy: 0.9534\n",
      "Epoch 18/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1484 - accuracy: 0.9649 - val_loss: 0.3006 - val_accuracy: 0.9393\n",
      "Epoch 19/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1543 - accuracy: 0.9637 - val_loss: 0.2755 - val_accuracy: 0.9578\n",
      "Epoch 20/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1390 - accuracy: 0.9676 - val_loss: 0.2728 - val_accuracy: 0.9637\n",
      "Epoch 21/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1457 - accuracy: 0.9654 - val_loss: 0.3102 - val_accuracy: 0.9322\n",
      "Epoch 22/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1345 - accuracy: 0.9674 - val_loss: 0.2503 - val_accuracy: 0.9685\n",
      "Epoch 23/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1344 - accuracy: 0.9680 - val_loss: 0.2708 - val_accuracy: 0.9679\n",
      "Epoch 24/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1313 - accuracy: 0.9701 - val_loss: 0.2588 - val_accuracy: 0.9591\n",
      "Epoch 25/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1380 - accuracy: 0.9681 - val_loss: 0.2410 - val_accuracy: 0.9666\n",
      "Epoch 26/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1354 - accuracy: 0.9682 - val_loss: 0.2407 - val_accuracy: 0.9644\n",
      "Epoch 27/40\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1277 - accuracy: 0.9703 - val_loss: 0.2388 - val_accuracy: 0.9635\n",
      "Epoch 28/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1252 - accuracy: 0.9707 - val_loss: 0.2566 - val_accuracy: 0.9628\n",
      "Epoch 29/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1289 - accuracy: 0.9708 - val_loss: 0.2533 - val_accuracy: 0.9584\n",
      "Epoch 30/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1286 - accuracy: 0.9698 - val_loss: 0.2427 - val_accuracy: 0.9608\n",
      "Epoch 31/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1148 - accuracy: 0.9721 - val_loss: 0.2335 - val_accuracy: 0.9624\n",
      "Epoch 32/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1160 - accuracy: 0.9741 - val_loss: 0.2723 - val_accuracy: 0.9558\n",
      "Epoch 33/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1247 - accuracy: 0.9704 - val_loss: 0.2424 - val_accuracy: 0.9661\n",
      "Epoch 34/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1193 - accuracy: 0.9708 - val_loss: 0.2385 - val_accuracy: 0.9672\n",
      "Epoch 35/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1170 - accuracy: 0.9721 - val_loss: 0.2454 - val_accuracy: 0.9648\n",
      "Epoch 36/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1217 - accuracy: 0.9714 - val_loss: 0.2388 - val_accuracy: 0.9639\n",
      "Epoch 37/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1134 - accuracy: 0.9729 - val_loss: 0.2750 - val_accuracy: 0.9496\n",
      "Epoch 38/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1095 - accuracy: 0.9745 - val_loss: 0.2342 - val_accuracy: 0.9657\n",
      "Epoch 39/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1115 - accuracy: 0.9737 - val_loss: 0.2629 - val_accuracy: 0.9630\n",
      "Epoch 40/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1186 - accuracy: 0.9697 - val_loss: 0.2280 - val_accuracy: 0.9637\n",
      "Average Validation Accuracy: 0.9710997045040131\n",
      "Average Validation Loss: 0.1564265862107277\n",
      "Average Test Accuracy: 0.9742757976055145\n",
      "Final Test Accuracy for each fold: 0.9753077030181885\n",
      "Number of input features: 15\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "1846/1846 [==============================] - 13s 6ms/step - loss: 3.3294 - accuracy: 0.3875 - val_loss: 2.0903 - val_accuracy: 0.6264\n",
      "Epoch 2/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 1.3130 - accuracy: 0.7492 - val_loss: 1.1178 - val_accuracy: 0.8033\n",
      "Epoch 3/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.7122 - accuracy: 0.8437 - val_loss: 0.7651 - val_accuracy: 0.8543\n",
      "Epoch 4/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.4851 - accuracy: 0.8861 - val_loss: 0.6023 - val_accuracy: 0.8768\n",
      "Epoch 5/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.3756 - accuracy: 0.9077 - val_loss: 0.5203 - val_accuracy: 0.8922\n",
      "Epoch 6/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.3246 - accuracy: 0.9207 - val_loss: 0.4564 - val_accuracy: 0.9243\n",
      "Epoch 7/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2813 - accuracy: 0.9297 - val_loss: 0.4612 - val_accuracy: 0.9188\n",
      "Epoch 8/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2694 - accuracy: 0.9323 - val_loss: 0.4411 - val_accuracy: 0.9061\n",
      "Epoch 9/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2400 - accuracy: 0.9393 - val_loss: 0.3892 - val_accuracy: 0.9342\n",
      "Epoch 10/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2228 - accuracy: 0.9443 - val_loss: 0.3614 - val_accuracy: 0.9320\n",
      "Epoch 11/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2148 - accuracy: 0.9475 - val_loss: 0.3203 - val_accuracy: 0.9487\n",
      "Epoch 12/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2051 - accuracy: 0.9492 - val_loss: 0.3852 - val_accuracy: 0.9355\n",
      "Epoch 13/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1894 - accuracy: 0.9536 - val_loss: 0.3290 - val_accuracy: 0.9362\n",
      "Epoch 14/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1830 - accuracy: 0.9562 - val_loss: 0.3370 - val_accuracy: 0.9417\n",
      "Epoch 15/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1810 - accuracy: 0.9570 - val_loss: 0.3032 - val_accuracy: 0.9584\n",
      "Epoch 16/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1698 - accuracy: 0.9584 - val_loss: 0.3144 - val_accuracy: 0.9485\n",
      "Epoch 17/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1695 - accuracy: 0.9572 - val_loss: 0.2983 - val_accuracy: 0.9518\n",
      "Epoch 18/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1640 - accuracy: 0.9587 - val_loss: 0.2996 - val_accuracy: 0.9446\n",
      "Epoch 19/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1627 - accuracy: 0.9610 - val_loss: 0.2932 - val_accuracy: 0.9481\n",
      "Epoch 20/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1481 - accuracy: 0.9662 - val_loss: 0.2691 - val_accuracy: 0.9582\n",
      "Epoch 21/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1503 - accuracy: 0.9642 - val_loss: 0.2974 - val_accuracy: 0.9509\n",
      "Epoch 22/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1496 - accuracy: 0.9646 - val_loss: 0.2846 - val_accuracy: 0.9518\n",
      "Epoch 23/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1434 - accuracy: 0.9670 - val_loss: 0.2692 - val_accuracy: 0.9494\n",
      "Epoch 24/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1388 - accuracy: 0.9677 - val_loss: 0.2839 - val_accuracy: 0.9472\n",
      "Epoch 25/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1461 - accuracy: 0.9667 - val_loss: 0.3216 - val_accuracy: 0.9441\n",
      "Epoch 26/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1291 - accuracy: 0.9699 - val_loss: 0.2537 - val_accuracy: 0.9646\n",
      "Epoch 27/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1334 - accuracy: 0.9689 - val_loss: 0.2556 - val_accuracy: 0.9604\n",
      "Epoch 28/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1321 - accuracy: 0.9695 - val_loss: 0.3335 - val_accuracy: 0.9344\n",
      "Epoch 29/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1321 - accuracy: 0.9693 - val_loss: 0.2438 - val_accuracy: 0.9732\n",
      "Epoch 30/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1294 - accuracy: 0.9684 - val_loss: 0.2638 - val_accuracy: 0.9611\n",
      "Epoch 31/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1311 - accuracy: 0.9705 - val_loss: 0.2515 - val_accuracy: 0.9604\n",
      "Epoch 32/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1345 - accuracy: 0.9674 - val_loss: 0.2521 - val_accuracy: 0.9712\n",
      "Epoch 33/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1193 - accuracy: 0.9705 - val_loss: 0.2951 - val_accuracy: 0.9496\n",
      "Epoch 34/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1195 - accuracy: 0.9725 - val_loss: 0.2591 - val_accuracy: 0.9630\n",
      "Epoch 35/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1254 - accuracy: 0.9700 - val_loss: 0.3385 - val_accuracy: 0.9406\n",
      "Epoch 36/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1228 - accuracy: 0.9720 - val_loss: 0.2498 - val_accuracy: 0.9619\n",
      "Epoch 37/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1164 - accuracy: 0.9726 - val_loss: 0.2515 - val_accuracy: 0.9626\n",
      "Epoch 38/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1170 - accuracy: 0.9721 - val_loss: 0.2393 - val_accuracy: 0.9646\n",
      "Epoch 39/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1166 - accuracy: 0.9747 - val_loss: 0.2317 - val_accuracy: 0.9688\n",
      "Epoch 40/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1244 - accuracy: 0.9695 - val_loss: 0.2423 - val_accuracy: 0.9639\n",
      "Fold: 2\n",
      "Epoch 1/40\n",
      "1846/1846 [==============================] - 13s 6ms/step - loss: 3.4403 - accuracy: 0.3523 - val_loss: 2.1706 - val_accuracy: 0.6392\n",
      "Epoch 2/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 1.3715 - accuracy: 0.7376 - val_loss: 1.2495 - val_accuracy: 0.7800\n",
      "Epoch 3/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.7698 - accuracy: 0.8314 - val_loss: 0.9526 - val_accuracy: 0.8288\n",
      "Epoch 4/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.5206 - accuracy: 0.8776 - val_loss: 0.7814 - val_accuracy: 0.8576\n",
      "Epoch 5/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.4081 - accuracy: 0.8996 - val_loss: 0.6182 - val_accuracy: 0.8933\n",
      "Epoch 6/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.3396 - accuracy: 0.9122 - val_loss: 0.5726 - val_accuracy: 0.8979\n",
      "Epoch 7/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2942 - accuracy: 0.9259 - val_loss: 0.4863 - val_accuracy: 0.9127\n",
      "Epoch 8/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2697 - accuracy: 0.9299 - val_loss: 0.4668 - val_accuracy: 0.9091\n",
      "Epoch 9/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2477 - accuracy: 0.9345 - val_loss: 0.4441 - val_accuracy: 0.9287\n",
      "Epoch 10/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2350 - accuracy: 0.9395 - val_loss: 0.4862 - val_accuracy: 0.9058\n",
      "Epoch 11/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2124 - accuracy: 0.9457 - val_loss: 0.3871 - val_accuracy: 0.9424\n",
      "Epoch 12/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2108 - accuracy: 0.9469 - val_loss: 0.4014 - val_accuracy: 0.9085\n",
      "Epoch 13/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1899 - accuracy: 0.9523 - val_loss: 0.3725 - val_accuracy: 0.9366\n",
      "Epoch 14/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1868 - accuracy: 0.9521 - val_loss: 0.3562 - val_accuracy: 0.9375\n",
      "Epoch 15/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1816 - accuracy: 0.9561 - val_loss: 0.3172 - val_accuracy: 0.9608\n",
      "Epoch 16/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1717 - accuracy: 0.9593 - val_loss: 0.3564 - val_accuracy: 0.9472\n",
      "Epoch 17/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1762 - accuracy: 0.9585 - val_loss: 0.3125 - val_accuracy: 0.9534\n",
      "Epoch 18/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1680 - accuracy: 0.9582 - val_loss: 0.3693 - val_accuracy: 0.9325\n",
      "Epoch 19/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1630 - accuracy: 0.9614 - val_loss: 0.3587 - val_accuracy: 0.9406\n",
      "Epoch 20/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1677 - accuracy: 0.9575 - val_loss: 0.3325 - val_accuracy: 0.9421\n",
      "Epoch 21/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1462 - accuracy: 0.9674 - val_loss: 0.3468 - val_accuracy: 0.9463\n",
      "Epoch 22/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1613 - accuracy: 0.9594 - val_loss: 0.3129 - val_accuracy: 0.9494\n",
      "Epoch 23/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1373 - accuracy: 0.9689 - val_loss: 0.3125 - val_accuracy: 0.9483\n",
      "Epoch 24/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1455 - accuracy: 0.9665 - val_loss: 0.3609 - val_accuracy: 0.9408\n",
      "Epoch 25/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1437 - accuracy: 0.9645 - val_loss: 0.3404 - val_accuracy: 0.9476\n",
      "Epoch 26/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1421 - accuracy: 0.9674 - val_loss: 0.2996 - val_accuracy: 0.9604\n",
      "Epoch 27/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1361 - accuracy: 0.9691 - val_loss: 0.2758 - val_accuracy: 0.9571\n",
      "Epoch 28/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1419 - accuracy: 0.9674 - val_loss: 0.2901 - val_accuracy: 0.9564\n",
      "Epoch 29/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1291 - accuracy: 0.9718 - val_loss: 0.3468 - val_accuracy: 0.9483\n",
      "Epoch 30/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1328 - accuracy: 0.9704 - val_loss: 0.3587 - val_accuracy: 0.9531\n",
      "Epoch 31/40\n",
      "1846/1846 [==============================] - 12s 6ms/step - loss: 0.1339 - accuracy: 0.9678 - val_loss: 0.2780 - val_accuracy: 0.9617\n",
      "Epoch 32/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1275 - accuracy: 0.9701 - val_loss: 0.2756 - val_accuracy: 0.9648\n",
      "Epoch 33/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1262 - accuracy: 0.9713 - val_loss: 0.2730 - val_accuracy: 0.9624\n",
      "Epoch 34/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1256 - accuracy: 0.9701 - val_loss: 0.2682 - val_accuracy: 0.9622\n",
      "Epoch 35/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1251 - accuracy: 0.9708 - val_loss: 0.3049 - val_accuracy: 0.9571\n",
      "Epoch 36/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1229 - accuracy: 0.9703 - val_loss: 0.2966 - val_accuracy: 0.9551\n",
      "Epoch 37/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1284 - accuracy: 0.9693 - val_loss: 0.2684 - val_accuracy: 0.9685\n",
      "Epoch 38/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1279 - accuracy: 0.9705 - val_loss: 0.2626 - val_accuracy: 0.9699\n",
      "Epoch 39/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1259 - accuracy: 0.9714 - val_loss: 0.2560 - val_accuracy: 0.9672\n",
      "Epoch 40/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1173 - accuracy: 0.9732 - val_loss: 0.2609 - val_accuracy: 0.9650\n",
      "Average Validation Accuracy: 0.9708093702793121\n",
      "Average Validation Loss: 0.15460553020238876\n",
      "Average Test Accuracy: 0.9726542532444\n",
      "Final Test Accuracy for each fold: 0.9739072918891907\n",
      "Number of input features: 16\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "1846/1846 [==============================] - 13s 6ms/step - loss: 3.5338 - accuracy: 0.3390 - val_loss: 2.2491 - val_accuracy: 0.5870\n",
      "Epoch 2/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 1.4548 - accuracy: 0.7227 - val_loss: 1.2201 - val_accuracy: 0.7881\n",
      "Epoch 3/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.7965 - accuracy: 0.8318 - val_loss: 0.8956 - val_accuracy: 0.8317\n",
      "Epoch 4/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.5383 - accuracy: 0.8765 - val_loss: 0.6679 - val_accuracy: 0.8821\n",
      "Epoch 5/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.4076 - accuracy: 0.9007 - val_loss: 0.5933 - val_accuracy: 0.8968\n",
      "Epoch 6/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.3414 - accuracy: 0.9138 - val_loss: 0.5596 - val_accuracy: 0.9045\n",
      "Epoch 7/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2948 - accuracy: 0.9259 - val_loss: 0.5834 - val_accuracy: 0.8792\n",
      "Epoch 8/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2567 - accuracy: 0.9345 - val_loss: 0.4644 - val_accuracy: 0.9019\n",
      "Epoch 9/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2331 - accuracy: 0.9389 - val_loss: 0.4072 - val_accuracy: 0.9256\n",
      "Epoch 10/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2134 - accuracy: 0.9470 - val_loss: 0.3793 - val_accuracy: 0.9344\n",
      "Epoch 11/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2006 - accuracy: 0.9493 - val_loss: 0.4002 - val_accuracy: 0.9274\n",
      "Epoch 12/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1826 - accuracy: 0.9539 - val_loss: 0.3900 - val_accuracy: 0.9276\n",
      "Epoch 13/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1801 - accuracy: 0.9544 - val_loss: 0.3443 - val_accuracy: 0.9492\n",
      "Epoch 14/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1784 - accuracy: 0.9558 - val_loss: 0.3502 - val_accuracy: 0.9371\n",
      "Epoch 15/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1673 - accuracy: 0.9573 - val_loss: 0.3303 - val_accuracy: 0.9496\n",
      "Epoch 16/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1562 - accuracy: 0.9622 - val_loss: 0.3375 - val_accuracy: 0.9435\n",
      "Epoch 17/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1598 - accuracy: 0.9612 - val_loss: 0.3108 - val_accuracy: 0.9633\n",
      "Epoch 18/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1500 - accuracy: 0.9625 - val_loss: 0.3174 - val_accuracy: 0.9496\n",
      "Epoch 19/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1433 - accuracy: 0.9655 - val_loss: 0.3015 - val_accuracy: 0.9562\n",
      "Epoch 20/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1362 - accuracy: 0.9674 - val_loss: 0.3049 - val_accuracy: 0.9633\n",
      "Epoch 21/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1427 - accuracy: 0.9654 - val_loss: 0.3022 - val_accuracy: 0.9538\n",
      "Epoch 22/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1349 - accuracy: 0.9670 - val_loss: 0.3019 - val_accuracy: 0.9626\n",
      "Epoch 23/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1360 - accuracy: 0.9684 - val_loss: 0.3022 - val_accuracy: 0.9606\n",
      "Epoch 24/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1341 - accuracy: 0.9665 - val_loss: 0.3010 - val_accuracy: 0.9481\n",
      "Epoch 25/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1290 - accuracy: 0.9685 - val_loss: 0.3232 - val_accuracy: 0.9538\n",
      "Epoch 26/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1277 - accuracy: 0.9704 - val_loss: 0.2747 - val_accuracy: 0.9694\n",
      "Epoch 27/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1238 - accuracy: 0.9711 - val_loss: 0.2841 - val_accuracy: 0.9679\n",
      "Epoch 28/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1188 - accuracy: 0.9740 - val_loss: 0.3044 - val_accuracy: 0.9545\n",
      "Epoch 29/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1270 - accuracy: 0.9689 - val_loss: 0.3057 - val_accuracy: 0.9622\n",
      "Epoch 30/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1166 - accuracy: 0.9741 - val_loss: 0.2844 - val_accuracy: 0.9767\n",
      "Epoch 31/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1150 - accuracy: 0.9728 - val_loss: 0.2772 - val_accuracy: 0.9703\n",
      "Epoch 32/40\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1202 - accuracy: 0.9713 - val_loss: 0.3850 - val_accuracy: 0.9248\n",
      "Epoch 33/40\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1127 - accuracy: 0.9750 - val_loss: 0.2722 - val_accuracy: 0.9712\n",
      "Epoch 34/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1103 - accuracy: 0.9740 - val_loss: 0.3268 - val_accuracy: 0.9597\n",
      "Epoch 35/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1134 - accuracy: 0.9753 - val_loss: 0.3371 - val_accuracy: 0.9450\n",
      "Epoch 36/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1207 - accuracy: 0.9713 - val_loss: 0.2785 - val_accuracy: 0.9672\n",
      "Epoch 37/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1039 - accuracy: 0.9765 - val_loss: 0.3036 - val_accuracy: 0.9646\n",
      "Epoch 38/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1125 - accuracy: 0.9739 - val_loss: 0.3174 - val_accuracy: 0.9503\n",
      "Epoch 39/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1036 - accuracy: 0.9767 - val_loss: 0.2859 - val_accuracy: 0.9626\n",
      "Epoch 40/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1103 - accuracy: 0.9740 - val_loss: 0.2683 - val_accuracy: 0.9765\n",
      "Fold: 2\n",
      "Epoch 1/40\n",
      "1846/1846 [==============================] - 10s 4ms/step - loss: 3.2575 - accuracy: 0.3904 - val_loss: 2.1621 - val_accuracy: 0.6174\n",
      "Epoch 2/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 1.4000 - accuracy: 0.7306 - val_loss: 1.2998 - val_accuracy: 0.7811\n",
      "Epoch 3/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.8041 - accuracy: 0.8352 - val_loss: 0.9845 - val_accuracy: 0.8433\n",
      "Epoch 4/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.5569 - accuracy: 0.8778 - val_loss: 0.7422 - val_accuracy: 0.8865\n",
      "Epoch 5/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.4228 - accuracy: 0.8994 - val_loss: 0.6549 - val_accuracy: 0.8840\n",
      "Epoch 6/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.3449 - accuracy: 0.9164 - val_loss: 0.5818 - val_accuracy: 0.9039\n",
      "Epoch 7/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.3071 - accuracy: 0.9241 - val_loss: 0.5302 - val_accuracy: 0.9186\n",
      "Epoch 8/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.2720 - accuracy: 0.9314 - val_loss: 0.4702 - val_accuracy: 0.9296\n",
      "Epoch 9/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.2401 - accuracy: 0.9387 - val_loss: 0.4483 - val_accuracy: 0.9175\n",
      "Epoch 10/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.2274 - accuracy: 0.9406 - val_loss: 0.4105 - val_accuracy: 0.9322\n",
      "Epoch 11/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.2085 - accuracy: 0.9464 - val_loss: 0.4468 - val_accuracy: 0.9285\n",
      "Epoch 12/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.2007 - accuracy: 0.9492 - val_loss: 0.4222 - val_accuracy: 0.9316\n",
      "Epoch 13/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1932 - accuracy: 0.9496 - val_loss: 0.3854 - val_accuracy: 0.9457\n",
      "Epoch 14/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1869 - accuracy: 0.9546 - val_loss: 0.3824 - val_accuracy: 0.9430\n",
      "Epoch 15/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1752 - accuracy: 0.9553 - val_loss: 0.3774 - val_accuracy: 0.9419\n",
      "Epoch 16/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1718 - accuracy: 0.9560 - val_loss: 0.5662 - val_accuracy: 0.9023\n",
      "Epoch 17/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1598 - accuracy: 0.9600 - val_loss: 0.3686 - val_accuracy: 0.9501\n",
      "Epoch 18/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1548 - accuracy: 0.9638 - val_loss: 0.3530 - val_accuracy: 0.9564\n",
      "Epoch 19/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1595 - accuracy: 0.9604 - val_loss: 0.3288 - val_accuracy: 0.9468\n",
      "Epoch 20/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1452 - accuracy: 0.9647 - val_loss: 0.4552 - val_accuracy: 0.9204\n",
      "Epoch 21/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1422 - accuracy: 0.9664 - val_loss: 0.3077 - val_accuracy: 0.9628\n",
      "Epoch 22/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1396 - accuracy: 0.9662 - val_loss: 0.3112 - val_accuracy: 0.9547\n",
      "Epoch 23/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1324 - accuracy: 0.9691 - val_loss: 0.3487 - val_accuracy: 0.9410\n",
      "Epoch 24/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1338 - accuracy: 0.9669 - val_loss: 0.3028 - val_accuracy: 0.9580\n",
      "Epoch 25/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1342 - accuracy: 0.9660 - val_loss: 0.2958 - val_accuracy: 0.9674\n",
      "Epoch 26/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1247 - accuracy: 0.9717 - val_loss: 0.3506 - val_accuracy: 0.9487\n",
      "Epoch 27/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1299 - accuracy: 0.9687 - val_loss: 0.2869 - val_accuracy: 0.9738\n",
      "Epoch 28/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1290 - accuracy: 0.9682 - val_loss: 0.2978 - val_accuracy: 0.9622\n",
      "Epoch 29/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1197 - accuracy: 0.9727 - val_loss: 0.3063 - val_accuracy: 0.9520\n",
      "Epoch 30/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1209 - accuracy: 0.9714 - val_loss: 0.3033 - val_accuracy: 0.9644\n",
      "Epoch 31/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1238 - accuracy: 0.9699 - val_loss: 0.4027 - val_accuracy: 0.9417\n",
      "Epoch 32/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1202 - accuracy: 0.9704 - val_loss: 0.3008 - val_accuracy: 0.9655\n",
      "Epoch 33/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1125 - accuracy: 0.9731 - val_loss: 0.2737 - val_accuracy: 0.9787\n",
      "Epoch 34/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1249 - accuracy: 0.9687 - val_loss: 0.2843 - val_accuracy: 0.9648\n",
      "Epoch 35/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1157 - accuracy: 0.9729 - val_loss: 0.2836 - val_accuracy: 0.9732\n",
      "Epoch 36/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1148 - accuracy: 0.9751 - val_loss: 0.2799 - val_accuracy: 0.9657\n",
      "Epoch 37/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1118 - accuracy: 0.9729 - val_loss: 0.4719 - val_accuracy: 0.9316\n",
      "Epoch 38/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1084 - accuracy: 0.9754 - val_loss: 0.3020 - val_accuracy: 0.9657\n",
      "Epoch 39/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1073 - accuracy: 0.9758 - val_loss: 0.3178 - val_accuracy: 0.9505\n",
      "Epoch 40/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1082 - accuracy: 0.9754 - val_loss: 0.2920 - val_accuracy: 0.9646\n",
      "Average Validation Accuracy: 0.9779253602027893\n",
      "Average Validation Loss: 0.1552867367863655\n",
      "Average Test Accuracy: 0.9792879819869995\n",
      "Final Test Accuracy for each fold: 0.9806147217750549\n",
      "Number of input features: 17\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "1846/1846 [==============================] - 10s 4ms/step - loss: 3.5330 - accuracy: 0.3351 - val_loss: 2.2953 - val_accuracy: 0.5985\n",
      "Epoch 2/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 1.5260 - accuracy: 0.7205 - val_loss: 1.3512 - val_accuracy: 0.7736\n",
      "Epoch 3/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.8901 - accuracy: 0.8251 - val_loss: 0.9372 - val_accuracy: 0.8484\n",
      "Epoch 4/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.6079 - accuracy: 0.8682 - val_loss: 0.7879 - val_accuracy: 0.8722\n",
      "Epoch 5/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.4453 - accuracy: 0.8972 - val_loss: 0.6442 - val_accuracy: 0.8975\n",
      "Epoch 6/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.3580 - accuracy: 0.9146 - val_loss: 0.5525 - val_accuracy: 0.9164\n",
      "Epoch 7/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.3008 - accuracy: 0.9284 - val_loss: 0.5352 - val_accuracy: 0.9245\n",
      "Epoch 8/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.2513 - accuracy: 0.9405 - val_loss: 0.4792 - val_accuracy: 0.9329\n",
      "Epoch 9/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.2320 - accuracy: 0.9419 - val_loss: 0.4272 - val_accuracy: 0.9375\n",
      "Epoch 10/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.2095 - accuracy: 0.9467 - val_loss: 0.4208 - val_accuracy: 0.9413\n",
      "Epoch 11/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1890 - accuracy: 0.9548 - val_loss: 0.4182 - val_accuracy: 0.9386\n",
      "Epoch 12/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1925 - accuracy: 0.9519 - val_loss: 0.3998 - val_accuracy: 0.9393\n",
      "Epoch 13/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1714 - accuracy: 0.9594 - val_loss: 0.3782 - val_accuracy: 0.9457\n",
      "Epoch 14/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1661 - accuracy: 0.9575 - val_loss: 0.3807 - val_accuracy: 0.9410\n",
      "Epoch 15/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1606 - accuracy: 0.9614 - val_loss: 0.3572 - val_accuracy: 0.9520\n",
      "Epoch 16/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1489 - accuracy: 0.9657 - val_loss: 0.3922 - val_accuracy: 0.9476\n",
      "Epoch 17/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1463 - accuracy: 0.9670 - val_loss: 0.3499 - val_accuracy: 0.9540\n",
      "Epoch 18/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1497 - accuracy: 0.9650 - val_loss: 0.3271 - val_accuracy: 0.9613\n",
      "Epoch 19/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1389 - accuracy: 0.9675 - val_loss: 0.3724 - val_accuracy: 0.9468\n",
      "Epoch 20/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1355 - accuracy: 0.9693 - val_loss: 0.4621 - val_accuracy: 0.9230\n",
      "Epoch 21/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1341 - accuracy: 0.9692 - val_loss: 0.3334 - val_accuracy: 0.9545\n",
      "Epoch 22/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1315 - accuracy: 0.9717 - val_loss: 0.3085 - val_accuracy: 0.9679\n",
      "Epoch 23/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1378 - accuracy: 0.9675 - val_loss: 0.3187 - val_accuracy: 0.9628\n",
      "Epoch 24/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1238 - accuracy: 0.9713 - val_loss: 0.3136 - val_accuracy: 0.9602\n",
      "Epoch 25/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1313 - accuracy: 0.9702 - val_loss: 0.3237 - val_accuracy: 0.9584\n",
      "Epoch 26/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1164 - accuracy: 0.9740 - val_loss: 0.3279 - val_accuracy: 0.9562\n",
      "Epoch 27/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1278 - accuracy: 0.9716 - val_loss: 0.3223 - val_accuracy: 0.9593\n",
      "Epoch 28/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1205 - accuracy: 0.9740 - val_loss: 0.2819 - val_accuracy: 0.9734\n",
      "Epoch 29/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1162 - accuracy: 0.9758 - val_loss: 0.2968 - val_accuracy: 0.9637\n",
      "Epoch 30/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1229 - accuracy: 0.9717 - val_loss: 0.3288 - val_accuracy: 0.9523\n",
      "Epoch 31/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1180 - accuracy: 0.9733 - val_loss: 0.2759 - val_accuracy: 0.9635\n",
      "Epoch 32/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1077 - accuracy: 0.9771 - val_loss: 0.3599 - val_accuracy: 0.9459\n",
      "Epoch 33/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1191 - accuracy: 0.9739 - val_loss: 0.2691 - val_accuracy: 0.9699\n",
      "Epoch 34/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1104 - accuracy: 0.9767 - val_loss: 0.3015 - val_accuracy: 0.9630\n",
      "Epoch 35/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1137 - accuracy: 0.9750 - val_loss: 0.2541 - val_accuracy: 0.9747\n",
      "Epoch 36/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1052 - accuracy: 0.9782 - val_loss: 0.2591 - val_accuracy: 0.9688\n",
      "Epoch 37/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1082 - accuracy: 0.9769 - val_loss: 0.2564 - val_accuracy: 0.9714\n",
      "Epoch 38/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1104 - accuracy: 0.9754 - val_loss: 0.2605 - val_accuracy: 0.9677\n",
      "Epoch 39/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.0990 - accuracy: 0.9793 - val_loss: 0.2790 - val_accuracy: 0.9617\n",
      "Epoch 40/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1047 - accuracy: 0.9777 - val_loss: 0.2435 - val_accuracy: 0.9725\n",
      "Fold: 2\n",
      "Epoch 1/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 3.0977 - accuracy: 0.4205 - val_loss: 1.9657 - val_accuracy: 0.6741\n",
      "Epoch 2/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 1.2643 - accuracy: 0.7634 - val_loss: 1.3151 - val_accuracy: 0.7707\n",
      "Epoch 3/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.7443 - accuracy: 0.8484 - val_loss: 0.9417 - val_accuracy: 0.8539\n",
      "Epoch 4/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.5078 - accuracy: 0.8911 - val_loss: 0.7503 - val_accuracy: 0.8915\n",
      "Epoch 5/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.3788 - accuracy: 0.9149 - val_loss: 0.6184 - val_accuracy: 0.9061\n",
      "Epoch 6/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.3049 - accuracy: 0.9298 - val_loss: 0.5578 - val_accuracy: 0.9171\n",
      "Epoch 7/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.2646 - accuracy: 0.9365 - val_loss: 0.4807 - val_accuracy: 0.9369\n",
      "Epoch 8/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.2300 - accuracy: 0.9441 - val_loss: 0.4236 - val_accuracy: 0.9426\n",
      "Epoch 9/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.2090 - accuracy: 0.9488 - val_loss: 0.3949 - val_accuracy: 0.9448\n",
      "Epoch 10/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1926 - accuracy: 0.9527 - val_loss: 0.4301 - val_accuracy: 0.9270\n",
      "Epoch 11/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1707 - accuracy: 0.9575 - val_loss: 0.3668 - val_accuracy: 0.9545\n",
      "Epoch 12/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1695 - accuracy: 0.9555 - val_loss: 0.4198 - val_accuracy: 0.9329\n",
      "Epoch 13/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1558 - accuracy: 0.9625 - val_loss: 0.3299 - val_accuracy: 0.9606\n",
      "Epoch 14/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1499 - accuracy: 0.9643 - val_loss: 0.3250 - val_accuracy: 0.9547\n",
      "Epoch 15/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1475 - accuracy: 0.9659 - val_loss: 0.3237 - val_accuracy: 0.9547\n",
      "Epoch 16/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1382 - accuracy: 0.9667 - val_loss: 0.4160 - val_accuracy: 0.9256\n",
      "Epoch 17/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1431 - accuracy: 0.9674 - val_loss: 0.3003 - val_accuracy: 0.9652\n",
      "Epoch 18/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1350 - accuracy: 0.9673 - val_loss: 0.3064 - val_accuracy: 0.9615\n",
      "Epoch 19/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1289 - accuracy: 0.9700 - val_loss: 0.2942 - val_accuracy: 0.9613\n",
      "Epoch 20/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1357 - accuracy: 0.9686 - val_loss: 0.2963 - val_accuracy: 0.9604\n",
      "Epoch 21/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1240 - accuracy: 0.9713 - val_loss: 0.2959 - val_accuracy: 0.9534\n",
      "Epoch 22/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1217 - accuracy: 0.9714 - val_loss: 0.2864 - val_accuracy: 0.9582\n",
      "Epoch 23/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1279 - accuracy: 0.9691 - val_loss: 0.2653 - val_accuracy: 0.9661\n",
      "Epoch 24/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1184 - accuracy: 0.9723 - val_loss: 0.2762 - val_accuracy: 0.9575\n",
      "Epoch 25/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1182 - accuracy: 0.9732 - val_loss: 0.2677 - val_accuracy: 0.9655\n",
      "Epoch 26/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1183 - accuracy: 0.9723 - val_loss: 0.2559 - val_accuracy: 0.9674\n",
      "Epoch 27/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1128 - accuracy: 0.9741 - val_loss: 0.3213 - val_accuracy: 0.9421\n",
      "Epoch 28/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1111 - accuracy: 0.9745 - val_loss: 0.2629 - val_accuracy: 0.9628\n",
      "Epoch 29/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1106 - accuracy: 0.9742 - val_loss: 0.2897 - val_accuracy: 0.9567\n",
      "Epoch 30/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1058 - accuracy: 0.9756 - val_loss: 0.2558 - val_accuracy: 0.9659\n",
      "Epoch 31/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1164 - accuracy: 0.9723 - val_loss: 0.2418 - val_accuracy: 0.9663\n",
      "Epoch 32/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1077 - accuracy: 0.9769 - val_loss: 0.2507 - val_accuracy: 0.9714\n",
      "Epoch 33/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1060 - accuracy: 0.9751 - val_loss: 0.3407 - val_accuracy: 0.9386\n",
      "Epoch 34/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1102 - accuracy: 0.9757 - val_loss: 0.2636 - val_accuracy: 0.9644\n",
      "Epoch 35/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1020 - accuracy: 0.9776 - val_loss: 0.2429 - val_accuracy: 0.9701\n",
      "Epoch 36/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1011 - accuracy: 0.9775 - val_loss: 0.2542 - val_accuracy: 0.9681\n",
      "Epoch 37/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1152 - accuracy: 0.9741 - val_loss: 0.2533 - val_accuracy: 0.9655\n",
      "Epoch 38/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.0965 - accuracy: 0.9790 - val_loss: 0.2832 - val_accuracy: 0.9611\n",
      "Epoch 39/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1070 - accuracy: 0.9755 - val_loss: 0.2390 - val_accuracy: 0.9685\n",
      "Epoch 40/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.0918 - accuracy: 0.9814 - val_loss: 0.2397 - val_accuracy: 0.9683\n",
      "Average Validation Accuracy: 0.9782522022724152\n",
      "Average Validation Loss: 0.1404494047164917\n",
      "Average Test Accuracy: 0.9799881875514984\n",
      "Final Test Accuracy for each fold: 0.9803935885429382\n",
      "Number of input features: 18\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 3.2781 - accuracy: 0.3892 - val_loss: 1.8763 - val_accuracy: 0.6706\n",
      "Epoch 2/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 1.1868 - accuracy: 0.7605 - val_loss: 1.0160 - val_accuracy: 0.8079\n",
      "Epoch 3/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.6577 - accuracy: 0.8565 - val_loss: 0.6958 - val_accuracy: 0.8658\n",
      "Epoch 4/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.4508 - accuracy: 0.8946 - val_loss: 0.5745 - val_accuracy: 0.8792\n",
      "Epoch 5/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.3518 - accuracy: 0.9134 - val_loss: 0.4988 - val_accuracy: 0.9067\n",
      "Epoch 6/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.2897 - accuracy: 0.9284 - val_loss: 0.4281 - val_accuracy: 0.9208\n",
      "Epoch 7/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.2527 - accuracy: 0.9363 - val_loss: 0.3868 - val_accuracy: 0.9325\n",
      "Epoch 8/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.2188 - accuracy: 0.9455 - val_loss: 0.3491 - val_accuracy: 0.9413\n",
      "Epoch 9/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.2027 - accuracy: 0.9512 - val_loss: 0.3423 - val_accuracy: 0.9428\n",
      "Epoch 10/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1845 - accuracy: 0.9553 - val_loss: 0.3054 - val_accuracy: 0.9494\n",
      "Epoch 11/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1781 - accuracy: 0.9558 - val_loss: 0.3549 - val_accuracy: 0.9441\n",
      "Epoch 12/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1676 - accuracy: 0.9576 - val_loss: 0.3121 - val_accuracy: 0.9523\n",
      "Epoch 13/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1512 - accuracy: 0.9635 - val_loss: 0.2758 - val_accuracy: 0.9586\n",
      "Epoch 14/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1512 - accuracy: 0.9644 - val_loss: 0.2633 - val_accuracy: 0.9593\n",
      "Epoch 15/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1484 - accuracy: 0.9635 - val_loss: 0.2774 - val_accuracy: 0.9613\n",
      "Epoch 16/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1428 - accuracy: 0.9669 - val_loss: 0.2593 - val_accuracy: 0.9648\n",
      "Epoch 17/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1336 - accuracy: 0.9670 - val_loss: 0.2731 - val_accuracy: 0.9624\n",
      "Epoch 18/40\n",
      "1846/1846 [==============================] - 8s 5ms/step - loss: 0.1227 - accuracy: 0.9719 - val_loss: 0.2637 - val_accuracy: 0.9659\n",
      "Epoch 19/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1311 - accuracy: 0.9713 - val_loss: 0.2625 - val_accuracy: 0.9655\n",
      "Epoch 20/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1220 - accuracy: 0.9727 - val_loss: 0.2765 - val_accuracy: 0.9593\n",
      "Epoch 21/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1270 - accuracy: 0.9712 - val_loss: 0.2502 - val_accuracy: 0.9710\n",
      "Epoch 22/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1169 - accuracy: 0.9734 - val_loss: 0.2389 - val_accuracy: 0.9760\n",
      "Epoch 23/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1167 - accuracy: 0.9752 - val_loss: 0.2515 - val_accuracy: 0.9703\n",
      "Epoch 24/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1123 - accuracy: 0.9759 - val_loss: 0.2836 - val_accuracy: 0.9582\n",
      "Epoch 25/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1135 - accuracy: 0.9738 - val_loss: 0.2610 - val_accuracy: 0.9692\n",
      "Epoch 26/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1085 - accuracy: 0.9768 - val_loss: 0.2592 - val_accuracy: 0.9694\n",
      "Epoch 27/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1095 - accuracy: 0.9745 - val_loss: 0.2905 - val_accuracy: 0.9593\n",
      "Epoch 28/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.0993 - accuracy: 0.9793 - val_loss: 0.2601 - val_accuracy: 0.9710\n",
      "Epoch 29/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1083 - accuracy: 0.9758 - val_loss: 0.2387 - val_accuracy: 0.9789\n",
      "Epoch 30/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1073 - accuracy: 0.9762 - val_loss: 0.3590 - val_accuracy: 0.9415\n",
      "Epoch 31/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1006 - accuracy: 0.9785 - val_loss: 0.2530 - val_accuracy: 0.9694\n",
      "Epoch 32/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.0993 - accuracy: 0.9779 - val_loss: 0.2574 - val_accuracy: 0.9705\n",
      "Epoch 33/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.0994 - accuracy: 0.9791 - val_loss: 0.2641 - val_accuracy: 0.9657\n",
      "Epoch 34/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.0978 - accuracy: 0.9797 - val_loss: 0.2403 - val_accuracy: 0.9754\n",
      "Epoch 35/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.0989 - accuracy: 0.9796 - val_loss: 0.2351 - val_accuracy: 0.9756\n",
      "Epoch 36/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.0952 - accuracy: 0.9797 - val_loss: 0.2456 - val_accuracy: 0.9749\n",
      "Epoch 37/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.0988 - accuracy: 0.9782 - val_loss: 0.2719 - val_accuracy: 0.9661\n",
      "Epoch 38/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.0904 - accuracy: 0.9818 - val_loss: 0.2557 - val_accuracy: 0.9701\n",
      "Epoch 39/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.0905 - accuracy: 0.9809 - val_loss: 0.2576 - val_accuracy: 0.9743\n",
      "Epoch 40/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.0932 - accuracy: 0.9815 - val_loss: 0.2532 - val_accuracy: 0.9758\n",
      "Fold: 2\n",
      "Epoch 1/40\n",
      "1846/1846 [==============================] - 6s 2ms/step - loss: 3.3655 - accuracy: 0.3583 - val_loss: 2.2413 - val_accuracy: 0.6700\n",
      "Epoch 2/40\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 1.4421 - accuracy: 0.7395 - val_loss: 1.3676 - val_accuracy: 0.7982\n",
      "Epoch 3/40\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 0.7890 - accuracy: 0.8453 - val_loss: 0.9590 - val_accuracy: 0.8702\n",
      "Epoch 4/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.5026 - accuracy: 0.8928 - val_loss: 0.7582 - val_accuracy: 0.8827\n",
      "Epoch 5/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3603 - accuracy: 0.9222 - val_loss: 0.6531 - val_accuracy: 0.9149\n",
      "Epoch 6/40\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 0.2935 - accuracy: 0.9320 - val_loss: 0.5874 - val_accuracy: 0.9111\n",
      "Epoch 7/40\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 0.2394 - accuracy: 0.9435 - val_loss: 0.5088 - val_accuracy: 0.9349\n",
      "Epoch 8/40\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 0.2127 - accuracy: 0.9494 - val_loss: 0.4537 - val_accuracy: 0.9446\n",
      "Epoch 9/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1952 - accuracy: 0.9551 - val_loss: 0.4650 - val_accuracy: 0.9404\n",
      "Epoch 10/40\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 0.1776 - accuracy: 0.9593 - val_loss: 0.3977 - val_accuracy: 0.9463\n",
      "Epoch 11/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1610 - accuracy: 0.9641 - val_loss: 0.4317 - val_accuracy: 0.9358\n",
      "Epoch 12/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1619 - accuracy: 0.9629 - val_loss: 0.4434 - val_accuracy: 0.9437\n",
      "Epoch 13/40\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 0.1474 - accuracy: 0.9667 - val_loss: 0.4084 - val_accuracy: 0.9430\n",
      "Epoch 14/40\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 0.1482 - accuracy: 0.9656 - val_loss: 0.3583 - val_accuracy: 0.9608\n",
      "Epoch 15/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1395 - accuracy: 0.9685 - val_loss: 0.3659 - val_accuracy: 0.9608\n",
      "Epoch 16/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1397 - accuracy: 0.9682 - val_loss: 0.3687 - val_accuracy: 0.9606\n",
      "Epoch 17/40\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 0.1360 - accuracy: 0.9704 - val_loss: 0.4010 - val_accuracy: 0.9481\n",
      "Epoch 18/40\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 0.1330 - accuracy: 0.9717 - val_loss: 0.3553 - val_accuracy: 0.9558\n",
      "Epoch 19/40\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 0.1298 - accuracy: 0.9702 - val_loss: 0.3235 - val_accuracy: 0.9661\n",
      "Epoch 20/40\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 0.1185 - accuracy: 0.9759 - val_loss: 0.3468 - val_accuracy: 0.9584\n",
      "Epoch 21/40\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 0.1206 - accuracy: 0.9741 - val_loss: 0.3878 - val_accuracy: 0.9349\n",
      "Epoch 22/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1193 - accuracy: 0.9760 - val_loss: 0.3294 - val_accuracy: 0.9622\n",
      "Epoch 23/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1164 - accuracy: 0.9738 - val_loss: 0.3291 - val_accuracy: 0.9613\n",
      "Epoch 24/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1118 - accuracy: 0.9756 - val_loss: 0.2948 - val_accuracy: 0.9723\n",
      "Epoch 25/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1281 - accuracy: 0.9736 - val_loss: 0.3679 - val_accuracy: 0.9520\n",
      "Epoch 26/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1152 - accuracy: 0.9763 - val_loss: 0.2860 - val_accuracy: 0.9743\n",
      "Epoch 27/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1066 - accuracy: 0.9771 - val_loss: 0.4116 - val_accuracy: 0.9496\n",
      "Epoch 28/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1070 - accuracy: 0.9780 - val_loss: 0.3953 - val_accuracy: 0.9432\n",
      "Epoch 29/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1054 - accuracy: 0.9781 - val_loss: 0.3123 - val_accuracy: 0.9558\n",
      "Epoch 30/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1107 - accuracy: 0.9770 - val_loss: 0.3120 - val_accuracy: 0.9670\n",
      "Epoch 31/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1041 - accuracy: 0.9784 - val_loss: 0.2854 - val_accuracy: 0.9683\n",
      "Epoch 32/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.0994 - accuracy: 0.9797 - val_loss: 0.2728 - val_accuracy: 0.9716\n",
      "Epoch 33/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.0964 - accuracy: 0.9804 - val_loss: 0.3175 - val_accuracy: 0.9635\n",
      "Epoch 34/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1006 - accuracy: 0.9806 - val_loss: 0.2879 - val_accuracy: 0.9701\n",
      "Epoch 35/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1065 - accuracy: 0.9780 - val_loss: 0.3340 - val_accuracy: 0.9644\n",
      "Epoch 36/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1003 - accuracy: 0.9796 - val_loss: 0.2869 - val_accuracy: 0.9725\n",
      "Epoch 37/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.0990 - accuracy: 0.9788 - val_loss: 0.3330 - val_accuracy: 0.9578\n",
      "Epoch 38/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.0979 - accuracy: 0.9805 - val_loss: 0.2896 - val_accuracy: 0.9650\n",
      "Epoch 39/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.0918 - accuracy: 0.9826 - val_loss: 0.2911 - val_accuracy: 0.9699\n",
      "Epoch 40/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.0937 - accuracy: 0.9803 - val_loss: 0.2852 - val_accuracy: 0.9696\n",
      "Average Validation Accuracy: 0.9783611297607422\n",
      "Average Validation Loss: 0.15134315192699432\n",
      "Average Test Accuracy: 0.9792879819869995\n",
      "Final Test Accuracy for each fold: 0.9802461862564087\n",
      "Number of input features: 19\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 3.2019 - accuracy: 0.4171 - val_loss: 1.9785 - val_accuracy: 0.6664\n",
      "Epoch 2/40\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 1.2928 - accuracy: 0.7658 - val_loss: 1.1272 - val_accuracy: 0.8088\n",
      "Epoch 3/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.7116 - accuracy: 0.8574 - val_loss: 0.7494 - val_accuracy: 0.8744\n",
      "Epoch 4/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.4504 - accuracy: 0.8989 - val_loss: 0.6243 - val_accuracy: 0.8999\n",
      "Epoch 5/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3176 - accuracy: 0.9279 - val_loss: 0.4452 - val_accuracy: 0.9344\n",
      "Epoch 6/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2506 - accuracy: 0.9411 - val_loss: 0.4023 - val_accuracy: 0.9358\n",
      "Epoch 7/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2117 - accuracy: 0.9471 - val_loss: 0.4551 - val_accuracy: 0.9054\n",
      "Epoch 8/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1847 - accuracy: 0.9526 - val_loss: 0.3360 - val_accuracy: 0.9496\n",
      "Epoch 9/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1676 - accuracy: 0.9591 - val_loss: 0.3353 - val_accuracy: 0.9485\n",
      "Epoch 10/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1595 - accuracy: 0.9601 - val_loss: 0.3196 - val_accuracy: 0.9483\n",
      "Epoch 11/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1542 - accuracy: 0.9621 - val_loss: 0.3383 - val_accuracy: 0.9454\n",
      "Epoch 12/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1360 - accuracy: 0.9676 - val_loss: 0.3085 - val_accuracy: 0.9558\n",
      "Epoch 13/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1431 - accuracy: 0.9643 - val_loss: 0.2956 - val_accuracy: 0.9562\n",
      "Epoch 14/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1296 - accuracy: 0.9698 - val_loss: 0.2622 - val_accuracy: 0.9646\n",
      "Epoch 15/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1258 - accuracy: 0.9717 - val_loss: 0.2979 - val_accuracy: 0.9560\n",
      "Epoch 16/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1205 - accuracy: 0.9720 - val_loss: 0.3853 - val_accuracy: 0.9331\n",
      "Epoch 17/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1196 - accuracy: 0.9728 - val_loss: 0.2595 - val_accuracy: 0.9619\n",
      "Epoch 18/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1128 - accuracy: 0.9764 - val_loss: 0.3340 - val_accuracy: 0.9492\n",
      "Epoch 19/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1150 - accuracy: 0.9725 - val_loss: 0.2789 - val_accuracy: 0.9611\n",
      "Epoch 20/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1124 - accuracy: 0.9743 - val_loss: 0.3768 - val_accuracy: 0.9342\n",
      "Epoch 21/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1140 - accuracy: 0.9737 - val_loss: 0.2407 - val_accuracy: 0.9696\n",
      "Epoch 22/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1113 - accuracy: 0.9752 - val_loss: 0.2665 - val_accuracy: 0.9666\n",
      "Epoch 23/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1054 - accuracy: 0.9768 - val_loss: 0.2184 - val_accuracy: 0.9813\n",
      "Epoch 24/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.0975 - accuracy: 0.9790 - val_loss: 0.2318 - val_accuracy: 0.9729\n",
      "Epoch 25/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1057 - accuracy: 0.9757 - val_loss: 0.2423 - val_accuracy: 0.9692\n",
      "Epoch 26/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1019 - accuracy: 0.9790 - val_loss: 0.2843 - val_accuracy: 0.9567\n",
      "Epoch 27/40\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.0987 - accuracy: 0.9788 - val_loss: 0.2538 - val_accuracy: 0.9617\n",
      "Epoch 28/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.0948 - accuracy: 0.9809 - val_loss: 0.2452 - val_accuracy: 0.9723\n",
      "Epoch 29/40\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.0945 - accuracy: 0.9789 - val_loss: 0.2626 - val_accuracy: 0.9655\n",
      "Epoch 30/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1036 - accuracy: 0.9778 - val_loss: 0.3292 - val_accuracy: 0.9393\n",
      "Epoch 31/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.0896 - accuracy: 0.9820 - val_loss: 0.2274 - val_accuracy: 0.9784\n",
      "Epoch 32/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.0978 - accuracy: 0.9797 - val_loss: 0.2391 - val_accuracy: 0.9734\n",
      "Epoch 33/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.0907 - accuracy: 0.9816 - val_loss: 0.2249 - val_accuracy: 0.9776\n",
      "Epoch 34/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.0932 - accuracy: 0.9807 - val_loss: 0.2261 - val_accuracy: 0.9780\n",
      "Epoch 35/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.0866 - accuracy: 0.9824 - val_loss: 0.2278 - val_accuracy: 0.9773\n",
      "Epoch 36/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.0896 - accuracy: 0.9820 - val_loss: 0.2198 - val_accuracy: 0.9831\n",
      "Epoch 37/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.0934 - accuracy: 0.9816 - val_loss: 0.2454 - val_accuracy: 0.9734\n",
      "Epoch 38/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.0872 - accuracy: 0.9816 - val_loss: 0.2424 - val_accuracy: 0.9760\n",
      "Epoch 39/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.0892 - accuracy: 0.9815 - val_loss: 0.2548 - val_accuracy: 0.9659\n",
      "Epoch 40/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.0876 - accuracy: 0.9816 - val_loss: 0.2360 - val_accuracy: 0.9762\n",
      "Fold: 2\n",
      "Epoch 1/40\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 3.2203 - accuracy: 0.3938 - val_loss: 1.9080 - val_accuracy: 0.6491\n",
      "Epoch 2/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 1.2154 - accuracy: 0.7634 - val_loss: 1.1334 - val_accuracy: 0.8200\n",
      "Epoch 3/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.6959 - accuracy: 0.8512 - val_loss: 0.8064 - val_accuracy: 0.8541\n",
      "Epoch 4/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.4641 - accuracy: 0.8941 - val_loss: 0.6381 - val_accuracy: 0.8893\n",
      "Epoch 5/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3483 - accuracy: 0.9170 - val_loss: 0.7343 - val_accuracy: 0.8414\n",
      "Epoch 6/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2960 - accuracy: 0.9234 - val_loss: 0.5295 - val_accuracy: 0.9087\n",
      "Epoch 7/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2531 - accuracy: 0.9346 - val_loss: 0.4467 - val_accuracy: 0.9276\n",
      "Epoch 8/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2294 - accuracy: 0.9415 - val_loss: 0.4287 - val_accuracy: 0.9241\n",
      "Epoch 9/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2119 - accuracy: 0.9446 - val_loss: 0.3851 - val_accuracy: 0.9116\n",
      "Epoch 10/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1939 - accuracy: 0.9488 - val_loss: 0.3803 - val_accuracy: 0.9318\n",
      "Epoch 11/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1879 - accuracy: 0.9522 - val_loss: 0.3692 - val_accuracy: 0.9415\n",
      "Epoch 12/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1709 - accuracy: 0.9545 - val_loss: 0.3389 - val_accuracy: 0.9369\n",
      "Epoch 13/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1752 - accuracy: 0.9551 - val_loss: 0.3582 - val_accuracy: 0.9333\n",
      "Epoch 14/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1601 - accuracy: 0.9599 - val_loss: 0.3040 - val_accuracy: 0.9525\n",
      "Epoch 15/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1528 - accuracy: 0.9615 - val_loss: 0.3187 - val_accuracy: 0.9562\n",
      "Epoch 16/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1524 - accuracy: 0.9627 - val_loss: 0.2802 - val_accuracy: 0.9602\n",
      "Epoch 17/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1454 - accuracy: 0.9643 - val_loss: 0.2898 - val_accuracy: 0.9556\n",
      "Epoch 18/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1412 - accuracy: 0.9655 - val_loss: 0.3001 - val_accuracy: 0.9571\n",
      "Epoch 19/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1311 - accuracy: 0.9672 - val_loss: 0.3195 - val_accuracy: 0.9492\n",
      "Epoch 20/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1377 - accuracy: 0.9660 - val_loss: 0.3130 - val_accuracy: 0.9507\n",
      "Epoch 21/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1276 - accuracy: 0.9707 - val_loss: 0.2716 - val_accuracy: 0.9624\n",
      "Epoch 22/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1311 - accuracy: 0.9674 - val_loss: 0.2669 - val_accuracy: 0.9648\n",
      "Epoch 23/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1212 - accuracy: 0.9718 - val_loss: 0.2675 - val_accuracy: 0.9668\n",
      "Epoch 24/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1223 - accuracy: 0.9721 - val_loss: 0.2576 - val_accuracy: 0.9672\n",
      "Epoch 25/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1241 - accuracy: 0.9710 - val_loss: 0.2646 - val_accuracy: 0.9670\n",
      "Epoch 26/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1155 - accuracy: 0.9733 - val_loss: 0.2925 - val_accuracy: 0.9514\n",
      "Epoch 27/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1161 - accuracy: 0.9736 - val_loss: 0.3014 - val_accuracy: 0.9507\n",
      "Epoch 28/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1135 - accuracy: 0.9728 - val_loss: 0.2450 - val_accuracy: 0.9732\n",
      "Epoch 29/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1066 - accuracy: 0.9757 - val_loss: 0.3821 - val_accuracy: 0.9287\n",
      "Epoch 30/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1162 - accuracy: 0.9741 - val_loss: 0.2546 - val_accuracy: 0.9685\n",
      "Epoch 31/40\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1058 - accuracy: 0.9776 - val_loss: 0.2741 - val_accuracy: 0.9545\n",
      "Epoch 32/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1047 - accuracy: 0.9781 - val_loss: 0.2735 - val_accuracy: 0.9685\n",
      "Epoch 33/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1024 - accuracy: 0.9772 - val_loss: 0.2556 - val_accuracy: 0.9716\n",
      "Epoch 34/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1030 - accuracy: 0.9762 - val_loss: 0.2444 - val_accuracy: 0.9714\n",
      "Epoch 35/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1083 - accuracy: 0.9769 - val_loss: 0.2479 - val_accuracy: 0.9685\n",
      "Epoch 36/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1008 - accuracy: 0.9776 - val_loss: 0.2690 - val_accuracy: 0.9683\n",
      "Epoch 37/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1052 - accuracy: 0.9777 - val_loss: 0.2493 - val_accuracy: 0.9734\n",
      "Epoch 38/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.0999 - accuracy: 0.9782 - val_loss: 0.2665 - val_accuracy: 0.9677\n",
      "Epoch 39/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.0979 - accuracy: 0.9782 - val_loss: 0.3063 - val_accuracy: 0.9540\n",
      "Epoch 40/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.0956 - accuracy: 0.9783 - val_loss: 0.2653 - val_accuracy: 0.9650\n",
      "Average Validation Accuracy: 0.9787239730358124\n",
      "Average Validation Loss: 0.13842106610536575\n",
      "Average Test Accuracy: 0.979435384273529\n",
      "Final Test Accuracy for each fold: 0.9845212697982788\n",
      "Number of input features: 20\n",
      "Fold: 1\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1846/1846 [==============================] - 5s 2ms/step - loss: 3.3437 - accuracy: 0.3763 - val_loss: 2.0792 - val_accuracy: 0.5853\n",
      "Epoch 2/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 1.2898 - accuracy: 0.7573 - val_loss: 1.1200 - val_accuracy: 0.8176\n",
      "Epoch 3/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.6889 - accuracy: 0.8588 - val_loss: 0.8340 - val_accuracy: 0.8502\n",
      "Epoch 4/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.4588 - accuracy: 0.8942 - val_loss: 0.6099 - val_accuracy: 0.8900\n",
      "Epoch 5/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.3519 - accuracy: 0.9119 - val_loss: 0.5269 - val_accuracy: 0.9107\n",
      "Epoch 6/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2860 - accuracy: 0.9288 - val_loss: 0.5083 - val_accuracy: 0.9113\n",
      "Epoch 7/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2544 - accuracy: 0.9343 - val_loss: 0.4174 - val_accuracy: 0.9360\n",
      "Epoch 8/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2269 - accuracy: 0.9402 - val_loss: 0.4367 - val_accuracy: 0.9226\n",
      "Epoch 9/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2077 - accuracy: 0.9459 - val_loss: 0.3783 - val_accuracy: 0.9349\n",
      "Epoch 10/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1894 - accuracy: 0.9496 - val_loss: 0.3964 - val_accuracy: 0.9285\n",
      "Epoch 11/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1782 - accuracy: 0.9518 - val_loss: 0.3470 - val_accuracy: 0.9439\n",
      "Epoch 12/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1659 - accuracy: 0.9570 - val_loss: 0.3335 - val_accuracy: 0.9507\n",
      "Epoch 13/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1626 - accuracy: 0.9590 - val_loss: 0.3134 - val_accuracy: 0.9542\n",
      "Epoch 14/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1515 - accuracy: 0.9630 - val_loss: 0.3325 - val_accuracy: 0.9463\n",
      "Epoch 15/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1496 - accuracy: 0.9620 - val_loss: 0.2962 - val_accuracy: 0.9567\n",
      "Epoch 16/40\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 0.1462 - accuracy: 0.9649 - val_loss: 0.2942 - val_accuracy: 0.9589\n",
      "Epoch 17/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1400 - accuracy: 0.9656 - val_loss: 0.2709 - val_accuracy: 0.9688\n",
      "Epoch 18/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1315 - accuracy: 0.9684 - val_loss: 0.2922 - val_accuracy: 0.9573\n",
      "Epoch 19/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1321 - accuracy: 0.9685 - val_loss: 0.2661 - val_accuracy: 0.9622\n",
      "Epoch 20/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1202 - accuracy: 0.9710 - val_loss: 0.3032 - val_accuracy: 0.9556\n",
      "Epoch 21/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1237 - accuracy: 0.9718 - val_loss: 0.3127 - val_accuracy: 0.9518\n",
      "Epoch 22/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1170 - accuracy: 0.9731 - val_loss: 0.2623 - val_accuracy: 0.9683\n",
      "Epoch 23/40\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1220 - accuracy: 0.9704 - val_loss: 0.2456 - val_accuracy: 0.9710\n",
      "Epoch 24/40\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 0.1085 - accuracy: 0.9759 - val_loss: 0.2659 - val_accuracy: 0.9652\n",
      "Epoch 25/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1101 - accuracy: 0.9747 - val_loss: 0.2757 - val_accuracy: 0.9668\n",
      "Epoch 26/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1135 - accuracy: 0.9744 - val_loss: 0.2555 - val_accuracy: 0.9683\n",
      "Epoch 27/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1109 - accuracy: 0.9763 - val_loss: 0.2697 - val_accuracy: 0.9646\n",
      "Epoch 28/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1108 - accuracy: 0.9760 - val_loss: 0.2723 - val_accuracy: 0.9626\n",
      "Epoch 29/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1083 - accuracy: 0.9760 - val_loss: 0.2443 - val_accuracy: 0.9758\n",
      "Epoch 30/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.0981 - accuracy: 0.9780 - val_loss: 0.2514 - val_accuracy: 0.9747\n",
      "Epoch 31/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1011 - accuracy: 0.9781 - val_loss: 0.2430 - val_accuracy: 0.9727\n",
      "Epoch 32/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1016 - accuracy: 0.9770 - val_loss: 0.2795 - val_accuracy: 0.9672\n",
      "Epoch 33/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1000 - accuracy: 0.9780 - val_loss: 0.2601 - val_accuracy: 0.9683\n",
      "Epoch 34/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.0955 - accuracy: 0.9792 - val_loss: 0.2424 - val_accuracy: 0.9727\n",
      "Epoch 35/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1014 - accuracy: 0.9777 - val_loss: 0.2601 - val_accuracy: 0.9721\n",
      "Epoch 36/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.0943 - accuracy: 0.9806 - val_loss: 0.2896 - val_accuracy: 0.9567\n",
      "Epoch 37/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.0964 - accuracy: 0.9789 - val_loss: 0.2341 - val_accuracy: 0.9802\n",
      "Epoch 38/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.0913 - accuracy: 0.9811 - val_loss: 0.2375 - val_accuracy: 0.9782\n",
      "Epoch 39/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.0913 - accuracy: 0.9804 - val_loss: 0.2479 - val_accuracy: 0.9791\n",
      "Epoch 40/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.0945 - accuracy: 0.9806 - val_loss: 0.3196 - val_accuracy: 0.9635\n",
      "Fold: 2\n",
      "Epoch 1/40\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 3.4639 - accuracy: 0.3480 - val_loss: 2.1321 - val_accuracy: 0.6425\n",
      "Epoch 2/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 1.3406 - accuracy: 0.7473 - val_loss: 1.1524 - val_accuracy: 0.8275\n",
      "Epoch 3/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.6752 - accuracy: 0.8667 - val_loss: 0.7871 - val_accuracy: 0.8799\n",
      "Epoch 4/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.4008 - accuracy: 0.9126 - val_loss: 0.5595 - val_accuracy: 0.9157\n",
      "Epoch 5/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2936 - accuracy: 0.9318 - val_loss: 0.4358 - val_accuracy: 0.9391\n",
      "Epoch 6/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2386 - accuracy: 0.9420 - val_loss: 0.3813 - val_accuracy: 0.9443\n",
      "Epoch 7/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2004 - accuracy: 0.9526 - val_loss: 0.3792 - val_accuracy: 0.9270\n",
      "Epoch 8/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1877 - accuracy: 0.9551 - val_loss: 0.3470 - val_accuracy: 0.9366\n",
      "Epoch 9/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1691 - accuracy: 0.9569 - val_loss: 0.3373 - val_accuracy: 0.9463\n",
      "Epoch 10/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1529 - accuracy: 0.9646 - val_loss: 0.2973 - val_accuracy: 0.9545\n",
      "Epoch 11/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1578 - accuracy: 0.9622 - val_loss: 0.2802 - val_accuracy: 0.9556\n",
      "Epoch 12/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1429 - accuracy: 0.9649 - val_loss: 0.3309 - val_accuracy: 0.9393\n",
      "Epoch 13/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1411 - accuracy: 0.9664 - val_loss: 0.2886 - val_accuracy: 0.9501\n",
      "Epoch 14/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1321 - accuracy: 0.9702 - val_loss: 0.2678 - val_accuracy: 0.9606\n",
      "Epoch 15/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1324 - accuracy: 0.9695 - val_loss: 0.2326 - val_accuracy: 0.9694\n",
      "Epoch 16/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1146 - accuracy: 0.9746 - val_loss: 0.2580 - val_accuracy: 0.9567\n",
      "Epoch 17/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1214 - accuracy: 0.9724 - val_loss: 0.2270 - val_accuracy: 0.9652\n",
      "Epoch 18/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1183 - accuracy: 0.9733 - val_loss: 0.2745 - val_accuracy: 0.9591\n",
      "Epoch 19/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1173 - accuracy: 0.9753 - val_loss: 0.2560 - val_accuracy: 0.9586\n",
      "Epoch 20/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1107 - accuracy: 0.9769 - val_loss: 0.2426 - val_accuracy: 0.9580\n",
      "Epoch 21/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1115 - accuracy: 0.9770 - val_loss: 0.2239 - val_accuracy: 0.9661\n",
      "Epoch 22/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1069 - accuracy: 0.9758 - val_loss: 0.2630 - val_accuracy: 0.9611\n",
      "Epoch 23/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1046 - accuracy: 0.9760 - val_loss: 0.2449 - val_accuracy: 0.9582\n",
      "Epoch 24/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1056 - accuracy: 0.9771 - val_loss: 0.2169 - val_accuracy: 0.9707\n",
      "Epoch 25/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1080 - accuracy: 0.9758 - val_loss: 0.2056 - val_accuracy: 0.9714\n",
      "Epoch 26/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1001 - accuracy: 0.9776 - val_loss: 0.2531 - val_accuracy: 0.9652\n",
      "Epoch 27/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1009 - accuracy: 0.9784 - val_loss: 0.2224 - val_accuracy: 0.9650\n",
      "Epoch 28/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1026 - accuracy: 0.9768 - val_loss: 0.2507 - val_accuracy: 0.9608\n",
      "Epoch 29/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.0959 - accuracy: 0.9806 - val_loss: 0.1866 - val_accuracy: 0.9811\n",
      "Epoch 30/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1005 - accuracy: 0.9777 - val_loss: 0.2328 - val_accuracy: 0.9644\n",
      "Epoch 31/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.0966 - accuracy: 0.9794 - val_loss: 0.2087 - val_accuracy: 0.9650\n",
      "Epoch 32/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.0895 - accuracy: 0.9810 - val_loss: 0.2099 - val_accuracy: 0.9677\n",
      "Epoch 33/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.0939 - accuracy: 0.9803 - val_loss: 0.1836 - val_accuracy: 0.9769\n",
      "Epoch 34/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.0926 - accuracy: 0.9798 - val_loss: 0.2206 - val_accuracy: 0.9721\n",
      "Epoch 35/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.0917 - accuracy: 0.9810 - val_loss: 0.2243 - val_accuracy: 0.9663\n",
      "Epoch 36/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.0970 - accuracy: 0.9798 - val_loss: 0.2330 - val_accuracy: 0.9624\n",
      "Epoch 37/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.0896 - accuracy: 0.9815 - val_loss: 0.2855 - val_accuracy: 0.9432\n",
      "Epoch 38/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.0935 - accuracy: 0.9810 - val_loss: 0.1975 - val_accuracy: 0.9769\n",
      "Epoch 39/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.0894 - accuracy: 0.9822 - val_loss: 0.1901 - val_accuracy: 0.9802\n",
      "Epoch 40/40\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.0931 - accuracy: 0.9817 - val_loss: 0.2056 - val_accuracy: 0.9765\n",
      "Average Validation Accuracy: 0.9753116369247437\n",
      "Average Validation Loss: 0.1587602198123932\n",
      "Average Test Accuracy: 0.9756394326686859\n",
      "Final Test Accuracy for each fold: 0.9841527342796326\n"
     ]
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_fs, Y, test_size=0.33, random_state=1)\n",
    "\n",
    "# prepare output data\n",
    "y_train_enc, y_test_enc = prepare_targets(y_train, y_test)\n",
    "\n",
    "# Define the number of folds for k-fold cross-validation\n",
    "k = 2\n",
    "\n",
    "# Define the cross-validation method\n",
    "cv_method = StratifiedKFold(n_splits=k)\n",
    "\n",
    "# Initialize the list to store the history, train & validation(accuracy & loss) for each model\n",
    "models = []\n",
    "model_history = []\n",
    "model_accuracy = []\n",
    "model_train_acc = []\n",
    "model_train_loss = []\n",
    "model_val_acc = []\n",
    "model_val_loss = []\n",
    "\n",
    "\n",
    "for i in range(1,21):\n",
    "\n",
    "    models_fold = []\n",
    "    hist = []\n",
    "    train_accuracy = []\n",
    "    train_loss = []\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "    test_accuracy = []\n",
    "\n",
    "    print(\"Number of input features:\",i)\n",
    "\n",
    "    # Select the input features from the input data\n",
    "    X_train_selected = X_train[:, :i]\n",
    "    X_test_selected = X_test[:, :i]\n",
    "\n",
    "    # Loop over the folds\n",
    "    for fold, (train_index, val_index) in enumerate(cv_method.split(X_train_selected, y_train)):\n",
    "\n",
    "        print(\"Fold:\", fold+1)\n",
    "\n",
    "        # Split the data into train and validation sets using the current fold index\n",
    "        X_train_fold  = X_train_selected[train_index]\n",
    "        y_train_fold  = y_train[train_index]\n",
    "        X_val_fold = X_train_selected[val_index]\n",
    "        y_val_fold = y_train[val_index]\n",
    "\n",
    "        # Prepare the target data\n",
    "        y_train_fold_enc, y_val_fold_enc = prepare_targets(y_train_fold, y_val_fold)\n",
    "\n",
    "        # build the model\n",
    "        model = MLP_model(i)\n",
    "\n",
    "        # Fit the model to the training data for the current fold\n",
    "        history = model.fit(X_train_fold, to_categorical(y_train_fold_enc, num_classes=373), epochs=40, batch_size=5, verbose=1, validation_split = 0.33)\n",
    "    \n",
    "        # Evaluate the model on the validation data for the current fold\n",
    "        val_scores = model.evaluate(X_val_fold, to_categorical(y_val_fold_enc, num_classes=373), verbose=0)\n",
    "        val_accuracy.append(val_scores[1])\n",
    "        val_loss.append(val_scores[0])\n",
    "\n",
    "        # Evaluate the model on the test data for the current fold\n",
    "        test_scores = model.evaluate(X_test_selected, to_categorical(y_test_enc, num_classes=373), verbose=0)\n",
    "        test_accuracy.append(test_scores[1])\n",
    "\n",
    "        # add the model to the list of models\n",
    "        models_fold.append(model)\n",
    "        hist.append(history)\n",
    "\n",
    "        # store the training accuracy and loss for each fold\n",
    "        train_accuracy.append(history.history['accuracy'])\n",
    "        train_loss.append(history.history['loss'])\n",
    "        \n",
    "    # Calculate the average test and validation accuracy and loss across all folds\n",
    "    avg_test_acc = sum(test_accuracy) / len(test_accuracy)\n",
    "    avg_val_acc = sum(val_accuracy) / len(val_accuracy)\n",
    "    avg_val_loss = sum(val_loss) / len(val_loss)\n",
    "\n",
    "    # Print the average validation and test accuracy and loss\n",
    "    print(\"Average Validation Accuracy:\", avg_val_acc)\n",
    "    print(\"Average Validation Loss:\",avg_val_loss)\n",
    "    print(\"Average Test Accuracy:\", avg_test_acc)\n",
    "\n",
    "    best_fold_index = test_accuracy.index(max(test_accuracy))\n",
    "    model_accuracy.append(test_accuracy[best_fold_index])\n",
    "    models.append(models_fold[best_fold_index])\n",
    "    model_history.append(hist[best_fold_index])\n",
    "    model_train_acc.append(train_accuracy[best_fold_index])\n",
    "    model_train_loss.append(train_loss[best_fold_index])\n",
    "    model_val_acc.append(val_accuracy[best_fold_index])\n",
    "    model_val_loss.append(val_loss[best_fold_index])\n",
    "\n",
    "\n",
    "    print(\"Final Test Accuracy for each fold:\", test_accuracy[best_fold_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to show the no of input features and its corresponding model accuracy\n",
    "model_list = []\n",
    "\n",
    "#Iterate through each model's accuracy \n",
    "for i in range (len(model_accuracy)):\n",
    "    #get the number of input features for the current model\n",
    "    no_features = i + 1\n",
    "\n",
    "    #round the model accuries to 3 d.p.\n",
    "    rounded_model_acc = round(model_accuracy[i], 3)\n",
    "    \n",
    "    model_list.append([no_features, rounded_model_acc])\n",
    "\n",
    "models_df = pd.DataFrame(model_list, columns=[\"No of input features\", \"Model accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 0.9845212697982788\n"
     ]
    }
   ],
   "source": [
    "# Select the final model based on the max test accuracy across all models\n",
    "\n",
    "best_model_index = model_accuracy.index(max(model_accuracy))\n",
    "\n",
    "best_model = models[best_model_index]\n",
    "best_model_history = model_history[best_model_index]\n",
    "best_model_train_acc = model_train_acc[best_model_index]\n",
    "best_model_train_loss = model_train_loss[best_model_index]\n",
    "best_model_val_acc = model_val_acc[best_model_index]\n",
    "best_model_val_loss = model_val_loss[best_model_index]\n",
    "\n",
    "# summarize history for accuracy  \n",
    "plt.subplot(211)  \n",
    "plt.plot(best_model_history.history['accuracy'])  \n",
    "plt.plot(best_model_history.history['val_accuracy'])  \n",
    "plt.title('model accuracy')  \n",
    "plt.ylabel('accuracy')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'test'], loc='lower right')  \n",
    "\n",
    "# summarize history for loss\n",
    "plt.subplot(212)  \n",
    "plt.plot(best_model_history.history['loss'])  \n",
    "plt.plot(best_model_history.history['val_loss'])  \n",
    "plt.title('model loss')  \n",
    "plt.ylabel('loss')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'test'], loc='upper right')  \n",
    "plt.subplots_adjust(hspace=0.7)\n",
    "\n",
    "plt.show() \n",
    "\n",
    "print(\"Final Test Accuracy:\", model_accuracy[best_model_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424/424 [==============================] - 1s 1ms/step\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       591\n",
      "           1       1.00      1.00      1.00       430\n",
      "           2       1.00      1.00      1.00       419\n",
      "           3       1.00      1.00      1.00       384\n",
      "           4       0.97      1.00      0.99       339\n",
      "           5       1.00      1.00      1.00       342\n",
      "           6       1.00      1.00      1.00       310\n",
      "           7       1.00      1.00      1.00       325\n",
      "           8       1.00      1.00      1.00       294\n",
      "           9       1.00      1.00      1.00       269\n",
      "          10       1.00      1.00      1.00       296\n",
      "          11       1.00      1.00      1.00       258\n",
      "          12       1.00      1.00      1.00       247\n",
      "          13       1.00      1.00      1.00       237\n",
      "          14       1.00      1.00      1.00       239\n",
      "          15       1.00      1.00      1.00       235\n",
      "          16       1.00      1.00      1.00       213\n",
      "          17       1.00      1.00      1.00       202\n",
      "          18       0.99      1.00      1.00       196\n",
      "          19       0.96      1.00      0.98       181\n",
      "          20       1.00      1.00      1.00       177\n",
      "          21       1.00      1.00      1.00       177\n",
      "          22       1.00      1.00      1.00       155\n",
      "          23       1.00      1.00      1.00       155\n",
      "          24       1.00      1.00      1.00       144\n",
      "          25       1.00      1.00      1.00       126\n",
      "          26       1.00      1.00      1.00       108\n",
      "          27       1.00      1.00      1.00       121\n",
      "          28       1.00      1.00      1.00        95\n",
      "          29       1.00      1.00      1.00       106\n",
      "          30       1.00      1.00      1.00       102\n",
      "          31       1.00      1.00      1.00        86\n",
      "          32       1.00      1.00      1.00       108\n",
      "          33       1.00      1.00      1.00        88\n",
      "          34       1.00      1.00      1.00       102\n",
      "          35       1.00      1.00      1.00        88\n",
      "          36       1.00      1.00      1.00        83\n",
      "          37       1.00      1.00      1.00        93\n",
      "          38       1.00      1.00      1.00        76\n",
      "          39       1.00      1.00      1.00        85\n",
      "          40       1.00      1.00      1.00        86\n",
      "          41       1.00      1.00      1.00        85\n",
      "          42       1.00      1.00      1.00        68\n",
      "          43       1.00      1.00      1.00        75\n",
      "          44       1.00      1.00      1.00        71\n",
      "          45       0.74      0.97      0.84        58\n",
      "          46       1.00      1.00      1.00        71\n",
      "          47       1.00      0.40      0.57        57\n",
      "          48       1.00      1.00      1.00        67\n",
      "          49       0.95      0.85      0.90        47\n",
      "          50       1.00      1.00      1.00        48\n",
      "          51       1.00      1.00      1.00        47\n",
      "          52       0.90      1.00      0.95        43\n",
      "          53       1.00      1.00      1.00        51\n",
      "          54       1.00      1.00      1.00        44\n",
      "          55       1.00      1.00      1.00        51\n",
      "          56       1.00      1.00      1.00        45\n",
      "          57       1.00      1.00      1.00        44\n",
      "          58       1.00      1.00      1.00        41\n",
      "          59       1.00      1.00      1.00        41\n",
      "          60       1.00      1.00      1.00        52\n",
      "          61       1.00      1.00      1.00        43\n",
      "          62       1.00      1.00      1.00        37\n",
      "          63       1.00      1.00      1.00        43\n",
      "          64       1.00      1.00      1.00        42\n",
      "          65       1.00      1.00      1.00        46\n",
      "          66       1.00      1.00      1.00        43\n",
      "          67       1.00      1.00      1.00        40\n",
      "          68       1.00      1.00      1.00        44\n",
      "          69       1.00      1.00      1.00        43\n",
      "          70       1.00      1.00      1.00        38\n",
      "          71       1.00      1.00      1.00        33\n",
      "          72       1.00      1.00      1.00        45\n",
      "          73       1.00      1.00      1.00        38\n",
      "          74       1.00      1.00      1.00        42\n",
      "          75       1.00      1.00      1.00        39\n",
      "          76       0.84      0.70      0.76        30\n",
      "          77       1.00      1.00      1.00        28\n",
      "          78       0.93      1.00      0.97        28\n",
      "          79       1.00      1.00      1.00        32\n",
      "          80       1.00      1.00      1.00        33\n",
      "          81       1.00      1.00      1.00        31\n",
      "          82       1.00      1.00      1.00        35\n",
      "          83       1.00      1.00      1.00        39\n",
      "          84       1.00      1.00      1.00        27\n",
      "          85       1.00      1.00      1.00        36\n",
      "          86       1.00      1.00      1.00        31\n",
      "          87       1.00      1.00      1.00        28\n",
      "          88       1.00      1.00      1.00        20\n",
      "          89       1.00      1.00      1.00        33\n",
      "          90       1.00      1.00      1.00        24\n",
      "          91       1.00      1.00      1.00        22\n",
      "          92       1.00      1.00      1.00        26\n",
      "          93       1.00      1.00      1.00        35\n",
      "          94       0.90      1.00      0.95        27\n",
      "          95       1.00      1.00      1.00        23\n",
      "          96       1.00      1.00      1.00        27\n",
      "          97       1.00      1.00      1.00        28\n",
      "          98       0.39      0.75      0.51        16\n",
      "          99       1.00      1.00      1.00        35\n",
      "         100       1.00      1.00      1.00        28\n",
      "         101       1.00      1.00      1.00        25\n",
      "         102       1.00      1.00      1.00        26\n",
      "         103       1.00      1.00      1.00        33\n",
      "         104       1.00      1.00      1.00        26\n",
      "         105       1.00      1.00      1.00        24\n",
      "         106       1.00      0.27      0.43        22\n",
      "         107       1.00      1.00      1.00        26\n",
      "         108       1.00      1.00      1.00        25\n",
      "         109       1.00      1.00      1.00        16\n",
      "         110       1.00      1.00      1.00        20\n",
      "         111       1.00      1.00      1.00        26\n",
      "         112       1.00      1.00      1.00        18\n",
      "         113       0.96      1.00      0.98        23\n",
      "         114       1.00      1.00      1.00        25\n",
      "         115       1.00      1.00      1.00        18\n",
      "         116       1.00      1.00      1.00        19\n",
      "         117       1.00      1.00      1.00        16\n",
      "         118       1.00      1.00      1.00        26\n",
      "         119       0.88      1.00      0.94        22\n",
      "         120       1.00      1.00      1.00        17\n",
      "         121       1.00      1.00      1.00        15\n",
      "         122       1.00      1.00      1.00        18\n",
      "         123       1.00      1.00      1.00        20\n",
      "         124       1.00      1.00      1.00        14\n",
      "         125       1.00      1.00      1.00        22\n",
      "         126       1.00      1.00      1.00        19\n",
      "         127       1.00      1.00      1.00        27\n",
      "         128       0.87      1.00      0.93        26\n",
      "         129       1.00      1.00      1.00        21\n",
      "         130       1.00      1.00      1.00        18\n",
      "         131       1.00      1.00      1.00        18\n",
      "         132       1.00      1.00      1.00        20\n",
      "         133       1.00      1.00      1.00        14\n",
      "         134       1.00      1.00      1.00        19\n",
      "         135       1.00      1.00      1.00        16\n",
      "         136       1.00      1.00      1.00        23\n",
      "         137       0.44      1.00      0.61        11\n",
      "         138       1.00      1.00      1.00        14\n",
      "         139       1.00      1.00      1.00        20\n",
      "         140       1.00      1.00      1.00        23\n",
      "         141       0.47      1.00      0.64        14\n",
      "         142       1.00      1.00      1.00        13\n",
      "         143       1.00      1.00      1.00        23\n",
      "         144       1.00      1.00      1.00        17\n",
      "         145       1.00      1.00      1.00        24\n",
      "         146       1.00      0.62      0.77        16\n",
      "         147       1.00      0.58      0.73        19\n",
      "         148       1.00      1.00      1.00        22\n",
      "         149       1.00      1.00      1.00        15\n",
      "         150       1.00      1.00      1.00        11\n",
      "         151       1.00      1.00      1.00        19\n",
      "         152       1.00      1.00      1.00        20\n",
      "         153       1.00      1.00      1.00        24\n",
      "         154       1.00      1.00      1.00        11\n",
      "         155       1.00      1.00      1.00        17\n",
      "         156       1.00      1.00      1.00        18\n",
      "         157       1.00      1.00      1.00        12\n",
      "         158       1.00      1.00      1.00        18\n",
      "         159       1.00      1.00      1.00        20\n",
      "         160       1.00      1.00      1.00        20\n",
      "         161       1.00      1.00      1.00        16\n",
      "         162       1.00      1.00      1.00        15\n",
      "         163       1.00      1.00      1.00        15\n",
      "         164       1.00      1.00      1.00        13\n",
      "         165       1.00      1.00      1.00        19\n",
      "         166       0.92      1.00      0.96        11\n",
      "         167       1.00      1.00      1.00         9\n",
      "         168       1.00      1.00      1.00        11\n",
      "         169       1.00      1.00      1.00        21\n",
      "         170       1.00      1.00      1.00        15\n",
      "         171       1.00      1.00      1.00        18\n",
      "         172       1.00      1.00      1.00        11\n",
      "         173       1.00      1.00      1.00        16\n",
      "         174       1.00      1.00      1.00        10\n",
      "         175       1.00      1.00      1.00        11\n",
      "         176       1.00      1.00      1.00        10\n",
      "         177       1.00      1.00      1.00        15\n",
      "         178       0.79      1.00      0.88        11\n",
      "         179       1.00      1.00      1.00        15\n",
      "         180       1.00      1.00      1.00        13\n",
      "         181       1.00      1.00      1.00        15\n",
      "         182       1.00      1.00      1.00         9\n",
      "         183       1.00      1.00      1.00        16\n",
      "         184       1.00      1.00      1.00         8\n",
      "         185       0.57      1.00      0.73        12\n",
      "         186       1.00      1.00      1.00        15\n",
      "         187       1.00      1.00      1.00        15\n",
      "         188       0.42      1.00      0.59        13\n",
      "         189       1.00      1.00      1.00        14\n",
      "         190       0.00      0.00      0.00        11\n",
      "         191       1.00      1.00      1.00        10\n",
      "         192       1.00      1.00      1.00        17\n",
      "         193       1.00      1.00      1.00         9\n",
      "         194       1.00      1.00      1.00         9\n",
      "         195       1.00      1.00      1.00         4\n",
      "         196       1.00      1.00      1.00         7\n",
      "         197       1.00      1.00      1.00         7\n",
      "         198       1.00      1.00      1.00        12\n",
      "         199       1.00      1.00      1.00        14\n",
      "         200       0.86      1.00      0.92         6\n",
      "         201       1.00      1.00      1.00         9\n",
      "         202       1.00      1.00      1.00         7\n",
      "         203       1.00      0.17      0.29         6\n",
      "         204       1.00      1.00      1.00        11\n",
      "         205       1.00      1.00      1.00        14\n",
      "         206       1.00      1.00      1.00        12\n",
      "         207       1.00      1.00      1.00        14\n",
      "         208       1.00      1.00      1.00        12\n",
      "         209       1.00      1.00      1.00         7\n",
      "         210       1.00      1.00      1.00        19\n",
      "         211       1.00      1.00      1.00         7\n",
      "         212       1.00      1.00      1.00        11\n",
      "         213       1.00      1.00      1.00         9\n",
      "         214       1.00      1.00      1.00         7\n",
      "         215       1.00      1.00      1.00         6\n",
      "         216       1.00      1.00      1.00        12\n",
      "         217       1.00      1.00      1.00        12\n",
      "         218       0.00      0.00      0.00         9\n",
      "         219       1.00      1.00      1.00         6\n",
      "         220       1.00      1.00      1.00         8\n",
      "         221       1.00      1.00      1.00         5\n",
      "         222       0.31      1.00      0.47         4\n",
      "         223       0.00      0.00      0.00        14\n",
      "         224       1.00      1.00      1.00        13\n",
      "         225       1.00      1.00      1.00         4\n",
      "         226       1.00      1.00      1.00        10\n",
      "         227       1.00      1.00      1.00        12\n",
      "         228       1.00      1.00      1.00        13\n",
      "         229       1.00      1.00      1.00        11\n",
      "         230       1.00      1.00      1.00         5\n",
      "         231       1.00      1.00      1.00         6\n",
      "         232       0.00      0.00      0.00         8\n",
      "         233       1.00      1.00      1.00        10\n",
      "         234       1.00      1.00      1.00         4\n",
      "         235       1.00      1.00      1.00         8\n",
      "         236       1.00      1.00      1.00         9\n",
      "         237       1.00      1.00      1.00         8\n",
      "         238       1.00      1.00      1.00        10\n",
      "         239       0.00      0.00      0.00         9\n",
      "         240       1.00      1.00      1.00         7\n",
      "         241       1.00      1.00      1.00         8\n",
      "         242       1.00      1.00      1.00         6\n",
      "         243       1.00      1.00      1.00         7\n",
      "         244       1.00      1.00      1.00         9\n",
      "         245       1.00      1.00      1.00         7\n",
      "         246       1.00      1.00      1.00         9\n",
      "         247       1.00      1.00      1.00         7\n",
      "         248       1.00      1.00      1.00        10\n",
      "         249       1.00      1.00      1.00        10\n",
      "         250       1.00      1.00      1.00         7\n",
      "         251       1.00      1.00      1.00         6\n",
      "         252       1.00      1.00      1.00        11\n",
      "         253       1.00      1.00      1.00         7\n",
      "         254       1.00      1.00      1.00         8\n",
      "         255       1.00      1.00      1.00         5\n",
      "         256       1.00      1.00      1.00         7\n",
      "         257       0.75      1.00      0.86         9\n",
      "         258       1.00      1.00      1.00         6\n",
      "         259       1.00      0.67      0.80         3\n",
      "         260       1.00      1.00      1.00         4\n",
      "         261       1.00      1.00      1.00         2\n",
      "         262       1.00      1.00      1.00         5\n",
      "         263       1.00      1.00      1.00        12\n",
      "         264       1.00      1.00      1.00         5\n",
      "         265       1.00      1.00      1.00         7\n",
      "         266       1.00      1.00      1.00        10\n",
      "         267       1.00      1.00      1.00         8\n",
      "         268       0.90      1.00      0.95         9\n",
      "         269       1.00      1.00      1.00         6\n",
      "         270       1.00      1.00      1.00         4\n",
      "         271       0.58      1.00      0.74         7\n",
      "         272       1.00      1.00      1.00        10\n",
      "         273       0.60      1.00      0.75         3\n",
      "         274       1.00      1.00      1.00         9\n",
      "         275       1.00      1.00      1.00         6\n",
      "         276       1.00      1.00      1.00         5\n",
      "         277       0.00      0.00      0.00         4\n",
      "         278       1.00      1.00      1.00         3\n",
      "         279       1.00      1.00      1.00         4\n",
      "         280       0.19      1.00      0.32         5\n",
      "         281       1.00      1.00      1.00         6\n",
      "         282       1.00      1.00      1.00        11\n",
      "         283       1.00      1.00      1.00         6\n",
      "         284       1.00      1.00      1.00         2\n",
      "         285       1.00      1.00      1.00         4\n",
      "         286       1.00      1.00      1.00         7\n",
      "         287       1.00      1.00      1.00         9\n",
      "         288       1.00      1.00      1.00         7\n",
      "         289       1.00      1.00      1.00         7\n",
      "         290       1.00      1.00      1.00         6\n",
      "         291       1.00      0.20      0.33         5\n",
      "         292       0.00      0.00      0.00         6\n",
      "         293       1.00      1.00      1.00         8\n",
      "         294       1.00      1.00      1.00         7\n",
      "         295       1.00      0.67      0.80         3\n",
      "         296       1.00      1.00      1.00         6\n",
      "         297       1.00      1.00      1.00         8\n",
      "         298       1.00      1.00      1.00         4\n",
      "         299       1.00      1.00      1.00         6\n",
      "         300       1.00      0.80      0.89         5\n",
      "         301       0.00      0.00      0.00         5\n",
      "         302       1.00      1.00      1.00         5\n",
      "         303       1.00      1.00      1.00         2\n",
      "         304       1.00      1.00      1.00         8\n",
      "         305       0.50      1.00      0.67         3\n",
      "         306       1.00      1.00      1.00         6\n",
      "         307       1.00      1.00      1.00         7\n",
      "         308       1.00      1.00      1.00         4\n",
      "         309       1.00      1.00      1.00         4\n",
      "         310       0.00      0.00      0.00         9\n",
      "         311       1.00      1.00      1.00         7\n",
      "         312       1.00      1.00      1.00         6\n",
      "         313       0.50      1.00      0.67         6\n",
      "         314       1.00      1.00      1.00         8\n",
      "         315       1.00      1.00      1.00         7\n",
      "         316       1.00      1.00      1.00         3\n",
      "         317       1.00      1.00      1.00         6\n",
      "         318       1.00      1.00      1.00        10\n",
      "         319       1.00      0.50      0.67         6\n",
      "         320       1.00      1.00      1.00         2\n",
      "         321       1.00      1.00      1.00         8\n",
      "         322       1.00      1.00      1.00         4\n",
      "         323       1.00      1.00      1.00         4\n",
      "         324       1.00      1.00      1.00         8\n",
      "         325       1.00      1.00      1.00         4\n",
      "         326       1.00      1.00      1.00         7\n",
      "         327       1.00      1.00      1.00         4\n",
      "         328       1.00      1.00      1.00         6\n",
      "         329       1.00      1.00      1.00         4\n",
      "         330       0.60      1.00      0.75         3\n",
      "         331       1.00      1.00      1.00         8\n",
      "         332       1.00      1.00      1.00         1\n",
      "         333       1.00      1.00      1.00         3\n",
      "         334       1.00      1.00      1.00         4\n",
      "         335       1.00      1.00      1.00         3\n",
      "         336       0.00      0.00      0.00         6\n",
      "         337       1.00      1.00      1.00         2\n",
      "         338       1.00      1.00      1.00         7\n",
      "         339       1.00      1.00      1.00         4\n",
      "         340       1.00      1.00      1.00         6\n",
      "         341       1.00      1.00      1.00         7\n",
      "         342       1.00      1.00      1.00         2\n",
      "         343       0.00      0.00      0.00         5\n",
      "         344       1.00      1.00      1.00         4\n",
      "         345       0.00      0.00      0.00         1\n",
      "         346       1.00      1.00      1.00         2\n",
      "         347       1.00      1.00      1.00         4\n",
      "         348       1.00      0.29      0.44         7\n",
      "         349       1.00      1.00      1.00         4\n",
      "         350       1.00      1.00      1.00         6\n",
      "         351       1.00      1.00      1.00         4\n",
      "         352       1.00      1.00      1.00         5\n",
      "         353       1.00      1.00      1.00         4\n",
      "         354       1.00      1.00      1.00         3\n",
      "         355       1.00      1.00      1.00         1\n",
      "         356       1.00      1.00      1.00         4\n",
      "         357       0.20      1.00      0.33         1\n",
      "         358       1.00      1.00      1.00         3\n",
      "         359       1.00      1.00      1.00         4\n",
      "         360       0.00      0.00      0.00         3\n",
      "         361       0.00      0.00      0.00         3\n",
      "         362       1.00      1.00      1.00         3\n",
      "         363       1.00      0.50      0.67         2\n",
      "         364       0.00      0.00      0.00         3\n",
      "         365       1.00      1.00      1.00         3\n",
      "         366       0.00      0.00      0.00         2\n",
      "         367       0.21      1.00      0.35         3\n",
      "         368       1.00      1.00      1.00         1\n",
      "         369       1.00      0.50      0.67         2\n",
      "         370       1.00      1.00      1.00         2\n",
      "         372       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.98     13567\n",
      "   macro avg       0.93      0.93      0.92     13567\n",
      "weighted avg       0.98      0.98      0.98     13567\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Select only the optimal number of input features for X_test\n",
    "X_test = X_test[:,:(best_model_index+1)]\n",
    "\n",
    "# Evaluate the best model on the test data\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# np.argmax() is used to convert the one-hot encoded predictions and test labels to class labels.\n",
    "y_pred_label = np.argmax(y_pred, axis = 1)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification report:\\n\", classification_report(y_test_enc, y_pred_label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           OsID  True Class  Predicted Class  True/False\n",
      "0  Os11g0704500         328              328        True\n",
      "1  Os09g0279600         161              161        True\n",
      "2  Os03g0669100          17               17        True\n",
      "3  Os05g0542500          34               34        True\n",
      "4  Os09g0522000           7                7        True\n"
     ]
    }
   ],
   "source": [
    "# extract class labels from test data\n",
    "class_test = y_test_enc\n",
    "\n",
    "# Invert OsID_labels dictionary\n",
    "inv_OsID_labels = {v: k for k, v in OsID_labels.items()}\n",
    "\n",
    "# map OsID values to the class labels\n",
    "OsID_test = [inv_OsID_labels.get(value, 'Unknown') for value in class_test]\n",
    "\n",
    "# create dataframe with OsID, true class, predicted class, and true/false columns\n",
    "results = pd.DataFrame({\n",
    "    'OsID': OsID_test,\n",
    "    'True Class': y_test_enc,\n",
    "    'Predicted Class': y_pred_label,\n",
    "    'True/False': class_test == y_pred_label\n",
    "})\n",
    "\n",
    "# display dataframe\n",
    "print(results.head())\n",
    "\n",
    "# save results_df to a CSV file\n",
    "results.to_csv('MLP_gene classification.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54645332476aec3a1589d49135d9c8280fdb5d7db877f5b7af7a1b58b8f996bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

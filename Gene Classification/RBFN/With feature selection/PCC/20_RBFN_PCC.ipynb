{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"G3AagWU9QsYg"},"outputs":[],"source":["import numpy as np\n","import pandas as pd \n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","\n","from keras.models import Sequential\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","#for RBN\n","from keras.layers import Layer, Flatten, Dense\n","from keras import backend as K\n","from sklearn.metrics import classification_report\n","\n","\n","from sklearn.model_selection import StratifiedKFold\n","from keras import regularizers\n","from tensorflow.keras.utils import to_categorical\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"TAShwIegQsYj"},"outputs":[],"source":["#fixed random seed for reproducibility \n","np.random.seed(0)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":216816,"status":"ok","timestamp":1682268983927,"user":{"displayName":"Good Notes UM","userId":"12493674840996884929"},"user_tz":-480},"id":"GgUSY9c4QsYl","outputId":"7f66c9df-3c26-4a25-c531-903faada4d63"},"outputs":[{"name":"stdout","output_type":"stream","text":["Summary of dataGene:\n","        log_2FoldChange            ET  CoExpression           PCC  \\\n","count     41110.000000  41110.000000  41110.000000  41110.000000   \n","mean         -0.037332      1.407395      0.991997     -0.361737   \n","std           0.391444      0.784327      0.089101      0.463979   \n","min          -1.000000      0.000000      0.000000     -1.000000   \n","25%          -0.251534      1.000000      1.000000     -0.747963   \n","50%           0.030675      2.000000      1.000000     -0.449089   \n","75%           0.251534      2.000000      1.000000     -0.051646   \n","max           1.000000      2.000000      1.000000      1.000000   \n","\n","                PPI  Root10DaysSeedling  Root14DaysSeedling  \\\n","count  41110.000000        41110.000000        41110.000000   \n","mean       0.914668           -0.522040           -0.646982   \n","std        0.279379            0.498568            0.393549   \n","min        0.000000           -1.000000           -1.000000   \n","25%        1.000000           -0.901371           -0.965084   \n","50%        1.000000           -0.663664           -0.680003   \n","75%        1.000000           -0.378497           -0.559627   \n","max        1.000000            1.000000            1.000000   \n","\n","       Root17DaysSeedling  Root21DaysSeedling  Root24DaysSeedling  ...  \\\n","count        41110.000000        41110.000000        41110.000000  ...   \n","mean            -0.700869           -0.669349           -0.670048  ...   \n","std              0.378219            0.405860            0.390751  ...   \n","min             -1.000000           -1.000000           -1.000000  ...   \n","25%             -0.980226           -1.000000           -0.982003  ...   \n","50%             -0.795609           -0.726665           -0.708584  ...   \n","75%             -0.601266           -0.543621           -0.482133  ...   \n","max              1.000000            1.000000            1.000000  ...   \n","\n","       Root52DaysSeedling  Shoot3DaysSeedling  Shoot10DaysSeedling  \\\n","count        41110.000000        41110.000000         41110.000000   \n","mean            -0.670345           -0.590806            -0.545055   \n","std              0.478222            0.443552             0.477438   \n","min             -1.000000           -1.000000            -1.000000   \n","25%             -1.000000           -1.000000            -0.906055   \n","50%             -0.853382           -0.676286            -0.698864   \n","75%             -0.542371           -0.409775            -0.250588   \n","max              1.000000            0.955179             1.000000   \n","\n","       Shoot14DaysSeedling  Shoot17DaysSeedling  Shoot21DaysSeedling  \\\n","count         41110.000000         41110.000000         41110.000000   \n","mean             -0.734141            -0.680810            -0.659443   \n","std               0.413716             0.478189             0.463838   \n","min              -1.000000            -1.000000            -1.000000   \n","25%              -1.000000            -1.000000            -1.000000   \n","50%              -0.924976            -0.954040            -0.874080   \n","75%              -0.513759            -0.420386            -0.440577   \n","max               0.997390             1.000000             1.000000   \n","\n","       Shoot35DaysSeedling  Leaf21DaysSeedling  Leaf45DaysOldPlant  \\\n","count         41110.000000        41110.000000        41110.000000   \n","mean             -0.558906           -0.828778           -0.585144   \n","std               0.506423            0.327542            0.399046   \n","min              -1.000000           -1.000000           -1.000000   \n","25%              -0.962199           -1.000000           -0.901444   \n","50%              -0.699035           -0.951894           -0.643376   \n","75%              -0.352995           -0.883755           -0.451900   \n","max               0.993958            1.000000            1.000000   \n","\n","              class  \n","count  41110.000000  \n","mean      60.092703  \n","std       77.624892  \n","min        1.000000  \n","25%        9.000000  \n","50%       26.000000  \n","75%       78.000000  \n","max      373.000000  \n","\n","[8 rows x 21 columns]\n"]}],"source":["# load dataset (input variables = X, output variables = Y)\n","df = pd.read_csv(\"TrainingData.csv\")\n","\n","#count the number of occurances for each osID\n","OsID_counts = df['OsID'].value_counts()\n","\n","#filter for osIDs that have 10 or more occurances\n","OsID_counts_filtered = OsID_counts[OsID_counts >= 10]\n","\n","#assign a label for each osID \n","OsID_labels = {}\n","class_no = 1\n","for osID in OsID_counts_filtered.index:\n","    OsID_labels[osID] = class_no\n","    class_no +=1\n","\n","#filter the dataset with osID that contain 10 or more occurances\n","dataGene = df[df['OsID'].isin(OsID_counts_filtered.index)]\n","\n","dataGene = dataGene.drop(['Class', 'Trait'],axis=1)\n","\n","# Add a new column 'class' to the filtered dataset\n","dataGene['class'] = dataGene['OsID'].map(OsID_labels)\n","\n","print(\"Summary of dataGene:\\n\",dataGene.describe())\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"ZQjM56LwvG4i"},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of X:\n"," (41110, 20)\n","Shape of Y:\n"," (41110,)\n","Summary of X:\n","        CoExpression           PCC           PPI  Root10DaysSeedling  \\\n","count  41110.000000  41110.000000  41110.000000        41110.000000   \n","mean       0.991997     -0.361737      0.914668           -0.522040   \n","std        0.089101      0.463979      0.279379            0.498568   \n","min        0.000000     -1.000000      0.000000           -1.000000   \n","25%        1.000000     -0.747963      1.000000           -0.901371   \n","50%        1.000000     -0.449089      1.000000           -0.663664   \n","75%        1.000000     -0.051646      1.000000           -0.378497   \n","max        1.000000      1.000000      1.000000            1.000000   \n","\n","       Leaf21DaysSeedling  Leaf45DaysOldPlant  log_2FoldChange            ET  \\\n","count        41110.000000        41110.000000     41110.000000  41110.000000   \n","mean            -0.828778           -0.585144        -0.037332      1.407395   \n","std              0.327542            0.399046         0.391444      0.784327   \n","min             -1.000000           -1.000000        -1.000000      0.000000   \n","25%             -1.000000           -0.901444        -0.251534      1.000000   \n","50%             -0.951894           -0.643376         0.030675      2.000000   \n","75%             -0.883755           -0.451900         0.251534      2.000000   \n","max              1.000000            1.000000         1.000000      2.000000   \n","\n","       Shoot10DaysSeedling  Shoot3DaysSeedling  Shoot35DaysSeedling  \\\n","count         41110.000000        41110.000000         41110.000000   \n","mean             -0.545055           -0.590806            -0.558906   \n","std               0.477438            0.443552             0.506423   \n","min              -1.000000           -1.000000            -1.000000   \n","25%              -0.906055           -1.000000            -0.962199   \n","50%              -0.698864           -0.676286            -0.699035   \n","75%              -0.250588           -0.409775            -0.352995   \n","max               1.000000            0.955179             0.993958   \n","\n","       Shoot14DaysSeedling  Root17DaysSeedling  Shoot17DaysSeedling  \\\n","count         41110.000000        41110.000000         41110.000000   \n","mean             -0.734141           -0.700869            -0.680810   \n","std               0.413716            0.378219             0.478189   \n","min              -1.000000           -1.000000            -1.000000   \n","25%              -1.000000           -0.980226            -1.000000   \n","50%              -0.924976           -0.795609            -0.954040   \n","75%              -0.513759           -0.601266            -0.420386   \n","max               0.997390            1.000000             1.000000   \n","\n","       Shoot21DaysSeedling  Root24DaysSeedling  Root14DaysSeedling  \\\n","count         41110.000000        41110.000000        41110.000000   \n","mean             -0.659443           -0.670048           -0.646982   \n","std               0.463838            0.390751            0.393549   \n","min              -1.000000           -1.000000           -1.000000   \n","25%              -1.000000           -0.982003           -0.965084   \n","50%              -0.874080           -0.708584           -0.680003   \n","75%              -0.440577           -0.482133           -0.559627   \n","max               1.000000            1.000000            1.000000   \n","\n","       Root21DaysSeedling  Root52DaysSeedling  Root35DaysSeedling  \n","count        41110.000000        41110.000000        41110.000000  \n","mean            -0.669349           -0.670345           -0.596196  \n","std              0.405860            0.478222            0.461679  \n","min             -1.000000           -1.000000           -1.000000  \n","25%             -1.000000           -1.000000           -0.937286  \n","50%             -0.726665           -0.853382           -0.769184  \n","75%             -0.543621           -0.542371           -0.323664  \n","max              1.000000            1.000000            1.000000  \n","Summary of Y:\n"," count    41110.000000\n","mean        60.092703\n","std         77.624892\n","min          1.000000\n","25%          9.000000\n","50%         26.000000\n","75%         78.000000\n","max        373.000000\n","Name: class, dtype: float64\n","class\n","1.0      1800\n","2.0      1296\n","3.0      1260\n","4.0      1218\n","5.0      1026\n","         ... \n","369.0      10\n","370.0      10\n","371.0      10\n","372.0      10\n","373.0      10\n","Length: 373, dtype: int64\n"]}],"source":["X = dataGene.drop(['class','OsID'], axis=1) # exclude class & OsID column\n","Y = dataGene['class']\n","\n","#input feature names in order of descending importance scores in MI feature selection method\n","feature_names = ['CoExpression', 'PCC', 'PPI', 'Root10DaysSeedling', 'Leaf21DaysSeedling', \n","                 'Leaf45DaysOldPlant', 'log_2FoldChange', 'ET', 'Shoot10DaysSeedling', 'Shoot3DaysSeedling', \n","                 'Shoot35DaysSeedling', 'Shoot14DaysSeedling', 'Root17DaysSeedling', 'Shoot17DaysSeedling', 'Shoot21DaysSeedling', \n","                 'Root24DaysSeedling', 'Root14DaysSeedling', 'Root21DaysSeedling', 'Root52DaysSeedling', 'Root35DaysSeedling']\n","\n","X_fs = X.reindex(columns=feature_names)\n","\n","print(\"Shape of X:\\n\",X_fs.shape)\n","print(\"Shape of Y:\\n\",Y.shape)\n","\n","# Statistical summary of the variables\n","print(\"Summary of X:\\n\",X_fs.describe())\n","print(\"Summary of Y:\\n\",Y.describe())\n","\n","# Check for class imbalance\n","print(df.groupby(Y).size())\n","\n","# change both input and target variables datatype to ndarray\n","X_fs = X_fs.values # 2-D array\n","\n","# select target variable \n","\n","Y = Y.values #1-D array\n","Y = Y.reshape(-1, 1)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"F2UQyOKPvMXF"},"outputs":[{"name":"stdout","output_type":"stream","text":["Class=0, n=1800 (4.378%)\n","Class=1, n=1296 (3.153%)\n","Class=2, n=1260 (3.065%)\n","Class=3, n=1218 (2.963%)\n","Class=4, n=1026 (2.496%)\n","Class=5, n=1008 (2.452%)\n","Class=6, n=930 (2.262%)\n","Class=7, n=912 (2.218%)\n","Class=8, n=880 (2.141%)\n","Class=9, n=798 (1.941%)\n","Class=10, n=792 (1.927%)\n","Class=11, n=759 (1.846%)\n","Class=12, n=729 (1.773%)\n","Class=13, n=720 (1.751%)\n","Class=14, n=702 (1.708%)\n","Class=15, n=693 (1.686%)\n","Class=16, n=672 (1.635%)\n","Class=17, n=640 (1.557%)\n","Class=18, n=625 (1.520%)\n","Class=19, n=570 (1.387%)\n","Class=20, n=546 (1.328%)\n","Class=21, n=506 (1.231%)\n","Class=22, n=483 (1.175%)\n","Class=23, n=448 (1.090%)\n","Class=24, n=432 (1.051%)\n","Class=25, n=384 (0.934%)\n","Class=26, n=360 (0.876%)\n","Class=27, n=360 (0.876%)\n","Class=28, n=320 (0.778%)\n","Class=29, n=312 (0.759%)\n","Class=30, n=312 (0.759%)\n","Class=31, n=306 (0.744%)\n","Class=32, n=304 (0.739%)\n","Class=33, n=299 (0.727%)\n","Class=34, n=297 (0.722%)\n","Class=35, n=296 (0.720%)\n","Class=36, n=280 (0.681%)\n","Class=37, n=264 (0.642%)\n","Class=38, n=260 (0.632%)\n","Class=39, n=253 (0.615%)\n","Class=40, n=252 (0.613%)\n","Class=41, n=248 (0.603%)\n","Class=42, n=242 (0.589%)\n","Class=43, n=228 (0.555%)\n","Class=44, n=216 (0.525%)\n","Class=45, n=210 (0.511%)\n","Class=46, n=200 (0.486%)\n","Class=47, n=192 (0.467%)\n","Class=48, n=180 (0.438%)\n","Class=49, n=171 (0.416%)\n","Class=50, n=168 (0.409%)\n","Class=51, n=168 (0.409%)\n","Class=52, n=162 (0.394%)\n","Class=53, n=150 (0.365%)\n","Class=54, n=148 (0.360%)\n","Class=55, n=138 (0.336%)\n","Class=56, n=135 (0.328%)\n","Class=57, n=135 (0.328%)\n","Class=58, n=133 (0.324%)\n","Class=59, n=132 (0.321%)\n","Class=60, n=132 (0.321%)\n","Class=61, n=130 (0.316%)\n","Class=62, n=130 (0.316%)\n","Class=63, n=130 (0.316%)\n","Class=64, n=128 (0.311%)\n","Class=65, n=128 (0.311%)\n","Class=66, n=126 (0.306%)\n","Class=67, n=124 (0.302%)\n","Class=68, n=124 (0.302%)\n","Class=69, n=124 (0.302%)\n","Class=70, n=120 (0.292%)\n","Class=71, n=120 (0.292%)\n","Class=72, n=118 (0.287%)\n","Class=73, n=116 (0.282%)\n","Class=74, n=114 (0.277%)\n","Class=75, n=105 (0.255%)\n","Class=76, n=104 (0.253%)\n","Class=77, n=102 (0.248%)\n","Class=78, n=99 (0.241%)\n","Class=79, n=98 (0.238%)\n","Class=80, n=98 (0.238%)\n","Class=81, n=98 (0.238%)\n","Class=82, n=98 (0.238%)\n","Class=83, n=96 (0.234%)\n","Class=84, n=96 (0.234%)\n","Class=85, n=96 (0.234%)\n","Class=86, n=93 (0.226%)\n","Class=87, n=92 (0.224%)\n","Class=88, n=92 (0.224%)\n","Class=89, n=91 (0.221%)\n","Class=90, n=88 (0.214%)\n","Class=91, n=88 (0.214%)\n","Class=92, n=86 (0.209%)\n","Class=93, n=86 (0.209%)\n","Class=94, n=84 (0.204%)\n","Class=95, n=84 (0.204%)\n","Class=96, n=84 (0.204%)\n","Class=97, n=78 (0.190%)\n","Class=98, n=78 (0.190%)\n","Class=99, n=76 (0.185%)\n","Class=100, n=75 (0.182%)\n","Class=101, n=75 (0.182%)\n","Class=102, n=73 (0.178%)\n","Class=103, n=72 (0.175%)\n","Class=104, n=72 (0.175%)\n","Class=105, n=70 (0.170%)\n","Class=106, n=69 (0.168%)\n","Class=107, n=68 (0.165%)\n","Class=108, n=67 (0.163%)\n","Class=109, n=66 (0.161%)\n","Class=110, n=66 (0.161%)\n","Class=111, n=66 (0.161%)\n","Class=112, n=66 (0.161%)\n","Class=113, n=66 (0.161%)\n","Class=114, n=65 (0.158%)\n","Class=115, n=64 (0.156%)\n","Class=116, n=63 (0.153%)\n","Class=117, n=63 (0.153%)\n","Class=118, n=62 (0.151%)\n","Class=119, n=61 (0.148%)\n","Class=120, n=60 (0.146%)\n","Class=121, n=60 (0.146%)\n","Class=122, n=60 (0.146%)\n","Class=123, n=60 (0.146%)\n","Class=124, n=60 (0.146%)\n","Class=125, n=60 (0.146%)\n","Class=126, n=60 (0.146%)\n","Class=127, n=60 (0.146%)\n","Class=128, n=60 (0.146%)\n","Class=129, n=60 (0.146%)\n","Class=130, n=59 (0.144%)\n","Class=131, n=59 (0.144%)\n","Class=132, n=58 (0.141%)\n","Class=133, n=56 (0.136%)\n","Class=134, n=56 (0.136%)\n","Class=135, n=56 (0.136%)\n","Class=136, n=56 (0.136%)\n","Class=137, n=56 (0.136%)\n","Class=138, n=56 (0.136%)\n","Class=139, n=56 (0.136%)\n","Class=140, n=56 (0.136%)\n","Class=141, n=56 (0.136%)\n","Class=142, n=55 (0.134%)\n","Class=143, n=55 (0.134%)\n","Class=144, n=54 (0.131%)\n","Class=145, n=54 (0.131%)\n","Class=146, n=54 (0.131%)\n","Class=147, n=54 (0.131%)\n","Class=148, n=54 (0.131%)\n","Class=149, n=53 (0.129%)\n","Class=150, n=52 (0.126%)\n","Class=151, n=52 (0.126%)\n","Class=152, n=52 (0.126%)\n","Class=153, n=52 (0.126%)\n","Class=154, n=50 (0.122%)\n","Class=155, n=50 (0.122%)\n","Class=156, n=49 (0.119%)\n","Class=157, n=49 (0.119%)\n","Class=158, n=48 (0.117%)\n","Class=159, n=48 (0.117%)\n","Class=160, n=48 (0.117%)\n","Class=161, n=46 (0.112%)\n","Class=162, n=45 (0.109%)\n","Class=163, n=44 (0.107%)\n","Class=164, n=44 (0.107%)\n","Class=165, n=44 (0.107%)\n","Class=166, n=42 (0.102%)\n","Class=167, n=42 (0.102%)\n","Class=168, n=42 (0.102%)\n","Class=169, n=42 (0.102%)\n","Class=170, n=42 (0.102%)\n","Class=171, n=42 (0.102%)\n","Class=172, n=42 (0.102%)\n","Class=173, n=41 (0.100%)\n","Class=174, n=41 (0.100%)\n","Class=175, n=40 (0.097%)\n","Class=176, n=40 (0.097%)\n","Class=177, n=39 (0.095%)\n","Class=178, n=39 (0.095%)\n","Class=179, n=38 (0.092%)\n","Class=180, n=37 (0.090%)\n","Class=181, n=36 (0.088%)\n","Class=182, n=35 (0.085%)\n","Class=183, n=35 (0.085%)\n","Class=184, n=35 (0.085%)\n","Class=185, n=35 (0.085%)\n","Class=186, n=34 (0.083%)\n","Class=187, n=34 (0.083%)\n","Class=188, n=34 (0.083%)\n","Class=189, n=34 (0.083%)\n","Class=190, n=32 (0.078%)\n","Class=191, n=32 (0.078%)\n","Class=192, n=32 (0.078%)\n","Class=193, n=32 (0.078%)\n","Class=194, n=32 (0.078%)\n","Class=195, n=32 (0.078%)\n","Class=196, n=31 (0.075%)\n","Class=197, n=31 (0.075%)\n","Class=198, n=31 (0.075%)\n","Class=199, n=31 (0.075%)\n","Class=200, n=30 (0.073%)\n","Class=201, n=30 (0.073%)\n","Class=202, n=30 (0.073%)\n","Class=203, n=30 (0.073%)\n","Class=204, n=30 (0.073%)\n","Class=205, n=30 (0.073%)\n","Class=206, n=30 (0.073%)\n","Class=207, n=30 (0.073%)\n","Class=208, n=30 (0.073%)\n","Class=209, n=29 (0.071%)\n","Class=210, n=29 (0.071%)\n","Class=211, n=28 (0.068%)\n","Class=212, n=28 (0.068%)\n","Class=213, n=28 (0.068%)\n","Class=214, n=28 (0.068%)\n","Class=215, n=28 (0.068%)\n","Class=216, n=28 (0.068%)\n","Class=217, n=27 (0.066%)\n","Class=218, n=27 (0.066%)\n","Class=219, n=27 (0.066%)\n","Class=220, n=27 (0.066%)\n","Class=221, n=27 (0.066%)\n","Class=222, n=27 (0.066%)\n","Class=223, n=26 (0.063%)\n","Class=224, n=26 (0.063%)\n","Class=225, n=26 (0.063%)\n","Class=226, n=26 (0.063%)\n","Class=227, n=26 (0.063%)\n","Class=228, n=25 (0.061%)\n","Class=229, n=25 (0.061%)\n","Class=230, n=25 (0.061%)\n","Class=231, n=25 (0.061%)\n","Class=232, n=24 (0.058%)\n","Class=233, n=24 (0.058%)\n","Class=234, n=24 (0.058%)\n","Class=235, n=24 (0.058%)\n","Class=236, n=24 (0.058%)\n","Class=237, n=24 (0.058%)\n","Class=238, n=24 (0.058%)\n","Class=239, n=24 (0.058%)\n","Class=240, n=24 (0.058%)\n","Class=241, n=24 (0.058%)\n","Class=242, n=24 (0.058%)\n","Class=243, n=24 (0.058%)\n","Class=244, n=23 (0.056%)\n","Class=245, n=23 (0.056%)\n","Class=246, n=22 (0.054%)\n","Class=247, n=22 (0.054%)\n","Class=248, n=22 (0.054%)\n","Class=249, n=22 (0.054%)\n","Class=250, n=22 (0.054%)\n","Class=251, n=22 (0.054%)\n","Class=252, n=22 (0.054%)\n","Class=253, n=22 (0.054%)\n","Class=254, n=22 (0.054%)\n","Class=255, n=22 (0.054%)\n","Class=256, n=22 (0.054%)\n","Class=257, n=22 (0.054%)\n","Class=258, n=22 (0.054%)\n","Class=259, n=22 (0.054%)\n","Class=260, n=22 (0.054%)\n","Class=261, n=22 (0.054%)\n","Class=262, n=22 (0.054%)\n","Class=263, n=22 (0.054%)\n","Class=264, n=21 (0.051%)\n","Class=265, n=21 (0.051%)\n","Class=266, n=21 (0.051%)\n","Class=267, n=21 (0.051%)\n","Class=268, n=21 (0.051%)\n","Class=269, n=20 (0.049%)\n","Class=270, n=20 (0.049%)\n","Class=271, n=20 (0.049%)\n","Class=272, n=20 (0.049%)\n","Class=273, n=20 (0.049%)\n","Class=274, n=20 (0.049%)\n","Class=275, n=20 (0.049%)\n","Class=276, n=20 (0.049%)\n","Class=277, n=20 (0.049%)\n","Class=278, n=20 (0.049%)\n","Class=279, n=20 (0.049%)\n","Class=280, n=19 (0.046%)\n","Class=281, n=19 (0.046%)\n","Class=282, n=19 (0.046%)\n","Class=283, n=18 (0.044%)\n","Class=284, n=18 (0.044%)\n","Class=285, n=18 (0.044%)\n","Class=286, n=18 (0.044%)\n","Class=287, n=18 (0.044%)\n","Class=288, n=18 (0.044%)\n","Class=289, n=18 (0.044%)\n","Class=290, n=18 (0.044%)\n","Class=291, n=18 (0.044%)\n","Class=292, n=17 (0.041%)\n","Class=293, n=17 (0.041%)\n","Class=294, n=17 (0.041%)\n","Class=295, n=17 (0.041%)\n","Class=296, n=17 (0.041%)\n","Class=297, n=17 (0.041%)\n","Class=298, n=16 (0.039%)\n","Class=299, n=16 (0.039%)\n","Class=300, n=16 (0.039%)\n","Class=301, n=16 (0.039%)\n","Class=302, n=16 (0.039%)\n","Class=303, n=16 (0.039%)\n","Class=304, n=16 (0.039%)\n","Class=305, n=16 (0.039%)\n","Class=306, n=15 (0.036%)\n","Class=307, n=15 (0.036%)\n","Class=308, n=15 (0.036%)\n","Class=309, n=15 (0.036%)\n","Class=310, n=15 (0.036%)\n","Class=311, n=14 (0.034%)\n","Class=312, n=14 (0.034%)\n","Class=313, n=14 (0.034%)\n","Class=314, n=14 (0.034%)\n","Class=315, n=14 (0.034%)\n","Class=316, n=14 (0.034%)\n","Class=317, n=14 (0.034%)\n","Class=318, n=14 (0.034%)\n","Class=319, n=14 (0.034%)\n","Class=320, n=14 (0.034%)\n","Class=321, n=14 (0.034%)\n","Class=322, n=14 (0.034%)\n","Class=323, n=14 (0.034%)\n","Class=324, n=14 (0.034%)\n","Class=325, n=14 (0.034%)\n","Class=326, n=14 (0.034%)\n","Class=327, n=14 (0.034%)\n","Class=328, n=13 (0.032%)\n","Class=329, n=13 (0.032%)\n","Class=330, n=13 (0.032%)\n","Class=331, n=13 (0.032%)\n","Class=332, n=13 (0.032%)\n","Class=333, n=13 (0.032%)\n","Class=334, n=13 (0.032%)\n","Class=335, n=13 (0.032%)\n","Class=336, n=13 (0.032%)\n","Class=337, n=12 (0.029%)\n","Class=338, n=12 (0.029%)\n","Class=339, n=12 (0.029%)\n","Class=340, n=12 (0.029%)\n","Class=341, n=12 (0.029%)\n","Class=342, n=12 (0.029%)\n","Class=343, n=12 (0.029%)\n","Class=344, n=12 (0.029%)\n","Class=345, n=12 (0.029%)\n","Class=346, n=12 (0.029%)\n","Class=347, n=12 (0.029%)\n","Class=348, n=12 (0.029%)\n","Class=349, n=12 (0.029%)\n","Class=350, n=12 (0.029%)\n","Class=351, n=12 (0.029%)\n","Class=352, n=12 (0.029%)\n","Class=353, n=12 (0.029%)\n","Class=354, n=12 (0.029%)\n","Class=355, n=11 (0.027%)\n","Class=356, n=11 (0.027%)\n","Class=357, n=11 (0.027%)\n","Class=358, n=11 (0.027%)\n","Class=359, n=11 (0.027%)\n","Class=360, n=11 (0.027%)\n","Class=361, n=10 (0.024%)\n","Class=362, n=10 (0.024%)\n","Class=363, n=10 (0.024%)\n","Class=364, n=10 (0.024%)\n","Class=365, n=10 (0.024%)\n","Class=366, n=10 (0.024%)\n","Class=367, n=10 (0.024%)\n","Class=368, n=10 (0.024%)\n","Class=369, n=10 (0.024%)\n","Class=370, n=10 (0.024%)\n","Class=371, n=10 (0.024%)\n","Class=372, n=10 (0.024%)\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["y_labelenc = LabelEncoder().fit_transform(Y)\n","\n","from collections import Counter\n","# summarize distribution\n","counter = Counter(y_labelenc)\n","\n","# sort counter by keys\n","counter = dict(sorted(counter.items()))\n","\n","\n","for k,v in counter.items():\n"," per = v / len(y_labelenc) * 100\n"," print('Class=%d, n=%d (%.3f%%)' % (k, v, per))\n","# plot the distribution\n","plt.bar( counter.keys(), counter.values())\n","\n","plt.ylabel('No of gene samples')\n","plt.xlabel('Class Index of OsID')\n","plt.title('Distribution of Target Classes (OsID)')\n","plt.show()"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"SyCZMH9wvSyC"},"outputs":[],"source":["# prepare target data\n","def prepare_targets(y_train, y_test):\n","\tle = LabelEncoder()\n","\t\n","\t#fit the encoders only to the training data and then transform both train and test data\n","\ty_train_enc = le.fit_transform(y_train)\n","\ty_test_enc = le.transform(y_test)\n","\n","\treturn y_train_enc, y_test_enc"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"wZNqq1dZQsYs"},"outputs":[],"source":["# define the RBFLayer layer as a custom layer\n","class RBFLayer(Layer):\n","    def __init__(self, units, gamma, **kwargs):\n","        super(RBFLayer, self).__init__(**kwargs)\n","        self.units = units\n","        self.gamma = K.cast_to_floatx(gamma)\n","\n","    def build(self, input_shape):\n","        self.mu = self.add_weight(name='mu',\n","                                  shape=(int(input_shape[1]), self.units),\n","                                  initializer='uniform',\n","                                  trainable=True)\n","        super(RBFLayer, self).build(input_shape)\n","\n","    def call(self, inputs):\n","        diff = K.expand_dims(inputs) - self.mu\n","        l2 = K.sum(K.pow(diff, 2), axis=1)\n","        res = K.exp(-1 * self.gamma * l2)\n","        return res\n","\n","    def compute_output_shape(self, input_shape):\n","        return (input_shape[0], self.units)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"0AqXG7_jQsYu"},"outputs":[],"source":["# prepare target data\n","def prepare_targets(y_train, y_test):\n","\tle = LabelEncoder()\n","\t\n","\t#fit the encoders only to the training data and then transform both train and test data\n","\ty_train_enc = le.fit_transform(y_train)\n","\ty_test_enc = le.transform(y_test)\n","\n","\treturn y_train_enc, y_test_enc"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"Nh4EHT34QsYv"},"outputs":[],"source":["# define baseline model (RBFN)\n","def RBFN_model(input_dim):\n","\n","    model = Sequential()\n","    model.add(Flatten(input_shape=(input_dim,)))\n","    #add the RBF layer\n","    model.add(RBFLayer(10, 0.5))\n","    \n","    model.add(Dense(60, input_dim=input_dim, activation='relu',bias_initializer='normal',kernel_regularizer=regularizers.l2(0.0001)))\n","    model.add(Dense(20,bias_initializer='normal',activation='relu',kernel_regularizer=regularizers.l2(0.0001)))\n","    model.add(Dense(20,bias_initializer='normal',activation='relu',kernel_regularizer=regularizers.l2(0.0001)))\n","    model.add(Dense(373,kernel_initializer='normal',activation='softmax'))\n","\n","    # Compile model\n","    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    \n","    return model"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"qGxBoDFmQsYw"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]}],"source":["# split into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X_fs, Y, test_size=0.33, random_state=1)\n","\n","# prepare output data\n","y_train_enc, y_test_enc = prepare_targets(y_train, y_test)\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5353788,"status":"ok","timestamp":1682274337707,"user":{"displayName":"Good Notes UM","userId":"12493674840996884929"},"user_tz":-480},"id":"21D3eb-NQsYx","outputId":"3cecf9fd-904d-4bc3-a803-fc2a3b30075d"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"name":"stdout","output_type":"stream","text":["Number of input features: 1\n","Fold: 1\n","Epoch 1/20\n","1846/1846 [==============================] - 8s 3ms/step - loss: 5.0983 - accuracy: 0.0415 - val_loss: 5.0695 - val_accuracy: 0.0436\n","Epoch 2/20\n","1846/1846 [==============================] - 6s 3ms/step - loss: 4.9837 - accuracy: 0.0440 - val_loss: 5.0399 - val_accuracy: 0.0436\n","Epoch 3/20\n","1846/1846 [==============================] - 5s 3ms/step - loss: 4.9769 - accuracy: 0.0440 - val_loss: 5.0387 - val_accuracy: 0.0436\n","Epoch 4/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 4.9737 - accuracy: 0.0440 - val_loss: 5.0382 - val_accuracy: 0.0436\n","Epoch 5/20\n","1846/1846 [==============================] - 6s 3ms/step - loss: 4.9700 - accuracy: 0.0440 - val_loss: 5.0508 - val_accuracy: 0.0436\n","Epoch 6/20\n","1846/1846 [==============================] - 5s 2ms/step - loss: 4.9663 - accuracy: 0.0440 - val_loss: 5.0350 - val_accuracy: 0.0436\n","Epoch 7/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 4.9650 - accuracy: 0.0440 - val_loss: 5.0303 - val_accuracy: 0.0436\n","Epoch 8/20\n","1846/1846 [==============================] - 6s 3ms/step - loss: 4.9574 - accuracy: 0.0440 - val_loss: 5.0293 - val_accuracy: 0.0436\n","Epoch 9/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 4.9518 - accuracy: 0.0440 - val_loss: 5.0497 - val_accuracy: 0.0436\n","Epoch 10/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.9458 - accuracy: 0.0451 - val_loss: 5.0224 - val_accuracy: 0.0451\n","Epoch 11/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.9367 - accuracy: 0.0454 - val_loss: 5.0181 - val_accuracy: 0.0451\n","Epoch 12/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.9290 - accuracy: 0.0454 - val_loss: 5.0163 - val_accuracy: 0.0451\n","Epoch 13/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.9270 - accuracy: 0.0454 - val_loss: 5.0170 - val_accuracy: 0.0451\n","Epoch 14/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.9240 - accuracy: 0.0452 - val_loss: 5.0146 - val_accuracy: 0.0451\n","Epoch 15/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.9225 - accuracy: 0.0454 - val_loss: 5.0137 - val_accuracy: 0.0451\n","Epoch 16/20\n","1846/1846 [==============================] - 12s 7ms/step - loss: 4.9211 - accuracy: 0.0453 - val_loss: 5.0126 - val_accuracy: 0.0451\n","Epoch 17/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.9208 - accuracy: 0.0454 - val_loss: 5.0326 - val_accuracy: 0.0451\n","Epoch 18/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.9197 - accuracy: 0.0451 - val_loss: 5.0223 - val_accuracy: 0.0451\n","Epoch 19/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.9201 - accuracy: 0.0454 - val_loss: 5.0154 - val_accuracy: 0.0451\n","Epoch 20/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.9182 - accuracy: 0.0453 - val_loss: 5.0136 - val_accuracy: 0.0451\n","Fold: 2\n","Epoch 1/20\n","1846/1846 [==============================] - 13s 5ms/step - loss: 5.0902 - accuracy: 0.0410 - val_loss: 5.0801 - val_accuracy: 0.0440\n","Epoch 2/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.9874 - accuracy: 0.0439 - val_loss: 5.0542 - val_accuracy: 0.0440\n","Epoch 3/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.9803 - accuracy: 0.0439 - val_loss: 5.0502 - val_accuracy: 0.0440\n","Epoch 4/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.9776 - accuracy: 0.0439 - val_loss: 5.0504 - val_accuracy: 0.0440\n","Epoch 5/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.9749 - accuracy: 0.0439 - val_loss: 5.0626 - val_accuracy: 0.0440\n","Epoch 6/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.9720 - accuracy: 0.0439 - val_loss: 5.0468 - val_accuracy: 0.0440\n","Epoch 7/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.9696 - accuracy: 0.0439 - val_loss: 5.0501 - val_accuracy: 0.0440\n","Epoch 8/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.9684 - accuracy: 0.0439 - val_loss: 5.0499 - val_accuracy: 0.0440\n","Epoch 9/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.9662 - accuracy: 0.0439 - val_loss: 5.0513 - val_accuracy: 0.0440\n","Epoch 10/20\n","1846/1846 [==============================] - 14s 7ms/step - loss: 4.9640 - accuracy: 0.0439 - val_loss: 5.0535 - val_accuracy: 0.0440\n","Epoch 11/20\n","1846/1846 [==============================] - 13s 7ms/step - loss: 4.9627 - accuracy: 0.0439 - val_loss: 5.0451 - val_accuracy: 0.0440\n","Epoch 12/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.9599 - accuracy: 0.0439 - val_loss: 5.0463 - val_accuracy: 0.0440\n","Epoch 13/20\n","1846/1846 [==============================] - 12s 6ms/step - loss: 4.9565 - accuracy: 0.0439 - val_loss: 5.0521 - val_accuracy: 0.0440\n","Epoch 14/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.9512 - accuracy: 0.0439 - val_loss: 5.0323 - val_accuracy: 0.0440\n","Epoch 15/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.9481 - accuracy: 0.0439 - val_loss: 5.0303 - val_accuracy: 0.0440\n","Epoch 16/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.9450 - accuracy: 0.0447 - val_loss: 5.0336 - val_accuracy: 0.0460\n","Epoch 17/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.9396 - accuracy: 0.0451 - val_loss: 5.0219 - val_accuracy: 0.0460\n","Epoch 18/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.9320 - accuracy: 0.0451 - val_loss: 5.0286 - val_accuracy: 0.0460\n","Epoch 19/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.9281 - accuracy: 0.0451 - val_loss: 5.0203 - val_accuracy: 0.0460\n","Epoch 20/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 4.9272 - accuracy: 0.0451 - val_loss: 5.0166 - val_accuracy: 0.0460\n","Average Validation Accuracy: 0.04534727334976196\n","Average Validation Loss: 4.944753408432007\n","Average Test Accuracy: 0.045035749673843384\n","Final Test Accuracy for each fold: 0.045035749673843384\n","Number of input features: 2\n","Fold: 1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","1846/1846 [==============================] - 11s 4ms/step - loss: 5.0950 - accuracy: 0.0426 - val_loss: 5.0573 - val_accuracy: 0.0436\n","Epoch 2/20\n","1846/1846 [==============================] - 8s 5ms/step - loss: 4.9827 - accuracy: 0.0426 - val_loss: 5.0391 - val_accuracy: 0.0436\n","Epoch 3/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 4.9756 - accuracy: 0.0439 - val_loss: 5.0403 - val_accuracy: 0.0436\n","Epoch 4/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 4.9702 - accuracy: 0.0426 - val_loss: 5.0316 - val_accuracy: 0.0436\n","Epoch 5/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 4.9690 - accuracy: 0.0440 - val_loss: 5.0320 - val_accuracy: 0.0436\n","Epoch 6/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 4.9619 - accuracy: 0.0448 - val_loss: 5.0300 - val_accuracy: 0.0436\n","Epoch 7/20\n","1846/1846 [==============================] - 8s 5ms/step - loss: 4.9373 - accuracy: 0.0490 - val_loss: 4.9795 - val_accuracy: 0.0528\n","Epoch 8/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.8884 - accuracy: 0.0506 - val_loss: 4.9504 - val_accuracy: 0.0508\n","Epoch 9/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.8626 - accuracy: 0.0512 - val_loss: 4.9376 - val_accuracy: 0.0488\n","Epoch 10/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.8455 - accuracy: 0.0530 - val_loss: 4.9413 - val_accuracy: 0.0524\n","Epoch 11/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.8329 - accuracy: 0.0551 - val_loss: 4.9354 - val_accuracy: 0.0508\n","Epoch 12/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.8224 - accuracy: 0.0532 - val_loss: 4.9327 - val_accuracy: 0.0510\n","Epoch 13/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.8124 - accuracy: 0.0555 - val_loss: 4.9172 - val_accuracy: 0.0561\n","Epoch 14/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.8008 - accuracy: 0.0565 - val_loss: 4.9102 - val_accuracy: 0.0541\n","Epoch 15/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.7872 - accuracy: 0.0579 - val_loss: 4.9108 - val_accuracy: 0.0574\n","Epoch 16/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.7747 - accuracy: 0.0581 - val_loss: 4.9150 - val_accuracy: 0.0526\n","Epoch 17/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.7647 - accuracy: 0.0589 - val_loss: 4.8978 - val_accuracy: 0.0572\n","Epoch 18/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.7546 - accuracy: 0.0590 - val_loss: 4.9078 - val_accuracy: 0.0561\n","Epoch 19/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.7457 - accuracy: 0.0595 - val_loss: 4.8983 - val_accuracy: 0.0631\n","Epoch 20/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.7395 - accuracy: 0.0606 - val_loss: 4.8972 - val_accuracy: 0.0532\n","Fold: 2\n","Epoch 1/20\n","1846/1846 [==============================] - 13s 6ms/step - loss: 5.0918 - accuracy: 0.0426 - val_loss: 5.0715 - val_accuracy: 0.0440\n","Epoch 2/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.9808 - accuracy: 0.0436 - val_loss: 5.0547 - val_accuracy: 0.0440\n","Epoch 3/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.9732 - accuracy: 0.0439 - val_loss: 5.0430 - val_accuracy: 0.0440\n","Epoch 4/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.9678 - accuracy: 0.0434 - val_loss: 5.0384 - val_accuracy: 0.0440\n","Epoch 5/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.9589 - accuracy: 0.0438 - val_loss: 5.0338 - val_accuracy: 0.0440\n","Epoch 6/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.9345 - accuracy: 0.0475 - val_loss: 4.9731 - val_accuracy: 0.0431\n","Epoch 7/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.8689 - accuracy: 0.0517 - val_loss: 4.9272 - val_accuracy: 0.0535\n","Epoch 8/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.8321 - accuracy: 0.0545 - val_loss: 4.9159 - val_accuracy: 0.0517\n","Epoch 9/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.8111 - accuracy: 0.0538 - val_loss: 4.9115 - val_accuracy: 0.0524\n","Epoch 10/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.7934 - accuracy: 0.0521 - val_loss: 4.8974 - val_accuracy: 0.0521\n","Epoch 11/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.7780 - accuracy: 0.0545 - val_loss: 4.9139 - val_accuracy: 0.0570\n","Epoch 12/20\n","1846/1846 [==============================] - 8s 5ms/step - loss: 4.7670 - accuracy: 0.0533 - val_loss: 4.8988 - val_accuracy: 0.0557\n","Epoch 13/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.7570 - accuracy: 0.0530 - val_loss: 4.9062 - val_accuracy: 0.0550\n","Epoch 14/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.7459 - accuracy: 0.0567 - val_loss: 4.8988 - val_accuracy: 0.0579\n","Epoch 15/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.7374 - accuracy: 0.0554 - val_loss: 4.9077 - val_accuracy: 0.0532\n","Epoch 16/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.7287 - accuracy: 0.0565 - val_loss: 4.9139 - val_accuracy: 0.0502\n","Epoch 17/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.7226 - accuracy: 0.0580 - val_loss: 4.9153 - val_accuracy: 0.0548\n","Epoch 18/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.7159 - accuracy: 0.0591 - val_loss: 4.9235 - val_accuracy: 0.0667\n","Epoch 19/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.7110 - accuracy: 0.0587 - val_loss: 4.9463 - val_accuracy: 0.0526\n","Epoch 20/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.7044 - accuracy: 0.0617 - val_loss: 4.9390 - val_accuracy: 0.0535\n","Average Validation Accuracy: 0.055694758892059326\n","Average Validation Loss: 4.835721969604492\n","Average Test Accuracy: 0.05664479918777943\n","Final Test Accuracy for each fold: 0.05690277740359306\n","Number of input features: 3\n","Fold: 1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","1846/1846 [==============================] - 16s 6ms/step - loss: 5.1020 - accuracy: 0.0423 - val_loss: 5.0390 - val_accuracy: 0.0271\n","Epoch 2/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.9647 - accuracy: 0.0508 - val_loss: 4.9660 - val_accuracy: 0.0706\n","Epoch 3/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.7876 - accuracy: 0.0758 - val_loss: 4.8046 - val_accuracy: 0.0706\n","Epoch 4/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.6818 - accuracy: 0.0778 - val_loss: 4.7636 - val_accuracy: 0.0761\n","Epoch 5/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.6251 - accuracy: 0.0822 - val_loss: 4.7104 - val_accuracy: 0.0801\n","Epoch 6/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.5963 - accuracy: 0.0834 - val_loss: 4.6955 - val_accuracy: 0.0803\n","Epoch 7/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.5767 - accuracy: 0.0832 - val_loss: 4.6837 - val_accuracy: 0.0774\n","Epoch 8/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.5637 - accuracy: 0.0845 - val_loss: 4.6758 - val_accuracy: 0.0772\n","Epoch 9/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 4.5488 - accuracy: 0.0851 - val_loss: 4.6714 - val_accuracy: 0.0779\n","Epoch 10/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.5324 - accuracy: 0.0849 - val_loss: 4.6732 - val_accuracy: 0.0788\n","Epoch 11/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.5165 - accuracy: 0.0837 - val_loss: 4.6598 - val_accuracy: 0.0750\n","Epoch 12/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.5018 - accuracy: 0.0855 - val_loss: 4.6516 - val_accuracy: 0.0752\n","Epoch 13/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.4885 - accuracy: 0.0847 - val_loss: 4.6505 - val_accuracy: 0.0774\n","Epoch 14/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.4785 - accuracy: 0.0880 - val_loss: 4.6489 - val_accuracy: 0.0722\n","Epoch 15/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.4675 - accuracy: 0.0877 - val_loss: 4.6611 - val_accuracy: 0.0796\n","Epoch 16/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.4588 - accuracy: 0.0901 - val_loss: 4.6697 - val_accuracy: 0.0818\n","Epoch 17/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.4526 - accuracy: 0.0874 - val_loss: 4.6688 - val_accuracy: 0.0834\n","Epoch 18/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.4449 - accuracy: 0.0892 - val_loss: 4.6718 - val_accuracy: 0.0840\n","Epoch 19/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.4416 - accuracy: 0.0908 - val_loss: 4.6739 - val_accuracy: 0.0766\n","Epoch 20/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.4349 - accuracy: 0.0905 - val_loss: 4.6750 - val_accuracy: 0.0821\n","Fold: 2\n","Epoch 1/20\n","1846/1846 [==============================] - 13s 5ms/step - loss: 5.0912 - accuracy: 0.0418 - val_loss: 5.0394 - val_accuracy: 0.0440\n","Epoch 2/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.8871 - accuracy: 0.0685 - val_loss: 4.8668 - val_accuracy: 0.0759\n","Epoch 3/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.7372 - accuracy: 0.0742 - val_loss: 4.8147 - val_accuracy: 0.0829\n","Epoch 4/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.6610 - accuracy: 0.0795 - val_loss: 4.7366 - val_accuracy: 0.0816\n","Epoch 5/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.6133 - accuracy: 0.0825 - val_loss: 4.7111 - val_accuracy: 0.0854\n","Epoch 6/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.5868 - accuracy: 0.0830 - val_loss: 4.7045 - val_accuracy: 0.0799\n","Epoch 7/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.5680 - accuracy: 0.0812 - val_loss: 4.6805 - val_accuracy: 0.0865\n","Epoch 8/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.5497 - accuracy: 0.0835 - val_loss: 4.6783 - val_accuracy: 0.0854\n","Epoch 9/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.5357 - accuracy: 0.0836 - val_loss: 4.6627 - val_accuracy: 0.0854\n","Epoch 10/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.5213 - accuracy: 0.0845 - val_loss: 4.6630 - val_accuracy: 0.0849\n","Epoch 11/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.5082 - accuracy: 0.0835 - val_loss: 4.6947 - val_accuracy: 0.0823\n","Epoch 12/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.4957 - accuracy: 0.0859 - val_loss: 4.6527 - val_accuracy: 0.0862\n","Epoch 13/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.4814 - accuracy: 0.0839 - val_loss: 4.6550 - val_accuracy: 0.0867\n","Epoch 14/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.4714 - accuracy: 0.0876 - val_loss: 4.6480 - val_accuracy: 0.0898\n","Epoch 15/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.4621 - accuracy: 0.0869 - val_loss: 4.6642 - val_accuracy: 0.0904\n","Epoch 16/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.4549 - accuracy: 0.0882 - val_loss: 4.6579 - val_accuracy: 0.0895\n","Epoch 17/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.4469 - accuracy: 0.0875 - val_loss: 4.6740 - val_accuracy: 0.0827\n","Epoch 18/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.4437 - accuracy: 0.0887 - val_loss: 4.6663 - val_accuracy: 0.0977\n","Epoch 19/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.4372 - accuracy: 0.0885 - val_loss: 4.6598 - val_accuracy: 0.0889\n","Epoch 20/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.4310 - accuracy: 0.0864 - val_loss: 4.6840 - val_accuracy: 0.0924\n","Average Validation Accuracy: 0.09087605029344559\n","Average Validation Loss: 4.580660820007324\n","Average Test Accuracy: 0.0881919376552105\n","Final Test Accuracy for each fold: 0.0893344134092331\n","Number of input features: 4\n","Fold: 1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","1846/1846 [==============================] - 16s 7ms/step - loss: 5.0321 - accuracy: 0.0466 - val_loss: 4.8734 - val_accuracy: 0.0750\n","Epoch 2/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.4392 - accuracy: 0.0996 - val_loss: 4.3012 - val_accuracy: 0.1219\n","Epoch 3/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.0640 - accuracy: 0.1344 - val_loss: 4.0602 - val_accuracy: 0.1285\n","Epoch 4/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 3.8019 - accuracy: 0.1638 - val_loss: 3.8158 - val_accuracy: 0.1677\n","Epoch 5/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.5239 - accuracy: 0.2075 - val_loss: 3.6288 - val_accuracy: 0.2301\n","Epoch 6/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 3.2910 - accuracy: 0.2496 - val_loss: 3.4422 - val_accuracy: 0.2341\n","Epoch 7/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.0908 - accuracy: 0.2643 - val_loss: 3.2899 - val_accuracy: 0.2847\n","Epoch 8/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.9218 - accuracy: 0.2929 - val_loss: 3.2641 - val_accuracy: 0.2988\n","Epoch 9/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.7933 - accuracy: 0.3152 - val_loss: 3.1685 - val_accuracy: 0.3153\n","Epoch 10/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.7006 - accuracy: 0.3235 - val_loss: 3.0573 - val_accuracy: 0.2931\n","Epoch 11/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.6371 - accuracy: 0.3418 - val_loss: 3.0517 - val_accuracy: 0.2865\n","Epoch 12/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.5947 - accuracy: 0.3459 - val_loss: 2.9197 - val_accuracy: 0.3602\n","Epoch 13/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.5472 - accuracy: 0.3504 - val_loss: 2.9151 - val_accuracy: 0.3514\n","Epoch 14/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.5167 - accuracy: 0.3506 - val_loss: 2.8748 - val_accuracy: 0.3329\n","Epoch 15/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.4746 - accuracy: 0.3562 - val_loss: 2.8720 - val_accuracy: 0.3606\n","Epoch 16/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.4412 - accuracy: 0.3646 - val_loss: 2.7861 - val_accuracy: 0.3789\n","Epoch 17/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.4120 - accuracy: 0.3634 - val_loss: 2.7677 - val_accuracy: 0.3520\n","Epoch 18/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.3822 - accuracy: 0.3697 - val_loss: 2.8354 - val_accuracy: 0.3358\n","Epoch 19/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.3553 - accuracy: 0.3789 - val_loss: 2.7174 - val_accuracy: 0.3364\n","Epoch 20/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.3452 - accuracy: 0.3767 - val_loss: 2.7580 - val_accuracy: 0.2937\n","Fold: 2\n","Epoch 1/20\n","1846/1846 [==============================] - 13s 6ms/step - loss: 5.0593 - accuracy: 0.0400 - val_loss: 4.9838 - val_accuracy: 0.0440\n","Epoch 2/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.6545 - accuracy: 0.0755 - val_loss: 4.3948 - val_accuracy: 0.1151\n","Epoch 3/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.1020 - accuracy: 0.1238 - val_loss: 4.1260 - val_accuracy: 0.1278\n","Epoch 4/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.8561 - accuracy: 0.1430 - val_loss: 3.9404 - val_accuracy: 0.1947\n","Epoch 5/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 3.6185 - accuracy: 0.1951 - val_loss: 3.7865 - val_accuracy: 0.2169\n","Epoch 6/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.3502 - accuracy: 0.2312 - val_loss: 3.5744 - val_accuracy: 0.2581\n","Epoch 7/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.1202 - accuracy: 0.2575 - val_loss: 3.3836 - val_accuracy: 0.2660\n","Epoch 8/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.9594 - accuracy: 0.2731 - val_loss: 3.2808 - val_accuracy: 0.3017\n","Epoch 9/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.8378 - accuracy: 0.2896 - val_loss: 3.2207 - val_accuracy: 0.2990\n","Epoch 10/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.7549 - accuracy: 0.3036 - val_loss: 3.0784 - val_accuracy: 0.3078\n","Epoch 11/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.6791 - accuracy: 0.3261 - val_loss: 3.0401 - val_accuracy: 0.3296\n","Epoch 12/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.6130 - accuracy: 0.3362 - val_loss: 3.0184 - val_accuracy: 0.3305\n","Epoch 13/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.5597 - accuracy: 0.3504 - val_loss: 2.9261 - val_accuracy: 0.3338\n","Epoch 14/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.5136 - accuracy: 0.3587 - val_loss: 2.9304 - val_accuracy: 0.3646\n","Epoch 15/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.4638 - accuracy: 0.3698 - val_loss: 2.9616 - val_accuracy: 0.3342\n","Epoch 16/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.4254 - accuracy: 0.3739 - val_loss: 2.8468 - val_accuracy: 0.3441\n","Epoch 17/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.3748 - accuracy: 0.3833 - val_loss: 2.7792 - val_accuracy: 0.3690\n","Epoch 18/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.3433 - accuracy: 0.3970 - val_loss: 2.7117 - val_accuracy: 0.4057\n","Epoch 19/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.3094 - accuracy: 0.3963 - val_loss: 2.7282 - val_accuracy: 0.3747\n","Epoch 20/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.2790 - accuracy: 0.3981 - val_loss: 2.6409 - val_accuracy: 0.3776\n","Average Validation Accuracy: 0.34629619121551514\n","Average Validation Loss: 2.424707055091858\n","Average Test Accuracy: 0.3486400842666626\n","Final Test Accuracy for each fold: 0.3825458884239197\n","Number of input features: 5\n","Fold: 1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","1846/1846 [==============================] - 13s 5ms/step - loss: 5.0647 - accuracy: 0.0429 - val_loss: 4.9344 - val_accuracy: 0.0623\n","Epoch 2/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.4520 - accuracy: 0.0976 - val_loss: 4.2442 - val_accuracy: 0.1239\n","Epoch 3/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 3.9775 - accuracy: 0.1456 - val_loss: 3.9460 - val_accuracy: 0.1859\n","Epoch 4/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.6438 - accuracy: 0.1889 - val_loss: 3.6517 - val_accuracy: 0.2020\n","Epoch 5/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.3092 - accuracy: 0.2484 - val_loss: 3.3574 - val_accuracy: 0.2768\n","Epoch 6/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.9675 - accuracy: 0.3100 - val_loss: 3.0273 - val_accuracy: 0.3575\n","Epoch 7/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.6485 - accuracy: 0.3672 - val_loss: 2.7817 - val_accuracy: 0.3793\n","Epoch 8/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.3949 - accuracy: 0.4107 - val_loss: 2.5973 - val_accuracy: 0.4328\n","Epoch 9/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.1921 - accuracy: 0.4411 - val_loss: 2.4526 - val_accuracy: 0.4293\n","Epoch 10/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.0487 - accuracy: 0.4627 - val_loss: 2.3542 - val_accuracy: 0.4191\n","Epoch 11/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.9121 - accuracy: 0.4940 - val_loss: 2.2371 - val_accuracy: 0.4816\n","Epoch 12/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.8249 - accuracy: 0.5060 - val_loss: 2.2036 - val_accuracy: 0.4774\n","Epoch 13/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.7402 - accuracy: 0.5175 - val_loss: 2.0772 - val_accuracy: 0.5045\n","Epoch 14/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.6728 - accuracy: 0.5299 - val_loss: 2.1592 - val_accuracy: 0.5034\n","Epoch 15/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.6123 - accuracy: 0.5364 - val_loss: 2.0576 - val_accuracy: 0.5102\n","Epoch 16/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.5608 - accuracy: 0.5502 - val_loss: 1.9676 - val_accuracy: 0.5116\n","Epoch 17/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.5199 - accuracy: 0.5561 - val_loss: 2.0426 - val_accuracy: 0.5039\n","Epoch 18/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.4707 - accuracy: 0.5653 - val_loss: 1.8632 - val_accuracy: 0.5736\n","Epoch 19/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.4429 - accuracy: 0.5739 - val_loss: 1.9210 - val_accuracy: 0.5168\n","Epoch 20/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.4021 - accuracy: 0.5785 - val_loss: 1.7937 - val_accuracy: 0.5914\n","Fold: 2\n","Epoch 1/20\n","1846/1846 [==============================] - 13s 6ms/step - loss: 5.0498 - accuracy: 0.0458 - val_loss: 4.9114 - val_accuracy: 0.0768\n","Epoch 2/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.3607 - accuracy: 0.0940 - val_loss: 4.2274 - val_accuracy: 0.1465\n","Epoch 3/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 3.9825 - accuracy: 0.1393 - val_loss: 4.0306 - val_accuracy: 0.1538\n","Epoch 4/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.7400 - accuracy: 0.1787 - val_loss: 3.8389 - val_accuracy: 0.2198\n","Epoch 5/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 3.4884 - accuracy: 0.2194 - val_loss: 3.6782 - val_accuracy: 0.2664\n","Epoch 6/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 3.2614 - accuracy: 0.2550 - val_loss: 3.5303 - val_accuracy: 0.3175\n","Epoch 7/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.0581 - accuracy: 0.2957 - val_loss: 3.4220 - val_accuracy: 0.3151\n","Epoch 8/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.8642 - accuracy: 0.3407 - val_loss: 3.2618 - val_accuracy: 0.3318\n","Epoch 9/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.6338 - accuracy: 0.3783 - val_loss: 3.1075 - val_accuracy: 0.3791\n","Epoch 10/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.4231 - accuracy: 0.4090 - val_loss: 2.9961 - val_accuracy: 0.3923\n","Epoch 11/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.2950 - accuracy: 0.4292 - val_loss: 2.9424 - val_accuracy: 0.3839\n","Epoch 12/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.1969 - accuracy: 0.4433 - val_loss: 2.9149 - val_accuracy: 0.4601\n","Epoch 13/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.1227 - accuracy: 0.4494 - val_loss: 2.7617 - val_accuracy: 0.4262\n","Epoch 14/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.0643 - accuracy: 0.4666 - val_loss: 2.7866 - val_accuracy: 0.4330\n","Epoch 15/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.0214 - accuracy: 0.4660 - val_loss: 2.6834 - val_accuracy: 0.4572\n","Epoch 16/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.9701 - accuracy: 0.4820 - val_loss: 2.6607 - val_accuracy: 0.3894\n","Epoch 17/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.9322 - accuracy: 0.4906 - val_loss: 2.5683 - val_accuracy: 0.4658\n","Epoch 18/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.8913 - accuracy: 0.4887 - val_loss: 2.4998 - val_accuracy: 0.4794\n","Epoch 19/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.8574 - accuracy: 0.4936 - val_loss: 2.5311 - val_accuracy: 0.4541\n","Epoch 20/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.8155 - accuracy: 0.4996 - val_loss: 2.4417 - val_accuracy: 0.5369\n","Average Validation Accuracy: 0.5771327316761017\n","Average Validation Loss: 1.743194043636322\n","Average Test Accuracy: 0.5797154903411865\n","Final Test Accuracy for each fold: 0.614063560962677\n","Number of input features: 6\n","Fold: 1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","1846/1846 [==============================] - 14s 6ms/step - loss: 5.0144 - accuracy: 0.0419 - val_loss: 4.8011 - val_accuracy: 0.0900\n","Epoch 2/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.2542 - accuracy: 0.1567 - val_loss: 3.9090 - val_accuracy: 0.2088\n","Epoch 3/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.4189 - accuracy: 0.2884 - val_loss: 3.2446 - val_accuracy: 0.3219\n","Epoch 4/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.8009 - accuracy: 0.3838 - val_loss: 2.7559 - val_accuracy: 0.4497\n","Epoch 5/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.3020 - accuracy: 0.4776 - val_loss: 2.3821 - val_accuracy: 0.5241\n","Epoch 6/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.9219 - accuracy: 0.5537 - val_loss: 2.0713 - val_accuracy: 0.5828\n","Epoch 7/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.6450 - accuracy: 0.5982 - val_loss: 1.8944 - val_accuracy: 0.6158\n","Epoch 8/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.4514 - accuracy: 0.6343 - val_loss: 1.8814 - val_accuracy: 0.6238\n","Epoch 9/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.3255 - accuracy: 0.6584 - val_loss: 1.6855 - val_accuracy: 0.6488\n","Epoch 10/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.2301 - accuracy: 0.6746 - val_loss: 1.5683 - val_accuracy: 0.7102\n","Epoch 11/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.1548 - accuracy: 0.6895 - val_loss: 1.5684 - val_accuracy: 0.6860\n","Epoch 12/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.1039 - accuracy: 0.7012 - val_loss: 1.5302 - val_accuracy: 0.6946\n","Epoch 13/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.0523 - accuracy: 0.7109 - val_loss: 1.4605 - val_accuracy: 0.6981\n","Epoch 14/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.0218 - accuracy: 0.7143 - val_loss: 1.4168 - val_accuracy: 0.7230\n","Epoch 15/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.9850 - accuracy: 0.7239 - val_loss: 1.3089 - val_accuracy: 0.7439\n","Epoch 16/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.9486 - accuracy: 0.7327 - val_loss: 1.4097 - val_accuracy: 0.7034\n","Epoch 17/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 0.9234 - accuracy: 0.7398 - val_loss: 1.3503 - val_accuracy: 0.7382\n","Epoch 18/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.8982 - accuracy: 0.7440 - val_loss: 1.3081 - val_accuracy: 0.7274\n","Epoch 19/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 0.8856 - accuracy: 0.7412 - val_loss: 1.2745 - val_accuracy: 0.7490\n","Epoch 20/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.8642 - accuracy: 0.7522 - val_loss: 1.2308 - val_accuracy: 0.7494\n","Fold: 2\n","Epoch 1/20\n","1846/1846 [==============================] - 14s 6ms/step - loss: 5.0119 - accuracy: 0.0414 - val_loss: 4.7362 - val_accuracy: 0.0667\n","Epoch 2/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.1387 - accuracy: 0.1565 - val_loss: 3.8985 - val_accuracy: 0.2015\n","Epoch 3/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 3.4734 - accuracy: 0.2757 - val_loss: 3.4845 - val_accuracy: 0.3241\n","Epoch 4/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.9788 - accuracy: 0.3738 - val_loss: 3.0681 - val_accuracy: 0.4099\n","Epoch 5/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.5562 - accuracy: 0.4530 - val_loss: 2.7815 - val_accuracy: 0.4939\n","Epoch 6/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.2070 - accuracy: 0.5127 - val_loss: 2.6096 - val_accuracy: 0.5061\n","Epoch 7/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.9458 - accuracy: 0.5528 - val_loss: 2.4363 - val_accuracy: 0.5815\n","Epoch 8/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.7464 - accuracy: 0.5892 - val_loss: 2.4820 - val_accuracy: 0.5371\n","Epoch 9/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.6057 - accuracy: 0.6193 - val_loss: 2.2318 - val_accuracy: 0.6354\n","Epoch 10/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.4900 - accuracy: 0.6414 - val_loss: 2.1330 - val_accuracy: 0.6675\n","Epoch 11/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.3959 - accuracy: 0.6607 - val_loss: 2.1891 - val_accuracy: 0.6354\n","Epoch 12/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.3116 - accuracy: 0.6756 - val_loss: 2.2105 - val_accuracy: 0.6251\n","Epoch 13/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.2560 - accuracy: 0.6872 - val_loss: 1.9406 - val_accuracy: 0.7135\n","Epoch 14/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.1926 - accuracy: 0.6984 - val_loss: 1.9424 - val_accuracy: 0.6803\n","Epoch 15/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.1419 - accuracy: 0.7082 - val_loss: 1.8655 - val_accuracy: 0.7329\n","Epoch 16/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.0971 - accuracy: 0.7167 - val_loss: 1.8512 - val_accuracy: 0.7036\n","Epoch 17/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.0540 - accuracy: 0.7229 - val_loss: 1.7826 - val_accuracy: 0.7173\n","Epoch 18/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.0151 - accuracy: 0.7388 - val_loss: 1.8095 - val_accuracy: 0.7217\n","Epoch 19/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 0.9928 - accuracy: 0.7385 - val_loss: 1.7314 - val_accuracy: 0.7415\n","Epoch 20/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.9666 - accuracy: 0.7493 - val_loss: 1.7057 - val_accuracy: 0.7250\n","Average Validation Accuracy: 0.7493734359741211\n","Average Validation Loss: 1.0784829556941986\n","Average Test Accuracy: 0.7419473826885223\n","Final Test Accuracy for each fold: 0.7464435696601868\n","Number of input features: 7\n","Fold: 1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","1846/1846 [==============================] - 16s 7ms/step - loss: 5.0095 - accuracy: 0.0470 - val_loss: 4.7724 - val_accuracy: 0.0748\n","Epoch 2/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.3997 - accuracy: 0.1123 - val_loss: 4.2602 - val_accuracy: 0.1573\n","Epoch 3/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 3.7805 - accuracy: 0.2094 - val_loss: 3.6584 - val_accuracy: 0.2436\n","Epoch 4/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 3.1923 - accuracy: 0.3000 - val_loss: 3.1041 - val_accuracy: 0.3534\n","Epoch 5/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.6926 - accuracy: 0.3777 - val_loss: 2.7429 - val_accuracy: 0.4084\n","Epoch 6/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.3133 - accuracy: 0.4487 - val_loss: 2.4972 - val_accuracy: 0.4722\n","Epoch 7/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.0473 - accuracy: 0.4935 - val_loss: 2.3418 - val_accuracy: 0.5089\n","Epoch 8/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.8651 - accuracy: 0.5295 - val_loss: 2.2787 - val_accuracy: 0.5089\n","Epoch 9/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.7243 - accuracy: 0.5555 - val_loss: 2.1753 - val_accuracy: 0.5241\n","Epoch 10/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.6275 - accuracy: 0.5669 - val_loss: 2.0709 - val_accuracy: 0.5688\n","Epoch 11/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.5600 - accuracy: 0.5793 - val_loss: 1.9406 - val_accuracy: 0.6020\n","Epoch 12/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.4867 - accuracy: 0.5971 - val_loss: 1.8931 - val_accuracy: 0.6031\n","Epoch 13/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.4259 - accuracy: 0.6044 - val_loss: 1.8661 - val_accuracy: 0.6114\n","Epoch 14/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.3865 - accuracy: 0.6189 - val_loss: 1.8100 - val_accuracy: 0.6246\n","Epoch 15/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.3486 - accuracy: 0.6252 - val_loss: 1.7193 - val_accuracy: 0.6464\n","Epoch 16/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.2972 - accuracy: 0.6421 - val_loss: 1.7922 - val_accuracy: 0.6172\n","Epoch 17/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.2688 - accuracy: 0.6393 - val_loss: 1.6820 - val_accuracy: 0.6510\n","Epoch 18/20\n","1846/1846 [==============================] - 13s 7ms/step - loss: 1.2415 - accuracy: 0.6536 - val_loss: 1.6183 - val_accuracy: 0.6832\n","Epoch 19/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.2169 - accuracy: 0.6566 - val_loss: 1.6365 - val_accuracy: 0.6275\n","Epoch 20/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.1848 - accuracy: 0.6613 - val_loss: 1.5708 - val_accuracy: 0.6783\n","Fold: 2\n","Epoch 1/20\n","1846/1846 [==============================] - 13s 6ms/step - loss: 5.0217 - accuracy: 0.0429 - val_loss: 4.7838 - val_accuracy: 0.0528\n","Epoch 2/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.3290 - accuracy: 0.1399 - val_loss: 4.0792 - val_accuracy: 0.1947\n","Epoch 3/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 3.4949 - accuracy: 0.2649 - val_loss: 3.3534 - val_accuracy: 0.3267\n","Epoch 4/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.8228 - accuracy: 0.3727 - val_loss: 2.8810 - val_accuracy: 0.4062\n","Epoch 5/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.3115 - accuracy: 0.4608 - val_loss: 2.5005 - val_accuracy: 0.5107\n","Epoch 6/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.9328 - accuracy: 0.5242 - val_loss: 2.3074 - val_accuracy: 0.5650\n","Epoch 7/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.6673 - accuracy: 0.5807 - val_loss: 2.1380 - val_accuracy: 0.5901\n","Epoch 8/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.5011 - accuracy: 0.6132 - val_loss: 2.0160 - val_accuracy: 0.6242\n","Epoch 9/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.3785 - accuracy: 0.6385 - val_loss: 1.8590 - val_accuracy: 0.6781\n","Epoch 10/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.2843 - accuracy: 0.6595 - val_loss: 1.7955 - val_accuracy: 0.6524\n","Epoch 11/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.2146 - accuracy: 0.6703 - val_loss: 1.7051 - val_accuracy: 0.7080\n","Epoch 12/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.1545 - accuracy: 0.6870 - val_loss: 1.6200 - val_accuracy: 0.6966\n","Epoch 13/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.1099 - accuracy: 0.6924 - val_loss: 1.6291 - val_accuracy: 0.6997\n","Epoch 14/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.0679 - accuracy: 0.7029 - val_loss: 1.6070 - val_accuracy: 0.6851\n","Epoch 15/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.0366 - accuracy: 0.7063 - val_loss: 1.5357 - val_accuracy: 0.7083\n","Epoch 16/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.0050 - accuracy: 0.7170 - val_loss: 1.5868 - val_accuracy: 0.7019\n","Epoch 17/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.9881 - accuracy: 0.7213 - val_loss: 1.4595 - val_accuracy: 0.7281\n","Epoch 18/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.9659 - accuracy: 0.7284 - val_loss: 1.4354 - val_accuracy: 0.7234\n","Epoch 19/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.9356 - accuracy: 0.7341 - val_loss: 1.4837 - val_accuracy: 0.6964\n","Epoch 20/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.9205 - accuracy: 0.7371 - val_loss: 1.3724 - val_accuracy: 0.7540\n","Average Validation Accuracy: 0.7256302833557129\n","Average Validation Loss: 1.1488035321235657\n","Average Test Accuracy: 0.7164811789989471\n","Final Test Accuracy for each fold: 0.7492445111274719\n","Number of input features: 8\n","Fold: 1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","1846/1846 [==============================] - 14s 6ms/step - loss: 4.7623 - accuracy: 0.0483 - val_loss: 4.5076 - val_accuracy: 0.0704\n","Epoch 2/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.2958 - accuracy: 0.0762 - val_loss: 4.1861 - val_accuracy: 0.1025\n","Epoch 3/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 3.9091 - accuracy: 0.1434 - val_loss: 3.8364 - val_accuracy: 0.1569\n","Epoch 4/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.5028 - accuracy: 0.2039 - val_loss: 3.4378 - val_accuracy: 0.2323\n","Epoch 5/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.1575 - accuracy: 0.2579 - val_loss: 3.1398 - val_accuracy: 0.2480\n","Epoch 6/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.8656 - accuracy: 0.2990 - val_loss: 2.8914 - val_accuracy: 0.3006\n","Epoch 7/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.6035 - accuracy: 0.3383 - val_loss: 2.6573 - val_accuracy: 0.3712\n","Epoch 8/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.3506 - accuracy: 0.3865 - val_loss: 2.4483 - val_accuracy: 0.3806\n","Epoch 9/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.1307 - accuracy: 0.4347 - val_loss: 2.2484 - val_accuracy: 0.4352\n","Epoch 10/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.9383 - accuracy: 0.4719 - val_loss: 2.0732 - val_accuracy: 0.4933\n","Epoch 11/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.7895 - accuracy: 0.5042 - val_loss: 1.9448 - val_accuracy: 0.5171\n","Epoch 12/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.6656 - accuracy: 0.5332 - val_loss: 1.8664 - val_accuracy: 0.5505\n","Epoch 13/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.5534 - accuracy: 0.5641 - val_loss: 1.8362 - val_accuracy: 0.5659\n","Epoch 14/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.4703 - accuracy: 0.5837 - val_loss: 1.6924 - val_accuracy: 0.5773\n","Epoch 15/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.3889 - accuracy: 0.6032 - val_loss: 1.7027 - val_accuracy: 0.5765\n","Epoch 16/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.3119 - accuracy: 0.6194 - val_loss: 1.5891 - val_accuracy: 0.6359\n","Epoch 17/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.2540 - accuracy: 0.6383 - val_loss: 1.5017 - val_accuracy: 0.6312\n","Epoch 18/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.2024 - accuracy: 0.6437 - val_loss: 1.4583 - val_accuracy: 0.6370\n","Epoch 19/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.1465 - accuracy: 0.6600 - val_loss: 1.4468 - val_accuracy: 0.6440\n","Epoch 20/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.0965 - accuracy: 0.6742 - val_loss: 1.3936 - val_accuracy: 0.6790\n","Fold: 2\n","Epoch 1/20\n","1846/1846 [==============================] - 14s 6ms/step - loss: 4.8285 - accuracy: 0.0458 - val_loss: 4.5915 - val_accuracy: 0.0519\n","Epoch 2/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.4135 - accuracy: 0.0642 - val_loss: 4.4107 - val_accuracy: 0.0805\n","Epoch 3/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.1794 - accuracy: 0.0855 - val_loss: 4.0906 - val_accuracy: 0.1239\n","Epoch 4/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 3.7279 - accuracy: 0.1799 - val_loss: 3.5918 - val_accuracy: 0.2352\n","Epoch 5/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.2553 - accuracy: 0.2621 - val_loss: 3.2073 - val_accuracy: 0.3012\n","Epoch 6/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.8302 - accuracy: 0.3352 - val_loss: 2.8506 - val_accuracy: 0.3696\n","Epoch 7/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.4433 - accuracy: 0.4188 - val_loss: 2.5634 - val_accuracy: 0.4689\n","Epoch 8/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.1469 - accuracy: 0.4711 - val_loss: 2.2737 - val_accuracy: 0.5083\n","Epoch 9/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.9153 - accuracy: 0.5200 - val_loss: 2.1028 - val_accuracy: 0.5465\n","Epoch 10/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.7375 - accuracy: 0.5573 - val_loss: 1.9759 - val_accuracy: 0.5861\n","Epoch 11/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.5914 - accuracy: 0.5889 - val_loss: 1.8851 - val_accuracy: 0.5930\n","Epoch 12/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.4660 - accuracy: 0.6099 - val_loss: 1.7650 - val_accuracy: 0.6156\n","Epoch 13/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.3614 - accuracy: 0.6333 - val_loss: 1.7397 - val_accuracy: 0.6143\n","Epoch 14/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.2875 - accuracy: 0.6474 - val_loss: 1.7105 - val_accuracy: 0.6229\n","Epoch 15/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.2217 - accuracy: 0.6641 - val_loss: 1.5723 - val_accuracy: 0.6528\n","Epoch 16/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.1580 - accuracy: 0.6767 - val_loss: 1.5606 - val_accuracy: 0.6387\n","Epoch 17/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.1179 - accuracy: 0.6827 - val_loss: 1.5250 - val_accuracy: 0.6623\n","Epoch 18/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.0720 - accuracy: 0.6958 - val_loss: 1.4716 - val_accuracy: 0.6713\n","Epoch 19/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.0346 - accuracy: 0.7032 - val_loss: 1.4152 - val_accuracy: 0.6849\n","Epoch 20/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 0.9942 - accuracy: 0.7145 - val_loss: 1.3899 - val_accuracy: 0.7098\n","Average Validation Accuracy: 0.7036275863647461\n","Average Validation Loss: 1.1584835648536682\n","Average Test Accuracy: 0.7020712196826935\n","Final Test Accuracy for each fold: 0.719613790512085\n","Number of input features: 9\n","Fold: 1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","1846/1846 [==============================] - 14s 6ms/step - loss: 4.7479 - accuracy: 0.0515 - val_loss: 4.5310 - val_accuracy: 0.0854\n","Epoch 2/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.3130 - accuracy: 0.0784 - val_loss: 4.3081 - val_accuracy: 0.0953\n","Epoch 3/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.0587 - accuracy: 0.1134 - val_loss: 4.0645 - val_accuracy: 0.1393\n","Epoch 4/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.8197 - accuracy: 0.1474 - val_loss: 3.8201 - val_accuracy: 0.1468\n","Epoch 5/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 3.5764 - accuracy: 0.1688 - val_loss: 3.5913 - val_accuracy: 0.1688\n","Epoch 6/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 3.3296 - accuracy: 0.1952 - val_loss: 3.3540 - val_accuracy: 0.2064\n","Epoch 7/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.1207 - accuracy: 0.2184 - val_loss: 3.1786 - val_accuracy: 0.2343\n","Epoch 8/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.9356 - accuracy: 0.2543 - val_loss: 3.0506 - val_accuracy: 0.2625\n","Epoch 9/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.7693 - accuracy: 0.2905 - val_loss: 2.9527 - val_accuracy: 0.2530\n","Epoch 10/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.5956 - accuracy: 0.3307 - val_loss: 2.7203 - val_accuracy: 0.3501\n","Epoch 11/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.4181 - accuracy: 0.3684 - val_loss: 2.5713 - val_accuracy: 0.3602\n","Epoch 12/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.2321 - accuracy: 0.4053 - val_loss: 2.4036 - val_accuracy: 0.3916\n","Epoch 13/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.0439 - accuracy: 0.4508 - val_loss: 2.2237 - val_accuracy: 0.4497\n","Epoch 14/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.8736 - accuracy: 0.4866 - val_loss: 2.1071 - val_accuracy: 0.4832\n","Epoch 15/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.7430 - accuracy: 0.5176 - val_loss: 1.9797 - val_accuracy: 0.5320\n","Epoch 16/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.6181 - accuracy: 0.5521 - val_loss: 1.9058 - val_accuracy: 0.5670\n","Epoch 17/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.5241 - accuracy: 0.5776 - val_loss: 1.8750 - val_accuracy: 0.5659\n","Epoch 18/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.4381 - accuracy: 0.6080 - val_loss: 1.7868 - val_accuracy: 0.5809\n","Epoch 19/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.3742 - accuracy: 0.6204 - val_loss: 1.7025 - val_accuracy: 0.6044\n","Epoch 20/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.3121 - accuracy: 0.6377 - val_loss: 1.7202 - val_accuracy: 0.5985\n","Fold: 2\n","Epoch 1/20\n","1846/1846 [==============================] - 14s 6ms/step - loss: 4.8154 - accuracy: 0.0471 - val_loss: 4.6123 - val_accuracy: 0.0596\n","Epoch 2/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.3320 - accuracy: 0.0805 - val_loss: 4.2626 - val_accuracy: 0.1019\n","Epoch 3/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 3.9712 - accuracy: 0.1297 - val_loss: 3.8996 - val_accuracy: 0.1956\n","Epoch 4/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 3.4558 - accuracy: 0.2381 - val_loss: 3.3411 - val_accuracy: 0.2959\n","Epoch 5/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.9395 - accuracy: 0.3174 - val_loss: 2.9606 - val_accuracy: 0.3527\n","Epoch 6/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.5753 - accuracy: 0.3641 - val_loss: 2.6561 - val_accuracy: 0.3899\n","Epoch 7/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.3004 - accuracy: 0.4258 - val_loss: 2.4382 - val_accuracy: 0.4647\n","Epoch 8/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.0987 - accuracy: 0.4711 - val_loss: 2.2449 - val_accuracy: 0.4968\n","Epoch 9/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.9317 - accuracy: 0.5060 - val_loss: 2.1576 - val_accuracy: 0.5283\n","Epoch 10/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.7760 - accuracy: 0.5406 - val_loss: 2.0449 - val_accuracy: 0.5327\n","Epoch 11/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.6589 - accuracy: 0.5638 - val_loss: 1.8944 - val_accuracy: 0.5547\n","Epoch 12/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.5510 - accuracy: 0.5840 - val_loss: 1.8469 - val_accuracy: 0.5767\n","Epoch 13/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.4658 - accuracy: 0.6045 - val_loss: 1.7685 - val_accuracy: 0.6154\n","Epoch 14/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.3807 - accuracy: 0.6278 - val_loss: 1.6857 - val_accuracy: 0.6229\n","Epoch 15/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.3151 - accuracy: 0.6380 - val_loss: 1.6046 - val_accuracy: 0.6376\n","Epoch 16/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.2532 - accuracy: 0.6592 - val_loss: 1.6197 - val_accuracy: 0.6466\n","Epoch 17/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.2035 - accuracy: 0.6655 - val_loss: 1.5169 - val_accuracy: 0.6616\n","Epoch 18/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.1655 - accuracy: 0.6784 - val_loss: 1.4418 - val_accuracy: 0.6851\n","Epoch 19/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.1143 - accuracy: 0.6924 - val_loss: 1.4013 - val_accuracy: 0.6706\n","Epoch 20/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.0818 - accuracy: 0.6996 - val_loss: 1.5014 - val_accuracy: 0.6304\n","Average Validation Accuracy: 0.624914288520813\n","Average Validation Loss: 1.384727954864502\n","Average Test Accuracy: 0.6246775090694427\n","Final Test Accuracy for each fold: 0.6441364884376526\n","Number of input features: 10\n","Fold: 1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","1846/1846 [==============================] - 18s 6ms/step - loss: 4.8390 - accuracy: 0.0449 - val_loss: 4.6346 - val_accuracy: 0.0491\n","Epoch 2/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.4208 - accuracy: 0.0672 - val_loss: 4.3230 - val_accuracy: 0.0937\n","Epoch 3/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.0917 - accuracy: 0.1021 - val_loss: 4.0314 - val_accuracy: 0.1014\n","Epoch 4/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 3.7702 - accuracy: 0.1388 - val_loss: 3.7040 - val_accuracy: 0.1518\n","Epoch 5/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 3.3704 - accuracy: 0.2080 - val_loss: 3.2964 - val_accuracy: 0.2480\n","Epoch 6/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.9004 - accuracy: 0.3008 - val_loss: 2.8784 - val_accuracy: 0.3353\n","Epoch 7/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.5142 - accuracy: 0.3796 - val_loss: 2.5562 - val_accuracy: 0.4044\n","Epoch 8/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.1892 - accuracy: 0.4575 - val_loss: 2.2601 - val_accuracy: 0.4728\n","Epoch 9/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.9272 - accuracy: 0.4971 - val_loss: 2.0395 - val_accuracy: 0.5133\n","Epoch 10/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.7029 - accuracy: 0.5526 - val_loss: 1.8765 - val_accuracy: 0.5635\n","Epoch 11/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.5214 - accuracy: 0.5954 - val_loss: 1.7990 - val_accuracy: 0.5800\n","Epoch 12/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.3820 - accuracy: 0.6271 - val_loss: 1.6528 - val_accuracy: 0.6396\n","Epoch 13/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.2707 - accuracy: 0.6508 - val_loss: 1.5662 - val_accuracy: 0.6449\n","Epoch 14/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.1781 - accuracy: 0.6798 - val_loss: 1.4747 - val_accuracy: 0.6803\n","Epoch 15/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.1199 - accuracy: 0.6907 - val_loss: 1.4602 - val_accuracy: 0.6854\n","Epoch 16/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.0564 - accuracy: 0.7063 - val_loss: 1.4017 - val_accuracy: 0.7058\n","Epoch 17/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.0152 - accuracy: 0.7158 - val_loss: 1.4240 - val_accuracy: 0.6781\n","Epoch 18/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.9718 - accuracy: 0.7320 - val_loss: 1.3744 - val_accuracy: 0.7179\n","Epoch 19/20\n","1846/1846 [==============================] - 13s 7ms/step - loss: 0.9453 - accuracy: 0.7304 - val_loss: 1.3746 - val_accuracy: 0.7105\n","Epoch 20/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.9074 - accuracy: 0.7417 - val_loss: 1.3133 - val_accuracy: 0.7311\n","Fold: 2\n","Epoch 1/20\n","1846/1846 [==============================] - 14s 6ms/step - loss: 4.7700 - accuracy: 0.0464 - val_loss: 4.5448 - val_accuracy: 0.0552\n","Epoch 2/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.2833 - accuracy: 0.0789 - val_loss: 4.2366 - val_accuracy: 0.1058\n","Epoch 3/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.9798 - accuracy: 0.1125 - val_loss: 3.9287 - val_accuracy: 0.1424\n","Epoch 4/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 3.6711 - accuracy: 0.1527 - val_loss: 3.6279 - val_accuracy: 0.1996\n","Epoch 5/20\n","1846/1846 [==============================] - 13s 7ms/step - loss: 3.3385 - accuracy: 0.2142 - val_loss: 3.3453 - val_accuracy: 0.2312\n","Epoch 6/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 3.0261 - accuracy: 0.2585 - val_loss: 3.0654 - val_accuracy: 0.2865\n","Epoch 7/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.7268 - accuracy: 0.3093 - val_loss: 2.7899 - val_accuracy: 0.3325\n","Epoch 8/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.4491 - accuracy: 0.3544 - val_loss: 2.5869 - val_accuracy: 0.3751\n","Epoch 9/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.2103 - accuracy: 0.4054 - val_loss: 2.3699 - val_accuracy: 0.4350\n","Epoch 10/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.0393 - accuracy: 0.4456 - val_loss: 2.3132 - val_accuracy: 0.4257\n","Epoch 11/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.8758 - accuracy: 0.4843 - val_loss: 2.1287 - val_accuracy: 0.4711\n","Epoch 12/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.7493 - accuracy: 0.5152 - val_loss: 2.0461 - val_accuracy: 0.5023\n","Epoch 13/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.6378 - accuracy: 0.5408 - val_loss: 1.9184 - val_accuracy: 0.5274\n","Epoch 14/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.5360 - accuracy: 0.5714 - val_loss: 1.8528 - val_accuracy: 0.5490\n","Epoch 15/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.4476 - accuracy: 0.5937 - val_loss: 1.7660 - val_accuracy: 0.5890\n","Epoch 16/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.3592 - accuracy: 0.6173 - val_loss: 1.7138 - val_accuracy: 0.6029\n","Epoch 17/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.2897 - accuracy: 0.6373 - val_loss: 1.6168 - val_accuracy: 0.6264\n","Epoch 18/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.2322 - accuracy: 0.6567 - val_loss: 1.5946 - val_accuracy: 0.6495\n","Epoch 19/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.1711 - accuracy: 0.6703 - val_loss: 1.5085 - val_accuracy: 0.6609\n","Epoch 20/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.1182 - accuracy: 0.6871 - val_loss: 1.5852 - val_accuracy: 0.6493\n","Average Validation Accuracy: 0.7098703384399414\n","Average Validation Loss: 1.1684093475341797\n","Average Test Accuracy: 0.7075993120670319\n","Final Test Accuracy for each fold: 0.7456327676773071\n","Number of input features: 11\n","Fold: 1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","1846/1846 [==============================] - 14s 6ms/step - loss: 4.7750 - accuracy: 0.0570 - val_loss: 4.4727 - val_accuracy: 0.0994\n","Epoch 2/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.1592 - accuracy: 0.1096 - val_loss: 4.0310 - val_accuracy: 0.1179\n","Epoch 3/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 3.7620 - accuracy: 0.1424 - val_loss: 3.7125 - val_accuracy: 0.1439\n","Epoch 4/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.4681 - accuracy: 0.1780 - val_loss: 3.4793 - val_accuracy: 0.1703\n","Epoch 5/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.2359 - accuracy: 0.2154 - val_loss: 3.2776 - val_accuracy: 0.2473\n","Epoch 6/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 3.0340 - accuracy: 0.2522 - val_loss: 3.1139 - val_accuracy: 0.2442\n","Epoch 7/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.8159 - accuracy: 0.2821 - val_loss: 2.8746 - val_accuracy: 0.2882\n","Epoch 8/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.5392 - accuracy: 0.3419 - val_loss: 2.6245 - val_accuracy: 0.3661\n","Epoch 9/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.2767 - accuracy: 0.4026 - val_loss: 2.3862 - val_accuracy: 0.4128\n","Epoch 10/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.0374 - accuracy: 0.4546 - val_loss: 2.1754 - val_accuracy: 0.4711\n","Epoch 11/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.8278 - accuracy: 0.5046 - val_loss: 2.0123 - val_accuracy: 0.5039\n","Epoch 12/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.6530 - accuracy: 0.5521 - val_loss: 1.8987 - val_accuracy: 0.5443\n","Epoch 13/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.5059 - accuracy: 0.5856 - val_loss: 1.7335 - val_accuracy: 0.5941\n","Epoch 14/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.3816 - accuracy: 0.6163 - val_loss: 1.6800 - val_accuracy: 0.6174\n","Epoch 15/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.2806 - accuracy: 0.6416 - val_loss: 1.5839 - val_accuracy: 0.6374\n","Epoch 16/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.1946 - accuracy: 0.6594 - val_loss: 1.5548 - val_accuracy: 0.6543\n","Epoch 17/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.1287 - accuracy: 0.6795 - val_loss: 1.5121 - val_accuracy: 0.6425\n","Epoch 18/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.0680 - accuracy: 0.6937 - val_loss: 1.4604 - val_accuracy: 0.6766\n","Epoch 19/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.0146 - accuracy: 0.7140 - val_loss: 1.3990 - val_accuracy: 0.7109\n","Epoch 20/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.9697 - accuracy: 0.7265 - val_loss: 1.3735 - val_accuracy: 0.7166\n","Fold: 2\n","Epoch 1/20\n","1846/1846 [==============================] - 14s 6ms/step - loss: 4.8368 - accuracy: 0.0471 - val_loss: 4.6480 - val_accuracy: 0.0821\n","Epoch 2/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.3548 - accuracy: 0.0853 - val_loss: 4.2757 - val_accuracy: 0.1045\n","Epoch 3/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.0224 - accuracy: 0.1097 - val_loss: 3.9844 - val_accuracy: 0.1417\n","Epoch 4/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.7300 - accuracy: 0.1413 - val_loss: 3.7130 - val_accuracy: 0.1864\n","Epoch 5/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.4207 - accuracy: 0.1894 - val_loss: 3.4278 - val_accuracy: 0.2429\n","Epoch 6/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 3.0963 - accuracy: 0.2521 - val_loss: 3.1300 - val_accuracy: 0.2741\n","Epoch 7/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.7499 - accuracy: 0.3225 - val_loss: 2.8037 - val_accuracy: 0.3435\n","Epoch 8/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.4169 - accuracy: 0.3997 - val_loss: 2.5351 - val_accuracy: 0.4130\n","Epoch 9/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.1399 - accuracy: 0.4602 - val_loss: 2.3008 - val_accuracy: 0.4770\n","Epoch 10/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.9132 - accuracy: 0.5071 - val_loss: 2.1449 - val_accuracy: 0.5388\n","Epoch 11/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.7273 - accuracy: 0.5575 - val_loss: 1.9847 - val_accuracy: 0.5725\n","Epoch 12/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.5728 - accuracy: 0.5938 - val_loss: 1.8647 - val_accuracy: 0.6213\n","Epoch 13/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.4520 - accuracy: 0.6230 - val_loss: 1.7867 - val_accuracy: 0.6282\n","Epoch 14/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.3484 - accuracy: 0.6490 - val_loss: 1.7646 - val_accuracy: 0.6295\n","Epoch 15/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.2750 - accuracy: 0.6668 - val_loss: 1.6533 - val_accuracy: 0.6700\n","Epoch 16/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.1980 - accuracy: 0.6841 - val_loss: 1.6373 - val_accuracy: 0.6656\n","Epoch 17/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.1433 - accuracy: 0.6943 - val_loss: 1.5270 - val_accuracy: 0.7010\n","Epoch 18/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.0986 - accuracy: 0.7046 - val_loss: 1.4993 - val_accuracy: 0.6942\n","Epoch 19/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.0512 - accuracy: 0.7115 - val_loss: 1.4661 - val_accuracy: 0.7157\n","Epoch 20/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.0224 - accuracy: 0.7226 - val_loss: 1.4553 - val_accuracy: 0.6928\n","Average Validation Accuracy: 0.7250114679336548\n","Average Validation Loss: 1.1128262281417847\n","Average Test Accuracy: 0.7169971168041229\n","Final Test Accuracy for each fold: 0.7264686226844788\n","Number of input features: 12\n","Fold: 1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","1846/1846 [==============================] - 15s 6ms/step - loss: 4.7706 - accuracy: 0.0569 - val_loss: 4.4995 - val_accuracy: 0.0860\n","Epoch 2/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.1928 - accuracy: 0.0995 - val_loss: 4.0864 - val_accuracy: 0.1094\n","Epoch 3/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.8223 - accuracy: 0.1372 - val_loss: 3.7762 - val_accuracy: 0.1474\n","Epoch 4/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.5352 - accuracy: 0.1692 - val_loss: 3.5379 - val_accuracy: 0.1881\n","Epoch 5/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 3.2813 - accuracy: 0.2037 - val_loss: 3.2981 - val_accuracy: 0.2172\n","Epoch 6/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 3.0459 - accuracy: 0.2443 - val_loss: 3.0662 - val_accuracy: 0.2438\n","Epoch 7/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.8023 - accuracy: 0.2944 - val_loss: 2.8908 - val_accuracy: 0.3089\n","Epoch 8/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.6002 - accuracy: 0.3317 - val_loss: 2.7388 - val_accuracy: 0.3067\n","Epoch 9/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.4241 - accuracy: 0.3695 - val_loss: 2.5969 - val_accuracy: 0.3294\n","Epoch 10/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.2781 - accuracy: 0.4084 - val_loss: 2.5250 - val_accuracy: 0.3982\n","Epoch 11/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.1596 - accuracy: 0.4315 - val_loss: 2.3826 - val_accuracy: 0.4167\n","Epoch 12/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.0715 - accuracy: 0.4518 - val_loss: 2.3478 - val_accuracy: 0.4279\n","Epoch 13/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.0029 - accuracy: 0.4663 - val_loss: 2.2560 - val_accuracy: 0.4803\n","Epoch 14/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.9253 - accuracy: 0.4847 - val_loss: 2.1556 - val_accuracy: 0.4867\n","Epoch 15/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.8804 - accuracy: 0.4915 - val_loss: 2.1222 - val_accuracy: 0.4900\n","Epoch 16/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.8297 - accuracy: 0.4965 - val_loss: 2.1137 - val_accuracy: 0.5019\n","Epoch 17/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.7805 - accuracy: 0.5118 - val_loss: 2.0311 - val_accuracy: 0.5329\n","Epoch 18/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.7360 - accuracy: 0.5214 - val_loss: 2.0112 - val_accuracy: 0.5155\n","Epoch 19/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.6861 - accuracy: 0.5409 - val_loss: 1.9128 - val_accuracy: 0.5604\n","Epoch 20/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.6344 - accuracy: 0.5478 - val_loss: 1.9171 - val_accuracy: 0.5586\n","Fold: 2\n","Epoch 1/20\n","1846/1846 [==============================] - 14s 6ms/step - loss: 4.7944 - accuracy: 0.0462 - val_loss: 4.5690 - val_accuracy: 0.0711\n","Epoch 2/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.2753 - accuracy: 0.0914 - val_loss: 4.1667 - val_accuracy: 0.1096\n","Epoch 3/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 3.8815 - accuracy: 0.1211 - val_loss: 3.8698 - val_accuracy: 0.1569\n","Epoch 4/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.5945 - accuracy: 0.1565 - val_loss: 3.6353 - val_accuracy: 0.1822\n","Epoch 5/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 3.3576 - accuracy: 0.1990 - val_loss: 3.4545 - val_accuracy: 0.2015\n","Epoch 6/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 3.1427 - accuracy: 0.2314 - val_loss: 3.2513 - val_accuracy: 0.2200\n","Epoch 7/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.9331 - accuracy: 0.2604 - val_loss: 3.0299 - val_accuracy: 0.3065\n","Epoch 8/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.7338 - accuracy: 0.3029 - val_loss: 2.9068 - val_accuracy: 0.3270\n","Epoch 9/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.5622 - accuracy: 0.3364 - val_loss: 2.7628 - val_accuracy: 0.3239\n","Epoch 10/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.3898 - accuracy: 0.3778 - val_loss: 2.6011 - val_accuracy: 0.3912\n","Epoch 11/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.2070 - accuracy: 0.4264 - val_loss: 2.3993 - val_accuracy: 0.4451\n","Epoch 12/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.0176 - accuracy: 0.4763 - val_loss: 2.2523 - val_accuracy: 0.4684\n","Epoch 13/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.8437 - accuracy: 0.5179 - val_loss: 2.1116 - val_accuracy: 0.5118\n","Epoch 14/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.7061 - accuracy: 0.5516 - val_loss: 2.0098 - val_accuracy: 0.5435\n","Epoch 15/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.5881 - accuracy: 0.5793 - val_loss: 1.9232 - val_accuracy: 0.5822\n","Epoch 16/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.5014 - accuracy: 0.5991 - val_loss: 1.8173 - val_accuracy: 0.5976\n","Epoch 17/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.4183 - accuracy: 0.6202 - val_loss: 1.7422 - val_accuracy: 0.6268\n","Epoch 18/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.3387 - accuracy: 0.6468 - val_loss: 1.6554 - val_accuracy: 0.6451\n","Epoch 19/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.2671 - accuracy: 0.6678 - val_loss: 1.6093 - val_accuracy: 0.6609\n","Epoch 20/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.2075 - accuracy: 0.6858 - val_loss: 1.5476 - val_accuracy: 0.7091\n","Average Validation Accuracy: 0.6466642320156097\n","Average Validation Loss: 1.4914288520812988\n","Average Test Accuracy: 0.6488538384437561\n","Final Test Accuracy for each fold: 0.7213090658187866\n","Number of input features: 13\n","Fold: 1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","1846/1846 [==============================] - 13s 5ms/step - loss: 4.8053 - accuracy: 0.0501 - val_loss: 4.5635 - val_accuracy: 0.0653\n","Epoch 2/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.2623 - accuracy: 0.1002 - val_loss: 4.1141 - val_accuracy: 0.1124\n","Epoch 3/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 3.8453 - accuracy: 0.1437 - val_loss: 3.7990 - val_accuracy: 0.1648\n","Epoch 4/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.5774 - accuracy: 0.1735 - val_loss: 3.6081 - val_accuracy: 0.1965\n","Epoch 5/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.3732 - accuracy: 0.1946 - val_loss: 3.4354 - val_accuracy: 0.1864\n","Epoch 6/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.1886 - accuracy: 0.2251 - val_loss: 3.2578 - val_accuracy: 0.2352\n","Epoch 7/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.9970 - accuracy: 0.2632 - val_loss: 3.0841 - val_accuracy: 0.2477\n","Epoch 8/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.8067 - accuracy: 0.2981 - val_loss: 2.9254 - val_accuracy: 0.2950\n","Epoch 9/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.6444 - accuracy: 0.3227 - val_loss: 2.8029 - val_accuracy: 0.3193\n","Epoch 10/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.5042 - accuracy: 0.3487 - val_loss: 2.7092 - val_accuracy: 0.3450\n","Epoch 11/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.3883 - accuracy: 0.3706 - val_loss: 2.6037 - val_accuracy: 0.3611\n","Epoch 12/20\n","1846/1846 [==============================] - 13s 7ms/step - loss: 2.2810 - accuracy: 0.3984 - val_loss: 2.5069 - val_accuracy: 0.4044\n","Epoch 13/20\n","1846/1846 [==============================] - 16s 9ms/step - loss: 2.1918 - accuracy: 0.4108 - val_loss: 2.4453 - val_accuracy: 0.4370\n","Epoch 14/20\n","1846/1846 [==============================] - 13s 7ms/step - loss: 2.1164 - accuracy: 0.4293 - val_loss: 2.4198 - val_accuracy: 0.4158\n","Epoch 15/20\n","1846/1846 [==============================] - 12s 6ms/step - loss: 2.0495 - accuracy: 0.4409 - val_loss: 2.3209 - val_accuracy: 0.4651\n","Epoch 16/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.9983 - accuracy: 0.4530 - val_loss: 2.2826 - val_accuracy: 0.4761\n","Epoch 17/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.9499 - accuracy: 0.4673 - val_loss: 2.2741 - val_accuracy: 0.4161\n","Epoch 18/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.9168 - accuracy: 0.4660 - val_loss: 2.2395 - val_accuracy: 0.4834\n","Epoch 19/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.8674 - accuracy: 0.4835 - val_loss: 2.2470 - val_accuracy: 0.4799\n","Epoch 20/20\n","1846/1846 [==============================] - 12s 6ms/step - loss: 1.8460 - accuracy: 0.4860 - val_loss: 2.1962 - val_accuracy: 0.4730\n","Fold: 2\n","Epoch 1/20\n","1846/1846 [==============================] - 15s 6ms/step - loss: 4.7870 - accuracy: 0.0578 - val_loss: 4.4724 - val_accuracy: 0.0917\n","Epoch 2/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.1231 - accuracy: 0.1075 - val_loss: 4.0934 - val_accuracy: 0.1217\n","Epoch 3/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 3.8134 - accuracy: 0.1399 - val_loss: 3.8591 - val_accuracy: 0.1536\n","Epoch 4/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 3.5959 - accuracy: 0.1581 - val_loss: 3.6680 - val_accuracy: 0.1760\n","Epoch 5/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 3.3691 - accuracy: 0.1948 - val_loss: 3.4543 - val_accuracy: 0.2128\n","Epoch 6/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 3.0614 - accuracy: 0.2595 - val_loss: 3.1206 - val_accuracy: 0.2893\n","Epoch 7/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.6852 - accuracy: 0.3452 - val_loss: 2.7682 - val_accuracy: 0.4079\n","Epoch 8/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.3208 - accuracy: 0.4231 - val_loss: 2.4808 - val_accuracy: 0.4594\n","Epoch 9/20\n","1846/1846 [==============================] - 12s 6ms/step - loss: 2.0085 - accuracy: 0.4899 - val_loss: 2.2327 - val_accuracy: 0.5270\n","Epoch 10/20\n","1846/1846 [==============================] - 12s 6ms/step - loss: 1.7723 - accuracy: 0.5531 - val_loss: 2.0401 - val_accuracy: 0.5886\n","Epoch 11/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.5891 - accuracy: 0.6020 - val_loss: 1.9091 - val_accuracy: 0.6136\n","Epoch 12/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.4490 - accuracy: 0.6370 - val_loss: 1.8240 - val_accuracy: 0.6077\n","Epoch 13/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.3407 - accuracy: 0.6566 - val_loss: 1.6703 - val_accuracy: 0.6691\n","Epoch 14/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.2482 - accuracy: 0.6818 - val_loss: 1.6846 - val_accuracy: 0.6565\n","Epoch 15/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.1687 - accuracy: 0.7025 - val_loss: 1.5792 - val_accuracy: 0.6955\n","Epoch 16/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.1086 - accuracy: 0.7137 - val_loss: 1.4906 - val_accuracy: 0.7208\n","Epoch 17/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.0627 - accuracy: 0.7255 - val_loss: 1.4459 - val_accuracy: 0.7237\n","Epoch 18/20\n","1846/1846 [==============================] - 12s 7ms/step - loss: 1.0132 - accuracy: 0.7363 - val_loss: 1.4315 - val_accuracy: 0.7261\n","Epoch 19/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.9840 - accuracy: 0.7426 - val_loss: 1.4000 - val_accuracy: 0.7190\n","Epoch 20/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.9426 - accuracy: 0.7520 - val_loss: 1.3641 - val_accuracy: 0.7380\n","Average Validation Accuracy: 0.6113399118185043\n","Average Validation Loss: 1.5001010298728943\n","Average Test Accuracy: 0.61177858710289\n","Final Test Accuracy for each fold: 0.7405469417572021\n","Number of input features: 14\n","Fold: 1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","1846/1846 [==============================] - 14s 6ms/step - loss: 4.7377 - accuracy: 0.0608 - val_loss: 4.4231 - val_accuracy: 0.0867\n","Epoch 2/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.0650 - accuracy: 0.1176 - val_loss: 3.9390 - val_accuracy: 0.1274\n","Epoch 3/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 3.7026 - accuracy: 0.1464 - val_loss: 3.7410 - val_accuracy: 0.1428\n","Epoch 4/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 3.4492 - accuracy: 0.1729 - val_loss: 3.4663 - val_accuracy: 0.2004\n","Epoch 5/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 3.2448 - accuracy: 0.1956 - val_loss: 3.2867 - val_accuracy: 0.2141\n","Epoch 6/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 3.0689 - accuracy: 0.2155 - val_loss: 3.1502 - val_accuracy: 0.2361\n","Epoch 7/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.8828 - accuracy: 0.2525 - val_loss: 2.9832 - val_accuracy: 0.2796\n","Epoch 8/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.6936 - accuracy: 0.2962 - val_loss: 2.8018 - val_accuracy: 0.3177\n","Epoch 9/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.5383 - accuracy: 0.3354 - val_loss: 2.7653 - val_accuracy: 0.3124\n","Epoch 10/20\n","1846/1846 [==============================] - 8s 5ms/step - loss: 2.4112 - accuracy: 0.3490 - val_loss: 2.6054 - val_accuracy: 0.3802\n","Epoch 11/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.3050 - accuracy: 0.3768 - val_loss: 2.5665 - val_accuracy: 0.3919\n","Epoch 12/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.2199 - accuracy: 0.3949 - val_loss: 2.4402 - val_accuracy: 0.3932\n","Epoch 13/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 2.1457 - accuracy: 0.4083 - val_loss: 2.3915 - val_accuracy: 0.4051\n","Epoch 14/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 2.0859 - accuracy: 0.4192 - val_loss: 2.3648 - val_accuracy: 0.3853\n","Epoch 15/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 2.0272 - accuracy: 0.4303 - val_loss: 2.3192 - val_accuracy: 0.4475\n","Epoch 16/20\n","1846/1846 [==============================] - 8s 5ms/step - loss: 1.9843 - accuracy: 0.4379 - val_loss: 2.3664 - val_accuracy: 0.3894\n","Epoch 17/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.9417 - accuracy: 0.4535 - val_loss: 2.2411 - val_accuracy: 0.5001\n","Epoch 18/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.9079 - accuracy: 0.4669 - val_loss: 2.2454 - val_accuracy: 0.4827\n","Epoch 19/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.8690 - accuracy: 0.4784 - val_loss: 2.2073 - val_accuracy: 0.4471\n","Epoch 20/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.8326 - accuracy: 0.4944 - val_loss: 2.1264 - val_accuracy: 0.4708\n","Fold: 2\n","Epoch 1/20\n","1846/1846 [==============================] - 14s 6ms/step - loss: 4.8162 - accuracy: 0.0483 - val_loss: 4.5566 - val_accuracy: 0.0583\n","Epoch 2/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.1471 - accuracy: 0.1083 - val_loss: 4.0479 - val_accuracy: 0.1327\n","Epoch 3/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 3.7450 - accuracy: 0.1421 - val_loss: 3.7472 - val_accuracy: 0.1611\n","Epoch 4/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 3.4635 - accuracy: 0.1720 - val_loss: 3.5239 - val_accuracy: 0.1813\n","Epoch 5/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 3.2229 - accuracy: 0.2094 - val_loss: 3.2959 - val_accuracy: 0.2240\n","Epoch 6/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 2.9988 - accuracy: 0.2493 - val_loss: 3.0731 - val_accuracy: 0.2741\n","Epoch 7/20\n","1846/1846 [==============================] - 8s 5ms/step - loss: 2.7522 - accuracy: 0.2964 - val_loss: 2.8368 - val_accuracy: 0.3155\n","Epoch 8/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 2.4811 - accuracy: 0.3630 - val_loss: 2.5745 - val_accuracy: 0.3914\n","Epoch 9/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.2268 - accuracy: 0.4279 - val_loss: 2.3769 - val_accuracy: 0.4295\n","Epoch 10/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.9866 - accuracy: 0.4898 - val_loss: 2.1482 - val_accuracy: 0.5102\n","Epoch 11/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.8015 - accuracy: 0.5266 - val_loss: 1.9874 - val_accuracy: 0.5278\n","Epoch 12/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.6438 - accuracy: 0.5650 - val_loss: 1.8760 - val_accuracy: 0.6018\n","Epoch 13/20\n","1846/1846 [==============================] - 8s 5ms/step - loss: 1.5139 - accuracy: 0.6003 - val_loss: 1.7497 - val_accuracy: 0.5765\n","Epoch 14/20\n","1846/1846 [==============================] - 8s 5ms/step - loss: 1.4070 - accuracy: 0.6291 - val_loss: 1.6955 - val_accuracy: 0.6433\n","Epoch 15/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.3088 - accuracy: 0.6581 - val_loss: 1.5530 - val_accuracy: 0.6675\n","Epoch 16/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.2193 - accuracy: 0.6815 - val_loss: 1.4888 - val_accuracy: 0.6669\n","Epoch 17/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.1466 - accuracy: 0.6985 - val_loss: 1.4311 - val_accuracy: 0.6900\n","Epoch 18/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.0902 - accuracy: 0.7132 - val_loss: 1.3791 - val_accuracy: 0.7091\n","Epoch 19/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.0480 - accuracy: 0.7164 - val_loss: 1.3321 - val_accuracy: 0.7186\n","Epoch 20/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.0082 - accuracy: 0.7289 - val_loss: 1.3677 - val_accuracy: 0.7177\n","Average Validation Accuracy: 0.6089068353176117\n","Average Validation Loss: 1.5003716945648193\n","Average Test Accuracy: 0.616311639547348\n","Final Test Accuracy for each fold: 0.7279428243637085\n","Number of input features: 15\n","Fold: 1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","1846/1846 [==============================] - 14s 6ms/step - loss: 4.8067 - accuracy: 0.0550 - val_loss: 4.4263 - val_accuracy: 0.0689\n","Epoch 2/20\n","1846/1846 [==============================] - 13s 7ms/step - loss: 4.0798 - accuracy: 0.1211 - val_loss: 3.9504 - val_accuracy: 0.1397\n","Epoch 3/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 3.6847 - accuracy: 0.1633 - val_loss: 3.6266 - val_accuracy: 0.1650\n","Epoch 4/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 3.3666 - accuracy: 0.2049 - val_loss: 3.3778 - val_accuracy: 0.2064\n","Epoch 5/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 3.0411 - accuracy: 0.2688 - val_loss: 3.0472 - val_accuracy: 0.2931\n","Epoch 6/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 2.7181 - accuracy: 0.3465 - val_loss: 2.7729 - val_accuracy: 0.3586\n","Epoch 7/20\n","1846/1846 [==============================] - 8s 5ms/step - loss: 2.3998 - accuracy: 0.4144 - val_loss: 2.4848 - val_accuracy: 0.4134\n","Epoch 8/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.1277 - accuracy: 0.4646 - val_loss: 2.2523 - val_accuracy: 0.4766\n","Epoch 9/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.8930 - accuracy: 0.5168 - val_loss: 2.0671 - val_accuracy: 0.5243\n","Epoch 10/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.6987 - accuracy: 0.5672 - val_loss: 1.8955 - val_accuracy: 0.5806\n","Epoch 11/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.5333 - accuracy: 0.5977 - val_loss: 1.8085 - val_accuracy: 0.5921\n","Epoch 12/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.4010 - accuracy: 0.6384 - val_loss: 1.6877 - val_accuracy: 0.6255\n","Epoch 13/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.2844 - accuracy: 0.6730 - val_loss: 1.5936 - val_accuracy: 0.6411\n","Epoch 14/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.2046 - accuracy: 0.6886 - val_loss: 1.5189 - val_accuracy: 0.6788\n","Epoch 15/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.1285 - accuracy: 0.7065 - val_loss: 1.4592 - val_accuracy: 0.7008\n","Epoch 16/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.0633 - accuracy: 0.7237 - val_loss: 1.4374 - val_accuracy: 0.7111\n","Epoch 17/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.0224 - accuracy: 0.7312 - val_loss: 1.3681 - val_accuracy: 0.7199\n","Epoch 18/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.9686 - accuracy: 0.7429 - val_loss: 1.3742 - val_accuracy: 0.7285\n","Epoch 19/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.9334 - accuracy: 0.7503 - val_loss: 1.3259 - val_accuracy: 0.7503\n","Epoch 20/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.8991 - accuracy: 0.7567 - val_loss: 1.2661 - val_accuracy: 0.7699\n","Fold: 2\n","Epoch 1/20\n","1846/1846 [==============================] - 10s 4ms/step - loss: 4.8468 - accuracy: 0.0544 - val_loss: 4.5783 - val_accuracy: 0.0928\n","Epoch 2/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 4.2480 - accuracy: 0.0984 - val_loss: 4.1820 - val_accuracy: 0.1206\n","Epoch 3/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 3.8906 - accuracy: 0.1372 - val_loss: 3.9415 - val_accuracy: 0.1705\n","Epoch 4/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 3.6495 - accuracy: 0.1625 - val_loss: 3.7115 - val_accuracy: 0.1743\n","Epoch 5/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 3.4293 - accuracy: 0.1899 - val_loss: 3.5172 - val_accuracy: 0.2198\n","Epoch 6/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 3.2170 - accuracy: 0.2179 - val_loss: 3.3546 - val_accuracy: 0.2502\n","Epoch 7/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 3.0357 - accuracy: 0.2487 - val_loss: 3.2081 - val_accuracy: 0.2627\n","Epoch 8/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.8898 - accuracy: 0.2766 - val_loss: 3.0915 - val_accuracy: 0.2697\n","Epoch 9/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.7586 - accuracy: 0.2941 - val_loss: 3.0042 - val_accuracy: 0.3058\n","Epoch 10/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.6314 - accuracy: 0.3201 - val_loss: 2.8616 - val_accuracy: 0.3276\n","Epoch 11/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.5051 - accuracy: 0.3449 - val_loss: 2.7294 - val_accuracy: 0.3549\n","Epoch 12/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.3691 - accuracy: 0.3687 - val_loss: 2.6977 - val_accuracy: 0.3514\n","Epoch 13/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.2240 - accuracy: 0.4075 - val_loss: 2.4812 - val_accuracy: 0.4561\n","Epoch 14/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.0633 - accuracy: 0.4494 - val_loss: 2.3787 - val_accuracy: 0.4376\n","Epoch 15/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.9451 - accuracy: 0.4733 - val_loss: 2.2471 - val_accuracy: 0.4878\n","Epoch 16/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.8323 - accuracy: 0.5032 - val_loss: 2.2029 - val_accuracy: 0.4968\n","Epoch 17/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.7338 - accuracy: 0.5249 - val_loss: 2.1184 - val_accuracy: 0.5045\n","Epoch 18/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.6438 - accuracy: 0.5464 - val_loss: 1.9947 - val_accuracy: 0.5494\n","Epoch 19/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.5531 - accuracy: 0.5714 - val_loss: 1.9209 - val_accuracy: 0.5608\n","Epoch 20/20\n","1846/1846 [==============================] - 8s 5ms/step - loss: 1.4675 - accuracy: 0.5975 - val_loss: 1.8645 - val_accuracy: 0.5800\n","Average Validation Accuracy: 0.6872130036354065\n","Average Validation Loss: 1.2765260934829712\n","Average Test Accuracy: 0.686776727437973\n","Final Test Accuracy for each fold: 0.7793911695480347\n","Number of input features: 16\n","Fold: 1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","1846/1846 [==============================] - 15s 6ms/step - loss: 4.7780 - accuracy: 0.0643 - val_loss: 4.4550 - val_accuracy: 0.0939\n","Epoch 2/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.0827 - accuracy: 0.1229 - val_loss: 3.9405 - val_accuracy: 0.1327\n","Epoch 3/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.6969 - accuracy: 0.1676 - val_loss: 3.6664 - val_accuracy: 0.1470\n","Epoch 4/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.4365 - accuracy: 0.1857 - val_loss: 3.4439 - val_accuracy: 0.1943\n","Epoch 5/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.2047 - accuracy: 0.2200 - val_loss: 3.2554 - val_accuracy: 0.2389\n","Epoch 6/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.9733 - accuracy: 0.2600 - val_loss: 3.0334 - val_accuracy: 0.2977\n","Epoch 7/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.7360 - accuracy: 0.3057 - val_loss: 2.8442 - val_accuracy: 0.3217\n","Epoch 8/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.4990 - accuracy: 0.3630 - val_loss: 2.6348 - val_accuracy: 0.3699\n","Epoch 9/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.2730 - accuracy: 0.4198 - val_loss: 2.4108 - val_accuracy: 0.4554\n","Epoch 10/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.0655 - accuracy: 0.4757 - val_loss: 2.2613 - val_accuracy: 0.4893\n","Epoch 11/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.8768 - accuracy: 0.5194 - val_loss: 2.0945 - val_accuracy: 0.5261\n","Epoch 12/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.7110 - accuracy: 0.5603 - val_loss: 2.0492 - val_accuracy: 0.5338\n","Epoch 13/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.5822 - accuracy: 0.5901 - val_loss: 1.8865 - val_accuracy: 0.5767\n","Epoch 14/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.4793 - accuracy: 0.6172 - val_loss: 1.8377 - val_accuracy: 0.5982\n","Epoch 15/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.4044 - accuracy: 0.6308 - val_loss: 1.7686 - val_accuracy: 0.6086\n","Epoch 16/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.3451 - accuracy: 0.6385 - val_loss: 1.6916 - val_accuracy: 0.6590\n","Epoch 17/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.2887 - accuracy: 0.6576 - val_loss: 1.6538 - val_accuracy: 0.6658\n","Epoch 18/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.2462 - accuracy: 0.6655 - val_loss: 1.6641 - val_accuracy: 0.6370\n","Epoch 19/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.2053 - accuracy: 0.6819 - val_loss: 1.5760 - val_accuracy: 0.6763\n","Epoch 20/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.1738 - accuracy: 0.6859 - val_loss: 1.5843 - val_accuracy: 0.6794\n","Fold: 2\n","Epoch 1/20\n","1846/1846 [==============================] - 15s 6ms/step - loss: 4.7464 - accuracy: 0.0678 - val_loss: 4.3711 - val_accuracy: 0.1052\n","Epoch 2/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 3.9831 - accuracy: 0.1258 - val_loss: 3.8959 - val_accuracy: 0.1589\n","Epoch 3/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 3.6326 - accuracy: 0.1655 - val_loss: 3.6702 - val_accuracy: 0.1800\n","Epoch 4/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 3.3988 - accuracy: 0.1913 - val_loss: 3.4582 - val_accuracy: 0.2158\n","Epoch 5/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.1408 - accuracy: 0.2436 - val_loss: 3.1918 - val_accuracy: 0.2704\n","Epoch 6/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 2.8336 - accuracy: 0.3023 - val_loss: 2.9586 - val_accuracy: 0.2865\n","Epoch 7/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 2.5499 - accuracy: 0.3627 - val_loss: 2.6758 - val_accuracy: 0.3947\n","Epoch 8/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.2841 - accuracy: 0.4296 - val_loss: 2.4466 - val_accuracy: 0.4656\n","Epoch 9/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 2.0454 - accuracy: 0.4809 - val_loss: 2.2299 - val_accuracy: 0.4988\n","Epoch 10/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.8349 - accuracy: 0.5296 - val_loss: 2.0798 - val_accuracy: 0.5567\n","Epoch 11/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.6825 - accuracy: 0.5583 - val_loss: 1.9598 - val_accuracy: 0.5340\n","Epoch 12/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.5531 - accuracy: 0.5891 - val_loss: 1.8629 - val_accuracy: 0.5947\n","Epoch 13/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.4541 - accuracy: 0.6166 - val_loss: 1.7327 - val_accuracy: 0.6414\n","Epoch 14/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.3765 - accuracy: 0.6351 - val_loss: 1.7553 - val_accuracy: 0.6139\n","Epoch 15/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.3164 - accuracy: 0.6448 - val_loss: 1.6855 - val_accuracy: 0.6165\n","Epoch 16/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.2500 - accuracy: 0.6675 - val_loss: 1.6265 - val_accuracy: 0.6548\n","Epoch 17/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.1984 - accuracy: 0.6804 - val_loss: 1.5526 - val_accuracy: 0.6900\n","Epoch 18/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.1605 - accuracy: 0.6852 - val_loss: 1.4950 - val_accuracy: 0.6933\n","Epoch 19/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.1218 - accuracy: 0.7039 - val_loss: 1.5231 - val_accuracy: 0.6693\n","Epoch 20/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.0879 - accuracy: 0.7041 - val_loss: 1.4901 - val_accuracy: 0.6911\n","Average Validation Accuracy: 0.6948411762714386\n","Average Validation Loss: 1.2539492845535278\n","Average Test Accuracy: 0.6957322657108307\n","Final Test Accuracy for each fold: 0.7064936757087708\n","Number of input features: 17\n","Fold: 1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","1846/1846 [==============================] - 13s 5ms/step - loss: 4.8065 - accuracy: 0.0595 - val_loss: 4.4910 - val_accuracy: 0.1028\n","Epoch 2/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.1637 - accuracy: 0.1207 - val_loss: 4.0196 - val_accuracy: 0.1413\n","Epoch 3/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 3.7616 - accuracy: 0.1599 - val_loss: 3.7353 - val_accuracy: 0.1639\n","Epoch 4/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 3.5070 - accuracy: 0.1882 - val_loss: 3.6123 - val_accuracy: 0.1509\n","Epoch 5/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 3.3083 - accuracy: 0.2113 - val_loss: 3.3723 - val_accuracy: 0.1903\n","Epoch 6/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 3.0841 - accuracy: 0.2473 - val_loss: 3.1535 - val_accuracy: 0.2585\n","Epoch 7/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 2.8346 - accuracy: 0.3031 - val_loss: 2.9251 - val_accuracy: 0.2924\n","Epoch 8/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.5845 - accuracy: 0.3581 - val_loss: 2.7004 - val_accuracy: 0.3448\n","Epoch 9/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 2.3426 - accuracy: 0.4104 - val_loss: 2.4750 - val_accuracy: 0.4330\n","Epoch 10/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.1139 - accuracy: 0.4724 - val_loss: 2.3011 - val_accuracy: 0.4845\n","Epoch 11/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.9151 - accuracy: 0.5140 - val_loss: 2.1751 - val_accuracy: 0.5162\n","Epoch 12/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.7644 - accuracy: 0.5469 - val_loss: 2.0062 - val_accuracy: 0.5369\n","Epoch 13/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.6435 - accuracy: 0.5758 - val_loss: 1.9096 - val_accuracy: 0.5923\n","Epoch 14/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.5524 - accuracy: 0.5951 - val_loss: 1.8737 - val_accuracy: 0.5771\n","Epoch 15/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.4728 - accuracy: 0.6091 - val_loss: 1.7952 - val_accuracy: 0.6073\n","Epoch 16/20\n","1846/1846 [==============================] - 8s 5ms/step - loss: 1.4081 - accuracy: 0.6284 - val_loss: 1.7449 - val_accuracy: 0.6187\n","Epoch 17/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.3499 - accuracy: 0.6433 - val_loss: 1.6897 - val_accuracy: 0.6546\n","Epoch 18/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.2989 - accuracy: 0.6538 - val_loss: 1.7067 - val_accuracy: 0.6341\n","Epoch 19/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.2648 - accuracy: 0.6633 - val_loss: 1.6591 - val_accuracy: 0.6381\n","Epoch 20/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.2260 - accuracy: 0.6705 - val_loss: 1.5904 - val_accuracy: 0.6546\n","Fold: 2\n","Epoch 1/20\n","1846/1846 [==============================] - 12s 5ms/step - loss: 4.8402 - accuracy: 0.0535 - val_loss: 4.5966 - val_accuracy: 0.0656\n","Epoch 2/20\n","1846/1846 [==============================] - 8s 5ms/step - loss: 4.2123 - accuracy: 0.1073 - val_loss: 4.0735 - val_accuracy: 0.1267\n","Epoch 3/20\n","1846/1846 [==============================] - 8s 5ms/step - loss: 3.7848 - accuracy: 0.1557 - val_loss: 3.7996 - val_accuracy: 0.1831\n","Epoch 4/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 3.5002 - accuracy: 0.1886 - val_loss: 3.5895 - val_accuracy: 0.2051\n","Epoch 5/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 3.2756 - accuracy: 0.2233 - val_loss: 3.4185 - val_accuracy: 0.2513\n","Epoch 6/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 3.1021 - accuracy: 0.2472 - val_loss: 3.2825 - val_accuracy: 0.2568\n","Epoch 7/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.9426 - accuracy: 0.2746 - val_loss: 3.1121 - val_accuracy: 0.2891\n","Epoch 8/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.7886 - accuracy: 0.3017 - val_loss: 2.9797 - val_accuracy: 0.3019\n","Epoch 9/20\n","1846/1846 [==============================] - 15s 8ms/step - loss: 2.6318 - accuracy: 0.3361 - val_loss: 2.8455 - val_accuracy: 0.3254\n","Epoch 10/20\n","1846/1846 [==============================] - 17s 9ms/step - loss: 2.4918 - accuracy: 0.3751 - val_loss: 2.6980 - val_accuracy: 0.3978\n","Epoch 11/20\n","1846/1846 [==============================] - 16s 9ms/step - loss: 2.3667 - accuracy: 0.4052 - val_loss: 2.5800 - val_accuracy: 0.4167\n","Epoch 12/20\n","1846/1846 [==============================] - 13s 7ms/step - loss: 2.2577 - accuracy: 0.4280 - val_loss: 2.5111 - val_accuracy: 0.4385\n","Epoch 13/20\n","1846/1846 [==============================] - 15s 8ms/step - loss: 2.1641 - accuracy: 0.4494 - val_loss: 2.4181 - val_accuracy: 0.4695\n","Epoch 14/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.0757 - accuracy: 0.4663 - val_loss: 2.4336 - val_accuracy: 0.4073\n","Epoch 15/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.0043 - accuracy: 0.4717 - val_loss: 2.3149 - val_accuracy: 0.4684\n","Epoch 16/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.9312 - accuracy: 0.4892 - val_loss: 2.2132 - val_accuracy: 0.5230\n","Epoch 17/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.8669 - accuracy: 0.5051 - val_loss: 2.1808 - val_accuracy: 0.4979\n","Epoch 18/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.8155 - accuracy: 0.5115 - val_loss: 2.1499 - val_accuracy: 0.5208\n","Epoch 19/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.7607 - accuracy: 0.5253 - val_loss: 2.1361 - val_accuracy: 0.5208\n","Epoch 20/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.7166 - accuracy: 0.5395 - val_loss: 2.0718 - val_accuracy: 0.5212\n","Average Validation Accuracy: 0.6017112731933594\n","Average Validation Loss: 1.567479431629181\n","Average Test Accuracy: 0.601348876953125\n","Final Test Accuracy for each fold: 0.6707451939582825\n","Number of input features: 18\n","Fold: 1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","1846/1846 [==============================] - 19s 8ms/step - loss: 4.8261 - accuracy: 0.0617 - val_loss: 4.4856 - val_accuracy: 0.1023\n","Epoch 2/20\n","1846/1846 [==============================] - 16s 9ms/step - loss: 4.0601 - accuracy: 0.1368 - val_loss: 3.8964 - val_accuracy: 0.1839\n","Epoch 3/20\n","1846/1846 [==============================] - 16s 9ms/step - loss: 3.6319 - accuracy: 0.1918 - val_loss: 3.6142 - val_accuracy: 0.1985\n","Epoch 4/20\n","1846/1846 [==============================] - 15s 8ms/step - loss: 3.3548 - accuracy: 0.2233 - val_loss: 3.3802 - val_accuracy: 0.2354\n","Epoch 5/20\n","1846/1846 [==============================] - 13s 7ms/step - loss: 3.1143 - accuracy: 0.2570 - val_loss: 3.1702 - val_accuracy: 0.2878\n","Epoch 6/20\n","1846/1846 [==============================] - 13s 7ms/step - loss: 2.8726 - accuracy: 0.2907 - val_loss: 2.9474 - val_accuracy: 0.2840\n","Epoch 7/20\n","1846/1846 [==============================] - 13s 7ms/step - loss: 2.6262 - accuracy: 0.3426 - val_loss: 2.7227 - val_accuracy: 0.3716\n","Epoch 8/20\n","1846/1846 [==============================] - 14s 7ms/step - loss: 2.3725 - accuracy: 0.4032 - val_loss: 2.4719 - val_accuracy: 0.4332\n","Epoch 9/20\n","1846/1846 [==============================] - 17s 9ms/step - loss: 2.1328 - accuracy: 0.4653 - val_loss: 2.3395 - val_accuracy: 0.4526\n","Epoch 10/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.9249 - accuracy: 0.5130 - val_loss: 2.0876 - val_accuracy: 0.5274\n","Epoch 11/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.7509 - accuracy: 0.5649 - val_loss: 1.9601 - val_accuracy: 0.5600\n","Epoch 12/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.6009 - accuracy: 0.5960 - val_loss: 1.8297 - val_accuracy: 0.5974\n","Epoch 13/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.4900 - accuracy: 0.6287 - val_loss: 1.7730 - val_accuracy: 0.6051\n","Epoch 14/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.4016 - accuracy: 0.6458 - val_loss: 1.7364 - val_accuracy: 0.6121\n","Epoch 15/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.3287 - accuracy: 0.6579 - val_loss: 1.6236 - val_accuracy: 0.6649\n","Epoch 16/20\n","1846/1846 [==============================] - 8s 5ms/step - loss: 1.2688 - accuracy: 0.6734 - val_loss: 1.5694 - val_accuracy: 0.6475\n","Epoch 17/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.2131 - accuracy: 0.6859 - val_loss: 1.5331 - val_accuracy: 0.6662\n","Epoch 18/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.1584 - accuracy: 0.7013 - val_loss: 1.4650 - val_accuracy: 0.7113\n","Epoch 19/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.1105 - accuracy: 0.7158 - val_loss: 1.5390 - val_accuracy: 0.6717\n","Epoch 20/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.0737 - accuracy: 0.7301 - val_loss: 1.4374 - val_accuracy: 0.7263\n","Fold: 2\n","Epoch 1/20\n","1846/1846 [==============================] - 13s 5ms/step - loss: 4.7769 - accuracy: 0.0651 - val_loss: 4.4757 - val_accuracy: 0.0961\n","Epoch 2/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.0727 - accuracy: 0.1276 - val_loss: 3.9120 - val_accuracy: 0.1525\n","Epoch 3/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 3.6120 - accuracy: 0.1809 - val_loss: 3.6122 - val_accuracy: 0.2152\n","Epoch 4/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 3.3662 - accuracy: 0.2085 - val_loss: 3.4452 - val_accuracy: 0.2227\n","Epoch 5/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 3.1967 - accuracy: 0.2268 - val_loss: 3.2908 - val_accuracy: 0.2354\n","Epoch 6/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 3.0347 - accuracy: 0.2531 - val_loss: 3.1457 - val_accuracy: 0.2893\n","Epoch 7/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.8741 - accuracy: 0.2923 - val_loss: 2.9874 - val_accuracy: 0.3164\n","Epoch 8/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.6898 - accuracy: 0.3301 - val_loss: 2.8106 - val_accuracy: 0.3450\n","Epoch 9/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.4957 - accuracy: 0.3707 - val_loss: 2.6685 - val_accuracy: 0.3875\n","Epoch 10/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.3041 - accuracy: 0.4157 - val_loss: 2.5132 - val_accuracy: 0.4053\n","Epoch 11/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.1136 - accuracy: 0.4644 - val_loss: 2.3366 - val_accuracy: 0.4924\n","Epoch 12/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.9379 - accuracy: 0.5083 - val_loss: 2.1728 - val_accuracy: 0.5182\n","Epoch 13/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.7816 - accuracy: 0.5436 - val_loss: 2.0390 - val_accuracy: 0.5672\n","Epoch 14/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.6650 - accuracy: 0.5655 - val_loss: 1.9724 - val_accuracy: 0.5373\n","Epoch 15/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.5717 - accuracy: 0.5829 - val_loss: 1.8760 - val_accuracy: 0.5921\n","Epoch 16/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.4965 - accuracy: 0.6017 - val_loss: 1.8106 - val_accuracy: 0.5958\n","Epoch 17/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.4382 - accuracy: 0.6144 - val_loss: 1.7909 - val_accuracy: 0.5941\n","Epoch 18/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.3821 - accuracy: 0.6297 - val_loss: 1.7034 - val_accuracy: 0.6172\n","Epoch 19/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.3287 - accuracy: 0.6365 - val_loss: 1.6633 - val_accuracy: 0.6427\n","Epoch 20/20\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.2893 - accuracy: 0.6521 - val_loss: 1.6393 - val_accuracy: 0.6312\n","Average Validation Accuracy: 0.6927333176136017\n","Average Validation Loss: 1.274277925491333\n","Average Test Accuracy: 0.6925996840000153\n","Final Test Accuracy for each fold: 0.7380408048629761\n","Number of input features: 19\n","Fold: 1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","1846/1846 [==============================] - 11s 4ms/step - loss: 4.9155 - accuracy: 0.0478 - val_loss: 4.6693 - val_accuracy: 0.0506\n","Epoch 2/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 4.2538 - accuracy: 0.1087 - val_loss: 3.9938 - val_accuracy: 0.1549\n","Epoch 3/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 3.6696 - accuracy: 0.1741 - val_loss: 3.6020 - val_accuracy: 0.1560\n","Epoch 4/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 3.3319 - accuracy: 0.2169 - val_loss: 3.3495 - val_accuracy: 0.2295\n","Epoch 5/20\n","1846/1846 [==============================] - 13s 7ms/step - loss: 3.0401 - accuracy: 0.2615 - val_loss: 3.0750 - val_accuracy: 0.3043\n","Epoch 6/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.7219 - accuracy: 0.3348 - val_loss: 2.8160 - val_accuracy: 0.3712\n","Epoch 7/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.4232 - accuracy: 0.4132 - val_loss: 2.5345 - val_accuracy: 0.4546\n","Epoch 8/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.1478 - accuracy: 0.4640 - val_loss: 2.3323 - val_accuracy: 0.4779\n","Epoch 9/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.9130 - accuracy: 0.5231 - val_loss: 2.1923 - val_accuracy: 0.5384\n","Epoch 10/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.7342 - accuracy: 0.5660 - val_loss: 2.0375 - val_accuracy: 0.5529\n","Epoch 11/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.5972 - accuracy: 0.5972 - val_loss: 1.9292 - val_accuracy: 0.6037\n","Epoch 12/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.4926 - accuracy: 0.6211 - val_loss: 1.8561 - val_accuracy: 0.6174\n","Epoch 13/20\n","1846/1846 [==============================] - 12s 7ms/step - loss: 1.4079 - accuracy: 0.6385 - val_loss: 1.7703 - val_accuracy: 0.6682\n","Epoch 14/20\n","1846/1846 [==============================] - 12s 7ms/step - loss: 1.3435 - accuracy: 0.6562 - val_loss: 1.7448 - val_accuracy: 0.6323\n","Epoch 15/20\n","1846/1846 [==============================] - 15s 8ms/step - loss: 1.2928 - accuracy: 0.6650 - val_loss: 1.7297 - val_accuracy: 0.6570\n","Epoch 16/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.2467 - accuracy: 0.6756 - val_loss: 1.7367 - val_accuracy: 0.6605\n","Epoch 17/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.2112 - accuracy: 0.6831 - val_loss: 1.6546 - val_accuracy: 0.6722\n","Epoch 18/20\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.1747 - accuracy: 0.6935 - val_loss: 1.6247 - val_accuracy: 0.6904\n","Epoch 19/20\n","1846/1846 [==============================] - 13s 7ms/step - loss: 1.1480 - accuracy: 0.7015 - val_loss: 1.6222 - val_accuracy: 0.6697\n","Epoch 20/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.1271 - accuracy: 0.7104 - val_loss: 1.6019 - val_accuracy: 0.6847\n","Fold: 2\n","Epoch 1/20\n","1846/1846 [==============================] - 11s 5ms/step - loss: 4.9617 - accuracy: 0.0442 - val_loss: 4.8225 - val_accuracy: 0.0477\n","Epoch 2/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.6698 - accuracy: 0.0580 - val_loss: 4.6275 - val_accuracy: 0.0777\n","Epoch 3/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 4.2960 - accuracy: 0.0992 - val_loss: 4.1301 - val_accuracy: 0.1331\n","Epoch 4/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 3.7885 - accuracy: 0.1567 - val_loss: 3.7492 - val_accuracy: 0.1677\n","Epoch 5/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 3.4609 - accuracy: 0.1901 - val_loss: 3.4938 - val_accuracy: 0.2035\n","Epoch 6/20\n","1846/1846 [==============================] - 9s 5ms/step - loss: 3.2453 - accuracy: 0.2120 - val_loss: 3.3697 - val_accuracy: 0.2073\n","Epoch 7/20\n","1846/1846 [==============================] - 6s 3ms/step - loss: 3.0827 - accuracy: 0.2346 - val_loss: 3.1865 - val_accuracy: 0.2394\n","Epoch 8/20\n","1846/1846 [==============================] - 6s 4ms/step - loss: 2.9232 - accuracy: 0.2769 - val_loss: 3.0414 - val_accuracy: 0.2878\n","Epoch 9/20\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.7773 - accuracy: 0.3056 - val_loss: 2.9566 - val_accuracy: 0.2953\n","Epoch 10/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.6329 - accuracy: 0.3316 - val_loss: 2.7794 - val_accuracy: 0.3325\n","Epoch 11/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.4841 - accuracy: 0.3664 - val_loss: 2.6300 - val_accuracy: 0.3619\n","Epoch 12/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.3275 - accuracy: 0.4042 - val_loss: 2.5253 - val_accuracy: 0.4163\n","Epoch 13/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.1805 - accuracy: 0.4442 - val_loss: 2.3930 - val_accuracy: 0.4497\n","Epoch 14/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.0399 - accuracy: 0.4837 - val_loss: 2.2773 - val_accuracy: 0.4909\n","Epoch 15/20\n","1846/1846 [==============================] - 12s 6ms/step - loss: 1.9182 - accuracy: 0.5149 - val_loss: 2.1734 - val_accuracy: 0.5274\n","Epoch 16/20\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.8162 - accuracy: 0.5425 - val_loss: 2.0694 - val_accuracy: 0.5424\n","Epoch 17/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.7291 - accuracy: 0.5571 - val_loss: 1.9908 - val_accuracy: 0.5806\n","Epoch 18/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.6494 - accuracy: 0.5773 - val_loss: 1.9388 - val_accuracy: 0.5723\n","Epoch 19/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.5852 - accuracy: 0.5872 - val_loss: 1.8418 - val_accuracy: 0.5820\n","Epoch 20/20\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.5184 - accuracy: 0.6038 - val_loss: 1.7896 - val_accuracy: 0.5943\n","Average Validation Accuracy: 0.6492741703987122\n","Average Validation Loss: 1.4238731265068054\n","Average Test Accuracy: 0.6504016816616058\n","Final Test Accuracy for each fold: 0.6932998895645142\n","Number of input features: 20\n","Fold: 1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n","1846/1846 [==============================] - 14s 5ms/step - loss: 4.9608 - accuracy: 0.0431 - val_loss: 4.7692 - val_accuracy: 0.0493\n","Epoch 2/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 4.3823 - accuracy: 0.0844 - val_loss: 4.1047 - val_accuracy: 0.1448\n","Epoch 3/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 3.7403 - accuracy: 0.1758 - val_loss: 3.6373 - val_accuracy: 0.1872\n","Epoch 4/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 3.3274 - accuracy: 0.2284 - val_loss: 3.3062 - val_accuracy: 0.2530\n","Epoch 5/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.9773 - accuracy: 0.2938 - val_loss: 3.0411 - val_accuracy: 0.3406\n","Epoch 6/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.6504 - accuracy: 0.3679 - val_loss: 2.7089 - val_accuracy: 0.3947\n","Epoch 7/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.3618 - accuracy: 0.4156 - val_loss: 2.4720 - val_accuracy: 0.4427\n","Epoch 8/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.1538 - accuracy: 0.4563 - val_loss: 2.3528 - val_accuracy: 0.4686\n","Epoch 9/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.9859 - accuracy: 0.4869 - val_loss: 2.1909 - val_accuracy: 0.4757\n","Epoch 10/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.8333 - accuracy: 0.5251 - val_loss: 2.0826 - val_accuracy: 0.5267\n","Epoch 11/20\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.7002 - accuracy: 0.5511 - val_loss: 2.0828 - val_accuracy: 0.5157\n","Epoch 12/20\n","1846/1846 [==============================] - 8s 5ms/step - loss: 1.5968 - accuracy: 0.5736 - val_loss: 1.8746 - val_accuracy: 0.5912\n","Epoch 13/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.4962 - accuracy: 0.5979 - val_loss: 1.7942 - val_accuracy: 0.6090\n","Epoch 14/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.4076 - accuracy: 0.6226 - val_loss: 1.7449 - val_accuracy: 0.6128\n","Epoch 15/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.3233 - accuracy: 0.6474 - val_loss: 1.7130 - val_accuracy: 0.6594\n","Epoch 16/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.2613 - accuracy: 0.6709 - val_loss: 1.6334 - val_accuracy: 0.6700\n","Epoch 17/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.2057 - accuracy: 0.6856 - val_loss: 1.6010 - val_accuracy: 0.6706\n","Epoch 18/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.1552 - accuracy: 0.6976 - val_loss: 1.5726 - val_accuracy: 0.6796\n","Epoch 19/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.1119 - accuracy: 0.7069 - val_loss: 1.5500 - val_accuracy: 0.7034\n","Epoch 20/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.0819 - accuracy: 0.7130 - val_loss: 1.4959 - val_accuracy: 0.7298\n","Fold: 2\n","Epoch 1/20\n","1846/1846 [==============================] - 9s 4ms/step - loss: 4.8955 - accuracy: 0.0535 - val_loss: 4.6260 - val_accuracy: 0.0664\n","Epoch 2/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 4.2274 - accuracy: 0.1112 - val_loss: 4.0885 - val_accuracy: 0.1276\n","Epoch 3/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 3.7883 - accuracy: 0.1632 - val_loss: 3.7931 - val_accuracy: 0.1901\n","Epoch 4/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 3.5327 - accuracy: 0.1981 - val_loss: 3.6182 - val_accuracy: 0.2077\n","Epoch 5/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 3.3359 - accuracy: 0.2274 - val_loss: 3.4914 - val_accuracy: 0.2260\n","Epoch 6/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 3.1582 - accuracy: 0.2475 - val_loss: 3.3264 - val_accuracy: 0.2497\n","Epoch 7/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.9966 - accuracy: 0.2767 - val_loss: 3.1962 - val_accuracy: 0.2783\n","Epoch 8/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.8444 - accuracy: 0.3002 - val_loss: 3.0700 - val_accuracy: 0.2913\n","Epoch 9/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.6975 - accuracy: 0.3231 - val_loss: 2.9183 - val_accuracy: 0.3239\n","Epoch 10/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.5572 - accuracy: 0.3531 - val_loss: 2.7955 - val_accuracy: 0.3802\n","Epoch 11/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.4048 - accuracy: 0.3867 - val_loss: 2.6899 - val_accuracy: 0.3846\n","Epoch 12/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.2534 - accuracy: 0.4240 - val_loss: 2.5193 - val_accuracy: 0.4416\n","Epoch 13/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.1042 - accuracy: 0.4539 - val_loss: 2.4078 - val_accuracy: 0.4706\n","Epoch 14/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.9778 - accuracy: 0.4848 - val_loss: 2.3264 - val_accuracy: 0.4772\n","Epoch 15/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.8643 - accuracy: 0.5110 - val_loss: 2.2368 - val_accuracy: 0.5089\n","Epoch 16/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.7630 - accuracy: 0.5366 - val_loss: 2.1145 - val_accuracy: 0.5443\n","Epoch 17/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.6760 - accuracy: 0.5524 - val_loss: 2.0592 - val_accuracy: 0.5604\n","Epoch 18/20\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.5950 - accuracy: 0.5774 - val_loss: 2.0079 - val_accuracy: 0.5553\n","Epoch 19/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.5170 - accuracy: 0.5950 - val_loss: 2.0707 - val_accuracy: 0.5547\n","Epoch 20/20\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.4604 - accuracy: 0.6165 - val_loss: 1.8526 - val_accuracy: 0.6172\n","Average Validation Accuracy: 0.685326337814331\n","Average Validation Loss: 1.366301417350769\n","Average Test Accuracy: 0.6858185231685638\n","Final Test Accuracy for each fold: 0.7406943440437317\n"]}],"source":["# Define the number of folds for k-fold cross-validation\n","k = 2\n","\n","# Define the cross-validation method\n","cv_method = StratifiedKFold(n_splits=k)\n","\n","# Initialize the list to store the history, train & validation(accuracy & loss) for each model\n","models = []\n","model_history = []\n","model_accuracy = []\n","model_train_acc = []\n","model_train_loss = []\n","model_val_acc = []\n","model_val_loss = []\n","\n","\n","for i in range(1,21):\n","\n","    models_fold = []\n","    hist = []\n","    train_accuracy = []\n","    train_loss = []\n","    val_accuracy = []\n","    val_loss = []\n","    test_accuracy = []\n","\n","    print(\"Number of input features:\",i)\n","\n","    # Select the input features from the input data\n","    X_train_selected = X_train[:, :i]\n","    X_test_selected = X_test[:, :i]\n","\n","    # Loop over the folds\n","    for fold, (train_index, val_index) in enumerate(cv_method.split(X_train_selected, y_train)):\n","\n","        print(\"Fold:\", fold+1)\n","\n","        # Split the data into train and validation sets using the current fold index\n","        X_train_fold  = X_train_selected[train_index]\n","        y_train_fold  = y_train[train_index]\n","        X_val_fold = X_train_selected[val_index]\n","        y_val_fold = y_train[val_index]\n","\n","        # Prepare the target data\n","        y_train_fold_enc, y_val_fold_enc = prepare_targets(y_train_fold, y_val_fold)\n","\n","        # build the model\n","        model = RBFN_model(i)\n","\n","        # Fit the model to the training data for the current fold\n","        history = model.fit(X_train_fold, to_categorical(y_train_fold_enc, num_classes=373), epochs=20, batch_size=5, verbose=1, validation_split = 0.33)\n","    \n","        # Evaluate the model on the validation data for the current fold\n","        val_scores = model.evaluate(X_val_fold, to_categorical(y_val_fold_enc, num_classes=373), verbose=0)\n","        val_accuracy.append(val_scores[1])\n","        val_loss.append(val_scores[0])\n","\n","        # Evaluate the model on the test data for the current fold\n","        test_scores = model.evaluate(X_test_selected, to_categorical(y_test_enc, num_classes=373), verbose=0)\n","        test_accuracy.append(test_scores[1])\n","\n","        # add the model to the list of models\n","        models_fold.append(model)\n","        hist.append(history)\n","\n","        # store the training accuracy and loss for each fold\n","        train_accuracy.append(history.history['accuracy'])\n","        train_loss.append(history.history['loss'])\n","        \n","    # Calculate the average test and validation accuracy and loss across all folds\n","    avg_test_acc = sum(test_accuracy) / len(test_accuracy)\n","    avg_val_acc = sum(val_accuracy) / len(val_accuracy)\n","    avg_val_loss = sum(val_loss) / len(val_loss)\n","\n","    # Print the average validation and test accuracy and loss\n","    print(\"Average Validation Accuracy:\", avg_val_acc)\n","    print(\"Average Validation Loss:\",avg_val_loss)\n","    print(\"Average Test Accuracy:\", avg_test_acc)\n","\n","    best_fold_index = test_accuracy.index(max(test_accuracy))\n","    model_accuracy.append(test_accuracy[best_fold_index])\n","    models.append(models_fold[best_fold_index])\n","    model_history.append(hist[best_fold_index])\n","    model_train_acc.append(train_accuracy[best_fold_index])\n","    model_train_loss.append(train_loss[best_fold_index])\n","    model_val_acc.append(val_accuracy[best_fold_index])\n","    model_val_loss.append(val_loss[best_fold_index])\n","\n","\n","    print(\"Final Test Accuracy for each fold:\", test_accuracy[best_fold_index])"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"xzn9CBYTQsYy"},"outputs":[],"source":["#to show the no of input features and its corresponding model accuracy\n","model_list = []\n","\n","#Iterate through each model's accuracy \n","for i in range (len(model_accuracy)):\n","    #get the number of input features for the current model\n","    no_features = i + 1\n","\n","    #round the model accuries to 3 d.p.\n","    rounded_model_acc = round(model_accuracy[i], 3)\n","    \n","    model_list.append([no_features, rounded_model_acc])\n","\n","models_df = pd.DataFrame(model_list, columns=[\"No of input features\", \"Model accuracy\"])"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":72,"status":"ok","timestamp":1682274337709,"user":{"displayName":"Good Notes UM","userId":"12493674840996884929"},"user_tz":-480},"id":"WAjs7sLtQsYz","outputId":"42b8ee3f-cf37-459f-e38e-c3b79cbd580a"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 2 Axes>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Final Test Accuracy: 0.7793911695480347\n"]}],"source":["# Select the final model based on the max test accuracy across all models\n","\n","best_model_index = model_accuracy.index(max(model_accuracy))\n","\n","best_model = models[best_model_index]\n","best_model_history = model_history[best_model_index]\n","best_model_train_acc = model_train_acc[best_model_index]\n","best_model_train_loss = model_train_loss[best_model_index]\n","best_model_val_acc = model_val_acc[best_model_index]\n","best_model_val_loss = model_val_loss[best_model_index]\n","\n","# summarize history for accuracy  \n","plt.subplot(211)  \n","plt.plot(best_model_history.history['accuracy'])  \n","plt.plot(best_model_history.history['val_accuracy'])  \n","plt.title('model accuracy')  \n","plt.ylabel('accuracy')  \n","plt.xlabel('epoch')  \n","plt.legend(['train', 'test'], loc='lower right')  \n","\n","# summarize history for loss\n","plt.subplot(212)  \n","plt.plot(best_model_history.history['loss'])  \n","plt.plot(best_model_history.history['val_loss'])  \n","plt.title('model loss')  \n","plt.ylabel('loss')  \n","plt.xlabel('epoch')  \n","plt.legend(['train', 'test'], loc='upper right')  \n","plt.subplots_adjust(hspace=0.7)\n","\n","plt.show() \n","\n","print(\"Final Test Accuracy:\", model_accuracy[best_model_index])"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1862,"status":"ok","timestamp":1682274339506,"user":{"displayName":"Good Notes UM","userId":"12493674840996884929"},"user_tz":-480},"id":"KfbFjY7JQsY1","outputId":"892dc1e7-c7ee-4897-cf17-ad1d9811caf4"},"outputs":[{"name":"stdout","output_type":"stream","text":["424/424 [==============================] - 1s 2ms/step\n","Classification report:\n","               precision    recall  f1-score   support\n","\n","           0       0.89      1.00      0.94       591\n","           1       0.92      1.00      0.96       430\n","           2       0.98      1.00      0.99       419\n","           3       0.96      1.00      0.98       384\n","           4       0.82      1.00      0.90       339\n","           5       0.99      1.00      1.00       342\n","           6       0.99      1.00      1.00       310\n","           7       1.00      0.96      0.98       325\n","           8       0.64      0.96      0.77       294\n","           9       0.96      0.99      0.97       269\n","          10       1.00      1.00      1.00       296\n","          11       0.89      1.00      0.94       258\n","          12       0.96      1.00      0.98       247\n","          13       0.91      0.99      0.95       237\n","          14       0.88      0.96      0.92       239\n","          15       1.00      1.00      1.00       235\n","          16       0.91      1.00      0.96       213\n","          17       0.94      1.00      0.97       202\n","          18       0.99      0.98      0.99       196\n","          19       0.92      1.00      0.96       181\n","          20       0.98      0.99      0.99       177\n","          21       0.82      1.00      0.90       177\n","          22       0.93      1.00      0.97       155\n","          23       0.95      1.00      0.97       155\n","          24       0.95      0.87      0.91       144\n","          25       0.88      0.90      0.89       126\n","          26       1.00      0.74      0.85       108\n","          27       0.64      0.98      0.77       121\n","          28       0.98      1.00      0.99        95\n","          29       0.79      0.89      0.84       106\n","          30       0.78      1.00      0.88       102\n","          31       0.99      1.00      0.99        86\n","          32       0.64      0.85      0.73       108\n","          33       0.96      0.82      0.88        88\n","          34       0.94      0.64      0.76       102\n","          35       0.85      1.00      0.92        88\n","          36       0.92      1.00      0.96        83\n","          37       0.62      1.00      0.76        93\n","          38       0.84      1.00      0.92        76\n","          39       0.78      0.76      0.77        85\n","          40       0.91      1.00      0.95        86\n","          41       1.00      0.99      0.99        85\n","          42       0.94      0.47      0.63        68\n","          43       0.72      1.00      0.84        75\n","          44       0.66      1.00      0.79        71\n","          45       0.00      0.00      0.00        58\n","          46       1.00      0.37      0.54        71\n","          47       0.29      1.00      0.45        57\n","          48       0.00      0.00      0.00        67\n","          49       0.00      0.00      0.00        47\n","          50       0.90      0.94      0.92        48\n","          51       0.00      0.00      0.00        47\n","          52       0.77      0.84      0.80        43\n","          53       0.75      1.00      0.86        51\n","          54       0.60      1.00      0.75        44\n","          55       0.00      0.00      0.00        51\n","          56       0.61      0.91      0.73        45\n","          57       0.77      1.00      0.87        44\n","          58       0.33      0.98      0.49        41\n","          59       0.38      0.88      0.53        41\n","          60       0.67      0.83      0.74        52\n","          61       0.65      1.00      0.79        43\n","          62       0.64      1.00      0.78        37\n","          63       0.86      1.00      0.92        43\n","          64       0.92      0.81      0.86        42\n","          65       0.87      1.00      0.93        46\n","          66       1.00      0.70      0.82        43\n","          67       0.38      0.95      0.54        40\n","          68       1.00      1.00      1.00        44\n","          69       0.54      0.95      0.69        43\n","          70       0.00      0.00      0.00        38\n","          71       0.38      1.00      0.55        33\n","          72       1.00      0.42      0.59        45\n","          73       0.75      1.00      0.85        38\n","          74       1.00      0.83      0.91        42\n","          75       0.00      0.00      0.00        39\n","          76       0.00      0.00      0.00        30\n","          77       0.30      1.00      0.47        28\n","          78       1.00      0.07      0.13        28\n","          79       1.00      1.00      1.00        32\n","          80       0.97      1.00      0.99        33\n","          81       1.00      0.32      0.49        31\n","          82       0.45      1.00      0.62        35\n","          83       1.00      0.33      0.50        39\n","          84       0.87      1.00      0.93        27\n","          85       1.00      0.06      0.11        36\n","          86       0.65      1.00      0.78        31\n","          87       0.00      0.00      0.00        28\n","          88       1.00      0.35      0.52        20\n","          89       0.46      0.85      0.60        33\n","          90       0.77      1.00      0.87        24\n","          91       0.38      1.00      0.55        22\n","          92       0.46      1.00      0.63        26\n","          93       0.49      1.00      0.65        35\n","          94       0.00      0.00      0.00        27\n","          95       0.74      1.00      0.85        23\n","          96       0.00      0.00      0.00        27\n","          97       0.29      1.00      0.44        28\n","          98       0.40      0.62      0.49        16\n","          99       0.00      0.00      0.00        35\n","         100       0.79      0.82      0.81        28\n","         101       1.00      0.64      0.78        25\n","         102       0.74      0.96      0.83        26\n","         103       1.00      0.30      0.47        33\n","         104       1.00      1.00      1.00        26\n","         105       0.62      0.88      0.72        24\n","         106       0.23      0.36      0.28        22\n","         107       0.60      1.00      0.75        26\n","         108       0.50      1.00      0.67        25\n","         109       0.21      0.62      0.31        16\n","         110       0.00      0.00      0.00        20\n","         111       0.00      0.00      0.00        26\n","         112       0.95      1.00      0.97        18\n","         113       1.00      1.00      1.00        23\n","         114       0.00      0.00      0.00        25\n","         115       1.00      0.94      0.97        18\n","         116       1.00      0.05      0.10        19\n","         117       0.00      0.00      0.00        16\n","         118       0.50      1.00      0.67        26\n","         119       0.00      0.00      0.00        22\n","         120       0.00      0.00      0.00        17\n","         121       0.54      0.47      0.50        15\n","         122       0.54      0.39      0.45        18\n","         123       0.80      1.00      0.89        20\n","         124       0.00      0.00      0.00        14\n","         125       1.00      1.00      1.00        22\n","         126       1.00      0.32      0.48        19\n","         127       0.00      0.00      0.00        27\n","         128       1.00      0.96      0.98        26\n","         129       0.00      0.00      0.00        21\n","         130       0.00      0.00      0.00        18\n","         131       0.56      1.00      0.72        18\n","         132       0.00      0.00      0.00        20\n","         133       0.64      1.00      0.78        14\n","         134       0.26      0.84      0.40        19\n","         135       1.00      0.38      0.55        16\n","         136       0.05      0.04      0.04        23\n","         137       0.85      1.00      0.92        11\n","         138       0.35      1.00      0.52        14\n","         139       0.00      0.00      0.00        20\n","         140       0.21      1.00      0.35        23\n","         141       0.00      0.00      0.00        14\n","         142       0.00      0.00      0.00        13\n","         143       0.00      0.00      0.00        23\n","         144       0.00      0.00      0.00        17\n","         145       0.50      0.29      0.37        24\n","         146       0.00      0.00      0.00        16\n","         147       1.00      0.21      0.35        19\n","         148       0.08      0.14      0.10        22\n","         149       1.00      0.07      0.12        15\n","         150       0.00      0.00      0.00        11\n","         151       0.58      0.74      0.65        19\n","         152       1.00      0.95      0.97        20\n","         153       0.40      1.00      0.57        24\n","         154       0.00      0.00      0.00        11\n","         155       0.00      0.00      0.00        17\n","         156       1.00      1.00      1.00        18\n","         157       0.00      0.00      0.00        12\n","         158       0.00      0.00      0.00        18\n","         159       1.00      0.75      0.86        20\n","         160       0.00      0.00      0.00        20\n","         161       1.00      0.94      0.97        16\n","         162       0.00      0.00      0.00        15\n","         163       1.00      0.47      0.64        15\n","         164       1.00      0.08      0.14        13\n","         165       0.00      0.00      0.00        19\n","         166       0.00      0.00      0.00        11\n","         167       1.00      0.89      0.94         9\n","         168       0.00      0.00      0.00        11\n","         169       0.82      0.67      0.74        21\n","         170       0.00      0.00      0.00        15\n","         171       0.00      0.00      0.00        18\n","         172       0.33      0.09      0.14        11\n","         173       0.00      0.00      0.00        16\n","         174       0.00      0.00      0.00        10\n","         175       0.30      1.00      0.46        11\n","         176       0.00      0.00      0.00        10\n","         177       1.00      0.07      0.12        15\n","         178       1.00      0.73      0.84        11\n","         179       0.68      1.00      0.81        15\n","         180       0.00      0.00      0.00        13\n","         181       0.54      1.00      0.70        15\n","         182       0.00      0.00      0.00         9\n","         183       0.45      0.56      0.50        16\n","         184       0.00      0.00      0.00         8\n","         185       0.00      0.00      0.00        12\n","         186       0.00      0.00      0.00        15\n","         187       0.25      0.20      0.22        15\n","         188       0.00      0.00      0.00        13\n","         189       0.00      0.00      0.00        14\n","         190       0.00      0.00      0.00        11\n","         191       0.38      0.30      0.33        10\n","         192       0.00      0.00      0.00        17\n","         193       0.12      1.00      0.21         9\n","         194       0.00      0.00      0.00         9\n","         195       0.00      0.00      0.00         4\n","         196       0.00      0.00      0.00         7\n","         197       0.07      0.43      0.11         7\n","         198       0.00      0.00      0.00        12\n","         199       1.00      0.29      0.44        14\n","         200       0.00      0.00      0.00         6\n","         201       1.00      0.33      0.50         9\n","         202       0.00      0.00      0.00         7\n","         203       0.00      0.00      0.00         6\n","         204       0.00      0.00      0.00        11\n","         205       0.00      0.00      0.00        14\n","         206       0.00      0.00      0.00        12\n","         207       0.00      0.00      0.00        14\n","         208       0.00      0.00      0.00        12\n","         209       0.00      0.00      0.00         7\n","         210       0.00      0.00      0.00        19\n","         211       0.00      0.00      0.00         7\n","         212       0.00      0.00      0.00        11\n","         213       0.00      0.00      0.00         9\n","         214       0.00      0.00      0.00         7\n","         215       1.00      0.17      0.29         6\n","         216       0.00      0.00      0.00        12\n","         217       0.00      0.00      0.00        12\n","         218       0.10      0.22      0.14         9\n","         219       0.71      0.83      0.77         6\n","         220       0.00      0.00      0.00         8\n","         221       0.00      0.00      0.00         5\n","         222       0.00      0.00      0.00         4\n","         223       0.00      0.00      0.00        14\n","         224       0.00      0.00      0.00        13\n","         225       0.00      0.00      0.00         4\n","         226       0.10      0.30      0.15        10\n","         227       1.00      1.00      1.00        12\n","         228       0.33      0.08      0.12        13\n","         229       0.15      0.82      0.26        11\n","         230       0.00      0.00      0.00         5\n","         231       0.00      0.00      0.00         6\n","         232       0.00      0.00      0.00         8\n","         233       0.77      1.00      0.87        10\n","         234       0.00      0.00      0.00         4\n","         235       0.29      0.62      0.40         8\n","         236       0.00      0.00      0.00         9\n","         237       0.00      0.00      0.00         8\n","         238       0.00      0.00      0.00        10\n","         239       0.00      0.00      0.00         9\n","         240       0.00      0.00      0.00         7\n","         241       0.00      0.00      0.00         8\n","         242       0.00      0.00      0.00         6\n","         243       0.00      0.00      0.00         7\n","         244       0.00      0.00      0.00         9\n","         245       0.00      0.00      0.00         7\n","         246       0.00      0.00      0.00         9\n","         247       0.00      0.00      0.00         7\n","         248       0.00      0.00      0.00        10\n","         249       0.06      0.40      0.11        10\n","         250       0.00      0.00      0.00         7\n","         251       0.00      0.00      0.00         6\n","         252       0.00      0.00      0.00        11\n","         253       0.00      0.00      0.00         7\n","         254       0.00      0.00      0.00         8\n","         255       0.00      0.00      0.00         5\n","         256       0.00      0.00      0.00         7\n","         257       0.25      0.11      0.15         9\n","         258       0.00      0.00      0.00         6\n","         259       0.00      0.00      0.00         3\n","         260       1.00      0.25      0.40         4\n","         261       0.00      0.00      0.00         2\n","         262       0.00      0.00      0.00         5\n","         263       0.00      0.00      0.00        12\n","         264       0.00      0.00      0.00         5\n","         265       0.00      0.00      0.00         7\n","         266       0.00      0.00      0.00        10\n","         267       0.00      0.00      0.00         8\n","         268       1.00      0.56      0.71         9\n","         269       0.00      0.00      0.00         6\n","         270       0.33      0.25      0.29         4\n","         271       0.00      0.00      0.00         7\n","         272       0.00      0.00      0.00        10\n","         273       0.00      0.00      0.00         3\n","         274       0.00      0.00      0.00         9\n","         275       0.00      0.00      0.00         6\n","         276       0.00      0.00      0.00         5\n","         277       0.00      0.00      0.00         4\n","         278       0.00      0.00      0.00         3\n","         279       0.00      0.00      0.00         4\n","         280       0.33      0.60      0.43         5\n","         281       0.00      0.00      0.00         6\n","         282       0.00      0.00      0.00        11\n","         283       0.00      0.00      0.00         6\n","         284       0.00      0.00      0.00         2\n","         285       0.00      0.00      0.00         4\n","         286       0.00      0.00      0.00         7\n","         287       1.00      0.67      0.80         9\n","         288       1.00      0.43      0.60         7\n","         289       0.00      0.00      0.00         7\n","         290       0.00      0.00      0.00         6\n","         291       0.00      0.00      0.00         5\n","         292       0.25      1.00      0.40         6\n","         293       0.00      0.00      0.00         8\n","         294       0.00      0.00      0.00         7\n","         295       0.00      0.00      0.00         3\n","         296       0.00      0.00      0.00         6\n","         297       0.00      0.00      0.00         8\n","         298       0.00      0.00      0.00         4\n","         299       0.00      0.00      0.00         6\n","         300       0.00      0.00      0.00         5\n","         301       0.00      0.00      0.00         5\n","         302       0.00      0.00      0.00         5\n","         303       0.07      1.00      0.14         2\n","         304       0.00      0.00      0.00         8\n","         305       0.00      0.00      0.00         3\n","         306       1.00      0.33      0.50         6\n","         307       0.00      0.00      0.00         7\n","         308       0.00      0.00      0.00         4\n","         309       0.00      0.00      0.00         4\n","         310       0.00      0.00      0.00         9\n","         311       0.00      0.00      0.00         7\n","         312       0.00      0.00      0.00         6\n","         313       0.00      0.00      0.00         6\n","         314       0.00      0.00      0.00         8\n","         315       0.00      0.00      0.00         7\n","         316       0.00      0.00      0.00         3\n","         317       0.00      0.00      0.00         6\n","         318       0.00      0.00      0.00        10\n","         319       0.00      0.00      0.00         6\n","         320       0.00      0.00      0.00         2\n","         321       0.00      0.00      0.00         8\n","         322       0.00      0.00      0.00         4\n","         323       0.00      0.00      0.00         4\n","         324       0.00      0.00      0.00         8\n","         325       0.00      0.00      0.00         4\n","         326       0.00      0.00      0.00         7\n","         327       0.00      0.00      0.00         4\n","         328       0.00      0.00      0.00         6\n","         329       0.00      0.00      0.00         4\n","         330       0.00      0.00      0.00         3\n","         331       0.00      0.00      0.00         8\n","         332       0.00      0.00      0.00         1\n","         333       0.00      0.00      0.00         3\n","         334       0.00      0.00      0.00         4\n","         335       0.00      0.00      0.00         3\n","         336       0.00      0.00      0.00         6\n","         337       0.00      0.00      0.00         2\n","         338       0.00      0.00      0.00         7\n","         339       0.00      0.00      0.00         4\n","         340       0.00      0.00      0.00         6\n","         341       0.00      0.00      0.00         7\n","         342       0.00      0.00      0.00         2\n","         343       0.00      0.00      0.00         5\n","         344       0.00      0.00      0.00         4\n","         345       0.00      0.00      0.00         1\n","         346       0.00      0.00      0.00         2\n","         347       0.00      0.00      0.00         4\n","         348       0.00      0.00      0.00         7\n","         349       0.15      1.00      0.26         4\n","         350       0.00      0.00      0.00         6\n","         351       0.00      0.00      0.00         4\n","         352       0.00      0.00      0.00         5\n","         353       0.00      0.00      0.00         4\n","         354       0.00      0.00      0.00         3\n","         355       0.00      0.00      0.00         1\n","         356       0.00      0.00      0.00         4\n","         357       0.00      0.00      0.00         1\n","         358       0.00      0.00      0.00         3\n","         359       0.22      1.00      0.36         4\n","         360       0.00      0.00      0.00         3\n","         361       0.00      0.00      0.00         3\n","         362       0.00      0.00      0.00         3\n","         363       0.00      0.00      0.00         2\n","         364       0.00      0.00      0.00         3\n","         365       0.00      0.00      0.00         3\n","         366       1.00      1.00      1.00         2\n","         367       0.00      0.00      0.00         3\n","         368       0.00      0.00      0.00         1\n","         369       0.00      0.00      0.00         2\n","         370       0.00      0.00      0.00         2\n","         372       0.00      0.00      0.00         4\n","\n","    accuracy                           0.78     13567\n","   macro avg       0.32      0.35      0.30     13567\n","weighted avg       0.72      0.78      0.73     13567\n","\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["#Select only the optimal number of input features for X_test\n","X_test = X_test[:,:(best_model_index+1)]\n","\n","# Evaluate the best model on the test data\n","y_pred = best_model.predict(X_test)\n","\n","# np.argmax() is used to convert the one-hot encoded predictions and test labels to class labels.\n","y_pred_label = np.argmax(y_pred, axis = 1)\n","\n","# Print the classification report\n","print(\"Classification report:\\n\", classification_report(y_test_enc, y_pred_label))"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1682274341951,"user":{"displayName":"Good Notes UM","userId":"12493674840996884929"},"user_tz":-480},"id":"zbyINa_nQsY4","outputId":"32acb779-1c44-4b3e-e550-95feded30db2"},"outputs":[{"name":"stdout","output_type":"stream","text":["           OsID  True Class  Predicted Class  True/False\n","0  Os04g0475500         328              191       False\n","1  Os04g0659100         161              161        True\n","2  Os01g0686800          17               17        True\n","3  Os04g0107900          34               21       False\n","4  Os04g0517100           7                7        True\n"]}],"source":["# extract class labels from test data\n","class_test = y_test_enc\n","\n","# Invert OsID_labels dictionary\n","inv_OsID_labels = {v: k for k, v in OsID_labels.items()}\n","\n","# map OsID values to the class labels\n","OsID_test = [inv_OsID_labels.get(value, 'Unknown') for value in class_test]\n","\n","# create dataframe with OsID, true class, predicted class, and true/false columns\n","results = pd.DataFrame({\n","    'OsID': OsID_test,\n","    'True Class': y_test_enc,\n","    'Predicted Class': y_pred_label,\n","    'True/False': class_test == y_pred_label\n","})\n","\n","# display dataframe\n","print(results.head())\n","\n","# save results_df to a CSV file\n","results.to_csv('MLP_gene classification.csv', index=False)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":676},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1682274341952,"user":{"displayName":"Good Notes UM","userId":"12493674840996884929"},"user_tz":-480},"id":"l2lvuFxIN3QQ","outputId":"33989bc3-f928-4279-954e-b86af3fc24da"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>No of input features</th>\n","      <th>Model accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0.045</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>0.057</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>0.089</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0.383</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>0.614</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>6</td>\n","      <td>0.746</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>7</td>\n","      <td>0.749</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>8</td>\n","      <td>0.720</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>9</td>\n","      <td>0.644</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>10</td>\n","      <td>0.746</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>11</td>\n","      <td>0.726</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>12</td>\n","      <td>0.721</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>13</td>\n","      <td>0.741</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>14</td>\n","      <td>0.728</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>15</td>\n","      <td>0.779</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>16</td>\n","      <td>0.706</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>17</td>\n","      <td>0.671</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>18</td>\n","      <td>0.738</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>19</td>\n","      <td>0.693</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>20</td>\n","      <td>0.741</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    No of input features  Model accuracy\n","0                      1           0.045\n","1                      2           0.057\n","2                      3           0.089\n","3                      4           0.383\n","4                      5           0.614\n","5                      6           0.746\n","6                      7           0.749\n","7                      8           0.720\n","8                      9           0.644\n","9                     10           0.746\n","10                    11           0.726\n","11                    12           0.721\n","12                    13           0.741\n","13                    14           0.728\n","14                    15           0.779\n","15                    16           0.706\n","16                    17           0.671\n","17                    18           0.738\n","18                    19           0.693\n","19                    20           0.741"]},"metadata":{},"output_type":"display_data"}],"source":["display(models_df)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"},"vscode":{"interpreter":{"hash":"54645332476aec3a1589d49135d9c8280fdb5d7db877f5b7af7a1b58b8f996bc"}}},"nbformat":4,"nbformat_minor":0}

{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"G3AagWU9QsYg"},"outputs":[],"source":["import numpy as np\n","import pandas as pd \n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","\n","from keras.models import Sequential\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","#for RBN\n","from keras.layers import Layer, Flatten, Dense\n","from keras import backend as K\n","from sklearn.metrics import classification_report\n","\n","\n","from sklearn.model_selection import StratifiedKFold\n","from keras import regularizers\n","from tensorflow.keras.utils import to_categorical\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"TAShwIegQsYj"},"outputs":[],"source":["#fixed random seed for reproducibility \n","np.random.seed(0)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":216816,"status":"ok","timestamp":1682268983927,"user":{"displayName":"Good Notes UM","userId":"12493674840996884929"},"user_tz":-480},"id":"GgUSY9c4QsYl","outputId":"7f66c9df-3c26-4a25-c531-903faada4d63"},"outputs":[{"name":"stdout","output_type":"stream","text":["Summary of dataGene:\n","        log_2FoldChange            ET  CoExpression           PCC  \\\n","count     41110.000000  41110.000000  41110.000000  41110.000000   \n","mean         -0.037332      1.407395      0.991997     -0.361737   \n","std           0.391444      0.784327      0.089101      0.463979   \n","min          -1.000000      0.000000      0.000000     -1.000000   \n","25%          -0.251534      1.000000      1.000000     -0.747963   \n","50%           0.030675      2.000000      1.000000     -0.449089   \n","75%           0.251534      2.000000      1.000000     -0.051646   \n","max           1.000000      2.000000      1.000000      1.000000   \n","\n","                PPI  Root10DaysSeedling  Root14DaysSeedling  \\\n","count  41110.000000        41110.000000        41110.000000   \n","mean       0.914668           -0.522040           -0.646982   \n","std        0.279379            0.498568            0.393549   \n","min        0.000000           -1.000000           -1.000000   \n","25%        1.000000           -0.901371           -0.965084   \n","50%        1.000000           -0.663664           -0.680003   \n","75%        1.000000           -0.378497           -0.559627   \n","max        1.000000            1.000000            1.000000   \n","\n","       Root17DaysSeedling  Root21DaysSeedling  Root24DaysSeedling  ...  \\\n","count        41110.000000        41110.000000        41110.000000  ...   \n","mean            -0.700869           -0.669349           -0.670048  ...   \n","std              0.378219            0.405860            0.390751  ...   \n","min             -1.000000           -1.000000           -1.000000  ...   \n","25%             -0.980226           -1.000000           -0.982003  ...   \n","50%             -0.795609           -0.726665           -0.708584  ...   \n","75%             -0.601266           -0.543621           -0.482133  ...   \n","max              1.000000            1.000000            1.000000  ...   \n","\n","       Root52DaysSeedling  Shoot3DaysSeedling  Shoot10DaysSeedling  \\\n","count        41110.000000        41110.000000         41110.000000   \n","mean            -0.670345           -0.590806            -0.545055   \n","std              0.478222            0.443552             0.477438   \n","min             -1.000000           -1.000000            -1.000000   \n","25%             -1.000000           -1.000000            -0.906055   \n","50%             -0.853382           -0.676286            -0.698864   \n","75%             -0.542371           -0.409775            -0.250588   \n","max              1.000000            0.955179             1.000000   \n","\n","       Shoot14DaysSeedling  Shoot17DaysSeedling  Shoot21DaysSeedling  \\\n","count         41110.000000         41110.000000         41110.000000   \n","mean             -0.734141            -0.680810            -0.659443   \n","std               0.413716             0.478189             0.463838   \n","min              -1.000000            -1.000000            -1.000000   \n","25%              -1.000000            -1.000000            -1.000000   \n","50%              -0.924976            -0.954040            -0.874080   \n","75%              -0.513759            -0.420386            -0.440577   \n","max               0.997390             1.000000             1.000000   \n","\n","       Shoot35DaysSeedling  Leaf21DaysSeedling  Leaf45DaysOldPlant  \\\n","count         41110.000000        41110.000000        41110.000000   \n","mean             -0.558906           -0.828778           -0.585144   \n","std               0.506423            0.327542            0.399046   \n","min              -1.000000           -1.000000           -1.000000   \n","25%              -0.962199           -1.000000           -0.901444   \n","50%              -0.699035           -0.951894           -0.643376   \n","75%              -0.352995           -0.883755           -0.451900   \n","max               0.993958            1.000000            1.000000   \n","\n","              class  \n","count  41110.000000  \n","mean      60.092703  \n","std       77.624892  \n","min        1.000000  \n","25%        9.000000  \n","50%       26.000000  \n","75%       78.000000  \n","max      373.000000  \n","\n","[8 rows x 21 columns]\n"]}],"source":["# load dataset (input variables = X, output variables = Y)\n","df = pd.read_csv(\"TrainingData.csv\")\n","\n","#count the number of occurances for each osID\n","OsID_counts = df['OsID'].value_counts()\n","\n","#filter for osIDs that have 10 or more occurances\n","OsID_counts_filtered = OsID_counts[OsID_counts >= 10]\n","\n","#assign a label for each osID \n","OsID_labels = {}\n","class_no = 1\n","for osID in OsID_counts_filtered.index:\n","    OsID_labels[osID] = class_no\n","    class_no +=1\n","\n","#filter the dataset with osID that contain 10 or more occurances\n","dataGene = df[df['OsID'].isin(OsID_counts_filtered.index)]\n","\n","dataGene = dataGene.drop(['Class', 'Trait'],axis=1)\n","\n","# Add a new column 'class' to the filtered dataset\n","dataGene['class'] = dataGene['OsID'].map(OsID_labels)\n","\n","print(\"Summary of dataGene:\\n\",dataGene.describe())\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"ZQjM56LwvG4i"},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of X:\n"," (41110, 20)\n","Shape of Y:\n"," (41110,)\n","Summary of X:\n","        CoExpression           PCC           PPI  Root10DaysSeedling  \\\n","count  41110.000000  41110.000000  41110.000000        41110.000000   \n","mean       0.991997     -0.361737      0.914668           -0.522040   \n","std        0.089101      0.463979      0.279379            0.498568   \n","min        0.000000     -1.000000      0.000000           -1.000000   \n","25%        1.000000     -0.747963      1.000000           -0.901371   \n","50%        1.000000     -0.449089      1.000000           -0.663664   \n","75%        1.000000     -0.051646      1.000000           -0.378497   \n","max        1.000000      1.000000      1.000000            1.000000   \n","\n","       Leaf21DaysSeedling  Leaf45DaysOldPlant  log_2FoldChange            ET  \\\n","count        41110.000000        41110.000000     41110.000000  41110.000000   \n","mean            -0.828778           -0.585144        -0.037332      1.407395   \n","std              0.327542            0.399046         0.391444      0.784327   \n","min             -1.000000           -1.000000        -1.000000      0.000000   \n","25%             -1.000000           -0.901444        -0.251534      1.000000   \n","50%             -0.951894           -0.643376         0.030675      2.000000   \n","75%             -0.883755           -0.451900         0.251534      2.000000   \n","max              1.000000            1.000000         1.000000      2.000000   \n","\n","       Shoot10DaysSeedling  Shoot3DaysSeedling  Shoot35DaysSeedling  \\\n","count         41110.000000        41110.000000         41110.000000   \n","mean             -0.545055           -0.590806            -0.558906   \n","std               0.477438            0.443552             0.506423   \n","min              -1.000000           -1.000000            -1.000000   \n","25%              -0.906055           -1.000000            -0.962199   \n","50%              -0.698864           -0.676286            -0.699035   \n","75%              -0.250588           -0.409775            -0.352995   \n","max               1.000000            0.955179             0.993958   \n","\n","       Shoot14DaysSeedling  Root17DaysSeedling  Shoot17DaysSeedling  \\\n","count         41110.000000        41110.000000         41110.000000   \n","mean             -0.734141           -0.700869            -0.680810   \n","std               0.413716            0.378219             0.478189   \n","min              -1.000000           -1.000000            -1.000000   \n","25%              -1.000000           -0.980226            -1.000000   \n","50%              -0.924976           -0.795609            -0.954040   \n","75%              -0.513759           -0.601266            -0.420386   \n","max               0.997390            1.000000             1.000000   \n","\n","       Shoot21DaysSeedling  Root24DaysSeedling  Root14DaysSeedling  \\\n","count         41110.000000        41110.000000        41110.000000   \n","mean             -0.659443           -0.670048           -0.646982   \n","std               0.463838            0.390751            0.393549   \n","min              -1.000000           -1.000000           -1.000000   \n","25%              -1.000000           -0.982003           -0.965084   \n","50%              -0.874080           -0.708584           -0.680003   \n","75%              -0.440577           -0.482133           -0.559627   \n","max               1.000000            1.000000            1.000000   \n","\n","       Root21DaysSeedling  Root52DaysSeedling  Root35DaysSeedling  \n","count        41110.000000        41110.000000        41110.000000  \n","mean            -0.669349           -0.670345           -0.596196  \n","std              0.405860            0.478222            0.461679  \n","min             -1.000000           -1.000000           -1.000000  \n","25%             -1.000000           -1.000000           -0.937286  \n","50%             -0.726665           -0.853382           -0.769184  \n","75%             -0.543621           -0.542371           -0.323664  \n","max              1.000000            1.000000            1.000000  \n","Summary of Y:\n"," count    41110.000000\n","mean        60.092703\n","std         77.624892\n","min          1.000000\n","25%          9.000000\n","50%         26.000000\n","75%         78.000000\n","max        373.000000\n","Name: class, dtype: float64\n","class\n","1.0      1800\n","2.0      1296\n","3.0      1260\n","4.0      1218\n","5.0      1026\n","         ... \n","369.0      10\n","370.0      10\n","371.0      10\n","372.0      10\n","373.0      10\n","Length: 373, dtype: int64\n"]}],"source":["X = dataGene.drop(['class','OsID'], axis=1) # exclude class & OsID column\n","Y = dataGene['class']\n","\n","#input feature names in order of descending importance scores in MI feature selection method\n","feature_names = ['CoExpression', 'PCC', 'PPI', 'Root10DaysSeedling', 'Leaf21DaysSeedling', \n","                 'Leaf45DaysOldPlant', 'log_2FoldChange', 'ET', 'Shoot10DaysSeedling', 'Shoot3DaysSeedling', \n","                 'Shoot35DaysSeedling', 'Shoot14DaysSeedling', 'Root17DaysSeedling', 'Shoot17DaysSeedling', 'Shoot21DaysSeedling', \n","                 'Root24DaysSeedling', 'Root14DaysSeedling', 'Root21DaysSeedling', 'Root52DaysSeedling', 'Root35DaysSeedling']\n","\n","X_fs = X.reindex(columns=feature_names)\n","\n","print(\"Shape of X:\\n\",X_fs.shape)\n","print(\"Shape of Y:\\n\",Y.shape)\n","\n","# Statistical summary of the variables\n","print(\"Summary of X:\\n\",X_fs.describe())\n","print(\"Summary of Y:\\n\",Y.describe())\n","\n","# Check for class imbalance\n","print(df.groupby(Y).size())\n","\n","# change both input and target variables datatype to ndarray\n","X_fs = X_fs.values # 2-D array\n","\n","# select target variable \n","\n","Y = Y.values #1-D array\n","Y = Y.reshape(-1, 1)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"F2UQyOKPvMXF"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"name":"stdout","output_type":"stream","text":["Class=0, n=1800 (4.378%)\n","Class=1, n=1296 (3.153%)\n","Class=2, n=1260 (3.065%)\n","Class=3, n=1218 (2.963%)\n","Class=4, n=1026 (2.496%)\n","Class=5, n=1008 (2.452%)\n","Class=6, n=930 (2.262%)\n","Class=7, n=912 (2.218%)\n","Class=8, n=880 (2.141%)\n","Class=9, n=798 (1.941%)\n","Class=10, n=792 (1.927%)\n","Class=11, n=759 (1.846%)\n","Class=12, n=729 (1.773%)\n","Class=13, n=720 (1.751%)\n","Class=14, n=702 (1.708%)\n","Class=15, n=693 (1.686%)\n","Class=16, n=672 (1.635%)\n","Class=17, n=640 (1.557%)\n","Class=18, n=625 (1.520%)\n","Class=19, n=570 (1.387%)\n","Class=20, n=546 (1.328%)\n","Class=21, n=506 (1.231%)\n","Class=22, n=483 (1.175%)\n","Class=23, n=448 (1.090%)\n","Class=24, n=432 (1.051%)\n","Class=25, n=384 (0.934%)\n","Class=26, n=360 (0.876%)\n","Class=27, n=360 (0.876%)\n","Class=28, n=320 (0.778%)\n","Class=29, n=312 (0.759%)\n","Class=30, n=312 (0.759%)\n","Class=31, n=306 (0.744%)\n","Class=32, n=304 (0.739%)\n","Class=33, n=299 (0.727%)\n","Class=34, n=297 (0.722%)\n","Class=35, n=296 (0.720%)\n","Class=36, n=280 (0.681%)\n","Class=37, n=264 (0.642%)\n","Class=38, n=260 (0.632%)\n","Class=39, n=253 (0.615%)\n","Class=40, n=252 (0.613%)\n","Class=41, n=248 (0.603%)\n","Class=42, n=242 (0.589%)\n","Class=43, n=228 (0.555%)\n","Class=44, n=216 (0.525%)\n","Class=45, n=210 (0.511%)\n","Class=46, n=200 (0.486%)\n","Class=47, n=192 (0.467%)\n","Class=48, n=180 (0.438%)\n","Class=49, n=171 (0.416%)\n","Class=50, n=168 (0.409%)\n","Class=51, n=168 (0.409%)\n","Class=52, n=162 (0.394%)\n","Class=53, n=150 (0.365%)\n","Class=54, n=148 (0.360%)\n","Class=55, n=138 (0.336%)\n","Class=56, n=135 (0.328%)\n","Class=57, n=135 (0.328%)\n","Class=58, n=133 (0.324%)\n","Class=59, n=132 (0.321%)\n","Class=60, n=132 (0.321%)\n","Class=61, n=130 (0.316%)\n","Class=62, n=130 (0.316%)\n","Class=63, n=130 (0.316%)\n","Class=64, n=128 (0.311%)\n","Class=65, n=128 (0.311%)\n","Class=66, n=126 (0.306%)\n","Class=67, n=124 (0.302%)\n","Class=68, n=124 (0.302%)\n","Class=69, n=124 (0.302%)\n","Class=70, n=120 (0.292%)\n","Class=71, n=120 (0.292%)\n","Class=72, n=118 (0.287%)\n","Class=73, n=116 (0.282%)\n","Class=74, n=114 (0.277%)\n","Class=75, n=105 (0.255%)\n","Class=76, n=104 (0.253%)\n","Class=77, n=102 (0.248%)\n","Class=78, n=99 (0.241%)\n","Class=79, n=98 (0.238%)\n","Class=80, n=98 (0.238%)\n","Class=81, n=98 (0.238%)\n","Class=82, n=98 (0.238%)\n","Class=83, n=96 (0.234%)\n","Class=84, n=96 (0.234%)\n","Class=85, n=96 (0.234%)\n","Class=86, n=93 (0.226%)\n","Class=87, n=92 (0.224%)\n","Class=88, n=92 (0.224%)\n","Class=89, n=91 (0.221%)\n","Class=90, n=88 (0.214%)\n","Class=91, n=88 (0.214%)\n","Class=92, n=86 (0.209%)\n","Class=93, n=86 (0.209%)\n","Class=94, n=84 (0.204%)\n","Class=95, n=84 (0.204%)\n","Class=96, n=84 (0.204%)\n","Class=97, n=78 (0.190%)\n","Class=98, n=78 (0.190%)\n","Class=99, n=76 (0.185%)\n","Class=100, n=75 (0.182%)\n","Class=101, n=75 (0.182%)\n","Class=102, n=73 (0.178%)\n","Class=103, n=72 (0.175%)\n","Class=104, n=72 (0.175%)\n","Class=105, n=70 (0.170%)\n","Class=106, n=69 (0.168%)\n","Class=107, n=68 (0.165%)\n","Class=108, n=67 (0.163%)\n","Class=109, n=66 (0.161%)\n","Class=110, n=66 (0.161%)\n","Class=111, n=66 (0.161%)\n","Class=112, n=66 (0.161%)\n","Class=113, n=66 (0.161%)\n","Class=114, n=65 (0.158%)\n","Class=115, n=64 (0.156%)\n","Class=116, n=63 (0.153%)\n","Class=117, n=63 (0.153%)\n","Class=118, n=62 (0.151%)\n","Class=119, n=61 (0.148%)\n","Class=120, n=60 (0.146%)\n","Class=121, n=60 (0.146%)\n","Class=122, n=60 (0.146%)\n","Class=123, n=60 (0.146%)\n","Class=124, n=60 (0.146%)\n","Class=125, n=60 (0.146%)\n","Class=126, n=60 (0.146%)\n","Class=127, n=60 (0.146%)\n","Class=128, n=60 (0.146%)\n","Class=129, n=60 (0.146%)\n","Class=130, n=59 (0.144%)\n","Class=131, n=59 (0.144%)\n","Class=132, n=58 (0.141%)\n","Class=133, n=56 (0.136%)\n","Class=134, n=56 (0.136%)\n","Class=135, n=56 (0.136%)\n","Class=136, n=56 (0.136%)\n","Class=137, n=56 (0.136%)\n","Class=138, n=56 (0.136%)\n","Class=139, n=56 (0.136%)\n","Class=140, n=56 (0.136%)\n","Class=141, n=56 (0.136%)\n","Class=142, n=55 (0.134%)\n","Class=143, n=55 (0.134%)\n","Class=144, n=54 (0.131%)\n","Class=145, n=54 (0.131%)\n","Class=146, n=54 (0.131%)\n","Class=147, n=54 (0.131%)\n","Class=148, n=54 (0.131%)\n","Class=149, n=53 (0.129%)\n","Class=150, n=52 (0.126%)\n","Class=151, n=52 (0.126%)\n","Class=152, n=52 (0.126%)\n","Class=153, n=52 (0.126%)\n","Class=154, n=50 (0.122%)\n","Class=155, n=50 (0.122%)\n","Class=156, n=49 (0.119%)\n","Class=157, n=49 (0.119%)\n","Class=158, n=48 (0.117%)\n","Class=159, n=48 (0.117%)\n","Class=160, n=48 (0.117%)\n","Class=161, n=46 (0.112%)\n","Class=162, n=45 (0.109%)\n","Class=163, n=44 (0.107%)\n","Class=164, n=44 (0.107%)\n","Class=165, n=44 (0.107%)\n","Class=166, n=42 (0.102%)\n","Class=167, n=42 (0.102%)\n","Class=168, n=42 (0.102%)\n","Class=169, n=42 (0.102%)\n","Class=170, n=42 (0.102%)\n","Class=171, n=42 (0.102%)\n","Class=172, n=42 (0.102%)\n","Class=173, n=41 (0.100%)\n","Class=174, n=41 (0.100%)\n","Class=175, n=40 (0.097%)\n","Class=176, n=40 (0.097%)\n","Class=177, n=39 (0.095%)\n","Class=178, n=39 (0.095%)\n","Class=179, n=38 (0.092%)\n","Class=180, n=37 (0.090%)\n","Class=181, n=36 (0.088%)\n","Class=182, n=35 (0.085%)\n","Class=183, n=35 (0.085%)\n","Class=184, n=35 (0.085%)\n","Class=185, n=35 (0.085%)\n","Class=186, n=34 (0.083%)\n","Class=187, n=34 (0.083%)\n","Class=188, n=34 (0.083%)\n","Class=189, n=34 (0.083%)\n","Class=190, n=32 (0.078%)\n","Class=191, n=32 (0.078%)\n","Class=192, n=32 (0.078%)\n","Class=193, n=32 (0.078%)\n","Class=194, n=32 (0.078%)\n","Class=195, n=32 (0.078%)\n","Class=196, n=31 (0.075%)\n","Class=197, n=31 (0.075%)\n","Class=198, n=31 (0.075%)\n","Class=199, n=31 (0.075%)\n","Class=200, n=30 (0.073%)\n","Class=201, n=30 (0.073%)\n","Class=202, n=30 (0.073%)\n","Class=203, n=30 (0.073%)\n","Class=204, n=30 (0.073%)\n","Class=205, n=30 (0.073%)\n","Class=206, n=30 (0.073%)\n","Class=207, n=30 (0.073%)\n","Class=208, n=30 (0.073%)\n","Class=209, n=29 (0.071%)\n","Class=210, n=29 (0.071%)\n","Class=211, n=28 (0.068%)\n","Class=212, n=28 (0.068%)\n","Class=213, n=28 (0.068%)\n","Class=214, n=28 (0.068%)\n","Class=215, n=28 (0.068%)\n","Class=216, n=28 (0.068%)\n","Class=217, n=27 (0.066%)\n","Class=218, n=27 (0.066%)\n","Class=219, n=27 (0.066%)\n","Class=220, n=27 (0.066%)\n","Class=221, n=27 (0.066%)\n","Class=222, n=27 (0.066%)\n","Class=223, n=26 (0.063%)\n","Class=224, n=26 (0.063%)\n","Class=225, n=26 (0.063%)\n","Class=226, n=26 (0.063%)\n","Class=227, n=26 (0.063%)\n","Class=228, n=25 (0.061%)\n","Class=229, n=25 (0.061%)\n","Class=230, n=25 (0.061%)\n","Class=231, n=25 (0.061%)\n","Class=232, n=24 (0.058%)\n","Class=233, n=24 (0.058%)\n","Class=234, n=24 (0.058%)\n","Class=235, n=24 (0.058%)\n","Class=236, n=24 (0.058%)\n","Class=237, n=24 (0.058%)\n","Class=238, n=24 (0.058%)\n","Class=239, n=24 (0.058%)\n","Class=240, n=24 (0.058%)\n","Class=241, n=24 (0.058%)\n","Class=242, n=24 (0.058%)\n","Class=243, n=24 (0.058%)\n","Class=244, n=23 (0.056%)\n","Class=245, n=23 (0.056%)\n","Class=246, n=22 (0.054%)\n","Class=247, n=22 (0.054%)\n","Class=248, n=22 (0.054%)\n","Class=249, n=22 (0.054%)\n","Class=250, n=22 (0.054%)\n","Class=251, n=22 (0.054%)\n","Class=252, n=22 (0.054%)\n","Class=253, n=22 (0.054%)\n","Class=254, n=22 (0.054%)\n","Class=255, n=22 (0.054%)\n","Class=256, n=22 (0.054%)\n","Class=257, n=22 (0.054%)\n","Class=258, n=22 (0.054%)\n","Class=259, n=22 (0.054%)\n","Class=260, n=22 (0.054%)\n","Class=261, n=22 (0.054%)\n","Class=262, n=22 (0.054%)\n","Class=263, n=22 (0.054%)\n","Class=264, n=21 (0.051%)\n","Class=265, n=21 (0.051%)\n","Class=266, n=21 (0.051%)\n","Class=267, n=21 (0.051%)\n","Class=268, n=21 (0.051%)\n","Class=269, n=20 (0.049%)\n","Class=270, n=20 (0.049%)\n","Class=271, n=20 (0.049%)\n","Class=272, n=20 (0.049%)\n","Class=273, n=20 (0.049%)\n","Class=274, n=20 (0.049%)\n","Class=275, n=20 (0.049%)\n","Class=276, n=20 (0.049%)\n","Class=277, n=20 (0.049%)\n","Class=278, n=20 (0.049%)\n","Class=279, n=20 (0.049%)\n","Class=280, n=19 (0.046%)\n","Class=281, n=19 (0.046%)\n","Class=282, n=19 (0.046%)\n","Class=283, n=18 (0.044%)\n","Class=284, n=18 (0.044%)\n","Class=285, n=18 (0.044%)\n","Class=286, n=18 (0.044%)\n","Class=287, n=18 (0.044%)\n","Class=288, n=18 (0.044%)\n","Class=289, n=18 (0.044%)\n","Class=290, n=18 (0.044%)\n","Class=291, n=18 (0.044%)\n","Class=292, n=17 (0.041%)\n","Class=293, n=17 (0.041%)\n","Class=294, n=17 (0.041%)\n","Class=295, n=17 (0.041%)\n","Class=296, n=17 (0.041%)\n","Class=297, n=17 (0.041%)\n","Class=298, n=16 (0.039%)\n","Class=299, n=16 (0.039%)\n","Class=300, n=16 (0.039%)\n","Class=301, n=16 (0.039%)\n","Class=302, n=16 (0.039%)\n","Class=303, n=16 (0.039%)\n","Class=304, n=16 (0.039%)\n","Class=305, n=16 (0.039%)\n","Class=306, n=15 (0.036%)\n","Class=307, n=15 (0.036%)\n","Class=308, n=15 (0.036%)\n","Class=309, n=15 (0.036%)\n","Class=310, n=15 (0.036%)\n","Class=311, n=14 (0.034%)\n","Class=312, n=14 (0.034%)\n","Class=313, n=14 (0.034%)\n","Class=314, n=14 (0.034%)\n","Class=315, n=14 (0.034%)\n","Class=316, n=14 (0.034%)\n","Class=317, n=14 (0.034%)\n","Class=318, n=14 (0.034%)\n","Class=319, n=14 (0.034%)\n","Class=320, n=14 (0.034%)\n","Class=321, n=14 (0.034%)\n","Class=322, n=14 (0.034%)\n","Class=323, n=14 (0.034%)\n","Class=324, n=14 (0.034%)\n","Class=325, n=14 (0.034%)\n","Class=326, n=14 (0.034%)\n","Class=327, n=14 (0.034%)\n","Class=328, n=13 (0.032%)\n","Class=329, n=13 (0.032%)\n","Class=330, n=13 (0.032%)\n","Class=331, n=13 (0.032%)\n","Class=332, n=13 (0.032%)\n","Class=333, n=13 (0.032%)\n","Class=334, n=13 (0.032%)\n","Class=335, n=13 (0.032%)\n","Class=336, n=13 (0.032%)\n","Class=337, n=12 (0.029%)\n","Class=338, n=12 (0.029%)\n","Class=339, n=12 (0.029%)\n","Class=340, n=12 (0.029%)\n","Class=341, n=12 (0.029%)\n","Class=342, n=12 (0.029%)\n","Class=343, n=12 (0.029%)\n","Class=344, n=12 (0.029%)\n","Class=345, n=12 (0.029%)\n","Class=346, n=12 (0.029%)\n","Class=347, n=12 (0.029%)\n","Class=348, n=12 (0.029%)\n","Class=349, n=12 (0.029%)\n","Class=350, n=12 (0.029%)\n","Class=351, n=12 (0.029%)\n","Class=352, n=12 (0.029%)\n","Class=353, n=12 (0.029%)\n","Class=354, n=12 (0.029%)\n","Class=355, n=11 (0.027%)\n","Class=356, n=11 (0.027%)\n","Class=357, n=11 (0.027%)\n","Class=358, n=11 (0.027%)\n","Class=359, n=11 (0.027%)\n","Class=360, n=11 (0.027%)\n","Class=361, n=10 (0.024%)\n","Class=362, n=10 (0.024%)\n","Class=363, n=10 (0.024%)\n","Class=364, n=10 (0.024%)\n","Class=365, n=10 (0.024%)\n","Class=366, n=10 (0.024%)\n","Class=367, n=10 (0.024%)\n","Class=368, n=10 (0.024%)\n","Class=369, n=10 (0.024%)\n","Class=370, n=10 (0.024%)\n","Class=371, n=10 (0.024%)\n","Class=372, n=10 (0.024%)\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["y_labelenc = LabelEncoder().fit_transform(Y)\n","\n","from collections import Counter\n","# summarize distribution\n","counter = Counter(y_labelenc)\n","\n","# sort counter by keys\n","counter = dict(sorted(counter.items()))\n","\n","\n","for k,v in counter.items():\n"," per = v / len(y_labelenc) * 100\n"," print('Class=%d, n=%d (%.3f%%)' % (k, v, per))\n","# plot the distribution\n","plt.bar( counter.keys(), counter.values())\n","\n","plt.ylabel('No of gene samples')\n","plt.xlabel('Class Index of OsID')\n","plt.title('Distribution of Target Classes (OsID)')\n","plt.show()"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"SyCZMH9wvSyC"},"outputs":[],"source":["# prepare target data\n","def prepare_targets(y_train, y_test):\n","\tle = LabelEncoder()\n","\t\n","\t#fit the encoders only to the training data and then transform both train and test data\n","\ty_train_enc = le.fit_transform(y_train)\n","\ty_test_enc = le.transform(y_test)\n","\n","\treturn y_train_enc, y_test_enc"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"wZNqq1dZQsYs"},"outputs":[],"source":["# define the RBFLayer layer as a custom layer\n","class RBFLayer(Layer):\n","    def __init__(self, units, gamma, **kwargs):\n","        super(RBFLayer, self).__init__(**kwargs)\n","        self.units = units\n","        self.gamma = K.cast_to_floatx(gamma)\n","\n","    def build(self, input_shape):\n","        self.mu = self.add_weight(name='mu',\n","                                  shape=(int(input_shape[1]), self.units),\n","                                  initializer='uniform',\n","                                  trainable=True)\n","        super(RBFLayer, self).build(input_shape)\n","\n","    def call(self, inputs):\n","        diff = K.expand_dims(inputs) - self.mu\n","        l2 = K.sum(K.pow(diff, 2), axis=1)\n","        res = K.exp(-1 * self.gamma * l2)\n","        return res\n","\n","    def compute_output_shape(self, input_shape):\n","        return (input_shape[0], self.units)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"0AqXG7_jQsYu"},"outputs":[],"source":["# prepare target data\n","def prepare_targets(y_train, y_test):\n","\tle = LabelEncoder()\n","\t\n","\t#fit the encoders only to the training data and then transform both train and test data\n","\ty_train_enc = le.fit_transform(y_train)\n","\ty_test_enc = le.transform(y_test)\n","\n","\treturn y_train_enc, y_test_enc"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"Nh4EHT34QsYv"},"outputs":[],"source":["# define baseline model (RBFN)\n","def RBFN_model(input_dim):\n","\n","    model = Sequential()\n","    model.add(Flatten(input_shape=(input_dim,)))\n","    #add the RBF layer\n","    model.add(RBFLayer(10, 0.5))\n","    \n","    model.add(Dense(60, input_dim=input_dim, activation='relu',bias_initializer='normal',kernel_regularizer=regularizers.l2(0.0001)))\n","    model.add(Dense(20,bias_initializer='normal',activation='relu',kernel_regularizer=regularizers.l2(0.0001)))\n","    model.add(Dense(20,bias_initializer='normal',activation='relu',kernel_regularizer=regularizers.l2(0.0001)))\n","    model.add(Dense(373,kernel_initializer='normal',activation='softmax'))\n","\n","    # Compile model\n","    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    \n","    return model"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"qGxBoDFmQsYw"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]}],"source":["# split into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X_fs, Y, test_size=0.33, random_state=1)\n","\n","# prepare output data\n","y_train_enc, y_test_enc = prepare_targets(y_train, y_test)\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5353788,"status":"ok","timestamp":1682274337707,"user":{"displayName":"Good Notes UM","userId":"12493674840996884929"},"user_tz":-480},"id":"21D3eb-NQsYx","outputId":"3cecf9fd-904d-4bc3-a803-fc2a3b30075d"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"name":"stdout","output_type":"stream","text":["Number of input features: 1\n","Fold: 1\n","Epoch 1/40\n","1846/1846 [==============================] - 12s 5ms/step - loss: 5.1042 - accuracy: 0.0403 - val_loss: 5.0441 - val_accuracy: 0.0436\n","Epoch 2/40\n","1846/1846 [==============================] - 8s 5ms/step - loss: 4.9823 - accuracy: 0.0438 - val_loss: 5.0351 - val_accuracy: 0.0436\n","Epoch 3/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.9749 - accuracy: 0.0440 - val_loss: 5.0359 - val_accuracy: 0.0436\n","Epoch 4/40\n","1846/1846 [==============================] - 12s 6ms/step - loss: 4.9713 - accuracy: 0.0440 - val_loss: 5.0351 - val_accuracy: 0.0436\n","Epoch 5/40\n","1846/1846 [==============================] - 12s 7ms/step - loss: 4.9688 - accuracy: 0.0440 - val_loss: 5.0338 - val_accuracy: 0.0436\n","Epoch 6/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.9666 - accuracy: 0.0440 - val_loss: 5.0352 - val_accuracy: 0.0436\n","Epoch 7/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.9605 - accuracy: 0.0440 - val_loss: 5.0292 - val_accuracy: 0.0436\n","Epoch 8/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.9546 - accuracy: 0.0440 - val_loss: 5.0272 - val_accuracy: 0.0436\n","Epoch 9/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.9480 - accuracy: 0.0449 - val_loss: 5.0224 - val_accuracy: 0.0451\n","Epoch 10/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.9368 - accuracy: 0.0454 - val_loss: 5.0117 - val_accuracy: 0.0451\n","Epoch 11/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.9272 - accuracy: 0.0454 - val_loss: 5.0313 - val_accuracy: 0.0451\n","Epoch 12/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.9249 - accuracy: 0.0454 - val_loss: 5.0102 - val_accuracy: 0.0451\n","Epoch 13/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.9239 - accuracy: 0.0454 - val_loss: 5.0112 - val_accuracy: 0.0451\n","Epoch 14/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.9218 - accuracy: 0.0452 - val_loss: 5.0120 - val_accuracy: 0.0451\n","Epoch 15/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.9216 - accuracy: 0.0452 - val_loss: 5.0117 - val_accuracy: 0.0451\n","Epoch 16/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.9203 - accuracy: 0.0454 - val_loss: 5.0163 - val_accuracy: 0.0451\n","Epoch 17/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.9203 - accuracy: 0.0453 - val_loss: 5.0204 - val_accuracy: 0.0451\n","Epoch 18/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.9184 - accuracy: 0.0454 - val_loss: 5.0133 - val_accuracy: 0.0451\n","Epoch 19/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.9186 - accuracy: 0.0454 - val_loss: 5.0201 - val_accuracy: 0.0451\n","Epoch 20/40\n","1846/1846 [==============================] - 12s 7ms/step - loss: 4.9186 - accuracy: 0.0451 - val_loss: 5.0149 - val_accuracy: 0.0451\n","Epoch 21/40\n","1846/1846 [==============================] - 13s 7ms/step - loss: 4.9174 - accuracy: 0.0452 - val_loss: 5.0154 - val_accuracy: 0.0451\n","Epoch 22/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.9167 - accuracy: 0.0451 - val_loss: 5.0166 - val_accuracy: 0.0451\n","Epoch 23/40\n","1846/1846 [==============================] - 12s 6ms/step - loss: 4.9170 - accuracy: 0.0453 - val_loss: 5.0116 - val_accuracy: 0.0451\n","Epoch 24/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.9161 - accuracy: 0.0454 - val_loss: 5.0132 - val_accuracy: 0.0451\n","Epoch 25/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.9163 - accuracy: 0.0453 - val_loss: 5.0149 - val_accuracy: 0.0451\n","Epoch 26/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.9154 - accuracy: 0.0451 - val_loss: 5.0148 - val_accuracy: 0.0451\n","Epoch 27/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.9151 - accuracy: 0.0453 - val_loss: 5.0149 - val_accuracy: 0.0451\n","Epoch 28/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.9149 - accuracy: 0.0454 - val_loss: 5.0130 - val_accuracy: 0.0451\n","Epoch 29/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.9143 - accuracy: 0.0452 - val_loss: 5.0147 - val_accuracy: 0.0451\n","Epoch 30/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 4.9145 - accuracy: 0.0454 - val_loss: 5.0160 - val_accuracy: 0.0451\n","Epoch 31/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 4.9140 - accuracy: 0.0452 - val_loss: 5.0145 - val_accuracy: 0.0451\n","Epoch 32/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.9145 - accuracy: 0.0450 - val_loss: 5.0139 - val_accuracy: 0.0451\n","Epoch 33/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 4.9135 - accuracy: 0.0454 - val_loss: 5.0162 - val_accuracy: 0.0451\n","Epoch 34/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 4.9133 - accuracy: 0.0449 - val_loss: 5.0192 - val_accuracy: 0.0451\n","Epoch 35/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 4.9134 - accuracy: 0.0454 - val_loss: 5.0147 - val_accuracy: 0.0451\n","Epoch 36/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 4.9138 - accuracy: 0.0452 - val_loss: 5.0169 - val_accuracy: 0.0451\n","Epoch 37/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 4.9130 - accuracy: 0.0453 - val_loss: 5.0173 - val_accuracy: 0.0451\n","Epoch 38/40\n","1846/1846 [==============================] - 8s 5ms/step - loss: 4.9125 - accuracy: 0.0452 - val_loss: 5.0290 - val_accuracy: 0.0451\n","Epoch 39/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.9133 - accuracy: 0.0451 - val_loss: 5.0197 - val_accuracy: 0.0451\n","Epoch 40/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.9127 - accuracy: 0.0454 - val_loss: 5.0189 - val_accuracy: 0.0451\n","Fold: 2\n","Epoch 1/40\n","1846/1846 [==============================] - 12s 5ms/step - loss: 5.0913 - accuracy: 0.0415 - val_loss: 5.0599 - val_accuracy: 0.0440\n","Epoch 2/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.9844 - accuracy: 0.0439 - val_loss: 5.0546 - val_accuracy: 0.0440\n","Epoch 3/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.9772 - accuracy: 0.0435 - val_loss: 5.0514 - val_accuracy: 0.0440\n","Epoch 4/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.9738 - accuracy: 0.0439 - val_loss: 5.0637 - val_accuracy: 0.0440\n","Epoch 5/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.9705 - accuracy: 0.0439 - val_loss: 5.0482 - val_accuracy: 0.0440\n","Epoch 6/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.9684 - accuracy: 0.0439 - val_loss: 5.0528 - val_accuracy: 0.0440\n","Epoch 7/40\n","1846/1846 [==============================] - 8s 5ms/step - loss: 4.9664 - accuracy: 0.0439 - val_loss: 5.0473 - val_accuracy: 0.0440\n","Epoch 8/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.9629 - accuracy: 0.0439 - val_loss: 5.0456 - val_accuracy: 0.0440\n","Epoch 9/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.9613 - accuracy: 0.0439 - val_loss: 5.0479 - val_accuracy: 0.0440\n","Epoch 10/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.9547 - accuracy: 0.0439 - val_loss: 5.0301 - val_accuracy: 0.0440\n","Epoch 11/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.9467 - accuracy: 0.0444 - val_loss: 5.0307 - val_accuracy: 0.0460\n","Epoch 12/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.9367 - accuracy: 0.0451 - val_loss: 5.0341 - val_accuracy: 0.0460\n","Epoch 13/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.9314 - accuracy: 0.0451 - val_loss: 5.0173 - val_accuracy: 0.0460\n","Epoch 14/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.9290 - accuracy: 0.0451 - val_loss: 5.0120 - val_accuracy: 0.0460\n","Epoch 15/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.9268 - accuracy: 0.0449 - val_loss: 5.0139 - val_accuracy: 0.0460\n","Epoch 16/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.9260 - accuracy: 0.0451 - val_loss: 5.0115 - val_accuracy: 0.0460\n","Epoch 17/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.9256 - accuracy: 0.0449 - val_loss: 5.0132 - val_accuracy: 0.0460\n","Epoch 18/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.9248 - accuracy: 0.0447 - val_loss: 5.0150 - val_accuracy: 0.0460\n","Epoch 19/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.9239 - accuracy: 0.0451 - val_loss: 5.0105 - val_accuracy: 0.0460\n","Epoch 20/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.9232 - accuracy: 0.0450 - val_loss: 5.0063 - val_accuracy: 0.0460\n","Epoch 21/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.9228 - accuracy: 0.0451 - val_loss: 5.0105 - val_accuracy: 0.0460\n","Epoch 22/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.9229 - accuracy: 0.0445 - val_loss: 5.0107 - val_accuracy: 0.0460\n","Epoch 23/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.9223 - accuracy: 0.0449 - val_loss: 5.0077 - val_accuracy: 0.0460\n","Epoch 24/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.9222 - accuracy: 0.0449 - val_loss: 5.0067 - val_accuracy: 0.0460\n","Epoch 25/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.9216 - accuracy: 0.0451 - val_loss: 5.0064 - val_accuracy: 0.0460\n","Epoch 26/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.9211 - accuracy: 0.0451 - val_loss: 5.0068 - val_accuracy: 0.0460\n","Epoch 27/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.9210 - accuracy: 0.0451 - val_loss: 5.0080 - val_accuracy: 0.0460\n","Epoch 28/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.9203 - accuracy: 0.0451 - val_loss: 5.0086 - val_accuracy: 0.0460\n","Epoch 29/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.9199 - accuracy: 0.0449 - val_loss: 5.0113 - val_accuracy: 0.0460\n","Epoch 30/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.9203 - accuracy: 0.0449 - val_loss: 5.0083 - val_accuracy: 0.0460\n","Epoch 31/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.9197 - accuracy: 0.0451 - val_loss: 5.0070 - val_accuracy: 0.0460\n","Epoch 32/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.9197 - accuracy: 0.0451 - val_loss: 5.0096 - val_accuracy: 0.0460\n","Epoch 33/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.9197 - accuracy: 0.0451 - val_loss: 5.0079 - val_accuracy: 0.0460\n","Epoch 34/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.9185 - accuracy: 0.0450 - val_loss: 5.0092 - val_accuracy: 0.0460\n","Epoch 35/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.9195 - accuracy: 0.0451 - val_loss: 5.0064 - val_accuracy: 0.0460\n","Epoch 36/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.9188 - accuracy: 0.0449 - val_loss: 5.0061 - val_accuracy: 0.0460\n","Epoch 37/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.9188 - accuracy: 0.0448 - val_loss: 5.0105 - val_accuracy: 0.0460\n","Epoch 38/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.9185 - accuracy: 0.0451 - val_loss: 5.0097 - val_accuracy: 0.0460\n","Epoch 39/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.9186 - accuracy: 0.0451 - val_loss: 5.0087 - val_accuracy: 0.0460\n","Epoch 40/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.9185 - accuracy: 0.0448 - val_loss: 5.0103 - val_accuracy: 0.0460\n","Average Validation Accuracy: 0.04534727334976196\n","Average Validation Loss: 4.942248821258545\n","Average Test Accuracy: 0.045035749673843384\n","Final Test Accuracy for each fold: 0.045035749673843384\n","Number of input features: 2\n","Fold: 1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/40\n","1846/1846 [==============================] - 12s 5ms/step - loss: 5.0965 - accuracy: 0.0403 - val_loss: 5.0382 - val_accuracy: 0.0436\n","Epoch 2/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.9781 - accuracy: 0.0440 - val_loss: 5.0316 - val_accuracy: 0.0436\n","Epoch 3/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.9704 - accuracy: 0.0439 - val_loss: 5.0333 - val_accuracy: 0.0436\n","Epoch 4/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.9611 - accuracy: 0.0440 - val_loss: 5.0197 - val_accuracy: 0.0436\n","Epoch 5/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.9115 - accuracy: 0.0501 - val_loss: 4.9496 - val_accuracy: 0.0473\n","Epoch 6/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.8687 - accuracy: 0.0527 - val_loss: 4.9458 - val_accuracy: 0.0574\n","Epoch 7/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.8482 - accuracy: 0.0533 - val_loss: 4.9343 - val_accuracy: 0.0510\n","Epoch 8/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.8370 - accuracy: 0.0544 - val_loss: 4.9305 - val_accuracy: 0.0515\n","Epoch 9/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.8256 - accuracy: 0.0525 - val_loss: 4.9187 - val_accuracy: 0.0583\n","Epoch 10/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.8166 - accuracy: 0.0553 - val_loss: 4.9180 - val_accuracy: 0.0515\n","Epoch 11/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.8064 - accuracy: 0.0544 - val_loss: 4.9186 - val_accuracy: 0.0515\n","Epoch 12/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.7965 - accuracy: 0.0553 - val_loss: 4.9083 - val_accuracy: 0.0548\n","Epoch 13/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.7884 - accuracy: 0.0571 - val_loss: 4.9119 - val_accuracy: 0.0570\n","Epoch 14/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.7786 - accuracy: 0.0592 - val_loss: 4.9088 - val_accuracy: 0.0563\n","Epoch 15/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.7700 - accuracy: 0.0594 - val_loss: 4.9092 - val_accuracy: 0.0535\n","Epoch 16/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.7638 - accuracy: 0.0566 - val_loss: 4.9120 - val_accuracy: 0.0587\n","Epoch 17/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.7588 - accuracy: 0.0595 - val_loss: 4.9138 - val_accuracy: 0.0504\n","Epoch 18/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.7539 - accuracy: 0.0583 - val_loss: 4.9131 - val_accuracy: 0.0581\n","Epoch 19/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.7496 - accuracy: 0.0589 - val_loss: 4.9166 - val_accuracy: 0.0552\n","Epoch 20/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.7473 - accuracy: 0.0590 - val_loss: 4.9245 - val_accuracy: 0.0592\n","Epoch 21/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.7455 - accuracy: 0.0591 - val_loss: 4.9396 - val_accuracy: 0.0493\n","Epoch 22/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.7427 - accuracy: 0.0591 - val_loss: 4.9214 - val_accuracy: 0.0557\n","Epoch 23/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.7421 - accuracy: 0.0591 - val_loss: 4.9269 - val_accuracy: 0.0541\n","Epoch 24/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.7378 - accuracy: 0.0596 - val_loss: 4.9322 - val_accuracy: 0.0519\n","Epoch 25/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.7356 - accuracy: 0.0596 - val_loss: 4.9406 - val_accuracy: 0.0557\n","Epoch 26/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.7332 - accuracy: 0.0613 - val_loss: 4.9330 - val_accuracy: 0.0554\n","Epoch 27/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.7309 - accuracy: 0.0619 - val_loss: 4.9595 - val_accuracy: 0.0607\n","Epoch 28/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.7282 - accuracy: 0.0621 - val_loss: 4.9337 - val_accuracy: 0.0565\n","Epoch 29/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.7254 - accuracy: 0.0613 - val_loss: 4.9441 - val_accuracy: 0.0629\n","Epoch 30/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.7217 - accuracy: 0.0619 - val_loss: 4.9512 - val_accuracy: 0.0572\n","Epoch 31/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.7167 - accuracy: 0.0603 - val_loss: 4.9528 - val_accuracy: 0.0548\n","Epoch 32/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.7128 - accuracy: 0.0616 - val_loss: 4.9447 - val_accuracy: 0.0598\n","Epoch 33/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.7106 - accuracy: 0.0620 - val_loss: 4.9548 - val_accuracy: 0.0550\n","Epoch 34/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.7063 - accuracy: 0.0602 - val_loss: 4.9511 - val_accuracy: 0.0590\n","Epoch 35/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.7020 - accuracy: 0.0608 - val_loss: 4.9551 - val_accuracy: 0.0557\n","Epoch 36/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.7011 - accuracy: 0.0604 - val_loss: 4.9662 - val_accuracy: 0.0557\n","Epoch 37/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.6980 - accuracy: 0.0628 - val_loss: 4.9718 - val_accuracy: 0.0557\n","Epoch 38/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.6947 - accuracy: 0.0615 - val_loss: 4.9776 - val_accuracy: 0.0541\n","Epoch 39/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.6921 - accuracy: 0.0595 - val_loss: 4.9758 - val_accuracy: 0.0550\n","Epoch 40/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.6915 - accuracy: 0.0607 - val_loss: 4.9865 - val_accuracy: 0.0583\n","Fold: 2\n","Epoch 1/40\n","1846/1846 [==============================] - 13s 5ms/step - loss: 5.0950 - accuracy: 0.0410 - val_loss: 5.0558 - val_accuracy: 0.0440\n","Epoch 2/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.9817 - accuracy: 0.0439 - val_loss: 5.0457 - val_accuracy: 0.0440\n","Epoch 3/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.9746 - accuracy: 0.0439 - val_loss: 5.0487 - val_accuracy: 0.0440\n","Epoch 4/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.9691 - accuracy: 0.0439 - val_loss: 5.0470 - val_accuracy: 0.0440\n","Epoch 5/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.9653 - accuracy: 0.0439 - val_loss: 5.0454 - val_accuracy: 0.0440\n","Epoch 6/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.9587 - accuracy: 0.0443 - val_loss: 5.0473 - val_accuracy: 0.0440\n","Epoch 7/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.9516 - accuracy: 0.0443 - val_loss: 5.0193 - val_accuracy: 0.0440\n","Epoch 8/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.9062 - accuracy: 0.0516 - val_loss: 4.9675 - val_accuracy: 0.0469\n","Epoch 9/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.8581 - accuracy: 0.0538 - val_loss: 4.9412 - val_accuracy: 0.0466\n","Epoch 10/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.8376 - accuracy: 0.0526 - val_loss: 4.9367 - val_accuracy: 0.0502\n","Epoch 11/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.8270 - accuracy: 0.0556 - val_loss: 4.9436 - val_accuracy: 0.0535\n","Epoch 12/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.8194 - accuracy: 0.0555 - val_loss: 4.9255 - val_accuracy: 0.0546\n","Epoch 13/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.8085 - accuracy: 0.0555 - val_loss: 4.9278 - val_accuracy: 0.0535\n","Epoch 14/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.7995 - accuracy: 0.0565 - val_loss: 4.9267 - val_accuracy: 0.0570\n","Epoch 15/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.7881 - accuracy: 0.0577 - val_loss: 4.9325 - val_accuracy: 0.0535\n","Epoch 16/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.7759 - accuracy: 0.0555 - val_loss: 4.9127 - val_accuracy: 0.0565\n","Epoch 17/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.7667 - accuracy: 0.0584 - val_loss: 4.9226 - val_accuracy: 0.0528\n","Epoch 18/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.7583 - accuracy: 0.0596 - val_loss: 4.9066 - val_accuracy: 0.0590\n","Epoch 19/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.7469 - accuracy: 0.0609 - val_loss: 4.9078 - val_accuracy: 0.0614\n","Epoch 20/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.7365 - accuracy: 0.0619 - val_loss: 4.9178 - val_accuracy: 0.0603\n","Epoch 21/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.7287 - accuracy: 0.0595 - val_loss: 4.9123 - val_accuracy: 0.0568\n","Epoch 22/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.7200 - accuracy: 0.0595 - val_loss: 4.9196 - val_accuracy: 0.0541\n","Epoch 23/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.7136 - accuracy: 0.0604 - val_loss: 4.9335 - val_accuracy: 0.0524\n","Epoch 24/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.7069 - accuracy: 0.0600 - val_loss: 4.9375 - val_accuracy: 0.0585\n","Epoch 25/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.7013 - accuracy: 0.0607 - val_loss: 4.9345 - val_accuracy: 0.0603\n","Epoch 26/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.6989 - accuracy: 0.0587 - val_loss: 4.9578 - val_accuracy: 0.0609\n","Epoch 27/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.6938 - accuracy: 0.0587 - val_loss: 4.9465 - val_accuracy: 0.0579\n","Epoch 28/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.6894 - accuracy: 0.0586 - val_loss: 4.9678 - val_accuracy: 0.0572\n","Epoch 29/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.6866 - accuracy: 0.0590 - val_loss: 4.9973 - val_accuracy: 0.0568\n","Epoch 30/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.6805 - accuracy: 0.0577 - val_loss: 4.9910 - val_accuracy: 0.0574\n","Epoch 31/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.6771 - accuracy: 0.0603 - val_loss: 4.9854 - val_accuracy: 0.0576\n","Epoch 32/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.6727 - accuracy: 0.0607 - val_loss: 4.9872 - val_accuracy: 0.0552\n","Epoch 33/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.6705 - accuracy: 0.0598 - val_loss: 5.0241 - val_accuracy: 0.0561\n","Epoch 34/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.6681 - accuracy: 0.0606 - val_loss: 5.0221 - val_accuracy: 0.0506\n","Epoch 35/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.6643 - accuracy: 0.0592 - val_loss: 5.0511 - val_accuracy: 0.0515\n","Epoch 36/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.6593 - accuracy: 0.0604 - val_loss: 5.0262 - val_accuracy: 0.0559\n","Epoch 37/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.6571 - accuracy: 0.0595 - val_loss: 5.0571 - val_accuracy: 0.0548\n","Epoch 38/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.6538 - accuracy: 0.0608 - val_loss: 5.0509 - val_accuracy: 0.0614\n","Epoch 39/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.6511 - accuracy: 0.0626 - val_loss: 5.0429 - val_accuracy: 0.0629\n","Epoch 40/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.6465 - accuracy: 0.0637 - val_loss: 5.0773 - val_accuracy: 0.0587\n","Average Validation Accuracy: 0.05903500132262707\n","Average Validation Loss: 4.911961078643799\n","Average Test Accuracy: 0.059445714578032494\n","Final Test Accuracy for each fold: 0.0597774013876915\n","Number of input features: 3\n","Fold: 1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/40\n","1846/1846 [==============================] - 17s 6ms/step - loss: 5.1002 - accuracy: 0.0427 - val_loss: 5.0307 - val_accuracy: 0.0627\n","Epoch 2/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.8380 - accuracy: 0.0735 - val_loss: 4.8182 - val_accuracy: 0.0706\n","Epoch 3/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.7069 - accuracy: 0.0759 - val_loss: 4.7628 - val_accuracy: 0.0706\n","Epoch 4/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.6518 - accuracy: 0.0793 - val_loss: 4.7398 - val_accuracy: 0.0706\n","Epoch 5/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.6197 - accuracy: 0.0834 - val_loss: 4.7118 - val_accuracy: 0.0759\n","Epoch 6/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.6028 - accuracy: 0.0832 - val_loss: 4.7268 - val_accuracy: 0.0772\n","Epoch 7/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.5881 - accuracy: 0.0836 - val_loss: 4.7043 - val_accuracy: 0.0779\n","Epoch 8/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.5751 - accuracy: 0.0841 - val_loss: 4.7054 - val_accuracy: 0.0759\n","Epoch 9/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.5626 - accuracy: 0.0843 - val_loss: 4.6925 - val_accuracy: 0.0785\n","Epoch 10/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.5514 - accuracy: 0.0848 - val_loss: 4.6843 - val_accuracy: 0.0733\n","Epoch 11/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.5403 - accuracy: 0.0843 - val_loss: 4.7082 - val_accuracy: 0.0796\n","Epoch 12/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.5318 - accuracy: 0.0847 - val_loss: 4.6949 - val_accuracy: 0.0779\n","Epoch 13/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.5247 - accuracy: 0.0845 - val_loss: 4.6986 - val_accuracy: 0.0739\n","Epoch 14/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.5198 - accuracy: 0.0852 - val_loss: 4.6885 - val_accuracy: 0.0733\n","Epoch 15/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.5153 - accuracy: 0.0854 - val_loss: 4.7056 - val_accuracy: 0.0799\n","Epoch 16/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.5101 - accuracy: 0.0866 - val_loss: 4.6905 - val_accuracy: 0.0834\n","Epoch 17/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.5089 - accuracy: 0.0864 - val_loss: 4.6992 - val_accuracy: 0.0812\n","Epoch 18/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.5044 - accuracy: 0.0875 - val_loss: 4.6943 - val_accuracy: 0.0761\n","Epoch 19/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.5003 - accuracy: 0.0865 - val_loss: 4.7066 - val_accuracy: 0.0807\n","Epoch 20/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.4963 - accuracy: 0.0871 - val_loss: 4.7032 - val_accuracy: 0.0733\n","Epoch 21/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.4945 - accuracy: 0.0884 - val_loss: 4.6918 - val_accuracy: 0.0750\n","Epoch 22/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.4905 - accuracy: 0.0902 - val_loss: 4.7093 - val_accuracy: 0.0774\n","Epoch 23/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.4867 - accuracy: 0.0879 - val_loss: 4.7035 - val_accuracy: 0.0772\n","Epoch 24/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.4829 - accuracy: 0.0900 - val_loss: 4.7056 - val_accuracy: 0.0774\n","Epoch 25/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.4802 - accuracy: 0.0900 - val_loss: 4.7018 - val_accuracy: 0.0772\n","Epoch 26/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.4778 - accuracy: 0.0890 - val_loss: 4.7134 - val_accuracy: 0.0766\n","Epoch 27/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.4745 - accuracy: 0.0882 - val_loss: 4.7070 - val_accuracy: 0.0807\n","Epoch 28/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.4726 - accuracy: 0.0890 - val_loss: 4.7060 - val_accuracy: 0.0757\n","Epoch 29/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.4696 - accuracy: 0.0880 - val_loss: 4.7085 - val_accuracy: 0.0792\n","Epoch 30/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.4663 - accuracy: 0.0939 - val_loss: 4.7007 - val_accuracy: 0.0832\n","Epoch 31/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.4624 - accuracy: 0.0912 - val_loss: 4.7229 - val_accuracy: 0.0843\n","Epoch 32/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.4612 - accuracy: 0.0897 - val_loss: 4.7155 - val_accuracy: 0.0807\n","Epoch 33/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.4566 - accuracy: 0.0915 - val_loss: 4.7101 - val_accuracy: 0.0834\n","Epoch 34/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.4557 - accuracy: 0.0906 - val_loss: 4.7139 - val_accuracy: 0.0836\n","Epoch 35/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.4537 - accuracy: 0.0910 - val_loss: 4.7176 - val_accuracy: 0.0829\n","Epoch 36/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.4518 - accuracy: 0.0926 - val_loss: 4.7082 - val_accuracy: 0.0862\n","Epoch 37/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.4487 - accuracy: 0.0941 - val_loss: 4.7133 - val_accuracy: 0.0814\n","Epoch 38/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.4480 - accuracy: 0.0910 - val_loss: 4.7198 - val_accuracy: 0.0843\n","Epoch 39/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.4453 - accuracy: 0.0938 - val_loss: 4.7047 - val_accuracy: 0.0867\n","Epoch 40/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.4428 - accuracy: 0.0954 - val_loss: 4.7376 - val_accuracy: 0.0801\n","Fold: 2\n","Epoch 1/40\n","1846/1846 [==============================] - 14s 6ms/step - loss: 5.0954 - accuracy: 0.0440 - val_loss: 5.0456 - val_accuracy: 0.0440\n","Epoch 2/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.8939 - accuracy: 0.0659 - val_loss: 4.8865 - val_accuracy: 0.0625\n","Epoch 3/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.7461 - accuracy: 0.0733 - val_loss: 4.8279 - val_accuracy: 0.0854\n","Epoch 4/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.6636 - accuracy: 0.0827 - val_loss: 4.7336 - val_accuracy: 0.0851\n","Epoch 5/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.6110 - accuracy: 0.0811 - val_loss: 4.7368 - val_accuracy: 0.0854\n","Epoch 6/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.5864 - accuracy: 0.0841 - val_loss: 4.6974 - val_accuracy: 0.0854\n","Epoch 7/40\n","1846/1846 [==============================] - 12s 7ms/step - loss: 4.5707 - accuracy: 0.0828 - val_loss: 4.6928 - val_accuracy: 0.0856\n","Epoch 8/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.5575 - accuracy: 0.0823 - val_loss: 4.6992 - val_accuracy: 0.0779\n","Epoch 9/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.5440 - accuracy: 0.0832 - val_loss: 4.6608 - val_accuracy: 0.0854\n","Epoch 10/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.5302 - accuracy: 0.0845 - val_loss: 4.6557 - val_accuracy: 0.0873\n","Epoch 11/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.5187 - accuracy: 0.0846 - val_loss: 4.6730 - val_accuracy: 0.0873\n","Epoch 12/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.5091 - accuracy: 0.0851 - val_loss: 4.6545 - val_accuracy: 0.0873\n","Epoch 13/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.4983 - accuracy: 0.0845 - val_loss: 4.6519 - val_accuracy: 0.0869\n","Epoch 14/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.4902 - accuracy: 0.0863 - val_loss: 4.6684 - val_accuracy: 0.0911\n","Epoch 15/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.4815 - accuracy: 0.0846 - val_loss: 4.7087 - val_accuracy: 0.0871\n","Epoch 16/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.4761 - accuracy: 0.0859 - val_loss: 4.6617 - val_accuracy: 0.0882\n","Epoch 17/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.4678 - accuracy: 0.0858 - val_loss: 4.6812 - val_accuracy: 0.0942\n","Epoch 18/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.4604 - accuracy: 0.0866 - val_loss: 4.6852 - val_accuracy: 0.0854\n","Epoch 19/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.4562 - accuracy: 0.0849 - val_loss: 4.6788 - val_accuracy: 0.0922\n","Epoch 20/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.4521 - accuracy: 0.0884 - val_loss: 4.6867 - val_accuracy: 0.0928\n","Epoch 21/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.4463 - accuracy: 0.0875 - val_loss: 4.6925 - val_accuracy: 0.0915\n","Epoch 22/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.4434 - accuracy: 0.0876 - val_loss: 4.6948 - val_accuracy: 0.0911\n","Epoch 23/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.4394 - accuracy: 0.0906 - val_loss: 4.6984 - val_accuracy: 0.0928\n","Epoch 24/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.4356 - accuracy: 0.0908 - val_loss: 4.7060 - val_accuracy: 0.0926\n","Epoch 25/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.4326 - accuracy: 0.0897 - val_loss: 4.7288 - val_accuracy: 0.0955\n","Epoch 26/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.4294 - accuracy: 0.0905 - val_loss: 4.7754 - val_accuracy: 0.0840\n","Epoch 27/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.4269 - accuracy: 0.0910 - val_loss: 4.7295 - val_accuracy: 0.0972\n","Epoch 28/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.4235 - accuracy: 0.0902 - val_loss: 4.7388 - val_accuracy: 0.0878\n","Epoch 29/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.4232 - accuracy: 0.0907 - val_loss: 4.7304 - val_accuracy: 0.0946\n","Epoch 30/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.4197 - accuracy: 0.0926 - val_loss: 4.7434 - val_accuracy: 0.0829\n","Epoch 31/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.4175 - accuracy: 0.0895 - val_loss: 4.7692 - val_accuracy: 0.0867\n","Epoch 32/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.4158 - accuracy: 0.0908 - val_loss: 4.7319 - val_accuracy: 0.0935\n","Epoch 33/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.4131 - accuracy: 0.0902 - val_loss: 4.7499 - val_accuracy: 0.0966\n","Epoch 34/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.4088 - accuracy: 0.0909 - val_loss: 4.7910 - val_accuracy: 0.0924\n","Epoch 35/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.4064 - accuracy: 0.0934 - val_loss: 4.7695 - val_accuracy: 0.0851\n","Epoch 36/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.4055 - accuracy: 0.0896 - val_loss: 4.7715 - val_accuracy: 0.0924\n","Epoch 37/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.4026 - accuracy: 0.0920 - val_loss: 4.7762 - val_accuracy: 0.0854\n","Epoch 38/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.3994 - accuracy: 0.0932 - val_loss: 4.7588 - val_accuracy: 0.0920\n","Epoch 39/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.3975 - accuracy: 0.0908 - val_loss: 4.8109 - val_accuracy: 0.0869\n","Epoch 40/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.3942 - accuracy: 0.0950 - val_loss: 4.7844 - val_accuracy: 0.0902\n","Average Validation Accuracy: 0.08648300543427467\n","Average Validation Loss: 4.629875183105469\n","Average Test Accuracy: 0.08561214432120323\n","Final Test Accuracy for each fold: 0.08719687163829803\n","Number of input features: 4\n","Fold: 1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/40\n","1846/1846 [==============================] - 14s 6ms/step - loss: 5.0491 - accuracy: 0.0399 - val_loss: 4.9450 - val_accuracy: 0.0436\n","Epoch 2/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.5993 - accuracy: 0.0904 - val_loss: 4.4336 - val_accuracy: 0.1153\n","Epoch 3/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.1633 - accuracy: 0.1256 - val_loss: 4.1707 - val_accuracy: 0.1135\n","Epoch 4/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 3.8966 - accuracy: 0.1514 - val_loss: 3.9299 - val_accuracy: 0.1575\n","Epoch 5/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.6537 - accuracy: 0.1891 - val_loss: 3.7085 - val_accuracy: 0.2026\n","Epoch 6/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 3.4029 - accuracy: 0.2320 - val_loss: 3.4885 - val_accuracy: 0.2363\n","Epoch 7/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.1895 - accuracy: 0.2667 - val_loss: 3.3145 - val_accuracy: 0.3078\n","Epoch 8/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 3.0079 - accuracy: 0.2916 - val_loss: 3.1919 - val_accuracy: 0.2770\n","Epoch 9/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.8615 - accuracy: 0.3155 - val_loss: 3.0977 - val_accuracy: 0.2959\n","Epoch 10/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.7454 - accuracy: 0.3283 - val_loss: 2.9991 - val_accuracy: 0.2680\n","Epoch 11/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.6542 - accuracy: 0.3380 - val_loss: 2.9039 - val_accuracy: 0.3157\n","Epoch 12/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.5850 - accuracy: 0.3513 - val_loss: 2.8447 - val_accuracy: 0.3386\n","Epoch 13/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.5168 - accuracy: 0.3577 - val_loss: 2.8314 - val_accuracy: 0.3604\n","Epoch 14/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.4719 - accuracy: 0.3659 - val_loss: 2.7392 - val_accuracy: 0.3545\n","Epoch 15/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.4151 - accuracy: 0.3762 - val_loss: 2.7913 - val_accuracy: 0.3303\n","Epoch 16/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.3733 - accuracy: 0.3814 - val_loss: 2.6685 - val_accuracy: 0.3791\n","Epoch 17/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.3356 - accuracy: 0.3863 - val_loss: 2.6521 - val_accuracy: 0.3822\n","Epoch 18/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.3041 - accuracy: 0.3911 - val_loss: 2.6695 - val_accuracy: 0.3800\n","Epoch 19/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.2854 - accuracy: 0.3876 - val_loss: 2.6974 - val_accuracy: 0.3252\n","Epoch 20/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.2481 - accuracy: 0.3940 - val_loss: 2.5648 - val_accuracy: 0.3809\n","Epoch 21/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.2281 - accuracy: 0.4035 - val_loss: 2.6256 - val_accuracy: 0.3523\n","Epoch 22/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.2083 - accuracy: 0.4027 - val_loss: 2.5149 - val_accuracy: 0.3912\n","Epoch 23/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.1917 - accuracy: 0.4106 - val_loss: 2.5483 - val_accuracy: 0.3490\n","Epoch 24/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.1660 - accuracy: 0.4142 - val_loss: 2.5317 - val_accuracy: 0.3927\n","Epoch 25/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.1464 - accuracy: 0.4168 - val_loss: 2.4903 - val_accuracy: 0.3949\n","Epoch 26/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.1296 - accuracy: 0.4215 - val_loss: 2.4563 - val_accuracy: 0.4205\n","Epoch 27/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.1208 - accuracy: 0.4204 - val_loss: 2.5741 - val_accuracy: 0.3663\n","Epoch 28/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.0977 - accuracy: 0.4183 - val_loss: 2.4901 - val_accuracy: 0.3787\n","Epoch 29/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.0851 - accuracy: 0.4300 - val_loss: 2.4593 - val_accuracy: 0.3806\n","Epoch 30/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.0641 - accuracy: 0.4350 - val_loss: 2.4452 - val_accuracy: 0.4202\n","Epoch 31/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.0585 - accuracy: 0.4332 - val_loss: 2.4206 - val_accuracy: 0.4262\n","Epoch 32/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.0372 - accuracy: 0.4431 - val_loss: 2.3772 - val_accuracy: 0.4572\n","Epoch 33/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.0249 - accuracy: 0.4401 - val_loss: 2.4860 - val_accuracy: 0.3949\n","Epoch 34/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.0166 - accuracy: 0.4467 - val_loss: 2.3670 - val_accuracy: 0.4081\n","Epoch 35/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.9918 - accuracy: 0.4522 - val_loss: 2.4647 - val_accuracy: 0.4097\n","Epoch 36/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.9955 - accuracy: 0.4481 - val_loss: 2.4458 - val_accuracy: 0.3663\n","Epoch 37/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.9832 - accuracy: 0.4476 - val_loss: 2.3773 - val_accuracy: 0.3971\n","Epoch 38/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.9694 - accuracy: 0.4518 - val_loss: 2.2686 - val_accuracy: 0.4594\n","Epoch 39/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.9598 - accuracy: 0.4555 - val_loss: 2.2928 - val_accuracy: 0.4678\n","Epoch 40/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.9488 - accuracy: 0.4596 - val_loss: 2.2436 - val_accuracy: 0.4640\n","Fold: 2\n","Epoch 1/40\n","1846/1846 [==============================] - 14s 6ms/step - loss: 5.0554 - accuracy: 0.0396 - val_loss: 4.9714 - val_accuracy: 0.0768\n","Epoch 2/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.5485 - accuracy: 0.0914 - val_loss: 4.4264 - val_accuracy: 0.1113\n","Epoch 3/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.1578 - accuracy: 0.1131 - val_loss: 4.2305 - val_accuracy: 0.1193\n","Epoch 4/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.9679 - accuracy: 0.1225 - val_loss: 4.1047 - val_accuracy: 0.1340\n","Epoch 5/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 3.8250 - accuracy: 0.1392 - val_loss: 3.9826 - val_accuracy: 0.1439\n","Epoch 6/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.6364 - accuracy: 0.1778 - val_loss: 3.8141 - val_accuracy: 0.2066\n","Epoch 7/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.4263 - accuracy: 0.2190 - val_loss: 3.6495 - val_accuracy: 0.2337\n","Epoch 8/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 3.2445 - accuracy: 0.2495 - val_loss: 3.5217 - val_accuracy: 0.2724\n","Epoch 9/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.0814 - accuracy: 0.2733 - val_loss: 3.4340 - val_accuracy: 0.2576\n","Epoch 10/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.9639 - accuracy: 0.2773 - val_loss: 3.3558 - val_accuracy: 0.3025\n","Epoch 11/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.8608 - accuracy: 0.2964 - val_loss: 3.3204 - val_accuracy: 0.2891\n","Epoch 12/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.7845 - accuracy: 0.2983 - val_loss: 3.2387 - val_accuracy: 0.2917\n","Epoch 13/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.7173 - accuracy: 0.3036 - val_loss: 3.1845 - val_accuracy: 0.3050\n","Epoch 14/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.6624 - accuracy: 0.3173 - val_loss: 3.1224 - val_accuracy: 0.3252\n","Epoch 15/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.6192 - accuracy: 0.3165 - val_loss: 3.2321 - val_accuracy: 0.2678\n","Epoch 16/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.5893 - accuracy: 0.3211 - val_loss: 3.1192 - val_accuracy: 0.3210\n","Epoch 17/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.5582 - accuracy: 0.3268 - val_loss: 3.0461 - val_accuracy: 0.3454\n","Epoch 18/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.5314 - accuracy: 0.3275 - val_loss: 2.9836 - val_accuracy: 0.3650\n","Epoch 19/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.5016 - accuracy: 0.3350 - val_loss: 3.0152 - val_accuracy: 0.3162\n","Epoch 20/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.4814 - accuracy: 0.3436 - val_loss: 2.9959 - val_accuracy: 0.3393\n","Epoch 21/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.4608 - accuracy: 0.3421 - val_loss: 2.9540 - val_accuracy: 0.3008\n","Epoch 22/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.4363 - accuracy: 0.3478 - val_loss: 2.9032 - val_accuracy: 0.3441\n","Epoch 23/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.4189 - accuracy: 0.3509 - val_loss: 2.8293 - val_accuracy: 0.3782\n","Epoch 24/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.4007 - accuracy: 0.3589 - val_loss: 2.8345 - val_accuracy: 0.4114\n","Epoch 25/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.3904 - accuracy: 0.3570 - val_loss: 2.8102 - val_accuracy: 0.3415\n","Epoch 26/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.3604 - accuracy: 0.3709 - val_loss: 2.9011 - val_accuracy: 0.3096\n","Epoch 27/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.3608 - accuracy: 0.3703 - val_loss: 2.7628 - val_accuracy: 0.4055\n","Epoch 28/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.3473 - accuracy: 0.3707 - val_loss: 2.9013 - val_accuracy: 0.3146\n","Epoch 29/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.3189 - accuracy: 0.3788 - val_loss: 2.8712 - val_accuracy: 0.3197\n","Epoch 30/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.3059 - accuracy: 0.3796 - val_loss: 2.7328 - val_accuracy: 0.3736\n","Epoch 31/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.2929 - accuracy: 0.3839 - val_loss: 2.6933 - val_accuracy: 0.3789\n","Epoch 32/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.2710 - accuracy: 0.3966 - val_loss: 2.6721 - val_accuracy: 0.4007\n","Epoch 33/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.2676 - accuracy: 0.3958 - val_loss: 2.7124 - val_accuracy: 0.3859\n","Epoch 34/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.2482 - accuracy: 0.4009 - val_loss: 2.6919 - val_accuracy: 0.3635\n","Epoch 35/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.2333 - accuracy: 0.4014 - val_loss: 2.6523 - val_accuracy: 0.4048\n","Epoch 36/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.2373 - accuracy: 0.4035 - val_loss: 2.5864 - val_accuracy: 0.4629\n","Epoch 37/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.2158 - accuracy: 0.4073 - val_loss: 2.6071 - val_accuracy: 0.4277\n","Epoch 38/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.1901 - accuracy: 0.4153 - val_loss: 2.6921 - val_accuracy: 0.3716\n","Epoch 39/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.1919 - accuracy: 0.4158 - val_loss: 2.6189 - val_accuracy: 0.3912\n","Epoch 40/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.1765 - accuracy: 0.4190 - val_loss: 2.5697 - val_accuracy: 0.4273\n","Average Validation Accuracy: 0.45826439559459686\n","Average Validation Loss: 2.13321053981781\n","Average Test Accuracy: 0.4565489739179611\n","Final Test Accuracy for each fold: 0.4795459508895874\n","Number of input features: 5\n","Fold: 1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/40\n","1846/1846 [==============================] - 14s 6ms/step - loss: 5.0652 - accuracy: 0.0408 - val_loss: 4.9434 - val_accuracy: 0.0704\n","Epoch 2/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.4951 - accuracy: 0.0934 - val_loss: 4.2963 - val_accuracy: 0.1212\n","Epoch 3/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.0339 - accuracy: 0.1330 - val_loss: 4.0377 - val_accuracy: 0.1408\n","Epoch 4/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 3.7645 - accuracy: 0.1718 - val_loss: 3.7897 - val_accuracy: 0.2106\n","Epoch 5/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.5242 - accuracy: 0.2207 - val_loss: 3.6094 - val_accuracy: 0.2499\n","Epoch 6/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 3.3251 - accuracy: 0.2425 - val_loss: 3.4818 - val_accuracy: 0.2480\n","Epoch 7/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 3.1806 - accuracy: 0.2562 - val_loss: 3.3874 - val_accuracy: 0.2816\n","Epoch 8/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 3.0677 - accuracy: 0.2760 - val_loss: 3.3955 - val_accuracy: 0.2475\n","Epoch 9/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.9820 - accuracy: 0.2914 - val_loss: 3.2573 - val_accuracy: 0.3250\n","Epoch 10/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.8970 - accuracy: 0.3001 - val_loss: 3.2376 - val_accuracy: 0.3144\n","Epoch 11/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.8354 - accuracy: 0.3196 - val_loss: 3.1903 - val_accuracy: 0.3265\n","Epoch 12/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.7809 - accuracy: 0.3213 - val_loss: 3.1278 - val_accuracy: 0.3190\n","Epoch 13/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.7260 - accuracy: 0.3341 - val_loss: 3.1248 - val_accuracy: 0.3098\n","Epoch 14/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.6800 - accuracy: 0.3371 - val_loss: 3.1148 - val_accuracy: 0.3481\n","Epoch 15/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.6293 - accuracy: 0.3520 - val_loss: 3.0702 - val_accuracy: 0.3151\n","Epoch 16/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.5888 - accuracy: 0.3613 - val_loss: 2.9449 - val_accuracy: 0.3501\n","Epoch 17/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.5402 - accuracy: 0.3684 - val_loss: 2.9841 - val_accuracy: 0.3327\n","Epoch 18/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.5009 - accuracy: 0.3738 - val_loss: 2.9214 - val_accuracy: 0.3762\n","Epoch 19/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.4466 - accuracy: 0.3843 - val_loss: 2.7968 - val_accuracy: 0.4231\n","Epoch 20/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.4194 - accuracy: 0.3862 - val_loss: 2.8015 - val_accuracy: 0.3806\n","Epoch 21/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.3896 - accuracy: 0.3922 - val_loss: 2.8183 - val_accuracy: 0.3699\n","Epoch 22/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.3365 - accuracy: 0.4015 - val_loss: 2.7215 - val_accuracy: 0.3947\n","Epoch 23/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.3248 - accuracy: 0.4070 - val_loss: 2.7362 - val_accuracy: 0.3630\n","Epoch 24/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.2952 - accuracy: 0.4018 - val_loss: 2.7014 - val_accuracy: 0.3833\n","Epoch 25/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.2672 - accuracy: 0.4155 - val_loss: 2.6564 - val_accuracy: 0.3897\n","Epoch 26/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.2298 - accuracy: 0.4182 - val_loss: 2.6288 - val_accuracy: 0.4240\n","Epoch 27/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.2216 - accuracy: 0.4237 - val_loss: 2.5956 - val_accuracy: 0.4370\n","Epoch 28/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.1851 - accuracy: 0.4319 - val_loss: 2.6938 - val_accuracy: 0.3824\n","Epoch 29/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.1846 - accuracy: 0.4312 - val_loss: 2.7742 - val_accuracy: 0.3740\n","Epoch 30/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.1464 - accuracy: 0.4356 - val_loss: 2.6271 - val_accuracy: 0.4101\n","Epoch 31/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.1438 - accuracy: 0.4397 - val_loss: 2.5162 - val_accuracy: 0.4139\n","Epoch 32/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.1204 - accuracy: 0.4430 - val_loss: 2.5712 - val_accuracy: 0.3927\n","Epoch 33/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.1148 - accuracy: 0.4470 - val_loss: 2.4861 - val_accuracy: 0.4480\n","Epoch 34/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.0936 - accuracy: 0.4473 - val_loss: 2.4689 - val_accuracy: 0.4746\n","Epoch 35/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.0738 - accuracy: 0.4519 - val_loss: 2.6040 - val_accuracy: 0.4260\n","Epoch 36/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.0571 - accuracy: 0.4573 - val_loss: 2.4552 - val_accuracy: 0.4689\n","Epoch 37/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.0183 - accuracy: 0.4644 - val_loss: 2.6001 - val_accuracy: 0.4315\n","Epoch 38/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.0295 - accuracy: 0.4611 - val_loss: 2.4735 - val_accuracy: 0.4499\n","Epoch 39/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.0005 - accuracy: 0.4639 - val_loss: 2.4233 - val_accuracy: 0.4845\n","Epoch 40/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.9971 - accuracy: 0.4620 - val_loss: 2.4953 - val_accuracy: 0.4264\n","Fold: 2\n","Epoch 1/40\n","1846/1846 [==============================] - 14s 6ms/step - loss: 5.0667 - accuracy: 0.0410 - val_loss: 4.9855 - val_accuracy: 0.0440\n","Epoch 2/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.5882 - accuracy: 0.0740 - val_loss: 4.4310 - val_accuracy: 0.0906\n","Epoch 3/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.0889 - accuracy: 0.1278 - val_loss: 4.0973 - val_accuracy: 0.1734\n","Epoch 4/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 3.6834 - accuracy: 0.1941 - val_loss: 3.6991 - val_accuracy: 0.2295\n","Epoch 5/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 3.2246 - accuracy: 0.2704 - val_loss: 3.3102 - val_accuracy: 0.3318\n","Epoch 6/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.8589 - accuracy: 0.3364 - val_loss: 3.0861 - val_accuracy: 0.3355\n","Epoch 7/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.5721 - accuracy: 0.3848 - val_loss: 2.8387 - val_accuracy: 0.4207\n","Epoch 8/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.3361 - accuracy: 0.4234 - val_loss: 2.7042 - val_accuracy: 0.4458\n","Epoch 9/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.1290 - accuracy: 0.4583 - val_loss: 2.5414 - val_accuracy: 0.4689\n","Epoch 10/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.9613 - accuracy: 0.4893 - val_loss: 2.4729 - val_accuracy: 0.4631\n","Epoch 11/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.8312 - accuracy: 0.5153 - val_loss: 2.3354 - val_accuracy: 0.5089\n","Epoch 12/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.7274 - accuracy: 0.5324 - val_loss: 2.2904 - val_accuracy: 0.4816\n","Epoch 13/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.6483 - accuracy: 0.5483 - val_loss: 2.1200 - val_accuracy: 0.5591\n","Epoch 14/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.5710 - accuracy: 0.5655 - val_loss: 2.1038 - val_accuracy: 0.5837\n","Epoch 15/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.5077 - accuracy: 0.5754 - val_loss: 2.0471 - val_accuracy: 0.5450\n","Epoch 16/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.4675 - accuracy: 0.5839 - val_loss: 1.9797 - val_accuracy: 0.5828\n","Epoch 17/40\n","1846/1846 [==============================] - 12s 6ms/step - loss: 1.4228 - accuracy: 0.5927 - val_loss: 2.0094 - val_accuracy: 0.5778\n","Epoch 18/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.3888 - accuracy: 0.5958 - val_loss: 1.9002 - val_accuracy: 0.5509\n","Epoch 19/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.3652 - accuracy: 0.6064 - val_loss: 1.9627 - val_accuracy: 0.5366\n","Epoch 20/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.3395 - accuracy: 0.6088 - val_loss: 1.7928 - val_accuracy: 0.6057\n","Epoch 21/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.3133 - accuracy: 0.6118 - val_loss: 1.7746 - val_accuracy: 0.5681\n","Epoch 22/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.2870 - accuracy: 0.6130 - val_loss: 1.7482 - val_accuracy: 0.6123\n","Epoch 23/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.2708 - accuracy: 0.6201 - val_loss: 1.7336 - val_accuracy: 0.5734\n","Epoch 24/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.2573 - accuracy: 0.6251 - val_loss: 1.6861 - val_accuracy: 0.5927\n","Epoch 25/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.2346 - accuracy: 0.6258 - val_loss: 1.7209 - val_accuracy: 0.6141\n","Epoch 26/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.2221 - accuracy: 0.6287 - val_loss: 1.6323 - val_accuracy: 0.6286\n","Epoch 27/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.2107 - accuracy: 0.6285 - val_loss: 1.6565 - val_accuracy: 0.6110\n","Epoch 28/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.2028 - accuracy: 0.6360 - val_loss: 1.6359 - val_accuracy: 0.6152\n","Epoch 29/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.1825 - accuracy: 0.6377 - val_loss: 1.6278 - val_accuracy: 0.6194\n","Epoch 30/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.1795 - accuracy: 0.6388 - val_loss: 1.6087 - val_accuracy: 0.5923\n","Epoch 31/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.1667 - accuracy: 0.6383 - val_loss: 1.6177 - val_accuracy: 0.5967\n","Epoch 32/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.1584 - accuracy: 0.6432 - val_loss: 1.6637 - val_accuracy: 0.6400\n","Epoch 33/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.1494 - accuracy: 0.6477 - val_loss: 1.6136 - val_accuracy: 0.6000\n","Epoch 34/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.1525 - accuracy: 0.6437 - val_loss: 1.5895 - val_accuracy: 0.6178\n","Epoch 35/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.1315 - accuracy: 0.6477 - val_loss: 1.5354 - val_accuracy: 0.6504\n","Epoch 36/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.1195 - accuracy: 0.6519 - val_loss: 1.5387 - val_accuracy: 0.6268\n","Epoch 37/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.1090 - accuracy: 0.6543 - val_loss: 1.5098 - val_accuracy: 0.6559\n","Epoch 38/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.1131 - accuracy: 0.6516 - val_loss: 1.4987 - val_accuracy: 0.6506\n","Epoch 39/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.1008 - accuracy: 0.6554 - val_loss: 1.5944 - val_accuracy: 0.6394\n","Epoch 40/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.0930 - accuracy: 0.6545 - val_loss: 1.5638 - val_accuracy: 0.6070\n","Average Validation Accuracy: 0.5342953354120255\n","Average Validation Loss: 1.7112799286842346\n","Average Test Accuracy: 0.5363013297319412\n","Final Test Accuracy for each fold: 0.6308690309524536\n","Number of input features: 6\n","Fold: 1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/40\n","1846/1846 [==============================] - 14s 6ms/step - loss: 5.0411 - accuracy: 0.0385 - val_loss: 4.9072 - val_accuracy: 0.0315\n","Epoch 2/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.5384 - accuracy: 0.0841 - val_loss: 4.2397 - val_accuracy: 0.1426\n","Epoch 3/40\n","1846/1846 [==============================] - 12s 6ms/step - loss: 3.7762 - accuracy: 0.2257 - val_loss: 3.5784 - val_accuracy: 0.2926\n","Epoch 4/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.1463 - accuracy: 0.3490 - val_loss: 3.0825 - val_accuracy: 0.3859\n","Epoch 5/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.6446 - accuracy: 0.4293 - val_loss: 2.6458 - val_accuracy: 0.4590\n","Epoch 6/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.2484 - accuracy: 0.4962 - val_loss: 2.3627 - val_accuracy: 0.5146\n","Epoch 7/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.9507 - accuracy: 0.5548 - val_loss: 2.1718 - val_accuracy: 0.5769\n","Epoch 8/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.7157 - accuracy: 0.5934 - val_loss: 2.0111 - val_accuracy: 0.5850\n","Epoch 9/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.5518 - accuracy: 0.6180 - val_loss: 1.8600 - val_accuracy: 0.6447\n","Epoch 10/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.4116 - accuracy: 0.6526 - val_loss: 1.7633 - val_accuracy: 0.6664\n","Epoch 11/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.3089 - accuracy: 0.6662 - val_loss: 1.7109 - val_accuracy: 0.6766\n","Epoch 12/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.2272 - accuracy: 0.6772 - val_loss: 1.6251 - val_accuracy: 0.6814\n","Epoch 13/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.1640 - accuracy: 0.6914 - val_loss: 1.5559 - val_accuracy: 0.7210\n","Epoch 14/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.1178 - accuracy: 0.6985 - val_loss: 1.6267 - val_accuracy: 0.6717\n","Epoch 15/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.0590 - accuracy: 0.7114 - val_loss: 1.5240 - val_accuracy: 0.7507\n","Epoch 16/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.0254 - accuracy: 0.7219 - val_loss: 1.4786 - val_accuracy: 0.7157\n","Epoch 17/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 0.9951 - accuracy: 0.7244 - val_loss: 1.5582 - val_accuracy: 0.7091\n","Epoch 18/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.9581 - accuracy: 0.7388 - val_loss: 1.4806 - val_accuracy: 0.7426\n","Epoch 19/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.9358 - accuracy: 0.7385 - val_loss: 1.3656 - val_accuracy: 0.7426\n","Epoch 20/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.9061 - accuracy: 0.7444 - val_loss: 1.4598 - val_accuracy: 0.7100\n","Epoch 21/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.8870 - accuracy: 0.7471 - val_loss: 1.4660 - val_accuracy: 0.6825\n","Epoch 22/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.8789 - accuracy: 0.7465 - val_loss: 1.3268 - val_accuracy: 0.7507\n","Epoch 23/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.8532 - accuracy: 0.7550 - val_loss: 1.3815 - val_accuracy: 0.7254\n","Epoch 24/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 0.8352 - accuracy: 0.7612 - val_loss: 1.2929 - val_accuracy: 0.7439\n","Epoch 25/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.8160 - accuracy: 0.7664 - val_loss: 1.4893 - val_accuracy: 0.7063\n","Epoch 26/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.8170 - accuracy: 0.7609 - val_loss: 1.2172 - val_accuracy: 0.7831\n","Epoch 27/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 0.7899 - accuracy: 0.7690 - val_loss: 1.2441 - val_accuracy: 0.7652\n","Epoch 28/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 0.7693 - accuracy: 0.7763 - val_loss: 1.1791 - val_accuracy: 0.7780\n","Epoch 29/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.7658 - accuracy: 0.7748 - val_loss: 1.2115 - val_accuracy: 0.7685\n","Epoch 30/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.7538 - accuracy: 0.7801 - val_loss: 1.2006 - val_accuracy: 0.7800\n","Epoch 31/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.7547 - accuracy: 0.7782 - val_loss: 1.3154 - val_accuracy: 0.7182\n","Epoch 32/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.7341 - accuracy: 0.7830 - val_loss: 1.2249 - val_accuracy: 0.7655\n","Epoch 33/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.7264 - accuracy: 0.7852 - val_loss: 1.1308 - val_accuracy: 0.7932\n","Epoch 34/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.7011 - accuracy: 0.7943 - val_loss: 1.1724 - val_accuracy: 0.7912\n","Epoch 35/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 0.7213 - accuracy: 0.7844 - val_loss: 1.1133 - val_accuracy: 0.7835\n","Epoch 36/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.7005 - accuracy: 0.7909 - val_loss: 1.1721 - val_accuracy: 0.7571\n","Epoch 37/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 0.6967 - accuracy: 0.7923 - val_loss: 1.1762 - val_accuracy: 0.7842\n","Epoch 38/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.6803 - accuracy: 0.7969 - val_loss: 1.0927 - val_accuracy: 0.7949\n","Epoch 39/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 0.6830 - accuracy: 0.7947 - val_loss: 1.0892 - val_accuracy: 0.7912\n","Epoch 40/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.6690 - accuracy: 0.7999 - val_loss: 1.1664 - val_accuracy: 0.7424\n","Fold: 2\n","Epoch 1/40\n","1846/1846 [==============================] - 15s 6ms/step - loss: 5.0510 - accuracy: 0.0399 - val_loss: 4.9168 - val_accuracy: 0.0680\n","Epoch 2/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.4833 - accuracy: 0.1064 - val_loss: 4.2113 - val_accuracy: 0.1771\n","Epoch 3/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 3.6985 - accuracy: 0.2336 - val_loss: 3.6027 - val_accuracy: 0.2818\n","Epoch 4/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.1234 - accuracy: 0.3268 - val_loss: 3.2110 - val_accuracy: 0.3723\n","Epoch 5/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.6524 - accuracy: 0.4209 - val_loss: 2.8522 - val_accuracy: 0.4922\n","Epoch 6/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.2527 - accuracy: 0.4997 - val_loss: 2.6226 - val_accuracy: 0.5349\n","Epoch 7/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.9409 - accuracy: 0.5537 - val_loss: 2.4329 - val_accuracy: 0.5639\n","Epoch 8/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.7138 - accuracy: 0.5908 - val_loss: 2.2745 - val_accuracy: 0.5875\n","Epoch 9/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.5399 - accuracy: 0.6239 - val_loss: 2.1819 - val_accuracy: 0.6018\n","Epoch 10/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.4185 - accuracy: 0.6497 - val_loss: 2.0853 - val_accuracy: 0.6209\n","Epoch 11/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.3209 - accuracy: 0.6621 - val_loss: 1.9809 - val_accuracy: 0.6766\n","Epoch 12/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.2456 - accuracy: 0.6754 - val_loss: 1.8856 - val_accuracy: 0.6867\n","Epoch 13/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.1831 - accuracy: 0.6861 - val_loss: 1.8216 - val_accuracy: 0.7052\n","Epoch 14/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.1236 - accuracy: 0.6957 - val_loss: 1.7491 - val_accuracy: 0.6975\n","Epoch 15/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.0745 - accuracy: 0.7094 - val_loss: 1.6965 - val_accuracy: 0.7094\n","Epoch 16/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.0374 - accuracy: 0.7164 - val_loss: 1.7006 - val_accuracy: 0.6640\n","Epoch 17/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 0.9978 - accuracy: 0.7226 - val_loss: 1.7771 - val_accuracy: 0.6854\n","Epoch 18/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.9796 - accuracy: 0.7252 - val_loss: 1.6271 - val_accuracy: 0.7012\n","Epoch 19/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.9406 - accuracy: 0.7343 - val_loss: 1.5404 - val_accuracy: 0.7435\n","Epoch 20/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.9246 - accuracy: 0.7429 - val_loss: 1.5902 - val_accuracy: 0.7146\n","Epoch 21/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.8933 - accuracy: 0.7482 - val_loss: 1.4874 - val_accuracy: 0.7245\n","Epoch 22/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.8814 - accuracy: 0.7434 - val_loss: 1.4590 - val_accuracy: 0.7421\n","Epoch 23/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 0.8530 - accuracy: 0.7545 - val_loss: 1.4698 - val_accuracy: 0.7201\n","Epoch 24/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.8393 - accuracy: 0.7554 - val_loss: 1.3551 - val_accuracy: 0.7716\n","Epoch 25/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.8323 - accuracy: 0.7598 - val_loss: 1.4498 - val_accuracy: 0.7479\n","Epoch 26/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.8154 - accuracy: 0.7649 - val_loss: 1.5820 - val_accuracy: 0.7061\n","Epoch 27/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 0.8061 - accuracy: 0.7631 - val_loss: 1.3322 - val_accuracy: 0.7839\n","Epoch 28/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.7882 - accuracy: 0.7671 - val_loss: 1.3322 - val_accuracy: 0.7747\n","Epoch 29/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 0.7771 - accuracy: 0.7719 - val_loss: 1.2765 - val_accuracy: 0.7943\n","Epoch 30/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.7629 - accuracy: 0.7763 - val_loss: 1.3966 - val_accuracy: 0.7353\n","Epoch 31/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.7639 - accuracy: 0.7741 - val_loss: 1.4239 - val_accuracy: 0.7459\n","Epoch 32/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 0.7536 - accuracy: 0.7772 - val_loss: 1.3252 - val_accuracy: 0.7639\n","Epoch 33/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 0.7502 - accuracy: 0.7775 - val_loss: 1.2379 - val_accuracy: 0.7927\n","Epoch 34/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 0.7337 - accuracy: 0.7812 - val_loss: 1.2565 - val_accuracy: 0.7776\n","Epoch 35/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.7377 - accuracy: 0.7814 - val_loss: 1.2834 - val_accuracy: 0.7503\n","Epoch 36/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.7196 - accuracy: 0.7875 - val_loss: 1.2471 - val_accuracy: 0.8044\n","Epoch 37/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.7143 - accuracy: 0.7871 - val_loss: 1.2564 - val_accuracy: 0.7622\n","Epoch 38/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 0.7015 - accuracy: 0.7913 - val_loss: 1.1610 - val_accuracy: 0.8123\n","Epoch 39/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.6835 - accuracy: 0.7923 - val_loss: 1.3667 - val_accuracy: 0.7567\n","Epoch 40/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.6911 - accuracy: 0.7949 - val_loss: 1.2193 - val_accuracy: 0.7718\n","Average Validation Accuracy: 0.7742444276809692\n","Average Validation Loss: 0.8627955615520477\n","Average Test Accuracy: 0.7692931294441223\n","Final Test Accuracy for each fold: 0.7824869155883789\n","Number of input features: 7\n","Fold: 1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/40\n","1846/1846 [==============================] - 14s 6ms/step - loss: 5.0351 - accuracy: 0.0421 - val_loss: 4.8302 - val_accuracy: 0.0486\n","Epoch 2/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.4890 - accuracy: 0.0910 - val_loss: 4.4094 - val_accuracy: 0.1122\n","Epoch 3/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 4.1552 - accuracy: 0.1202 - val_loss: 4.1270 - val_accuracy: 0.1536\n","Epoch 4/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.7915 - accuracy: 0.2081 - val_loss: 3.7648 - val_accuracy: 0.2260\n","Epoch 5/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 3.3466 - accuracy: 0.2844 - val_loss: 3.3352 - val_accuracy: 0.3331\n","Epoch 6/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.9068 - accuracy: 0.3503 - val_loss: 2.9786 - val_accuracy: 0.3941\n","Epoch 7/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.5200 - accuracy: 0.4226 - val_loss: 2.6585 - val_accuracy: 0.4477\n","Epoch 8/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.1957 - accuracy: 0.4783 - val_loss: 2.4490 - val_accuracy: 0.5184\n","Epoch 9/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.9246 - accuracy: 0.5238 - val_loss: 2.2844 - val_accuracy: 0.5388\n","Epoch 10/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.7391 - accuracy: 0.5578 - val_loss: 2.1630 - val_accuracy: 0.5602\n","Epoch 11/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.5864 - accuracy: 0.5902 - val_loss: 2.0223 - val_accuracy: 0.6196\n","Epoch 12/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.4878 - accuracy: 0.6061 - val_loss: 1.9514 - val_accuracy: 0.6392\n","Epoch 13/40\n","1846/1846 [==============================] - 13s 7ms/step - loss: 1.3924 - accuracy: 0.6319 - val_loss: 1.9765 - val_accuracy: 0.6233\n","Epoch 14/40\n","1846/1846 [==============================] - 15s 8ms/step - loss: 1.3140 - accuracy: 0.6472 - val_loss: 1.8781 - val_accuracy: 0.6678\n","Epoch 15/40\n","1846/1846 [==============================] - 14s 7ms/step - loss: 1.2587 - accuracy: 0.6616 - val_loss: 1.8022 - val_accuracy: 0.6572\n","Epoch 16/40\n","1846/1846 [==============================] - 13s 7ms/step - loss: 1.2056 - accuracy: 0.6800 - val_loss: 1.7945 - val_accuracy: 0.6675\n","Epoch 17/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.1639 - accuracy: 0.6873 - val_loss: 1.7115 - val_accuracy: 0.6728\n","Epoch 18/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.1092 - accuracy: 0.6990 - val_loss: 1.7058 - val_accuracy: 0.6803\n","Epoch 19/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.0796 - accuracy: 0.7016 - val_loss: 1.5800 - val_accuracy: 0.7223\n","Epoch 20/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.0343 - accuracy: 0.7118 - val_loss: 1.5646 - val_accuracy: 0.7186\n","Epoch 21/40\n","1846/1846 [==============================] - 12s 7ms/step - loss: 1.0169 - accuracy: 0.7160 - val_loss: 1.5789 - val_accuracy: 0.6836\n","Epoch 22/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 0.9951 - accuracy: 0.7222 - val_loss: 1.5009 - val_accuracy: 0.7384\n","Epoch 23/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.9521 - accuracy: 0.7347 - val_loss: 1.4913 - val_accuracy: 0.7333\n","Epoch 24/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.9312 - accuracy: 0.7355 - val_loss: 1.6536 - val_accuracy: 0.6814\n","Epoch 25/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.9071 - accuracy: 0.7438 - val_loss: 1.4293 - val_accuracy: 0.7534\n","Epoch 26/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.9037 - accuracy: 0.7412 - val_loss: 1.4419 - val_accuracy: 0.7382\n","Epoch 27/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.8713 - accuracy: 0.7507 - val_loss: 1.4228 - val_accuracy: 0.7417\n","Epoch 28/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.8547 - accuracy: 0.7543 - val_loss: 1.4522 - val_accuracy: 0.7272\n","Epoch 29/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.8369 - accuracy: 0.7602 - val_loss: 1.4377 - val_accuracy: 0.7586\n","Epoch 30/40\n","1846/1846 [==============================] - 12s 6ms/step - loss: 0.8313 - accuracy: 0.7591 - val_loss: 1.4290 - val_accuracy: 0.7314\n","Epoch 31/40\n","1846/1846 [==============================] - 12s 6ms/step - loss: 0.8110 - accuracy: 0.7662 - val_loss: 1.4131 - val_accuracy: 0.7496\n","Epoch 32/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.7937 - accuracy: 0.7752 - val_loss: 1.3903 - val_accuracy: 0.7571\n","Epoch 33/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.7859 - accuracy: 0.7775 - val_loss: 1.4467 - val_accuracy: 0.7428\n","Epoch 34/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.7612 - accuracy: 0.7803 - val_loss: 1.3606 - val_accuracy: 0.7549\n","Epoch 35/40\n","1846/1846 [==============================] - 12s 6ms/step - loss: 0.7782 - accuracy: 0.7770 - val_loss: 1.3444 - val_accuracy: 0.7657\n","Epoch 36/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.7347 - accuracy: 0.7889 - val_loss: 1.3054 - val_accuracy: 0.7611\n","Epoch 37/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.7498 - accuracy: 0.7830 - val_loss: 1.3667 - val_accuracy: 0.7591\n","Epoch 38/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.7337 - accuracy: 0.7907 - val_loss: 1.3802 - val_accuracy: 0.7470\n","Epoch 39/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.7140 - accuracy: 0.7933 - val_loss: 1.3082 - val_accuracy: 0.7626\n","Epoch 40/40\n","1846/1846 [==============================] - 12s 7ms/step - loss: 0.7207 - accuracy: 0.7893 - val_loss: 1.2332 - val_accuracy: 0.7941\n","Fold: 2\n","Epoch 1/40\n","1846/1846 [==============================] - 14s 6ms/step - loss: 5.0255 - accuracy: 0.0434 - val_loss: 4.8036 - val_accuracy: 0.0752\n","Epoch 2/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.3849 - accuracy: 0.1208 - val_loss: 4.1651 - val_accuracy: 0.1875\n","Epoch 3/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.6814 - accuracy: 0.2368 - val_loss: 3.6263 - val_accuracy: 0.2818\n","Epoch 4/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 3.1685 - accuracy: 0.3170 - val_loss: 3.3009 - val_accuracy: 0.3450\n","Epoch 5/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.7624 - accuracy: 0.3924 - val_loss: 2.9424 - val_accuracy: 0.4286\n","Epoch 6/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.4358 - accuracy: 0.4398 - val_loss: 2.7584 - val_accuracy: 0.4455\n","Epoch 7/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.1724 - accuracy: 0.4833 - val_loss: 2.5523 - val_accuracy: 0.5276\n","Epoch 8/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.9641 - accuracy: 0.5237 - val_loss: 2.3908 - val_accuracy: 0.5694\n","Epoch 9/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.8096 - accuracy: 0.5488 - val_loss: 2.3435 - val_accuracy: 0.5564\n","Epoch 10/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.6846 - accuracy: 0.5812 - val_loss: 2.2214 - val_accuracy: 0.6143\n","Epoch 11/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.5842 - accuracy: 0.5961 - val_loss: 2.1714 - val_accuracy: 0.6185\n","Epoch 12/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.4959 - accuracy: 0.6198 - val_loss: 2.1649 - val_accuracy: 0.6222\n","Epoch 13/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.4353 - accuracy: 0.6213 - val_loss: 2.0030 - val_accuracy: 0.6403\n","Epoch 14/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.3609 - accuracy: 0.6426 - val_loss: 1.9394 - val_accuracy: 0.6543\n","Epoch 15/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.3150 - accuracy: 0.6479 - val_loss: 1.9189 - val_accuracy: 0.6469\n","Epoch 16/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.2665 - accuracy: 0.6636 - val_loss: 2.0676 - val_accuracy: 0.5798\n","Epoch 17/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.2243 - accuracy: 0.6725 - val_loss: 1.7781 - val_accuracy: 0.6887\n","Epoch 18/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.1847 - accuracy: 0.6831 - val_loss: 1.7959 - val_accuracy: 0.6757\n","Epoch 19/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.1587 - accuracy: 0.6897 - val_loss: 1.7138 - val_accuracy: 0.6851\n","Epoch 20/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.1301 - accuracy: 0.6935 - val_loss: 1.6760 - val_accuracy: 0.7091\n","Epoch 21/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.1010 - accuracy: 0.6982 - val_loss: 1.6411 - val_accuracy: 0.7157\n","Epoch 22/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.0736 - accuracy: 0.7081 - val_loss: 1.6884 - val_accuracy: 0.6935\n","Epoch 23/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.0488 - accuracy: 0.7132 - val_loss: 1.6764 - val_accuracy: 0.7003\n","Epoch 24/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.0253 - accuracy: 0.7182 - val_loss: 1.6467 - val_accuracy: 0.7052\n","Epoch 25/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.0088 - accuracy: 0.7203 - val_loss: 1.5694 - val_accuracy: 0.7428\n","Epoch 26/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.9843 - accuracy: 0.7254 - val_loss: 1.6095 - val_accuracy: 0.7254\n","Epoch 27/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.9678 - accuracy: 0.7349 - val_loss: 1.5451 - val_accuracy: 0.7298\n","Epoch 28/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.9508 - accuracy: 0.7376 - val_loss: 1.5400 - val_accuracy: 0.7118\n","Epoch 29/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.9400 - accuracy: 0.7401 - val_loss: 1.5415 - val_accuracy: 0.7204\n","Epoch 30/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.9186 - accuracy: 0.7449 - val_loss: 1.6072 - val_accuracy: 0.6860\n","Epoch 31/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.8891 - accuracy: 0.7521 - val_loss: 1.6178 - val_accuracy: 0.6827\n","Epoch 32/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.8845 - accuracy: 0.7554 - val_loss: 1.5225 - val_accuracy: 0.7311\n","Epoch 33/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.8816 - accuracy: 0.7551 - val_loss: 1.4937 - val_accuracy: 0.7391\n","Epoch 34/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.8523 - accuracy: 0.7576 - val_loss: 1.4215 - val_accuracy: 0.7399\n","Epoch 35/40\n","1846/1846 [==============================] - 8s 5ms/step - loss: 0.8487 - accuracy: 0.7610 - val_loss: 1.3861 - val_accuracy: 0.7624\n","Epoch 36/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.8321 - accuracy: 0.7629 - val_loss: 1.4746 - val_accuracy: 0.7536\n","Epoch 37/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.8244 - accuracy: 0.7698 - val_loss: 1.3822 - val_accuracy: 0.7542\n","Epoch 38/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.8203 - accuracy: 0.7686 - val_loss: 1.4023 - val_accuracy: 0.7745\n","Epoch 39/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.8189 - accuracy: 0.7698 - val_loss: 1.4803 - val_accuracy: 0.7254\n","Epoch 40/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.7907 - accuracy: 0.7787 - val_loss: 1.4259 - val_accuracy: 0.7611\n","Average Validation Accuracy: 0.7902545928955078\n","Average Validation Loss: 0.9391189515590668\n","Average Test Accuracy: 0.7866514325141907\n","Final Test Accuracy for each fold: 0.8068106174468994\n","Number of input features: 8\n","Fold: 1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/40\n","1846/1846 [==============================] - 13s 5ms/step - loss: 4.7514 - accuracy: 0.0452 - val_loss: 4.5126 - val_accuracy: 0.0546\n","Epoch 2/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 4.3083 - accuracy: 0.0746 - val_loss: 4.2235 - val_accuracy: 0.1043\n","Epoch 3/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 3.9169 - accuracy: 0.1436 - val_loss: 3.7600 - val_accuracy: 0.1993\n","Epoch 4/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.4513 - accuracy: 0.2282 - val_loss: 3.3290 - val_accuracy: 0.2783\n","Epoch 5/40\n","1846/1846 [==============================] - 13s 7ms/step - loss: 2.9746 - accuracy: 0.2984 - val_loss: 2.9067 - val_accuracy: 0.3190\n","Epoch 6/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.5426 - accuracy: 0.3690 - val_loss: 2.5823 - val_accuracy: 0.3787\n","Epoch 7/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.2425 - accuracy: 0.4285 - val_loss: 2.3301 - val_accuracy: 0.4594\n","Epoch 8/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 2.0130 - accuracy: 0.4775 - val_loss: 2.1441 - val_accuracy: 0.4920\n","Epoch 9/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.8449 - accuracy: 0.5114 - val_loss: 2.0520 - val_accuracy: 0.5316\n","Epoch 10/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.7184 - accuracy: 0.5323 - val_loss: 1.9485 - val_accuracy: 0.5188\n","Epoch 11/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.6198 - accuracy: 0.5545 - val_loss: 1.9020 - val_accuracy: 0.5393\n","Epoch 12/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.5387 - accuracy: 0.5664 - val_loss: 1.7699 - val_accuracy: 0.5833\n","Epoch 13/40\n","1846/1846 [==============================] - 12s 6ms/step - loss: 1.4439 - accuracy: 0.5832 - val_loss: 1.6653 - val_accuracy: 0.5941\n","Epoch 14/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.3601 - accuracy: 0.6054 - val_loss: 1.5924 - val_accuracy: 0.5943\n","Epoch 15/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.2833 - accuracy: 0.6306 - val_loss: 1.5174 - val_accuracy: 0.6284\n","Epoch 16/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.2256 - accuracy: 0.6364 - val_loss: 1.4867 - val_accuracy: 0.6304\n","Epoch 17/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.1754 - accuracy: 0.6513 - val_loss: 1.3839 - val_accuracy: 0.6739\n","Epoch 18/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.1362 - accuracy: 0.6612 - val_loss: 1.3432 - val_accuracy: 0.6673\n","Epoch 19/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.0891 - accuracy: 0.6685 - val_loss: 1.3697 - val_accuracy: 0.6741\n","Epoch 20/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.0579 - accuracy: 0.6742 - val_loss: 1.2860 - val_accuracy: 0.6895\n","Epoch 21/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.0205 - accuracy: 0.6889 - val_loss: 1.2568 - val_accuracy: 0.6902\n","Epoch 22/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.9857 - accuracy: 0.6981 - val_loss: 1.2763 - val_accuracy: 0.7003\n","Epoch 23/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.9550 - accuracy: 0.7064 - val_loss: 1.4128 - val_accuracy: 0.6502\n","Epoch 24/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.9370 - accuracy: 0.7107 - val_loss: 1.1936 - val_accuracy: 0.7446\n","Epoch 25/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.9086 - accuracy: 0.7161 - val_loss: 1.2215 - val_accuracy: 0.7234\n","Epoch 26/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.8878 - accuracy: 0.7254 - val_loss: 1.2573 - val_accuracy: 0.7003\n","Epoch 27/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.8749 - accuracy: 0.7250 - val_loss: 1.1407 - val_accuracy: 0.7395\n","Epoch 28/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.8479 - accuracy: 0.7368 - val_loss: 1.1167 - val_accuracy: 0.7432\n","Epoch 29/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.8221 - accuracy: 0.7451 - val_loss: 1.2729 - val_accuracy: 0.6931\n","Epoch 30/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.8232 - accuracy: 0.7395 - val_loss: 1.1036 - val_accuracy: 0.7558\n","Epoch 31/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7889 - accuracy: 0.7491 - val_loss: 1.1147 - val_accuracy: 0.7485\n","Epoch 32/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7902 - accuracy: 0.7511 - val_loss: 1.0343 - val_accuracy: 0.7703\n","Epoch 33/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7706 - accuracy: 0.7554 - val_loss: 1.0719 - val_accuracy: 0.7659\n","Epoch 34/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7548 - accuracy: 0.7609 - val_loss: 1.1266 - val_accuracy: 0.7397\n","Epoch 35/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7491 - accuracy: 0.7669 - val_loss: 1.0592 - val_accuracy: 0.7611\n","Epoch 36/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7330 - accuracy: 0.7697 - val_loss: 1.1276 - val_accuracy: 0.7589\n","Epoch 37/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7256 - accuracy: 0.7722 - val_loss: 1.0207 - val_accuracy: 0.7727\n","Epoch 38/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7148 - accuracy: 0.7767 - val_loss: 1.0317 - val_accuracy: 0.7864\n","Epoch 39/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.6973 - accuracy: 0.7780 - val_loss: 1.0091 - val_accuracy: 0.7802\n","Epoch 40/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.6940 - accuracy: 0.7788 - val_loss: 1.0315 - val_accuracy: 0.7710\n","Fold: 2\n","Epoch 1/40\n","1846/1846 [==============================] - 9s 4ms/step - loss: 4.7414 - accuracy: 0.0487 - val_loss: 4.5469 - val_accuracy: 0.0537\n","Epoch 2/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 4.3271 - accuracy: 0.0698 - val_loss: 4.2649 - val_accuracy: 0.0904\n","Epoch 3/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 3.9009 - accuracy: 0.1389 - val_loss: 3.7324 - val_accuracy: 0.2088\n","Epoch 4/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 3.3907 - accuracy: 0.2271 - val_loss: 3.3431 - val_accuracy: 0.2543\n","Epoch 5/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.9381 - accuracy: 0.3075 - val_loss: 2.9535 - val_accuracy: 0.3490\n","Epoch 6/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.4919 - accuracy: 0.3895 - val_loss: 2.5479 - val_accuracy: 0.4183\n","Epoch 7/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.1252 - accuracy: 0.4589 - val_loss: 2.2978 - val_accuracy: 0.5065\n","Epoch 8/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.8437 - accuracy: 0.5325 - val_loss: 2.1221 - val_accuracy: 0.5054\n","Epoch 9/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.6346 - accuracy: 0.5721 - val_loss: 1.8749 - val_accuracy: 0.5890\n","Epoch 10/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.4726 - accuracy: 0.6050 - val_loss: 1.8226 - val_accuracy: 0.5824\n","Epoch 11/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.3422 - accuracy: 0.6340 - val_loss: 1.6369 - val_accuracy: 0.6339\n","Epoch 12/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.2455 - accuracy: 0.6522 - val_loss: 1.5531 - val_accuracy: 0.6647\n","Epoch 13/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.1591 - accuracy: 0.6691 - val_loss: 1.5333 - val_accuracy: 0.6539\n","Epoch 14/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.0901 - accuracy: 0.6916 - val_loss: 1.4044 - val_accuracy: 0.6977\n","Epoch 15/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.0197 - accuracy: 0.7112 - val_loss: 1.3506 - val_accuracy: 0.6810\n","Epoch 16/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.9688 - accuracy: 0.7217 - val_loss: 1.2882 - val_accuracy: 0.7056\n","Epoch 17/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.9160 - accuracy: 0.7318 - val_loss: 1.2475 - val_accuracy: 0.7296\n","Epoch 18/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.8770 - accuracy: 0.7386 - val_loss: 1.1625 - val_accuracy: 0.7551\n","Epoch 19/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.8409 - accuracy: 0.7529 - val_loss: 1.2807 - val_accuracy: 0.7287\n","Epoch 20/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 0.8116 - accuracy: 0.7572 - val_loss: 1.2294 - val_accuracy: 0.6920\n","Epoch 21/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.7771 - accuracy: 0.7711 - val_loss: 1.0593 - val_accuracy: 0.7615\n","Epoch 22/40\n","1846/1846 [==============================] - 13s 7ms/step - loss: 0.7613 - accuracy: 0.7773 - val_loss: 1.1114 - val_accuracy: 0.7608\n","Epoch 23/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.7317 - accuracy: 0.7817 - val_loss: 1.1242 - val_accuracy: 0.7292\n","Epoch 24/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.7210 - accuracy: 0.7832 - val_loss: 1.1553 - val_accuracy: 0.7320\n","Epoch 25/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.7066 - accuracy: 0.7855 - val_loss: 1.0876 - val_accuracy: 0.7399\n","Epoch 26/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.6895 - accuracy: 0.7942 - val_loss: 1.0649 - val_accuracy: 0.7551\n","Epoch 27/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.6616 - accuracy: 0.8047 - val_loss: 1.0662 - val_accuracy: 0.7573\n","Epoch 28/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.6539 - accuracy: 0.7985 - val_loss: 0.9873 - val_accuracy: 0.7868\n","Epoch 29/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.6534 - accuracy: 0.8036 - val_loss: 0.9550 - val_accuracy: 0.7923\n","Epoch 30/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.6297 - accuracy: 0.8058 - val_loss: 1.1335 - val_accuracy: 0.7413\n","Epoch 31/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.6459 - accuracy: 0.8007 - val_loss: 0.9988 - val_accuracy: 0.7967\n","Epoch 32/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.6095 - accuracy: 0.8158 - val_loss: 1.0166 - val_accuracy: 0.7608\n","Epoch 33/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.6097 - accuracy: 0.8178 - val_loss: 0.9467 - val_accuracy: 0.7855\n","Epoch 34/40\n","1846/1846 [==============================] - 8s 5ms/step - loss: 0.6056 - accuracy: 0.8153 - val_loss: 0.9063 - val_accuracy: 0.8145\n","Epoch 35/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.6005 - accuracy: 0.8190 - val_loss: 0.9326 - val_accuracy: 0.7813\n","Epoch 36/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.5823 - accuracy: 0.8254 - val_loss: 0.9313 - val_accuracy: 0.8062\n","Epoch 37/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.5750 - accuracy: 0.8237 - val_loss: 0.8750 - val_accuracy: 0.8306\n","Epoch 38/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.5642 - accuracy: 0.8278 - val_loss: 0.8984 - val_accuracy: 0.7936\n","Epoch 39/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.5681 - accuracy: 0.8311 - val_loss: 0.8505 - val_accuracy: 0.8264\n","Epoch 40/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.5518 - accuracy: 0.8337 - val_loss: 0.8204 - val_accuracy: 0.8365\n","Average Validation Accuracy: 0.8109152615070343\n","Average Validation Loss: 0.7122560739517212\n","Average Test Accuracy: 0.8084690570831299\n","Final Test Accuracy for each fold: 0.8432224988937378\n","Number of input features: 9\n","Fold: 1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/40\n","1846/1846 [==============================] - 11s 4ms/step - loss: 4.8238 - accuracy: 0.0467 - val_loss: 4.5841 - val_accuracy: 0.0486\n","Epoch 2/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 4.3382 - accuracy: 0.0686 - val_loss: 4.2209 - val_accuracy: 0.0869\n","Epoch 3/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 3.8190 - accuracy: 0.1662 - val_loss: 3.5849 - val_accuracy: 0.2051\n","Epoch 4/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 3.1805 - accuracy: 0.3011 - val_loss: 3.0664 - val_accuracy: 0.3402\n","Epoch 5/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.7072 - accuracy: 0.3730 - val_loss: 2.6991 - val_accuracy: 0.3952\n","Epoch 6/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.3440 - accuracy: 0.4422 - val_loss: 2.3956 - val_accuracy: 0.4515\n","Epoch 7/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.0773 - accuracy: 0.4904 - val_loss: 2.1792 - val_accuracy: 0.5085\n","Epoch 8/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.8824 - accuracy: 0.5331 - val_loss: 2.0451 - val_accuracy: 0.5135\n","Epoch 9/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.7268 - accuracy: 0.5612 - val_loss: 1.8999 - val_accuracy: 0.5848\n","Epoch 10/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.6029 - accuracy: 0.5893 - val_loss: 1.8808 - val_accuracy: 0.5767\n","Epoch 11/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.4938 - accuracy: 0.6114 - val_loss: 1.7506 - val_accuracy: 0.6183\n","Epoch 12/40\n","1846/1846 [==============================] - 8s 5ms/step - loss: 1.4066 - accuracy: 0.6318 - val_loss: 1.6857 - val_accuracy: 0.6246\n","Epoch 13/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.3318 - accuracy: 0.6387 - val_loss: 1.6079 - val_accuracy: 0.6502\n","Epoch 14/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.2551 - accuracy: 0.6608 - val_loss: 1.5775 - val_accuracy: 0.6510\n","Epoch 15/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.2030 - accuracy: 0.6702 - val_loss: 1.7612 - val_accuracy: 0.6033\n","Epoch 16/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.1512 - accuracy: 0.6792 - val_loss: 1.5006 - val_accuracy: 0.6779\n","Epoch 17/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.1091 - accuracy: 0.6848 - val_loss: 1.4401 - val_accuracy: 0.6900\n","Epoch 18/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.0772 - accuracy: 0.6926 - val_loss: 1.4255 - val_accuracy: 0.7124\n","Epoch 19/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.0245 - accuracy: 0.7054 - val_loss: 1.5120 - val_accuracy: 0.6684\n","Epoch 20/40\n","1846/1846 [==============================] - 8s 5ms/step - loss: 1.0031 - accuracy: 0.7141 - val_loss: 1.4350 - val_accuracy: 0.6920\n","Epoch 21/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.9729 - accuracy: 0.7208 - val_loss: 1.4431 - val_accuracy: 0.6851\n","Epoch 22/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.9596 - accuracy: 0.7222 - val_loss: 1.3782 - val_accuracy: 0.7012\n","Epoch 23/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.9362 - accuracy: 0.7265 - val_loss: 1.3317 - val_accuracy: 0.7206\n","Epoch 24/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.9039 - accuracy: 0.7398 - val_loss: 1.3164 - val_accuracy: 0.7256\n","Epoch 25/40\n","1846/1846 [==============================] - 8s 5ms/step - loss: 0.8919 - accuracy: 0.7395 - val_loss: 1.2534 - val_accuracy: 0.7459\n","Epoch 26/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.8706 - accuracy: 0.7457 - val_loss: 1.3592 - val_accuracy: 0.7017\n","Epoch 27/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.8645 - accuracy: 0.7479 - val_loss: 1.3235 - val_accuracy: 0.7085\n","Epoch 28/40\n","1846/1846 [==============================] - 8s 5ms/step - loss: 0.8340 - accuracy: 0.7575 - val_loss: 1.2357 - val_accuracy: 0.7613\n","Epoch 29/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.8226 - accuracy: 0.7611 - val_loss: 1.2099 - val_accuracy: 0.7644\n","Epoch 30/40\n","1846/1846 [==============================] - 12s 6ms/step - loss: 0.8057 - accuracy: 0.7626 - val_loss: 1.1719 - val_accuracy: 0.7729\n","Epoch 31/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.7973 - accuracy: 0.7644 - val_loss: 1.1384 - val_accuracy: 0.7971\n","Epoch 32/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 0.7793 - accuracy: 0.7675 - val_loss: 1.1793 - val_accuracy: 0.7734\n","Epoch 33/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.7628 - accuracy: 0.7725 - val_loss: 1.2034 - val_accuracy: 0.7608\n","Epoch 34/40\n","1846/1846 [==============================] - 12s 7ms/step - loss: 0.7529 - accuracy: 0.7812 - val_loss: 1.1521 - val_accuracy: 0.7855\n","Epoch 35/40\n","1846/1846 [==============================] - 18s 10ms/step - loss: 0.7403 - accuracy: 0.7827 - val_loss: 1.1287 - val_accuracy: 0.7930\n","Epoch 36/40\n","1846/1846 [==============================] - 17s 9ms/step - loss: 0.7253 - accuracy: 0.7869 - val_loss: 1.1296 - val_accuracy: 0.7936\n","Epoch 37/40\n","1846/1846 [==============================] - 14s 8ms/step - loss: 0.7266 - accuracy: 0.7822 - val_loss: 1.1216 - val_accuracy: 0.8020\n","Epoch 38/40\n","1846/1846 [==============================] - 14s 8ms/step - loss: 0.7151 - accuracy: 0.7873 - val_loss: 1.3133 - val_accuracy: 0.7179\n","Epoch 39/40\n","1846/1846 [==============================] - 13s 7ms/step - loss: 0.7067 - accuracy: 0.7915 - val_loss: 1.2314 - val_accuracy: 0.7611\n","Epoch 40/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.6960 - accuracy: 0.7927 - val_loss: 1.0880 - val_accuracy: 0.8035\n","Fold: 2\n","Epoch 1/40\n","1846/1846 [==============================] - 12s 5ms/step - loss: 4.7629 - accuracy: 0.0543 - val_loss: 4.5321 - val_accuracy: 0.0587\n","Epoch 2/40\n","1846/1846 [==============================] - 12s 6ms/step - loss: 4.2837 - accuracy: 0.0852 - val_loss: 4.2310 - val_accuracy: 0.1085\n","Epoch 3/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 3.9930 - accuracy: 0.1203 - val_loss: 3.9810 - val_accuracy: 0.1487\n","Epoch 4/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 3.7456 - accuracy: 0.1524 - val_loss: 3.7663 - val_accuracy: 0.1710\n","Epoch 5/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 3.5053 - accuracy: 0.1757 - val_loss: 3.5730 - val_accuracy: 0.1875\n","Epoch 6/40\n","1846/1846 [==============================] - 13s 7ms/step - loss: 3.2530 - accuracy: 0.2159 - val_loss: 3.2950 - val_accuracy: 0.2686\n","Epoch 7/40\n","1846/1846 [==============================] - 15s 8ms/step - loss: 2.9181 - accuracy: 0.2923 - val_loss: 2.9868 - val_accuracy: 0.3314\n","Epoch 8/40\n","1846/1846 [==============================] - 15s 8ms/step - loss: 2.5839 - accuracy: 0.3650 - val_loss: 2.7260 - val_accuracy: 0.4000\n","Epoch 9/40\n","1846/1846 [==============================] - 14s 8ms/step - loss: 2.3152 - accuracy: 0.4239 - val_loss: 2.5187 - val_accuracy: 0.4444\n","Epoch 10/40\n","1846/1846 [==============================] - 14s 8ms/step - loss: 2.1076 - accuracy: 0.4706 - val_loss: 2.4462 - val_accuracy: 0.4779\n","Epoch 11/40\n","1846/1846 [==============================] - 14s 7ms/step - loss: 1.9557 - accuracy: 0.5030 - val_loss: 2.3305 - val_accuracy: 0.5012\n","Epoch 12/40\n","1846/1846 [==============================] - 13s 7ms/step - loss: 1.8349 - accuracy: 0.5230 - val_loss: 2.1824 - val_accuracy: 0.5459\n","Epoch 13/40\n","1846/1846 [==============================] - 14s 7ms/step - loss: 1.7459 - accuracy: 0.5342 - val_loss: 2.1651 - val_accuracy: 0.5320\n","Epoch 14/40\n","1846/1846 [==============================] - 17s 9ms/step - loss: 1.6566 - accuracy: 0.5508 - val_loss: 2.0814 - val_accuracy: 0.5699\n","Epoch 15/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.5828 - accuracy: 0.5746 - val_loss: 2.0636 - val_accuracy: 0.5584\n","Epoch 16/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.5215 - accuracy: 0.5796 - val_loss: 2.0964 - val_accuracy: 0.5432\n","Epoch 17/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.4676 - accuracy: 0.5935 - val_loss: 2.0132 - val_accuracy: 0.5593\n","Epoch 18/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.4045 - accuracy: 0.6070 - val_loss: 1.8998 - val_accuracy: 0.6213\n","Epoch 19/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.3532 - accuracy: 0.6239 - val_loss: 1.8824 - val_accuracy: 0.6286\n","Epoch 20/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.3100 - accuracy: 0.6364 - val_loss: 1.8751 - val_accuracy: 0.6110\n","Epoch 21/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.2591 - accuracy: 0.6500 - val_loss: 1.9056 - val_accuracy: 0.6330\n","Epoch 22/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.2214 - accuracy: 0.6586 - val_loss: 1.8316 - val_accuracy: 0.6196\n","Epoch 23/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.1858 - accuracy: 0.6622 - val_loss: 1.7644 - val_accuracy: 0.6506\n","Epoch 24/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.1462 - accuracy: 0.6768 - val_loss: 1.8298 - val_accuracy: 0.6337\n","Epoch 25/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.1132 - accuracy: 0.6872 - val_loss: 1.7492 - val_accuracy: 0.6887\n","Epoch 26/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.0738 - accuracy: 0.6954 - val_loss: 1.7956 - val_accuracy: 0.6728\n","Epoch 27/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.0506 - accuracy: 0.7059 - val_loss: 1.6874 - val_accuracy: 0.7091\n","Epoch 28/40\n","1846/1846 [==============================] - 8s 5ms/step - loss: 1.0302 - accuracy: 0.7054 - val_loss: 1.6708 - val_accuracy: 0.7116\n","Epoch 29/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.9893 - accuracy: 0.7183 - val_loss: 1.7872 - val_accuracy: 0.6484\n","Epoch 30/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.9708 - accuracy: 0.7217 - val_loss: 1.7350 - val_accuracy: 0.7074\n","Epoch 31/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.9491 - accuracy: 0.7301 - val_loss: 1.6462 - val_accuracy: 0.7245\n","Epoch 32/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.9328 - accuracy: 0.7319 - val_loss: 1.6539 - val_accuracy: 0.7333\n","Epoch 33/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.9102 - accuracy: 0.7434 - val_loss: 1.6505 - val_accuracy: 0.7338\n","Epoch 34/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.8894 - accuracy: 0.7535 - val_loss: 1.5898 - val_accuracy: 0.7448\n","Epoch 35/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.8804 - accuracy: 0.7472 - val_loss: 1.6863 - val_accuracy: 0.7254\n","Epoch 36/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.8566 - accuracy: 0.7541 - val_loss: 1.6067 - val_accuracy: 0.7384\n","Epoch 37/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.8398 - accuracy: 0.7596 - val_loss: 1.6557 - val_accuracy: 0.7342\n","Epoch 38/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.8167 - accuracy: 0.7675 - val_loss: 1.5403 - val_accuracy: 0.7738\n","Epoch 39/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.8183 - accuracy: 0.7631 - val_loss: 1.6041 - val_accuracy: 0.7421\n","Epoch 40/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.7993 - accuracy: 0.7709 - val_loss: 1.6034 - val_accuracy: 0.7608\n","Average Validation Accuracy: 0.7925055027008057\n","Average Validation Loss: 0.9254288375377655\n","Average Test Accuracy: 0.7933957576751709\n","Final Test Accuracy for each fold: 0.8170561194419861\n","Number of input features: 10\n","Fold: 1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/40\n","1846/1846 [==============================] - 17s 5ms/step - loss: 4.7666 - accuracy: 0.0537 - val_loss: 4.4921 - val_accuracy: 0.0880\n","Epoch 2/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 4.2318 - accuracy: 0.1020 - val_loss: 4.1356 - val_accuracy: 0.1098\n","Epoch 3/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.9062 - accuracy: 0.1253 - val_loss: 3.8696 - val_accuracy: 0.1331\n","Epoch 4/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 3.6092 - accuracy: 0.1682 - val_loss: 3.5646 - val_accuracy: 0.1657\n","Epoch 5/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 3.3040 - accuracy: 0.2135 - val_loss: 3.3120 - val_accuracy: 0.2293\n","Epoch 6/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 3.0043 - accuracy: 0.2587 - val_loss: 3.0343 - val_accuracy: 0.2671\n","Epoch 7/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.7286 - accuracy: 0.3181 - val_loss: 2.7869 - val_accuracy: 0.3388\n","Epoch 8/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 2.4480 - accuracy: 0.3834 - val_loss: 2.5175 - val_accuracy: 0.4011\n","Epoch 9/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.1766 - accuracy: 0.4471 - val_loss: 2.2711 - val_accuracy: 0.4744\n","Epoch 10/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.9434 - accuracy: 0.4913 - val_loss: 2.0715 - val_accuracy: 0.5043\n","Epoch 11/40\n","1846/1846 [==============================] - 13s 7ms/step - loss: 1.7216 - accuracy: 0.5405 - val_loss: 1.9047 - val_accuracy: 0.5505\n","Epoch 12/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.5322 - accuracy: 0.5834 - val_loss: 1.7292 - val_accuracy: 0.5980\n","Epoch 13/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.3728 - accuracy: 0.6213 - val_loss: 1.6969 - val_accuracy: 0.6198\n","Epoch 14/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.2556 - accuracy: 0.6462 - val_loss: 1.5477 - val_accuracy: 0.6499\n","Epoch 15/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.1507 - accuracy: 0.6779 - val_loss: 1.4498 - val_accuracy: 0.6770\n","Epoch 16/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.0822 - accuracy: 0.6951 - val_loss: 1.4245 - val_accuracy: 0.6757\n","Epoch 17/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.0124 - accuracy: 0.7153 - val_loss: 1.3324 - val_accuracy: 0.7371\n","Epoch 18/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.9705 - accuracy: 0.7202 - val_loss: 1.3179 - val_accuracy: 0.7190\n","Epoch 19/40\n","1846/1846 [==============================] - 12s 6ms/step - loss: 0.9279 - accuracy: 0.7331 - val_loss: 1.2793 - val_accuracy: 0.7274\n","Epoch 20/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.8904 - accuracy: 0.7429 - val_loss: 1.3049 - val_accuracy: 0.7076\n","Epoch 21/40\n","1846/1846 [==============================] - 13s 7ms/step - loss: 0.8586 - accuracy: 0.7515 - val_loss: 1.3351 - val_accuracy: 0.7050\n","Epoch 22/40\n","1846/1846 [==============================] - 13s 7ms/step - loss: 0.8397 - accuracy: 0.7592 - val_loss: 1.2854 - val_accuracy: 0.7237\n","Epoch 23/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.8198 - accuracy: 0.7651 - val_loss: 1.2200 - val_accuracy: 0.7485\n","Epoch 24/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.7871 - accuracy: 0.7713 - val_loss: 1.2181 - val_accuracy: 0.7523\n","Epoch 25/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.7664 - accuracy: 0.7786 - val_loss: 1.2638 - val_accuracy: 0.7514\n","Epoch 26/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.7564 - accuracy: 0.7865 - val_loss: 1.2223 - val_accuracy: 0.7525\n","Epoch 27/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.7461 - accuracy: 0.7795 - val_loss: 1.1662 - val_accuracy: 0.7771\n","Epoch 28/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.7232 - accuracy: 0.7873 - val_loss: 1.1044 - val_accuracy: 0.7894\n","Epoch 29/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.7042 - accuracy: 0.7955 - val_loss: 1.1299 - val_accuracy: 0.7809\n","Epoch 30/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.6921 - accuracy: 0.8008 - val_loss: 1.0860 - val_accuracy: 0.8075\n","Epoch 31/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.6855 - accuracy: 0.8012 - val_loss: 1.1283 - val_accuracy: 0.7864\n","Epoch 32/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.6694 - accuracy: 0.8050 - val_loss: 1.1049 - val_accuracy: 0.8128\n","Epoch 33/40\n","1846/1846 [==============================] - 8s 5ms/step - loss: 0.6610 - accuracy: 0.8053 - val_loss: 1.0855 - val_accuracy: 0.8141\n","Epoch 34/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.6552 - accuracy: 0.8129 - val_loss: 1.1341 - val_accuracy: 0.7776\n","Epoch 35/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6420 - accuracy: 0.8170 - val_loss: 1.1201 - val_accuracy: 0.7925\n","Epoch 36/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6312 - accuracy: 0.8160 - val_loss: 1.1105 - val_accuracy: 0.7985\n","Epoch 37/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6257 - accuracy: 0.8175 - val_loss: 1.0299 - val_accuracy: 0.8176\n","Epoch 38/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.6126 - accuracy: 0.8200 - val_loss: 1.1540 - val_accuracy: 0.7831\n","Epoch 39/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.6161 - accuracy: 0.8210 - val_loss: 1.2078 - val_accuracy: 0.7463\n","Epoch 40/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.5967 - accuracy: 0.8260 - val_loss: 1.0349 - val_accuracy: 0.8086\n","Fold: 2\n","Epoch 1/40\n","1846/1846 [==============================] - 15s 6ms/step - loss: 4.8016 - accuracy: 0.0528 - val_loss: 4.5741 - val_accuracy: 0.0803\n","Epoch 2/40\n","1846/1846 [==============================] - 12s 6ms/step - loss: 4.3067 - accuracy: 0.0929 - val_loss: 4.2504 - val_accuracy: 0.1001\n","Epoch 3/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.0222 - accuracy: 0.1118 - val_loss: 3.9706 - val_accuracy: 0.1283\n","Epoch 4/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 3.6851 - accuracy: 0.1458 - val_loss: 3.6510 - val_accuracy: 0.1773\n","Epoch 5/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 3.2961 - accuracy: 0.2160 - val_loss: 3.2764 - val_accuracy: 0.2715\n","Epoch 6/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.8865 - accuracy: 0.3132 - val_loss: 2.9526 - val_accuracy: 0.3496\n","Epoch 7/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.5514 - accuracy: 0.3854 - val_loss: 2.6797 - val_accuracy: 0.3855\n","Epoch 8/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.3123 - accuracy: 0.4235 - val_loss: 2.4465 - val_accuracy: 0.4420\n","Epoch 9/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.1004 - accuracy: 0.4712 - val_loss: 2.3196 - val_accuracy: 0.4667\n","Epoch 10/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.9133 - accuracy: 0.5096 - val_loss: 2.1424 - val_accuracy: 0.5175\n","Epoch 11/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.7432 - accuracy: 0.5552 - val_loss: 2.0435 - val_accuracy: 0.5441\n","Epoch 12/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.6033 - accuracy: 0.5885 - val_loss: 1.8775 - val_accuracy: 0.5993\n","Epoch 13/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.4872 - accuracy: 0.6236 - val_loss: 1.8130 - val_accuracy: 0.6055\n","Epoch 14/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.3831 - accuracy: 0.6531 - val_loss: 1.7088 - val_accuracy: 0.6603\n","Epoch 15/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.2917 - accuracy: 0.6672 - val_loss: 1.6660 - val_accuracy: 0.6649\n","Epoch 16/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.2183 - accuracy: 0.6860 - val_loss: 1.5498 - val_accuracy: 0.6913\n","Epoch 17/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.1631 - accuracy: 0.6988 - val_loss: 1.5258 - val_accuracy: 0.6779\n","Epoch 18/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.0985 - accuracy: 0.7140 - val_loss: 1.4476 - val_accuracy: 0.7010\n","Epoch 19/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.0418 - accuracy: 0.7318 - val_loss: 1.3953 - val_accuracy: 0.7078\n","Epoch 20/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.0041 - accuracy: 0.7352 - val_loss: 1.3479 - val_accuracy: 0.7153\n","Epoch 21/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.9559 - accuracy: 0.7450 - val_loss: 1.2882 - val_accuracy: 0.7232\n","Epoch 22/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.9223 - accuracy: 0.7571 - val_loss: 1.3353 - val_accuracy: 0.7197\n","Epoch 23/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.8866 - accuracy: 0.7641 - val_loss: 1.2424 - val_accuracy: 0.7472\n","Epoch 24/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.8642 - accuracy: 0.7696 - val_loss: 1.1991 - val_accuracy: 0.7505\n","Epoch 25/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.8303 - accuracy: 0.7763 - val_loss: 1.1462 - val_accuracy: 0.7734\n","Epoch 26/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.8003 - accuracy: 0.7812 - val_loss: 1.1658 - val_accuracy: 0.7551\n","Epoch 27/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7965 - accuracy: 0.7806 - val_loss: 1.1332 - val_accuracy: 0.7811\n","Epoch 28/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7642 - accuracy: 0.7932 - val_loss: 1.0812 - val_accuracy: 0.7685\n","Epoch 29/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7401 - accuracy: 0.7972 - val_loss: 1.3126 - val_accuracy: 0.7289\n","Epoch 30/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7263 - accuracy: 0.8010 - val_loss: 1.0471 - val_accuracy: 0.7982\n","Epoch 31/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7041 - accuracy: 0.8064 - val_loss: 1.0409 - val_accuracy: 0.8011\n","Epoch 32/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.6909 - accuracy: 0.8117 - val_loss: 1.0817 - val_accuracy: 0.7848\n","Epoch 33/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.6795 - accuracy: 0.8128 - val_loss: 1.0302 - val_accuracy: 0.7758\n","Epoch 34/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.6641 - accuracy: 0.8158 - val_loss: 1.0769 - val_accuracy: 0.7809\n","Epoch 35/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.6535 - accuracy: 0.8140 - val_loss: 0.9705 - val_accuracy: 0.8024\n","Epoch 36/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.6348 - accuracy: 0.8193 - val_loss: 1.0157 - val_accuracy: 0.7868\n","Epoch 37/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.6278 - accuracy: 0.8263 - val_loss: 0.9840 - val_accuracy: 0.8035\n","Epoch 38/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.6098 - accuracy: 0.8259 - val_loss: 0.9912 - val_accuracy: 0.8004\n","Epoch 39/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.5956 - accuracy: 0.8341 - val_loss: 0.9114 - val_accuracy: 0.8222\n","Epoch 40/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.5838 - accuracy: 0.8357 - val_loss: 0.9116 - val_accuracy: 0.8262\n","Average Validation Accuracy: 0.8342593610286713\n","Average Validation Loss: 0.6960675120353699\n","Average Test Accuracy: 0.8351514637470245\n","Final Test Accuracy for each fold: 0.8435173630714417\n","Number of input features: 11\n","Fold: 1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/40\n","1846/1846 [==============================] - 10s 4ms/step - loss: 4.7879 - accuracy: 0.0493 - val_loss: 4.5177 - val_accuracy: 0.0880\n","Epoch 2/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 4.2022 - accuracy: 0.1012 - val_loss: 4.0418 - val_accuracy: 0.1065\n","Epoch 3/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 3.7812 - accuracy: 0.1352 - val_loss: 3.6844 - val_accuracy: 0.1536\n","Epoch 4/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 3.3543 - accuracy: 0.2165 - val_loss: 3.2246 - val_accuracy: 0.2398\n","Epoch 5/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.7983 - accuracy: 0.3350 - val_loss: 2.7554 - val_accuracy: 0.3604\n","Epoch 6/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.4112 - accuracy: 0.4151 - val_loss: 2.4504 - val_accuracy: 0.4396\n","Epoch 7/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.1309 - accuracy: 0.4767 - val_loss: 2.2643 - val_accuracy: 0.4559\n","Epoch 8/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.9228 - accuracy: 0.5201 - val_loss: 2.0673 - val_accuracy: 0.5298\n","Epoch 9/40\n","1846/1846 [==============================] - 6s 4ms/step - loss: 1.7635 - accuracy: 0.5597 - val_loss: 1.9264 - val_accuracy: 0.5553\n","Epoch 10/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.6403 - accuracy: 0.5798 - val_loss: 1.8205 - val_accuracy: 0.5985\n","Epoch 11/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.5296 - accuracy: 0.6050 - val_loss: 1.7969 - val_accuracy: 0.5945\n","Epoch 12/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.4366 - accuracy: 0.6244 - val_loss: 1.6793 - val_accuracy: 0.6264\n","Epoch 13/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.3554 - accuracy: 0.6475 - val_loss: 1.6104 - val_accuracy: 0.6367\n","Epoch 14/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.2840 - accuracy: 0.6582 - val_loss: 1.5753 - val_accuracy: 0.6440\n","Epoch 15/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.2204 - accuracy: 0.6704 - val_loss: 1.5717 - val_accuracy: 0.6451\n","Epoch 16/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.1691 - accuracy: 0.6793 - val_loss: 1.4477 - val_accuracy: 0.6902\n","Epoch 17/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.1291 - accuracy: 0.6866 - val_loss: 1.4894 - val_accuracy: 0.6557\n","Epoch 18/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.0790 - accuracy: 0.6999 - val_loss: 1.3890 - val_accuracy: 0.7019\n","Epoch 19/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.0381 - accuracy: 0.7115 - val_loss: 1.3634 - val_accuracy: 0.7034\n","Epoch 20/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.0054 - accuracy: 0.7219 - val_loss: 1.3942 - val_accuracy: 0.6948\n","Epoch 21/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.9875 - accuracy: 0.7221 - val_loss: 1.4412 - val_accuracy: 0.6752\n","Epoch 22/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.9560 - accuracy: 0.7272 - val_loss: 1.3514 - val_accuracy: 0.7199\n","Epoch 23/40\n","1846/1846 [==============================] - 8s 5ms/step - loss: 0.9279 - accuracy: 0.7400 - val_loss: 1.2890 - val_accuracy: 0.7294\n","Epoch 24/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.9089 - accuracy: 0.7409 - val_loss: 1.3047 - val_accuracy: 0.7294\n","Epoch 25/40\n","1846/1846 [==============================] - 8s 5ms/step - loss: 0.8927 - accuracy: 0.7492 - val_loss: 1.4347 - val_accuracy: 0.6959\n","Epoch 26/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.8692 - accuracy: 0.7521 - val_loss: 1.3073 - val_accuracy: 0.7245\n","Epoch 27/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.8592 - accuracy: 0.7532 - val_loss: 1.2382 - val_accuracy: 0.7459\n","Epoch 28/40\n","1846/1846 [==============================] - 8s 5ms/step - loss: 0.8439 - accuracy: 0.7600 - val_loss: 1.2488 - val_accuracy: 0.7437\n","Epoch 29/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.8274 - accuracy: 0.7654 - val_loss: 1.2103 - val_accuracy: 0.7729\n","Epoch 30/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.8139 - accuracy: 0.7671 - val_loss: 1.2565 - val_accuracy: 0.7404\n","Epoch 31/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.8088 - accuracy: 0.7714 - val_loss: 1.2119 - val_accuracy: 0.7608\n","Epoch 32/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.8017 - accuracy: 0.7724 - val_loss: 1.1949 - val_accuracy: 0.7652\n","Epoch 33/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7775 - accuracy: 0.7793 - val_loss: 1.3066 - val_accuracy: 0.7384\n","Epoch 34/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.7735 - accuracy: 0.7826 - val_loss: 1.2195 - val_accuracy: 0.7703\n","Epoch 35/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7613 - accuracy: 0.7845 - val_loss: 1.1959 - val_accuracy: 0.7721\n","Epoch 36/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7493 - accuracy: 0.7869 - val_loss: 1.2205 - val_accuracy: 0.7558\n","Epoch 37/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7463 - accuracy: 0.7872 - val_loss: 1.1732 - val_accuracy: 0.7853\n","Epoch 38/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7313 - accuracy: 0.7932 - val_loss: 1.1459 - val_accuracy: 0.7897\n","Epoch 39/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.7256 - accuracy: 0.7918 - val_loss: 1.1167 - val_accuracy: 0.7932\n","Epoch 40/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7091 - accuracy: 0.7994 - val_loss: 1.1577 - val_accuracy: 0.7740\n","Fold: 2\n","Epoch 1/40\n","1846/1846 [==============================] - 9s 4ms/step - loss: 4.7689 - accuracy: 0.0491 - val_loss: 4.5111 - val_accuracy: 0.0684\n","Epoch 2/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 4.2010 - accuracy: 0.0943 - val_loss: 4.0951 - val_accuracy: 0.1146\n","Epoch 3/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 3.7932 - accuracy: 0.1353 - val_loss: 3.7079 - val_accuracy: 0.1868\n","Epoch 4/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 3.3766 - accuracy: 0.2120 - val_loss: 3.2902 - val_accuracy: 0.2526\n","Epoch 5/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.8742 - accuracy: 0.3156 - val_loss: 2.8792 - val_accuracy: 0.3868\n","Epoch 6/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.4397 - accuracy: 0.4118 - val_loss: 2.5139 - val_accuracy: 0.4634\n","Epoch 7/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.1293 - accuracy: 0.4751 - val_loss: 2.2601 - val_accuracy: 0.5228\n","Epoch 8/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.8942 - accuracy: 0.5234 - val_loss: 2.1121 - val_accuracy: 0.5617\n","Epoch 9/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.7180 - accuracy: 0.5636 - val_loss: 1.9816 - val_accuracy: 0.5608\n","Epoch 10/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.5709 - accuracy: 0.5994 - val_loss: 1.8587 - val_accuracy: 0.6185\n","Epoch 11/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.4649 - accuracy: 0.6158 - val_loss: 1.7657 - val_accuracy: 0.6207\n","Epoch 12/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.3643 - accuracy: 0.6405 - val_loss: 1.6827 - val_accuracy: 0.6414\n","Epoch 13/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.2865 - accuracy: 0.6589 - val_loss: 1.6463 - val_accuracy: 0.6484\n","Epoch 14/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.2241 - accuracy: 0.6723 - val_loss: 1.5787 - val_accuracy: 0.6755\n","Epoch 15/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.1659 - accuracy: 0.6909 - val_loss: 1.5223 - val_accuracy: 0.6785\n","Epoch 16/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.1182 - accuracy: 0.6921 - val_loss: 1.4624 - val_accuracy: 0.6942\n","Epoch 17/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0750 - accuracy: 0.7078 - val_loss: 1.4226 - val_accuracy: 0.6865\n","Epoch 18/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.0413 - accuracy: 0.7158 - val_loss: 1.4072 - val_accuracy: 0.6970\n","Epoch 19/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.0069 - accuracy: 0.7224 - val_loss: 1.3456 - val_accuracy: 0.7118\n","Epoch 20/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9791 - accuracy: 0.7282 - val_loss: 1.3923 - val_accuracy: 0.6827\n","Epoch 21/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9464 - accuracy: 0.7411 - val_loss: 1.2891 - val_accuracy: 0.7248\n","Epoch 22/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9323 - accuracy: 0.7450 - val_loss: 1.2978 - val_accuracy: 0.7175\n","Epoch 23/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8989 - accuracy: 0.7527 - val_loss: 1.3003 - val_accuracy: 0.6946\n","Epoch 24/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8843 - accuracy: 0.7525 - val_loss: 1.2279 - val_accuracy: 0.7318\n","Epoch 25/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8550 - accuracy: 0.7608 - val_loss: 1.2357 - val_accuracy: 0.7054\n","Epoch 26/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8358 - accuracy: 0.7634 - val_loss: 1.1959 - val_accuracy: 0.7428\n","Epoch 27/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8222 - accuracy: 0.7702 - val_loss: 1.3418 - val_accuracy: 0.7056\n","Epoch 28/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7984 - accuracy: 0.7774 - val_loss: 1.1622 - val_accuracy: 0.7512\n","Epoch 29/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8013 - accuracy: 0.7806 - val_loss: 1.1749 - val_accuracy: 0.7476\n","Epoch 30/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7702 - accuracy: 0.7870 - val_loss: 1.1193 - val_accuracy: 0.7630\n","Epoch 31/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7630 - accuracy: 0.7874 - val_loss: 1.0865 - val_accuracy: 0.7795\n","Epoch 32/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7520 - accuracy: 0.7909 - val_loss: 1.1168 - val_accuracy: 0.7679\n","Epoch 33/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7328 - accuracy: 0.7964 - val_loss: 1.1360 - val_accuracy: 0.7503\n","Epoch 34/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7236 - accuracy: 0.7972 - val_loss: 1.0410 - val_accuracy: 0.8011\n","Epoch 35/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7072 - accuracy: 0.8002 - val_loss: 1.0375 - val_accuracy: 0.7945\n","Epoch 36/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7076 - accuracy: 0.8032 - val_loss: 1.0383 - val_accuracy: 0.7897\n","Epoch 37/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6968 - accuracy: 0.8069 - val_loss: 0.9805 - val_accuracy: 0.8136\n","Epoch 38/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.6856 - accuracy: 0.8110 - val_loss: 0.9997 - val_accuracy: 0.8022\n","Epoch 39/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6680 - accuracy: 0.8134 - val_loss: 0.9853 - val_accuracy: 0.8075\n","Epoch 40/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6685 - accuracy: 0.8132 - val_loss: 1.0084 - val_accuracy: 0.7974\n","Average Validation Accuracy: 0.8016197979450226\n","Average Validation Loss: 0.8106436133384705\n","Average Test Accuracy: 0.7999557554721832\n","Final Test Accuracy for each fold: 0.8135917782783508\n","Number of input features: 12\n","Fold: 1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/40\n","1846/1846 [==============================] - 12s 5ms/step - loss: 4.7939 - accuracy: 0.0504 - val_loss: 4.5194 - val_accuracy: 0.0906\n","Epoch 2/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 4.1844 - accuracy: 0.1052 - val_loss: 4.0256 - val_accuracy: 0.1153\n","Epoch 3/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 3.7518 - accuracy: 0.1356 - val_loss: 3.6946 - val_accuracy: 0.1360\n","Epoch 4/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 3.4549 - accuracy: 0.1741 - val_loss: 3.4298 - val_accuracy: 0.1716\n","Epoch 5/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 3.1870 - accuracy: 0.2167 - val_loss: 3.1817 - val_accuracy: 0.2552\n","Epoch 6/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.9119 - accuracy: 0.2678 - val_loss: 2.9100 - val_accuracy: 0.3083\n","Epoch 7/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.6116 - accuracy: 0.3431 - val_loss: 2.6948 - val_accuracy: 0.3771\n","Epoch 8/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.3165 - accuracy: 0.4124 - val_loss: 2.4058 - val_accuracy: 0.3925\n","Epoch 9/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 2.0849 - accuracy: 0.4519 - val_loss: 2.2304 - val_accuracy: 0.4530\n","Epoch 10/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.8947 - accuracy: 0.4866 - val_loss: 2.0341 - val_accuracy: 0.4845\n","Epoch 11/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.7315 - accuracy: 0.5216 - val_loss: 1.9121 - val_accuracy: 0.5556\n","Epoch 12/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.5942 - accuracy: 0.5618 - val_loss: 1.8204 - val_accuracy: 0.5336\n","Epoch 13/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.4854 - accuracy: 0.5773 - val_loss: 1.7194 - val_accuracy: 0.5881\n","Epoch 14/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.3942 - accuracy: 0.6022 - val_loss: 1.6435 - val_accuracy: 0.6169\n","Epoch 15/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.3109 - accuracy: 0.6210 - val_loss: 1.6294 - val_accuracy: 0.6128\n","Epoch 16/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.2484 - accuracy: 0.6345 - val_loss: 1.5608 - val_accuracy: 0.6158\n","Epoch 17/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.1915 - accuracy: 0.6550 - val_loss: 1.5246 - val_accuracy: 0.6543\n","Epoch 18/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.1350 - accuracy: 0.6684 - val_loss: 1.4984 - val_accuracy: 0.6515\n","Epoch 19/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.0905 - accuracy: 0.6797 - val_loss: 1.4517 - val_accuracy: 0.6836\n","Epoch 20/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.0438 - accuracy: 0.6933 - val_loss: 1.3884 - val_accuracy: 0.6944\n","Epoch 21/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.0048 - accuracy: 0.7050 - val_loss: 1.3975 - val_accuracy: 0.6891\n","Epoch 22/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.9802 - accuracy: 0.7133 - val_loss: 1.3271 - val_accuracy: 0.7030\n","Epoch 23/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.9424 - accuracy: 0.7238 - val_loss: 1.3960 - val_accuracy: 0.6977\n","Epoch 24/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.9185 - accuracy: 0.7279 - val_loss: 1.2885 - val_accuracy: 0.7173\n","Epoch 25/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.8900 - accuracy: 0.7343 - val_loss: 1.2993 - val_accuracy: 0.7234\n","Epoch 26/40\n","1846/1846 [==============================] - 8s 5ms/step - loss: 0.8656 - accuracy: 0.7465 - val_loss: 1.2426 - val_accuracy: 0.7578\n","Epoch 27/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.8371 - accuracy: 0.7573 - val_loss: 1.3019 - val_accuracy: 0.7294\n","Epoch 28/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.8380 - accuracy: 0.7494 - val_loss: 1.2122 - val_accuracy: 0.7573\n","Epoch 29/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7988 - accuracy: 0.7644 - val_loss: 1.2262 - val_accuracy: 0.7527\n","Epoch 30/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.7914 - accuracy: 0.7699 - val_loss: 1.2225 - val_accuracy: 0.7206\n","Epoch 31/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.7691 - accuracy: 0.7732 - val_loss: 1.1908 - val_accuracy: 0.7685\n","Epoch 32/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.7582 - accuracy: 0.7777 - val_loss: 1.2226 - val_accuracy: 0.7542\n","Epoch 33/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.7490 - accuracy: 0.7794 - val_loss: 1.1891 - val_accuracy: 0.7762\n","Epoch 34/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.7234 - accuracy: 0.7893 - val_loss: 1.1547 - val_accuracy: 0.7844\n","Epoch 35/40\n","1846/1846 [==============================] - 8s 5ms/step - loss: 0.7323 - accuracy: 0.7824 - val_loss: 1.1787 - val_accuracy: 0.7787\n","Epoch 36/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.7026 - accuracy: 0.7956 - val_loss: 1.1634 - val_accuracy: 0.7956\n","Epoch 37/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.6911 - accuracy: 0.7975 - val_loss: 1.2014 - val_accuracy: 0.7604\n","Epoch 38/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.6869 - accuracy: 0.7966 - val_loss: 1.1160 - val_accuracy: 0.8046\n","Epoch 39/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.6687 - accuracy: 0.8039 - val_loss: 1.1235 - val_accuracy: 0.7956\n","Epoch 40/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.6649 - accuracy: 0.8078 - val_loss: 1.1903 - val_accuracy: 0.7736\n","Fold: 2\n","Epoch 1/40\n","1846/1846 [==============================] - 10s 4ms/step - loss: 4.7803 - accuracy: 0.0564 - val_loss: 4.4973 - val_accuracy: 0.1021\n","Epoch 2/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 4.1420 - accuracy: 0.1052 - val_loss: 4.0203 - val_accuracy: 0.1402\n","Epoch 3/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 3.7688 - accuracy: 0.1372 - val_loss: 3.8057 - val_accuracy: 0.1391\n","Epoch 4/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 3.5300 - accuracy: 0.1683 - val_loss: 3.5848 - val_accuracy: 0.1881\n","Epoch 5/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 3.3356 - accuracy: 0.1974 - val_loss: 3.4419 - val_accuracy: 0.2163\n","Epoch 6/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 3.1298 - accuracy: 0.2320 - val_loss: 3.2297 - val_accuracy: 0.2319\n","Epoch 7/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.9108 - accuracy: 0.2773 - val_loss: 3.0324 - val_accuracy: 0.2867\n","Epoch 8/40\n","1846/1846 [==============================] - 6s 4ms/step - loss: 2.6920 - accuracy: 0.3217 - val_loss: 2.8381 - val_accuracy: 0.3619\n","Epoch 9/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.4661 - accuracy: 0.3656 - val_loss: 2.6520 - val_accuracy: 0.3716\n","Epoch 10/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.2326 - accuracy: 0.4155 - val_loss: 2.4157 - val_accuracy: 0.4277\n","Epoch 11/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.0240 - accuracy: 0.4622 - val_loss: 2.2421 - val_accuracy: 0.4728\n","Epoch 12/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.8632 - accuracy: 0.5033 - val_loss: 2.0993 - val_accuracy: 0.5102\n","Epoch 13/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.7174 - accuracy: 0.5351 - val_loss: 1.9349 - val_accuracy: 0.5685\n","Epoch 14/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.5825 - accuracy: 0.5714 - val_loss: 1.8426 - val_accuracy: 0.5881\n","Epoch 15/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.4805 - accuracy: 0.5986 - val_loss: 1.7883 - val_accuracy: 0.5769\n","Epoch 16/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.4054 - accuracy: 0.6137 - val_loss: 1.6848 - val_accuracy: 0.5813\n","Epoch 17/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.3208 - accuracy: 0.6325 - val_loss: 1.5923 - val_accuracy: 0.6163\n","Epoch 18/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.2585 - accuracy: 0.6491 - val_loss: 1.4803 - val_accuracy: 0.6502\n","Epoch 19/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.1990 - accuracy: 0.6633 - val_loss: 1.4515 - val_accuracy: 0.6550\n","Epoch 20/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.1519 - accuracy: 0.6760 - val_loss: 1.3873 - val_accuracy: 0.6935\n","Epoch 21/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.1015 - accuracy: 0.6878 - val_loss: 1.3906 - val_accuracy: 0.6759\n","Epoch 22/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.0677 - accuracy: 0.6933 - val_loss: 1.3950 - val_accuracy: 0.6748\n","Epoch 23/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.0215 - accuracy: 0.7079 - val_loss: 1.2969 - val_accuracy: 0.6889\n","Epoch 24/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9952 - accuracy: 0.7169 - val_loss: 1.2532 - val_accuracy: 0.7151\n","Epoch 25/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9668 - accuracy: 0.7265 - val_loss: 1.2482 - val_accuracy: 0.7122\n","Epoch 26/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.9365 - accuracy: 0.7294 - val_loss: 1.2175 - val_accuracy: 0.7122\n","Epoch 27/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.9190 - accuracy: 0.7364 - val_loss: 1.2200 - val_accuracy: 0.7149\n","Epoch 28/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8835 - accuracy: 0.7481 - val_loss: 1.1844 - val_accuracy: 0.7201\n","Epoch 29/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.8786 - accuracy: 0.7489 - val_loss: 1.1217 - val_accuracy: 0.7520\n","Epoch 30/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.8511 - accuracy: 0.7564 - val_loss: 1.1328 - val_accuracy: 0.7342\n","Epoch 31/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.8361 - accuracy: 0.7620 - val_loss: 1.1174 - val_accuracy: 0.7380\n","Epoch 32/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.8258 - accuracy: 0.7620 - val_loss: 1.1183 - val_accuracy: 0.7485\n","Epoch 33/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.8172 - accuracy: 0.7671 - val_loss: 1.0534 - val_accuracy: 0.7780\n","Epoch 34/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7987 - accuracy: 0.7694 - val_loss: 1.0361 - val_accuracy: 0.7630\n","Epoch 35/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7881 - accuracy: 0.7737 - val_loss: 1.0670 - val_accuracy: 0.7661\n","Epoch 36/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7701 - accuracy: 0.7854 - val_loss: 1.0178 - val_accuracy: 0.7773\n","Epoch 37/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7753 - accuracy: 0.7772 - val_loss: 1.0374 - val_accuracy: 0.7758\n","Epoch 38/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7565 - accuracy: 0.7847 - val_loss: 1.1368 - val_accuracy: 0.7490\n","Epoch 39/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7449 - accuracy: 0.7887 - val_loss: 1.0068 - val_accuracy: 0.7826\n","Epoch 40/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7432 - accuracy: 0.7842 - val_loss: 0.9916 - val_accuracy: 0.7890\n","Average Validation Accuracy: 0.7929057776927948\n","Average Validation Loss: 0.8390403687953949\n","Average Test Accuracy: 0.7925849556922913\n","Final Test Accuracy for each fold: 0.7972285747528076\n","Number of input features: 13\n","Fold: 1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/40\n","1846/1846 [==============================] - 9s 4ms/step - loss: 4.7643 - accuracy: 0.0580 - val_loss: 4.4601 - val_accuracy: 0.0790\n","Epoch 2/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 4.1302 - accuracy: 0.1154 - val_loss: 3.9881 - val_accuracy: 0.1278\n","Epoch 3/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 3.7312 - accuracy: 0.1457 - val_loss: 3.6934 - val_accuracy: 0.1428\n","Epoch 4/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 3.4624 - accuracy: 0.1860 - val_loss: 3.4511 - val_accuracy: 0.1965\n","Epoch 5/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 3.2322 - accuracy: 0.2180 - val_loss: 3.2635 - val_accuracy: 0.2229\n","Epoch 6/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.9997 - accuracy: 0.2564 - val_loss: 3.0677 - val_accuracy: 0.2695\n","Epoch 7/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.7629 - accuracy: 0.3123 - val_loss: 2.8745 - val_accuracy: 0.2975\n","Epoch 8/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.5380 - accuracy: 0.3635 - val_loss: 2.6892 - val_accuracy: 0.3701\n","Epoch 9/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.3280 - accuracy: 0.4261 - val_loss: 2.5259 - val_accuracy: 0.4070\n","Epoch 10/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.1247 - accuracy: 0.4692 - val_loss: 2.4118 - val_accuracy: 0.4002\n","Epoch 11/40\n","1846/1846 [==============================] - 5s 2ms/step - loss: 1.9417 - accuracy: 0.5088 - val_loss: 2.1394 - val_accuracy: 0.5001\n","Epoch 12/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.7878 - accuracy: 0.5402 - val_loss: 2.0833 - val_accuracy: 0.5391\n","Epoch 13/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.6642 - accuracy: 0.5718 - val_loss: 1.9440 - val_accuracy: 0.5870\n","Epoch 14/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.5569 - accuracy: 0.5944 - val_loss: 1.8559 - val_accuracy: 0.5941\n","Epoch 15/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.4695 - accuracy: 0.6160 - val_loss: 1.7723 - val_accuracy: 0.6044\n","Epoch 16/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.3908 - accuracy: 0.6319 - val_loss: 1.7091 - val_accuracy: 0.6273\n","Epoch 17/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.3234 - accuracy: 0.6508 - val_loss: 1.6699 - val_accuracy: 0.6455\n","Epoch 18/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.2610 - accuracy: 0.6633 - val_loss: 1.6190 - val_accuracy: 0.6466\n","Epoch 19/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.2008 - accuracy: 0.6736 - val_loss: 1.5583 - val_accuracy: 0.6724\n","Epoch 20/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.1545 - accuracy: 0.6863 - val_loss: 1.5517 - val_accuracy: 0.6554\n","Epoch 21/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.1014 - accuracy: 0.7028 - val_loss: 1.5132 - val_accuracy: 0.6821\n","Epoch 22/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.0756 - accuracy: 0.7027 - val_loss: 1.4228 - val_accuracy: 0.7076\n","Epoch 23/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.0379 - accuracy: 0.7161 - val_loss: 1.4019 - val_accuracy: 0.7166\n","Epoch 24/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.0138 - accuracy: 0.7184 - val_loss: 1.4271 - val_accuracy: 0.7149\n","Epoch 25/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.9805 - accuracy: 0.7283 - val_loss: 1.4035 - val_accuracy: 0.7050\n","Epoch 26/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.9578 - accuracy: 0.7338 - val_loss: 1.3925 - val_accuracy: 0.7217\n","Epoch 27/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.9308 - accuracy: 0.7426 - val_loss: 1.3699 - val_accuracy: 0.7230\n","Epoch 28/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.9134 - accuracy: 0.7468 - val_loss: 1.4217 - val_accuracy: 0.7127\n","Epoch 29/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.8808 - accuracy: 0.7550 - val_loss: 1.3639 - val_accuracy: 0.7107\n","Epoch 30/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.8723 - accuracy: 0.7560 - val_loss: 1.4114 - val_accuracy: 0.7045\n","Epoch 31/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.8469 - accuracy: 0.7673 - val_loss: 1.3234 - val_accuracy: 0.7347\n","Epoch 32/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.8264 - accuracy: 0.7708 - val_loss: 1.2541 - val_accuracy: 0.7509\n","Epoch 33/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.8131 - accuracy: 0.7738 - val_loss: 1.2589 - val_accuracy: 0.7558\n","Epoch 34/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.8027 - accuracy: 0.7766 - val_loss: 1.2562 - val_accuracy: 0.7439\n","Epoch 35/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.7861 - accuracy: 0.7821 - val_loss: 1.3128 - val_accuracy: 0.7364\n","Epoch 36/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.7766 - accuracy: 0.7841 - val_loss: 1.2188 - val_accuracy: 0.7894\n","Epoch 37/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.7652 - accuracy: 0.7858 - val_loss: 1.1925 - val_accuracy: 0.7890\n","Epoch 38/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.7485 - accuracy: 0.7895 - val_loss: 1.2361 - val_accuracy: 0.7685\n","Epoch 39/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.7422 - accuracy: 0.7957 - val_loss: 1.1849 - val_accuracy: 0.7897\n","Epoch 40/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.7269 - accuracy: 0.7966 - val_loss: 1.1976 - val_accuracy: 0.7872\n","Fold: 2\n","Epoch 1/40\n","1846/1846 [==============================] - 11s 4ms/step - loss: 4.7969 - accuracy: 0.0516 - val_loss: 4.5234 - val_accuracy: 0.0964\n","Epoch 2/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 4.2249 - accuracy: 0.1026 - val_loss: 4.1371 - val_accuracy: 0.1089\n","Epoch 3/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 3.8569 - accuracy: 0.1288 - val_loss: 3.8562 - val_accuracy: 0.1399\n","Epoch 4/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 3.5742 - accuracy: 0.1655 - val_loss: 3.6280 - val_accuracy: 0.1861\n","Epoch 5/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 3.3262 - accuracy: 0.1957 - val_loss: 3.4066 - val_accuracy: 0.2198\n","Epoch 6/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 3.0996 - accuracy: 0.2275 - val_loss: 3.1988 - val_accuracy: 0.2200\n","Epoch 7/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.8711 - accuracy: 0.2686 - val_loss: 3.0013 - val_accuracy: 0.2889\n","Epoch 8/40\n","1846/1846 [==============================] - 5s 2ms/step - loss: 2.6591 - accuracy: 0.3119 - val_loss: 2.8182 - val_accuracy: 0.3336\n","Epoch 9/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.4897 - accuracy: 0.3480 - val_loss: 2.6500 - val_accuracy: 0.3582\n","Epoch 10/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.3408 - accuracy: 0.3792 - val_loss: 2.5895 - val_accuracy: 0.3602\n","Epoch 11/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.2192 - accuracy: 0.4085 - val_loss: 2.4566 - val_accuracy: 0.4013\n","Epoch 12/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.1122 - accuracy: 0.4336 - val_loss: 2.3437 - val_accuracy: 0.4579\n","Epoch 13/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.0222 - accuracy: 0.4525 - val_loss: 2.2461 - val_accuracy: 0.5171\n","Epoch 14/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.9339 - accuracy: 0.4723 - val_loss: 2.1867 - val_accuracy: 0.4750\n","Epoch 15/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.8596 - accuracy: 0.4911 - val_loss: 2.1732 - val_accuracy: 0.4906\n","Epoch 16/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.7861 - accuracy: 0.5126 - val_loss: 2.0746 - val_accuracy: 0.4818\n","Epoch 17/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.7158 - accuracy: 0.5366 - val_loss: 2.0108 - val_accuracy: 0.5168\n","Epoch 18/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.6447 - accuracy: 0.5534 - val_loss: 1.9340 - val_accuracy: 0.5547\n","Epoch 19/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.5743 - accuracy: 0.5715 - val_loss: 1.8767 - val_accuracy: 0.5738\n","Epoch 20/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.5122 - accuracy: 0.5914 - val_loss: 1.7972 - val_accuracy: 0.5855\n","Epoch 21/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.4410 - accuracy: 0.6104 - val_loss: 1.6956 - val_accuracy: 0.6242\n","Epoch 22/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.3810 - accuracy: 0.6236 - val_loss: 1.6298 - val_accuracy: 0.6447\n","Epoch 23/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.3233 - accuracy: 0.6363 - val_loss: 1.6580 - val_accuracy: 0.6174\n","Epoch 24/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.2717 - accuracy: 0.6513 - val_loss: 1.5708 - val_accuracy: 0.6484\n","Epoch 25/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.2112 - accuracy: 0.6683 - val_loss: 1.5631 - val_accuracy: 0.6759\n","Epoch 26/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.1649 - accuracy: 0.6834 - val_loss: 1.4704 - val_accuracy: 0.6724\n","Epoch 27/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.1188 - accuracy: 0.6949 - val_loss: 1.4128 - val_accuracy: 0.6889\n","Epoch 28/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.0796 - accuracy: 0.6983 - val_loss: 1.3995 - val_accuracy: 0.7001\n","Epoch 29/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.0329 - accuracy: 0.7131 - val_loss: 1.3404 - val_accuracy: 0.7146\n","Epoch 30/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.0014 - accuracy: 0.7207 - val_loss: 1.3314 - val_accuracy: 0.7201\n","Epoch 31/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.9664 - accuracy: 0.7312 - val_loss: 1.2842 - val_accuracy: 0.7105\n","Epoch 32/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.9402 - accuracy: 0.7365 - val_loss: 1.3463 - val_accuracy: 0.7217\n","Epoch 33/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.9013 - accuracy: 0.7452 - val_loss: 1.1888 - val_accuracy: 0.7657\n","Epoch 34/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.8766 - accuracy: 0.7546 - val_loss: 1.2195 - val_accuracy: 0.7386\n","Epoch 35/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8525 - accuracy: 0.7630 - val_loss: 1.1973 - val_accuracy: 0.7362\n","Epoch 36/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8316 - accuracy: 0.7674 - val_loss: 1.1783 - val_accuracy: 0.7562\n","Epoch 37/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8120 - accuracy: 0.7722 - val_loss: 1.1483 - val_accuracy: 0.7514\n","Epoch 38/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.7875 - accuracy: 0.7819 - val_loss: 1.1843 - val_accuracy: 0.7307\n","Epoch 39/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 0.7805 - accuracy: 0.7803 - val_loss: 1.1173 - val_accuracy: 0.7688\n","Epoch 40/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7495 - accuracy: 0.7904 - val_loss: 1.1215 - val_accuracy: 0.7743\n","Average Validation Accuracy: 0.7998399436473846\n","Average Validation Loss: 0.8829600811004639\n","Average Test Accuracy: 0.7993661165237427\n","Final Test Accuracy for each fold: 0.8080636858940125\n","Number of input features: 14\n","Fold: 1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 4.8660 - accuracy: 0.0479 - val_loss: 4.6143 - val_accuracy: 0.0570\n","Epoch 2/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 4.3798 - accuracy: 0.0821 - val_loss: 4.3171 - val_accuracy: 0.0933\n","Epoch 3/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 4.0594 - accuracy: 0.1214 - val_loss: 4.0131 - val_accuracy: 0.1028\n","Epoch 4/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 3.7692 - accuracy: 0.1559 - val_loss: 3.7784 - val_accuracy: 0.1681\n","Epoch 5/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 3.5426 - accuracy: 0.1832 - val_loss: 3.5780 - val_accuracy: 0.1868\n","Epoch 6/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 3.3373 - accuracy: 0.2028 - val_loss: 3.4185 - val_accuracy: 0.2018\n","Epoch 7/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 3.1322 - accuracy: 0.2340 - val_loss: 3.2197 - val_accuracy: 0.2453\n","Epoch 8/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.9419 - accuracy: 0.2693 - val_loss: 3.0873 - val_accuracy: 0.2827\n","Epoch 9/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.7840 - accuracy: 0.2990 - val_loss: 2.9454 - val_accuracy: 0.2939\n","Epoch 10/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.6538 - accuracy: 0.3203 - val_loss: 2.8479 - val_accuracy: 0.3283\n","Epoch 11/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.5528 - accuracy: 0.3418 - val_loss: 2.8065 - val_accuracy: 0.3023\n","Epoch 12/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.4688 - accuracy: 0.3524 - val_loss: 2.7024 - val_accuracy: 0.3648\n","Epoch 13/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.3890 - accuracy: 0.3765 - val_loss: 2.5944 - val_accuracy: 0.3954\n","Epoch 14/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.3139 - accuracy: 0.3918 - val_loss: 2.5837 - val_accuracy: 0.3635\n","Epoch 15/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.2480 - accuracy: 0.4068 - val_loss: 2.5055 - val_accuracy: 0.4018\n","Epoch 16/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.1939 - accuracy: 0.4105 - val_loss: 2.4619 - val_accuracy: 0.4051\n","Epoch 17/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.1461 - accuracy: 0.4250 - val_loss: 2.3904 - val_accuracy: 0.4535\n","Epoch 18/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 2.0932 - accuracy: 0.4383 - val_loss: 2.4055 - val_accuracy: 0.4194\n","Epoch 19/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 2.0511 - accuracy: 0.4473 - val_loss: 2.3464 - val_accuracy: 0.4275\n","Epoch 20/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.0063 - accuracy: 0.4577 - val_loss: 2.2675 - val_accuracy: 0.4724\n","Epoch 21/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.9617 - accuracy: 0.4686 - val_loss: 2.2425 - val_accuracy: 0.4818\n","Epoch 22/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.9216 - accuracy: 0.4767 - val_loss: 2.2355 - val_accuracy: 0.4693\n","Epoch 23/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.8826 - accuracy: 0.4869 - val_loss: 2.1894 - val_accuracy: 0.4939\n","Epoch 24/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.8449 - accuracy: 0.4980 - val_loss: 2.2561 - val_accuracy: 0.5050\n","Epoch 25/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.8120 - accuracy: 0.5049 - val_loss: 2.1470 - val_accuracy: 0.4959\n","Epoch 26/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.7740 - accuracy: 0.5157 - val_loss: 2.1159 - val_accuracy: 0.4893\n","Epoch 27/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.7367 - accuracy: 0.5250 - val_loss: 2.0590 - val_accuracy: 0.5435\n","Epoch 28/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.6932 - accuracy: 0.5434 - val_loss: 2.1198 - val_accuracy: 0.5146\n","Epoch 29/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.6552 - accuracy: 0.5561 - val_loss: 2.0382 - val_accuracy: 0.5377\n","Epoch 30/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.6141 - accuracy: 0.5632 - val_loss: 1.9635 - val_accuracy: 0.5780\n","Epoch 31/40\n","1846/1846 [==============================] - 8s 5ms/step - loss: 1.5722 - accuracy: 0.5779 - val_loss: 1.9462 - val_accuracy: 0.5421\n","Epoch 32/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.5279 - accuracy: 0.5886 - val_loss: 1.9042 - val_accuracy: 0.5853\n","Epoch 33/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.4906 - accuracy: 0.5976 - val_loss: 1.8672 - val_accuracy: 0.5919\n","Epoch 34/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.4497 - accuracy: 0.6094 - val_loss: 1.8075 - val_accuracy: 0.6218\n","Epoch 35/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.4119 - accuracy: 0.6270 - val_loss: 1.7675 - val_accuracy: 0.6231\n","Epoch 36/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.3710 - accuracy: 0.6348 - val_loss: 1.7566 - val_accuracy: 0.6398\n","Epoch 37/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.3354 - accuracy: 0.6447 - val_loss: 1.7042 - val_accuracy: 0.6229\n","Epoch 38/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.3032 - accuracy: 0.6515 - val_loss: 1.7507 - val_accuracy: 0.5943\n","Epoch 39/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.2615 - accuracy: 0.6703 - val_loss: 1.6473 - val_accuracy: 0.6620\n","Epoch 40/40\n","1846/1846 [==============================] - 12s 6ms/step - loss: 1.2351 - accuracy: 0.6729 - val_loss: 1.6834 - val_accuracy: 0.6477\n","Fold: 2\n","Epoch 1/40\n","1846/1846 [==============================] - 12s 5ms/step - loss: 4.7803 - accuracy: 0.0539 - val_loss: 4.5118 - val_accuracy: 0.0706\n","Epoch 2/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.1221 - accuracy: 0.1108 - val_loss: 4.0011 - val_accuracy: 0.1314\n","Epoch 3/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 3.7210 - accuracy: 0.1511 - val_loss: 3.7064 - val_accuracy: 0.1454\n","Epoch 4/40\n","1846/1846 [==============================] - 8s 5ms/step - loss: 3.4460 - accuracy: 0.1809 - val_loss: 3.4757 - val_accuracy: 0.2002\n","Epoch 5/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.1888 - accuracy: 0.2200 - val_loss: 3.2589 - val_accuracy: 0.2563\n","Epoch 6/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.8853 - accuracy: 0.2896 - val_loss: 2.9741 - val_accuracy: 0.3107\n","Epoch 7/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.5577 - accuracy: 0.3643 - val_loss: 2.6953 - val_accuracy: 0.3760\n","Epoch 8/40\n","1846/1846 [==============================] - 8s 5ms/step - loss: 2.2974 - accuracy: 0.4215 - val_loss: 2.4929 - val_accuracy: 0.4106\n","Epoch 9/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 2.0744 - accuracy: 0.4732 - val_loss: 2.2846 - val_accuracy: 0.5003\n","Epoch 10/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.9034 - accuracy: 0.5044 - val_loss: 2.1508 - val_accuracy: 0.5184\n","Epoch 11/40\n","1846/1846 [==============================] - 12s 6ms/step - loss: 1.7737 - accuracy: 0.5311 - val_loss: 2.0396 - val_accuracy: 0.5140\n","Epoch 12/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.6470 - accuracy: 0.5596 - val_loss: 1.9638 - val_accuracy: 0.5430\n","Epoch 13/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.5405 - accuracy: 0.5863 - val_loss: 1.8278 - val_accuracy: 0.5707\n","Epoch 14/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.4422 - accuracy: 0.6055 - val_loss: 1.7445 - val_accuracy: 0.6101\n","Epoch 15/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.3672 - accuracy: 0.6263 - val_loss: 1.7403 - val_accuracy: 0.6121\n","Epoch 16/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.2955 - accuracy: 0.6441 - val_loss: 1.6593 - val_accuracy: 0.6403\n","Epoch 17/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.2343 - accuracy: 0.6603 - val_loss: 1.5523 - val_accuracy: 0.6803\n","Epoch 18/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.1786 - accuracy: 0.6778 - val_loss: 1.5349 - val_accuracy: 0.6634\n","Epoch 19/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.1151 - accuracy: 0.6958 - val_loss: 1.5137 - val_accuracy: 0.6680\n","Epoch 20/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.0874 - accuracy: 0.7007 - val_loss: 1.4527 - val_accuracy: 0.6796\n","Epoch 21/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.0370 - accuracy: 0.7170 - val_loss: 1.4877 - val_accuracy: 0.6708\n","Epoch 22/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.0023 - accuracy: 0.7268 - val_loss: 1.4446 - val_accuracy: 0.6944\n","Epoch 23/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.9745 - accuracy: 0.7328 - val_loss: 1.3615 - val_accuracy: 0.7336\n","Epoch 24/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.9501 - accuracy: 0.7435 - val_loss: 1.3335 - val_accuracy: 0.7256\n","Epoch 25/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.9190 - accuracy: 0.7498 - val_loss: 1.3295 - val_accuracy: 0.7393\n","Epoch 26/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.8986 - accuracy: 0.7566 - val_loss: 1.3101 - val_accuracy: 0.7417\n","Epoch 27/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.8775 - accuracy: 0.7612 - val_loss: 1.2724 - val_accuracy: 0.7424\n","Epoch 28/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.8686 - accuracy: 0.7640 - val_loss: 1.2484 - val_accuracy: 0.7729\n","Epoch 29/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.8463 - accuracy: 0.7718 - val_loss: 1.1979 - val_accuracy: 0.7699\n","Epoch 30/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.8206 - accuracy: 0.7805 - val_loss: 1.2185 - val_accuracy: 0.7674\n","Epoch 31/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.8145 - accuracy: 0.7775 - val_loss: 1.2171 - val_accuracy: 0.7743\n","Epoch 32/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.7941 - accuracy: 0.7855 - val_loss: 1.1897 - val_accuracy: 0.7692\n","Epoch 33/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.7906 - accuracy: 0.7845 - val_loss: 1.1719 - val_accuracy: 0.7844\n","Epoch 34/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.7730 - accuracy: 0.7894 - val_loss: 1.2108 - val_accuracy: 0.7595\n","Epoch 35/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.7633 - accuracy: 0.7870 - val_loss: 1.1495 - val_accuracy: 0.7993\n","Epoch 36/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.7465 - accuracy: 0.7986 - val_loss: 1.1765 - val_accuracy: 0.7886\n","Epoch 37/40\n","1846/1846 [==============================] - 8s 5ms/step - loss: 0.7424 - accuracy: 0.8004 - val_loss: 1.2150 - val_accuracy: 0.7476\n","Epoch 38/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7288 - accuracy: 0.7973 - val_loss: 1.1217 - val_accuracy: 0.8035\n","Epoch 39/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.7163 - accuracy: 0.8036 - val_loss: 1.1204 - val_accuracy: 0.7802\n","Epoch 40/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.7068 - accuracy: 0.8056 - val_loss: 1.1309 - val_accuracy: 0.7828\n","Average Validation Accuracy: 0.7331107258796692\n","Average Validation Loss: 1.1144736409187317\n","Average Test Accuracy: 0.7329549789428711\n","Final Test Accuracy for each fold: 0.7999557852745056\n","Number of input features: 15\n","Fold: 1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/40\n","1846/1846 [==============================] - 10s 4ms/step - loss: 4.8372 - accuracy: 0.0571 - val_loss: 4.5369 - val_accuracy: 0.0631\n","Epoch 2/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 4.2126 - accuracy: 0.1095 - val_loss: 4.0813 - val_accuracy: 0.1195\n","Epoch 3/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 3.8303 - accuracy: 0.1392 - val_loss: 3.7898 - val_accuracy: 0.1529\n","Epoch 4/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 3.5509 - accuracy: 0.1771 - val_loss: 3.5626 - val_accuracy: 0.1681\n","Epoch 5/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 3.3211 - accuracy: 0.2167 - val_loss: 3.3588 - val_accuracy: 0.2372\n","Epoch 6/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 3.1046 - accuracy: 0.2589 - val_loss: 3.1687 - val_accuracy: 0.2612\n","Epoch 7/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.9004 - accuracy: 0.2884 - val_loss: 3.0383 - val_accuracy: 0.2730\n","Epoch 8/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 2.6988 - accuracy: 0.3199 - val_loss: 2.8447 - val_accuracy: 0.3146\n","Epoch 9/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.4921 - accuracy: 0.3704 - val_loss: 2.6470 - val_accuracy: 0.3938\n","Epoch 10/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.3004 - accuracy: 0.4184 - val_loss: 2.4939 - val_accuracy: 0.4339\n","Epoch 11/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.1378 - accuracy: 0.4499 - val_loss: 2.3658 - val_accuracy: 0.4572\n","Epoch 12/40\n","1846/1846 [==============================] - 12s 6ms/step - loss: 1.9851 - accuracy: 0.4806 - val_loss: 2.2705 - val_accuracy: 0.4675\n","Epoch 13/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.8595 - accuracy: 0.4961 - val_loss: 2.2138 - val_accuracy: 0.5025\n","Epoch 14/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.7551 - accuracy: 0.5256 - val_loss: 2.0821 - val_accuracy: 0.4955\n","Epoch 15/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.6634 - accuracy: 0.5393 - val_loss: 2.0228 - val_accuracy: 0.5333\n","Epoch 16/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.5929 - accuracy: 0.5576 - val_loss: 2.0133 - val_accuracy: 0.5263\n","Epoch 17/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.5243 - accuracy: 0.5715 - val_loss: 1.8644 - val_accuracy: 0.5789\n","Epoch 18/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.4724 - accuracy: 0.5865 - val_loss: 1.8217 - val_accuracy: 0.5949\n","Epoch 19/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.4168 - accuracy: 0.6033 - val_loss: 1.8388 - val_accuracy: 0.6020\n","Epoch 20/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.3645 - accuracy: 0.6203 - val_loss: 1.7631 - val_accuracy: 0.6209\n","Epoch 21/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.3346 - accuracy: 0.6281 - val_loss: 1.7541 - val_accuracy: 0.6222\n","Epoch 22/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.2905 - accuracy: 0.6407 - val_loss: 1.7220 - val_accuracy: 0.6128\n","Epoch 23/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.2584 - accuracy: 0.6500 - val_loss: 1.6622 - val_accuracy: 0.6691\n","Epoch 24/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.2166 - accuracy: 0.6687 - val_loss: 1.6273 - val_accuracy: 0.6774\n","Epoch 25/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.1911 - accuracy: 0.6741 - val_loss: 1.6092 - val_accuracy: 0.6781\n","Epoch 26/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.1423 - accuracy: 0.6918 - val_loss: 1.5965 - val_accuracy: 0.6823\n","Epoch 27/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.1264 - accuracy: 0.6976 - val_loss: 1.5577 - val_accuracy: 0.7014\n","Epoch 28/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.0975 - accuracy: 0.7038 - val_loss: 1.5510 - val_accuracy: 0.7001\n","Epoch 29/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.0739 - accuracy: 0.7084 - val_loss: 1.5441 - val_accuracy: 0.6920\n","Epoch 30/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.0478 - accuracy: 0.7130 - val_loss: 1.5343 - val_accuracy: 0.6768\n","Epoch 31/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.0322 - accuracy: 0.7133 - val_loss: 1.4865 - val_accuracy: 0.7285\n","Epoch 32/40\n","1846/1846 [==============================] - 12s 6ms/step - loss: 1.0142 - accuracy: 0.7201 - val_loss: 1.5131 - val_accuracy: 0.6931\n","Epoch 33/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.9932 - accuracy: 0.7285 - val_loss: 1.4553 - val_accuracy: 0.7078\n","Epoch 34/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.9758 - accuracy: 0.7349 - val_loss: 1.4279 - val_accuracy: 0.7221\n","Epoch 35/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.9529 - accuracy: 0.7390 - val_loss: 1.4189 - val_accuracy: 0.7184\n","Epoch 36/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.9395 - accuracy: 0.7439 - val_loss: 1.4051 - val_accuracy: 0.7490\n","Epoch 37/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.9322 - accuracy: 0.7414 - val_loss: 1.4231 - val_accuracy: 0.7311\n","Epoch 38/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.9121 - accuracy: 0.7428 - val_loss: 1.4029 - val_accuracy: 0.7446\n","Epoch 39/40\n","1846/1846 [==============================] - 14s 7ms/step - loss: 0.8891 - accuracy: 0.7562 - val_loss: 1.3751 - val_accuracy: 0.7421\n","Epoch 40/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.8836 - accuracy: 0.7570 - val_loss: 1.3866 - val_accuracy: 0.7320\n","Fold: 2\n","Epoch 1/40\n","1846/1846 [==============================] - 10s 4ms/step - loss: 4.8099 - accuracy: 0.0530 - val_loss: 4.5531 - val_accuracy: 0.0715\n","Epoch 2/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.1617 - accuracy: 0.1062 - val_loss: 4.0489 - val_accuracy: 0.1325\n","Epoch 3/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 3.7664 - accuracy: 0.1471 - val_loss: 3.7770 - val_accuracy: 0.1701\n","Epoch 4/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 3.4956 - accuracy: 0.1750 - val_loss: 3.5822 - val_accuracy: 0.1844\n","Epoch 5/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 3.2449 - accuracy: 0.2163 - val_loss: 3.3297 - val_accuracy: 0.2376\n","Epoch 6/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 2.9811 - accuracy: 0.2632 - val_loss: 3.0924 - val_accuracy: 0.2986\n","Epoch 7/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.6963 - accuracy: 0.3310 - val_loss: 2.8357 - val_accuracy: 0.3549\n","Epoch 8/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.3995 - accuracy: 0.4027 - val_loss: 2.5725 - val_accuracy: 0.4429\n","Epoch 9/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.1154 - accuracy: 0.4795 - val_loss: 2.3030 - val_accuracy: 0.5239\n","Epoch 10/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.8527 - accuracy: 0.5442 - val_loss: 2.1200 - val_accuracy: 0.5562\n","Epoch 11/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.6378 - accuracy: 0.5961 - val_loss: 1.9218 - val_accuracy: 0.6002\n","Epoch 12/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.4702 - accuracy: 0.6303 - val_loss: 1.8279 - val_accuracy: 0.6504\n","Epoch 13/40\n","1846/1846 [==============================] - 12s 7ms/step - loss: 1.3480 - accuracy: 0.6593 - val_loss: 1.7042 - val_accuracy: 0.6568\n","Epoch 14/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.2504 - accuracy: 0.6851 - val_loss: 1.6472 - val_accuracy: 0.6741\n","Epoch 15/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.1708 - accuracy: 0.7063 - val_loss: 1.6306 - val_accuracy: 0.6823\n","Epoch 16/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.1132 - accuracy: 0.7196 - val_loss: 1.6059 - val_accuracy: 0.6724\n","Epoch 17/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.0562 - accuracy: 0.7341 - val_loss: 1.5063 - val_accuracy: 0.7155\n","Epoch 18/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.0093 - accuracy: 0.7360 - val_loss: 1.4197 - val_accuracy: 0.7410\n","Epoch 19/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.9751 - accuracy: 0.7463 - val_loss: 1.3777 - val_accuracy: 0.7448\n","Epoch 20/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.9390 - accuracy: 0.7576 - val_loss: 1.3767 - val_accuracy: 0.7454\n","Epoch 21/40\n","1846/1846 [==============================] - 12s 6ms/step - loss: 0.9101 - accuracy: 0.7582 - val_loss: 1.3371 - val_accuracy: 0.7553\n","Epoch 22/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.8810 - accuracy: 0.7688 - val_loss: 1.3366 - val_accuracy: 0.7589\n","Epoch 23/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.8503 - accuracy: 0.7760 - val_loss: 1.3118 - val_accuracy: 0.7562\n","Epoch 24/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 0.8282 - accuracy: 0.7827 - val_loss: 1.2376 - val_accuracy: 0.7771\n","Epoch 25/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.8083 - accuracy: 0.7843 - val_loss: 1.2521 - val_accuracy: 0.7624\n","Epoch 26/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7896 - accuracy: 0.7884 - val_loss: 1.2220 - val_accuracy: 0.7835\n","Epoch 27/40\n","1846/1846 [==============================] - 8s 5ms/step - loss: 0.7662 - accuracy: 0.7943 - val_loss: 1.2156 - val_accuracy: 0.7956\n","Epoch 28/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.7524 - accuracy: 0.8011 - val_loss: 1.1569 - val_accuracy: 0.7976\n","Epoch 29/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.7327 - accuracy: 0.8006 - val_loss: 1.1998 - val_accuracy: 0.7743\n","Epoch 30/40\n","1846/1846 [==============================] - 8s 5ms/step - loss: 0.7142 - accuracy: 0.8100 - val_loss: 1.1983 - val_accuracy: 0.7837\n","Epoch 31/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.7026 - accuracy: 0.8138 - val_loss: 1.1785 - val_accuracy: 0.8029\n","Epoch 32/40\n","1846/1846 [==============================] - 12s 6ms/step - loss: 0.6885 - accuracy: 0.8184 - val_loss: 1.1260 - val_accuracy: 0.8209\n","Epoch 33/40\n","1846/1846 [==============================] - 13s 7ms/step - loss: 0.6766 - accuracy: 0.8181 - val_loss: 1.1474 - val_accuracy: 0.8139\n","Epoch 34/40\n","1846/1846 [==============================] - 8s 5ms/step - loss: 0.6627 - accuracy: 0.8256 - val_loss: 1.0964 - val_accuracy: 0.8112\n","Epoch 35/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.6561 - accuracy: 0.8225 - val_loss: 1.0867 - val_accuracy: 0.8312\n","Epoch 36/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.6378 - accuracy: 0.8300 - val_loss: 1.1327 - val_accuracy: 0.8150\n","Epoch 37/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.6362 - accuracy: 0.8265 - val_loss: 1.0670 - val_accuracy: 0.8277\n","Epoch 38/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.6246 - accuracy: 0.8340 - val_loss: 1.0322 - val_accuracy: 0.8416\n","Epoch 39/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.6167 - accuracy: 0.8355 - val_loss: 1.0269 - val_accuracy: 0.8414\n","Epoch 40/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.5996 - accuracy: 0.8383 - val_loss: 1.1572 - val_accuracy: 0.8070\n","Average Validation Accuracy: 0.7837939858436584\n","Average Validation Loss: 0.9381156861782074\n","Average Test Accuracy: 0.7869831025600433\n","Final Test Accuracy for each fold: 0.8248691558837891\n","Number of input features: 16\n","Fold: 1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/40\n","1846/1846 [==============================] - 12s 5ms/step - loss: 4.7716 - accuracy: 0.0615 - val_loss: 4.4439 - val_accuracy: 0.1001\n","Epoch 2/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.1125 - accuracy: 0.1219 - val_loss: 3.9637 - val_accuracy: 0.1443\n","Epoch 3/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 3.7290 - accuracy: 0.1569 - val_loss: 3.7109 - val_accuracy: 0.1450\n","Epoch 4/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 3.4909 - accuracy: 0.1785 - val_loss: 3.5135 - val_accuracy: 0.1879\n","Epoch 5/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 3.3055 - accuracy: 0.1993 - val_loss: 3.3599 - val_accuracy: 0.1905\n","Epoch 6/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 3.1191 - accuracy: 0.2294 - val_loss: 3.2022 - val_accuracy: 0.2418\n","Epoch 7/40\n","1846/1846 [==============================] - 12s 6ms/step - loss: 2.8949 - accuracy: 0.2763 - val_loss: 2.9761 - val_accuracy: 0.2880\n","Epoch 8/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.6550 - accuracy: 0.3357 - val_loss: 2.7823 - val_accuracy: 0.3432\n","Epoch 9/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.4390 - accuracy: 0.3843 - val_loss: 2.5989 - val_accuracy: 0.3688\n","Epoch 10/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.2531 - accuracy: 0.4207 - val_loss: 2.4437 - val_accuracy: 0.4249\n","Epoch 11/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.0991 - accuracy: 0.4482 - val_loss: 2.3034 - val_accuracy: 0.4513\n","Epoch 12/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.9698 - accuracy: 0.4758 - val_loss: 2.2265 - val_accuracy: 0.4862\n","Epoch 13/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.8695 - accuracy: 0.5010 - val_loss: 2.1207 - val_accuracy: 0.5061\n","Epoch 14/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.7834 - accuracy: 0.5146 - val_loss: 2.0369 - val_accuracy: 0.5111\n","Epoch 15/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.7112 - accuracy: 0.5322 - val_loss: 2.0060 - val_accuracy: 0.5336\n","Epoch 16/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.6422 - accuracy: 0.5502 - val_loss: 2.0739 - val_accuracy: 0.4898\n","Epoch 17/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.5718 - accuracy: 0.5686 - val_loss: 1.8780 - val_accuracy: 0.5747\n","Epoch 18/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.5083 - accuracy: 0.5854 - val_loss: 1.7858 - val_accuracy: 0.5912\n","Epoch 19/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.4541 - accuracy: 0.6009 - val_loss: 1.7602 - val_accuracy: 0.6000\n","Epoch 20/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.4026 - accuracy: 0.6160 - val_loss: 1.7323 - val_accuracy: 0.6161\n","Epoch 21/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.3465 - accuracy: 0.6331 - val_loss: 1.7473 - val_accuracy: 0.6152\n","Epoch 22/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.3057 - accuracy: 0.6447 - val_loss: 1.6337 - val_accuracy: 0.6462\n","Epoch 23/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.2635 - accuracy: 0.6559 - val_loss: 1.6542 - val_accuracy: 0.6579\n","Epoch 24/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.2317 - accuracy: 0.6616 - val_loss: 1.6555 - val_accuracy: 0.6297\n","Epoch 25/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.1966 - accuracy: 0.6755 - val_loss: 1.5651 - val_accuracy: 0.6596\n","Epoch 26/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.1675 - accuracy: 0.6812 - val_loss: 1.5425 - val_accuracy: 0.6827\n","Epoch 27/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.1454 - accuracy: 0.6923 - val_loss: 1.5839 - val_accuracy: 0.6532\n","Epoch 28/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.1337 - accuracy: 0.6920 - val_loss: 1.5500 - val_accuracy: 0.6761\n","Epoch 29/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.0956 - accuracy: 0.7045 - val_loss: 1.4977 - val_accuracy: 0.6768\n","Epoch 30/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.0813 - accuracy: 0.7054 - val_loss: 1.4808 - val_accuracy: 0.6920\n","Epoch 31/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.0646 - accuracy: 0.7093 - val_loss: 1.5071 - val_accuracy: 0.6935\n","Epoch 32/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.0427 - accuracy: 0.7181 - val_loss: 1.4494 - val_accuracy: 0.7223\n","Epoch 33/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.0297 - accuracy: 0.7210 - val_loss: 1.4199 - val_accuracy: 0.7303\n","Epoch 34/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.0040 - accuracy: 0.7310 - val_loss: 1.3901 - val_accuracy: 0.7322\n","Epoch 35/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.9897 - accuracy: 0.7352 - val_loss: 1.4191 - val_accuracy: 0.7160\n","Epoch 36/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.9724 - accuracy: 0.7405 - val_loss: 1.4061 - val_accuracy: 0.7208\n","Epoch 37/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.9516 - accuracy: 0.7458 - val_loss: 1.3716 - val_accuracy: 0.7547\n","Epoch 38/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.9472 - accuracy: 0.7419 - val_loss: 1.3446 - val_accuracy: 0.7421\n","Epoch 39/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.9315 - accuracy: 0.7511 - val_loss: 1.3605 - val_accuracy: 0.7479\n","Epoch 40/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.9228 - accuracy: 0.7529 - val_loss: 1.3797 - val_accuracy: 0.7377\n","Fold: 2\n","Epoch 1/40\n","1846/1846 [==============================] - 10s 4ms/step - loss: 4.8292 - accuracy: 0.0613 - val_loss: 4.5373 - val_accuracy: 0.0999\n","Epoch 2/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 4.1458 - accuracy: 0.1186 - val_loss: 4.0154 - val_accuracy: 0.1331\n","Epoch 3/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 3.7425 - accuracy: 0.1571 - val_loss: 3.7593 - val_accuracy: 0.1696\n","Epoch 4/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 3.4774 - accuracy: 0.1807 - val_loss: 3.5472 - val_accuracy: 0.1927\n","Epoch 5/40\n","1846/1846 [==============================] - 4s 2ms/step - loss: 3.2509 - accuracy: 0.2138 - val_loss: 3.3715 - val_accuracy: 0.2132\n","Epoch 6/40\n","1846/1846 [==============================] - 6s 4ms/step - loss: 3.0407 - accuracy: 0.2444 - val_loss: 3.1399 - val_accuracy: 0.2733\n","Epoch 7/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.8086 - accuracy: 0.3029 - val_loss: 2.9375 - val_accuracy: 0.3228\n","Epoch 8/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.5601 - accuracy: 0.3601 - val_loss: 2.6997 - val_accuracy: 0.3868\n","Epoch 9/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.3234 - accuracy: 0.4060 - val_loss: 2.5119 - val_accuracy: 0.4112\n","Epoch 10/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.1216 - accuracy: 0.4473 - val_loss: 2.3657 - val_accuracy: 0.4700\n","Epoch 11/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.9577 - accuracy: 0.4930 - val_loss: 2.2072 - val_accuracy: 0.5025\n","Epoch 12/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.8112 - accuracy: 0.5247 - val_loss: 2.1376 - val_accuracy: 0.5155\n","Epoch 13/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.6950 - accuracy: 0.5473 - val_loss: 1.9953 - val_accuracy: 0.5611\n","Epoch 14/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.5875 - accuracy: 0.5703 - val_loss: 1.9667 - val_accuracy: 0.5531\n","Epoch 15/40\n","1846/1846 [==============================] - 5s 2ms/step - loss: 1.5079 - accuracy: 0.5905 - val_loss: 1.8186 - val_accuracy: 0.6059\n","Epoch 16/40\n","1846/1846 [==============================] - 5s 2ms/step - loss: 1.4236 - accuracy: 0.6135 - val_loss: 1.7738 - val_accuracy: 0.6233\n","Epoch 17/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.3425 - accuracy: 0.6457 - val_loss: 1.7307 - val_accuracy: 0.6400\n","Epoch 18/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.2864 - accuracy: 0.6577 - val_loss: 1.6655 - val_accuracy: 0.6462\n","Epoch 19/40\n","1846/1846 [==============================] - 5s 2ms/step - loss: 1.2198 - accuracy: 0.6765 - val_loss: 1.5974 - val_accuracy: 0.6667\n","Epoch 20/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.1671 - accuracy: 0.6881 - val_loss: 1.5576 - val_accuracy: 0.6697\n","Epoch 21/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.1322 - accuracy: 0.6943 - val_loss: 1.5360 - val_accuracy: 0.6702\n","Epoch 22/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.0936 - accuracy: 0.7052 - val_loss: 1.4920 - val_accuracy: 0.6944\n","Epoch 23/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0517 - accuracy: 0.7139 - val_loss: 1.4643 - val_accuracy: 0.6926\n","Epoch 24/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.0300 - accuracy: 0.7203 - val_loss: 1.3923 - val_accuracy: 0.7190\n","Epoch 25/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.9948 - accuracy: 0.7274 - val_loss: 1.3991 - val_accuracy: 0.7201\n","Epoch 26/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9662 - accuracy: 0.7389 - val_loss: 1.3778 - val_accuracy: 0.7175\n","Epoch 27/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9464 - accuracy: 0.7438 - val_loss: 1.3290 - val_accuracy: 0.7256\n","Epoch 28/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9228 - accuracy: 0.7455 - val_loss: 1.4070 - val_accuracy: 0.7160\n","Epoch 29/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9020 - accuracy: 0.7585 - val_loss: 1.2751 - val_accuracy: 0.7496\n","Epoch 30/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8883 - accuracy: 0.7576 - val_loss: 1.3438 - val_accuracy: 0.7298\n","Epoch 31/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8611 - accuracy: 0.7701 - val_loss: 1.2829 - val_accuracy: 0.7503\n","Epoch 32/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8485 - accuracy: 0.7650 - val_loss: 1.2561 - val_accuracy: 0.7545\n","Epoch 33/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8426 - accuracy: 0.7707 - val_loss: 1.2708 - val_accuracy: 0.7375\n","Epoch 34/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8227 - accuracy: 0.7764 - val_loss: 1.1924 - val_accuracy: 0.7683\n","Epoch 35/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8134 - accuracy: 0.7754 - val_loss: 1.2487 - val_accuracy: 0.7652\n","Epoch 36/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7978 - accuracy: 0.7837 - val_loss: 1.2529 - val_accuracy: 0.7589\n","Epoch 37/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7852 - accuracy: 0.7875 - val_loss: 1.2131 - val_accuracy: 0.7718\n","Epoch 38/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7693 - accuracy: 0.7964 - val_loss: 1.2239 - val_accuracy: 0.7789\n","Epoch 39/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7719 - accuracy: 0.7893 - val_loss: 1.1811 - val_accuracy: 0.7853\n","Epoch 40/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7509 - accuracy: 0.7973 - val_loss: 1.2083 - val_accuracy: 0.7734\n","Average Validation Accuracy: 0.7701419591903687\n","Average Validation Loss: 0.9887448251247406\n","Average Test Accuracy: 0.7680769562721252\n","Final Test Accuracy for each fold: 0.7848455905914307\n","Number of input features: 17\n","Fold: 1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/40\n","1846/1846 [==============================] - 8s 3ms/step - loss: 4.7959 - accuracy: 0.0594 - val_loss: 4.4423 - val_accuracy: 0.0975\n","Epoch 2/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 4.0437 - accuracy: 0.1293 - val_loss: 3.8952 - val_accuracy: 0.1597\n","Epoch 3/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 3.6391 - accuracy: 0.1763 - val_loss: 3.5977 - val_accuracy: 0.1749\n","Epoch 4/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 3.3440 - accuracy: 0.2159 - val_loss: 3.3532 - val_accuracy: 0.2231\n","Epoch 5/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.0516 - accuracy: 0.2700 - val_loss: 3.0967 - val_accuracy: 0.3307\n","Epoch 6/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.7334 - accuracy: 0.3409 - val_loss: 2.8030 - val_accuracy: 0.3718\n","Epoch 7/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.3988 - accuracy: 0.4078 - val_loss: 2.4760 - val_accuracy: 0.4398\n","Epoch 8/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.1018 - accuracy: 0.4720 - val_loss: 2.2145 - val_accuracy: 0.5001\n","Epoch 9/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.8592 - accuracy: 0.5315 - val_loss: 2.0220 - val_accuracy: 0.5520\n","Epoch 10/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.6717 - accuracy: 0.5806 - val_loss: 1.9402 - val_accuracy: 0.5833\n","Epoch 11/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.5402 - accuracy: 0.6056 - val_loss: 1.8243 - val_accuracy: 0.6145\n","Epoch 12/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.4296 - accuracy: 0.6304 - val_loss: 1.7663 - val_accuracy: 0.6312\n","Epoch 13/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.3474 - accuracy: 0.6499 - val_loss: 1.6732 - val_accuracy: 0.6616\n","Epoch 14/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.2681 - accuracy: 0.6681 - val_loss: 1.6657 - val_accuracy: 0.6451\n","Epoch 15/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.2158 - accuracy: 0.6844 - val_loss: 1.6518 - val_accuracy: 0.6334\n","Epoch 16/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.1712 - accuracy: 0.6873 - val_loss: 1.5256 - val_accuracy: 0.6838\n","Epoch 17/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.1245 - accuracy: 0.6974 - val_loss: 1.5402 - val_accuracy: 0.6750\n","Epoch 18/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.0871 - accuracy: 0.7107 - val_loss: 1.4924 - val_accuracy: 0.7043\n","Epoch 19/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.0640 - accuracy: 0.7160 - val_loss: 1.4485 - val_accuracy: 0.7133\n","Epoch 20/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.0273 - accuracy: 0.7258 - val_loss: 1.4505 - val_accuracy: 0.7168\n","Epoch 21/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.0056 - accuracy: 0.7320 - val_loss: 1.4247 - val_accuracy: 0.7069\n","Epoch 22/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.9866 - accuracy: 0.7368 - val_loss: 1.4081 - val_accuracy: 0.7087\n","Epoch 23/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.9679 - accuracy: 0.7418 - val_loss: 1.3642 - val_accuracy: 0.7360\n","Epoch 24/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.9540 - accuracy: 0.7484 - val_loss: 1.3518 - val_accuracy: 0.7353\n","Epoch 25/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.9320 - accuracy: 0.7497 - val_loss: 1.3456 - val_accuracy: 0.7344\n","Epoch 26/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.9171 - accuracy: 0.7543 - val_loss: 1.3482 - val_accuracy: 0.7406\n","Epoch 27/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.8994 - accuracy: 0.7595 - val_loss: 1.3045 - val_accuracy: 0.7509\n","Epoch 28/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.8779 - accuracy: 0.7612 - val_loss: 1.4581 - val_accuracy: 0.6781\n","Epoch 29/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.8655 - accuracy: 0.7633 - val_loss: 1.3490 - val_accuracy: 0.7305\n","Epoch 30/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.8507 - accuracy: 0.7701 - val_loss: 1.2797 - val_accuracy: 0.7503\n","Epoch 31/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.8273 - accuracy: 0.7747 - val_loss: 1.4067 - val_accuracy: 0.7129\n","Epoch 32/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.8275 - accuracy: 0.7731 - val_loss: 1.3603 - val_accuracy: 0.7107\n","Epoch 33/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.8158 - accuracy: 0.7799 - val_loss: 1.2582 - val_accuracy: 0.7705\n","Epoch 34/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7799 - accuracy: 0.7884 - val_loss: 1.2421 - val_accuracy: 0.7650\n","Epoch 35/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.7784 - accuracy: 0.7880 - val_loss: 1.2582 - val_accuracy: 0.7608\n","Epoch 36/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7646 - accuracy: 0.7949 - val_loss: 1.2589 - val_accuracy: 0.7729\n","Epoch 37/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7602 - accuracy: 0.7947 - val_loss: 1.2022 - val_accuracy: 0.7723\n","Epoch 38/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7425 - accuracy: 0.7963 - val_loss: 1.2670 - val_accuracy: 0.7670\n","Epoch 39/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7434 - accuracy: 0.7934 - val_loss: 1.1905 - val_accuracy: 0.7872\n","Epoch 40/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.7283 - accuracy: 0.8001 - val_loss: 1.2074 - val_accuracy: 0.7738\n","Fold: 2\n","Epoch 1/40\n","1846/1846 [==============================] - 10s 4ms/step - loss: 4.8828 - accuracy: 0.0503 - val_loss: 4.6467 - val_accuracy: 0.0730\n","Epoch 2/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 4.2279 - accuracy: 0.1095 - val_loss: 4.0540 - val_accuracy: 0.1655\n","Epoch 3/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 3.7524 - accuracy: 0.1676 - val_loss: 3.7658 - val_accuracy: 0.1776\n","Epoch 4/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 3.4996 - accuracy: 0.1904 - val_loss: 3.6255 - val_accuracy: 0.1813\n","Epoch 5/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 3.3121 - accuracy: 0.2087 - val_loss: 3.4278 - val_accuracy: 0.2103\n","Epoch 6/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 3.1300 - accuracy: 0.2404 - val_loss: 3.2908 - val_accuracy: 0.2614\n","Epoch 7/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 2.9434 - accuracy: 0.2733 - val_loss: 3.0917 - val_accuracy: 0.2871\n","Epoch 8/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.7591 - accuracy: 0.3062 - val_loss: 2.9650 - val_accuracy: 0.3023\n","Epoch 9/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 2.6078 - accuracy: 0.3350 - val_loss: 2.8373 - val_accuracy: 0.3732\n","Epoch 10/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.4875 - accuracy: 0.3654 - val_loss: 2.7603 - val_accuracy: 0.3408\n","Epoch 11/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.3848 - accuracy: 0.3897 - val_loss: 2.6197 - val_accuracy: 0.3842\n","Epoch 12/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.2794 - accuracy: 0.4029 - val_loss: 2.5312 - val_accuracy: 0.4200\n","Epoch 13/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.1932 - accuracy: 0.4242 - val_loss: 2.4821 - val_accuracy: 0.4117\n","Epoch 14/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.1067 - accuracy: 0.4371 - val_loss: 2.4108 - val_accuracy: 0.4543\n","Epoch 15/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.0319 - accuracy: 0.4579 - val_loss: 2.3458 - val_accuracy: 0.4684\n","Epoch 16/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.9562 - accuracy: 0.4746 - val_loss: 2.2778 - val_accuracy: 0.4801\n","Epoch 17/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.8858 - accuracy: 0.4965 - val_loss: 2.2122 - val_accuracy: 0.5025\n","Epoch 18/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.8148 - accuracy: 0.5159 - val_loss: 2.1650 - val_accuracy: 0.5083\n","Epoch 19/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.7564 - accuracy: 0.5270 - val_loss: 2.1316 - val_accuracy: 0.5388\n","Epoch 20/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.6886 - accuracy: 0.5525 - val_loss: 2.0557 - val_accuracy: 0.5661\n","Epoch 21/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.6272 - accuracy: 0.5734 - val_loss: 2.0180 - val_accuracy: 0.5765\n","Epoch 22/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.5646 - accuracy: 0.5928 - val_loss: 1.9255 - val_accuracy: 0.5721\n","Epoch 23/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.5101 - accuracy: 0.6090 - val_loss: 1.8878 - val_accuracy: 0.6178\n","Epoch 24/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.4496 - accuracy: 0.6256 - val_loss: 1.8104 - val_accuracy: 0.6363\n","Epoch 25/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.3933 - accuracy: 0.6442 - val_loss: 1.7431 - val_accuracy: 0.6647\n","Epoch 26/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.3494 - accuracy: 0.6500 - val_loss: 1.7081 - val_accuracy: 0.6559\n","Epoch 27/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.2978 - accuracy: 0.6626 - val_loss: 1.6848 - val_accuracy: 0.6623\n","Epoch 28/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.2612 - accuracy: 0.6692 - val_loss: 1.6144 - val_accuracy: 0.6891\n","Epoch 29/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.2227 - accuracy: 0.6804 - val_loss: 1.5973 - val_accuracy: 0.6700\n","Epoch 30/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.1858 - accuracy: 0.6905 - val_loss: 1.5683 - val_accuracy: 0.6825\n","Epoch 31/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.1447 - accuracy: 0.7009 - val_loss: 1.5553 - val_accuracy: 0.6926\n","Epoch 32/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.1255 - accuracy: 0.7035 - val_loss: 1.5050 - val_accuracy: 0.6920\n","Epoch 33/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.0914 - accuracy: 0.7136 - val_loss: 1.4648 - val_accuracy: 0.7091\n","Epoch 34/40\n","1846/1846 [==============================] - 8s 5ms/step - loss: 1.0595 - accuracy: 0.7211 - val_loss: 1.4468 - val_accuracy: 0.7012\n","Epoch 35/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.0336 - accuracy: 0.7252 - val_loss: 1.4395 - val_accuracy: 0.7063\n","Epoch 36/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.0174 - accuracy: 0.7267 - val_loss: 1.3976 - val_accuracy: 0.7085\n","Epoch 37/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9933 - accuracy: 0.7365 - val_loss: 1.5358 - val_accuracy: 0.6867\n","Epoch 38/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9646 - accuracy: 0.7394 - val_loss: 1.3427 - val_accuracy: 0.7542\n","Epoch 39/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9431 - accuracy: 0.7474 - val_loss: 1.3514 - val_accuracy: 0.7327\n","Epoch 40/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9205 - accuracy: 0.7568 - val_loss: 1.3219 - val_accuracy: 0.7503\n","Average Validation Accuracy: 0.772863894701004\n","Average Validation Loss: 0.9598332643508911\n","Average Test Accuracy: 0.7733839452266693\n","Final Test Accuracy for each fold: 0.7854352593421936\n","Number of input features: 18\n","Fold: 1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/40\n","1846/1846 [==============================] - 7s 3ms/step - loss: 4.8581 - accuracy: 0.0570 - val_loss: 4.5702 - val_accuracy: 0.0750\n","Epoch 2/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 4.2792 - accuracy: 0.1157 - val_loss: 4.1300 - val_accuracy: 0.1344\n","Epoch 3/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 3.8530 - accuracy: 0.1603 - val_loss: 3.8273 - val_accuracy: 0.1674\n","Epoch 4/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.5940 - accuracy: 0.1901 - val_loss: 3.6460 - val_accuracy: 0.1853\n","Epoch 5/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.3980 - accuracy: 0.2118 - val_loss: 3.4501 - val_accuracy: 0.2037\n","Epoch 6/40\n","1846/1846 [==============================] - 5s 2ms/step - loss: 3.2156 - accuracy: 0.2370 - val_loss: 3.3147 - val_accuracy: 0.2400\n","Epoch 7/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.0391 - accuracy: 0.2722 - val_loss: 3.1388 - val_accuracy: 0.2935\n","Epoch 8/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.8482 - accuracy: 0.3091 - val_loss: 2.9774 - val_accuracy: 0.3166\n","Epoch 9/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.6511 - accuracy: 0.3439 - val_loss: 2.7857 - val_accuracy: 0.3679\n","Epoch 10/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.4525 - accuracy: 0.3900 - val_loss: 2.6347 - val_accuracy: 0.3974\n","Epoch 11/40\n","1846/1846 [==============================] - 5s 2ms/step - loss: 2.2654 - accuracy: 0.4336 - val_loss: 2.4852 - val_accuracy: 0.4403\n","Epoch 12/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.0978 - accuracy: 0.4728 - val_loss: 2.3673 - val_accuracy: 0.4684\n","Epoch 13/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.9713 - accuracy: 0.5039 - val_loss: 2.2285 - val_accuracy: 0.5138\n","Epoch 14/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.8502 - accuracy: 0.5326 - val_loss: 2.1780 - val_accuracy: 0.5102\n","Epoch 15/40\n","1846/1846 [==============================] - 4s 2ms/step - loss: 1.7492 - accuracy: 0.5508 - val_loss: 2.1332 - val_accuracy: 0.5595\n","Epoch 16/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.6636 - accuracy: 0.5783 - val_loss: 2.0210 - val_accuracy: 0.5960\n","Epoch 17/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.5905 - accuracy: 0.5910 - val_loss: 1.9866 - val_accuracy: 0.5760\n","Epoch 18/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.5239 - accuracy: 0.6093 - val_loss: 1.9348 - val_accuracy: 0.5872\n","Epoch 19/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.4654 - accuracy: 0.6257 - val_loss: 1.8954 - val_accuracy: 0.6453\n","Epoch 20/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.4216 - accuracy: 0.6313 - val_loss: 1.8116 - val_accuracy: 0.6341\n","Epoch 21/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.3669 - accuracy: 0.6424 - val_loss: 1.8135 - val_accuracy: 0.6442\n","Epoch 22/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.3223 - accuracy: 0.6624 - val_loss: 1.7821 - val_accuracy: 0.6383\n","Epoch 23/40\n","1846/1846 [==============================] - 5s 2ms/step - loss: 1.2900 - accuracy: 0.6659 - val_loss: 1.7435 - val_accuracy: 0.6735\n","Epoch 24/40\n","1846/1846 [==============================] - 5s 2ms/step - loss: 1.2541 - accuracy: 0.6804 - val_loss: 1.7492 - val_accuracy: 0.6609\n","Epoch 25/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.2292 - accuracy: 0.6796 - val_loss: 1.6893 - val_accuracy: 0.6763\n","Epoch 26/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.1934 - accuracy: 0.6929 - val_loss: 1.6487 - val_accuracy: 0.6966\n","Epoch 27/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.1694 - accuracy: 0.7003 - val_loss: 1.6501 - val_accuracy: 0.7008\n","Epoch 28/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.1375 - accuracy: 0.7081 - val_loss: 1.6490 - val_accuracy: 0.6981\n","Epoch 29/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.1284 - accuracy: 0.7066 - val_loss: 1.6247 - val_accuracy: 0.7135\n","Epoch 30/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0969 - accuracy: 0.7201 - val_loss: 1.6412 - val_accuracy: 0.7019\n","Epoch 31/40\n","1846/1846 [==============================] - 5s 2ms/step - loss: 1.0765 - accuracy: 0.7240 - val_loss: 1.6153 - val_accuracy: 0.7074\n","Epoch 32/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0538 - accuracy: 0.7326 - val_loss: 1.5596 - val_accuracy: 0.7217\n","Epoch 33/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0394 - accuracy: 0.7310 - val_loss: 1.5485 - val_accuracy: 0.7234\n","Epoch 34/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0199 - accuracy: 0.7383 - val_loss: 1.6110 - val_accuracy: 0.6915\n","Epoch 35/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0087 - accuracy: 0.7393 - val_loss: 1.5581 - val_accuracy: 0.7201\n","Epoch 36/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9905 - accuracy: 0.7429 - val_loss: 1.5369 - val_accuracy: 0.7217\n","Epoch 37/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9880 - accuracy: 0.7414 - val_loss: 1.5403 - val_accuracy: 0.7344\n","Epoch 38/40\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.9668 - accuracy: 0.7458 - val_loss: 1.5117 - val_accuracy: 0.7309\n","Epoch 39/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9511 - accuracy: 0.7524 - val_loss: 1.5010 - val_accuracy: 0.7307\n","Epoch 40/40\n","1846/1846 [==============================] - 4s 2ms/step - loss: 0.9429 - accuracy: 0.7570 - val_loss: 1.5145 - val_accuracy: 0.7316\n","Fold: 2\n","Epoch 1/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 4.9088 - accuracy: 0.0471 - val_loss: 4.7142 - val_accuracy: 0.0552\n","Epoch 2/40\n","1846/1846 [==============================] - 5s 2ms/step - loss: 4.3050 - accuracy: 0.1023 - val_loss: 4.0855 - val_accuracy: 0.1384\n","Epoch 3/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.7332 - accuracy: 0.1658 - val_loss: 3.6740 - val_accuracy: 0.1853\n","Epoch 4/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.3684 - accuracy: 0.1996 - val_loss: 3.4181 - val_accuracy: 0.2007\n","Epoch 5/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.0522 - accuracy: 0.2605 - val_loss: 3.1545 - val_accuracy: 0.2297\n","Epoch 6/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.7705 - accuracy: 0.3134 - val_loss: 2.8641 - val_accuracy: 0.3226\n","Epoch 7/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.5079 - accuracy: 0.3682 - val_loss: 2.6574 - val_accuracy: 0.4026\n","Epoch 8/40\n","1846/1846 [==============================] - 5s 2ms/step - loss: 2.2612 - accuracy: 0.4330 - val_loss: 2.4956 - val_accuracy: 0.4308\n","Epoch 9/40\n","1846/1846 [==============================] - 5s 2ms/step - loss: 2.0503 - accuracy: 0.4936 - val_loss: 2.2400 - val_accuracy: 0.5355\n","Epoch 10/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.8595 - accuracy: 0.5481 - val_loss: 2.1285 - val_accuracy: 0.5340\n","Epoch 11/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.7268 - accuracy: 0.5813 - val_loss: 2.0001 - val_accuracy: 0.5938\n","Epoch 12/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.6162 - accuracy: 0.6027 - val_loss: 1.9019 - val_accuracy: 0.6079\n","Epoch 13/40\n","1846/1846 [==============================] - 5s 2ms/step - loss: 1.5223 - accuracy: 0.6284 - val_loss: 1.8641 - val_accuracy: 0.6084\n","Epoch 14/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.4512 - accuracy: 0.6404 - val_loss: 1.7746 - val_accuracy: 0.6326\n","Epoch 15/40\n","1846/1846 [==============================] - 5s 2ms/step - loss: 1.3842 - accuracy: 0.6557 - val_loss: 1.7667 - val_accuracy: 0.6365\n","Epoch 16/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.3310 - accuracy: 0.6735 - val_loss: 1.6660 - val_accuracy: 0.6744\n","Epoch 17/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.2885 - accuracy: 0.6689 - val_loss: 1.6553 - val_accuracy: 0.6673\n","Epoch 18/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.2444 - accuracy: 0.6867 - val_loss: 1.6455 - val_accuracy: 0.6695\n","Epoch 19/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.1998 - accuracy: 0.6974 - val_loss: 1.8595 - val_accuracy: 0.6286\n","Epoch 20/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.1630 - accuracy: 0.7014 - val_loss: 1.5821 - val_accuracy: 0.6620\n","Epoch 21/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.1222 - accuracy: 0.7071 - val_loss: 1.5427 - val_accuracy: 0.6902\n","Epoch 22/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.0960 - accuracy: 0.7188 - val_loss: 1.5585 - val_accuracy: 0.6878\n","Epoch 23/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.0699 - accuracy: 0.7202 - val_loss: 1.5226 - val_accuracy: 0.7021\n","Epoch 24/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.0368 - accuracy: 0.7363 - val_loss: 1.4282 - val_accuracy: 0.7311\n","Epoch 25/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.0132 - accuracy: 0.7371 - val_loss: 1.4240 - val_accuracy: 0.7397\n","Epoch 26/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.9912 - accuracy: 0.7440 - val_loss: 1.4095 - val_accuracy: 0.7274\n","Epoch 27/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.9651 - accuracy: 0.7483 - val_loss: 1.4306 - val_accuracy: 0.7303\n","Epoch 28/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.9549 - accuracy: 0.7525 - val_loss: 1.3373 - val_accuracy: 0.7553\n","Epoch 29/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.9281 - accuracy: 0.7580 - val_loss: 1.3481 - val_accuracy: 0.7492\n","Epoch 30/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.9082 - accuracy: 0.7602 - val_loss: 1.3332 - val_accuracy: 0.7441\n","Epoch 31/40\n","1846/1846 [==============================] - 6s 4ms/step - loss: 0.9003 - accuracy: 0.7650 - val_loss: 1.3787 - val_accuracy: 0.7377\n","Epoch 32/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.8817 - accuracy: 0.7683 - val_loss: 1.3149 - val_accuracy: 0.7481\n","Epoch 33/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.8654 - accuracy: 0.7720 - val_loss: 1.2969 - val_accuracy: 0.7701\n","Epoch 34/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.8504 - accuracy: 0.7729 - val_loss: 1.2400 - val_accuracy: 0.7734\n","Epoch 35/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8302 - accuracy: 0.7841 - val_loss: 1.2553 - val_accuracy: 0.7751\n","Epoch 36/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8256 - accuracy: 0.7797 - val_loss: 1.2645 - val_accuracy: 0.7795\n","Epoch 37/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8101 - accuracy: 0.7863 - val_loss: 1.2860 - val_accuracy: 0.7613\n","Epoch 38/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8014 - accuracy: 0.7886 - val_loss: 1.2740 - val_accuracy: 0.7611\n","Epoch 39/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7908 - accuracy: 0.7887 - val_loss: 1.2110 - val_accuracy: 0.7815\n","Epoch 40/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7847 - accuracy: 0.7928 - val_loss: 1.3282 - val_accuracy: 0.7424\n","Average Validation Accuracy: 0.7509714365005493\n","Average Validation Loss: 1.0684707462787628\n","Average Test Accuracy: 0.7500184178352356\n","Final Test Accuracy for each fold: 0.7538143992424011\n","Number of input features: 19\n","Fold: 1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/40\n","1846/1846 [==============================] - 9s 4ms/step - loss: 4.8746 - accuracy: 0.0571 - val_loss: 4.5532 - val_accuracy: 0.0636\n","Epoch 2/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 4.2010 - accuracy: 0.1184 - val_loss: 4.0368 - val_accuracy: 0.1388\n","Epoch 3/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 3.7590 - accuracy: 0.1690 - val_loss: 3.7333 - val_accuracy: 0.1791\n","Epoch 4/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.4878 - accuracy: 0.2039 - val_loss: 3.5193 - val_accuracy: 0.2086\n","Epoch 5/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 3.2389 - accuracy: 0.2399 - val_loss: 3.2764 - val_accuracy: 0.2653\n","Epoch 6/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.9910 - accuracy: 0.2928 - val_loss: 3.0324 - val_accuracy: 0.3250\n","Epoch 7/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.7300 - accuracy: 0.3452 - val_loss: 2.8212 - val_accuracy: 0.3536\n","Epoch 8/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.4669 - accuracy: 0.4014 - val_loss: 2.5376 - val_accuracy: 0.4240\n","Epoch 9/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.1951 - accuracy: 0.4675 - val_loss: 2.3167 - val_accuracy: 0.4942\n","Epoch 10/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.9774 - accuracy: 0.5196 - val_loss: 2.1482 - val_accuracy: 0.5285\n","Epoch 11/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.8202 - accuracy: 0.5570 - val_loss: 2.0326 - val_accuracy: 0.5586\n","Epoch 12/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.6939 - accuracy: 0.5794 - val_loss: 1.9084 - val_accuracy: 0.5908\n","Epoch 13/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.6007 - accuracy: 0.6004 - val_loss: 1.8609 - val_accuracy: 0.5923\n","Epoch 14/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.5142 - accuracy: 0.6203 - val_loss: 1.7987 - val_accuracy: 0.6035\n","Epoch 15/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.4503 - accuracy: 0.6330 - val_loss: 1.7897 - val_accuracy: 0.6114\n","Epoch 16/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.4034 - accuracy: 0.6423 - val_loss: 1.7049 - val_accuracy: 0.6257\n","Epoch 17/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.3513 - accuracy: 0.6516 - val_loss: 1.6971 - val_accuracy: 0.6356\n","Epoch 18/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.3090 - accuracy: 0.6587 - val_loss: 1.6743 - val_accuracy: 0.6396\n","Epoch 19/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.2677 - accuracy: 0.6709 - val_loss: 1.6445 - val_accuracy: 0.6546\n","Epoch 20/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.2292 - accuracy: 0.6740 - val_loss: 1.6193 - val_accuracy: 0.6462\n","Epoch 21/40\n","1846/1846 [==============================] - 6s 4ms/step - loss: 1.2057 - accuracy: 0.6855 - val_loss: 1.5836 - val_accuracy: 0.6532\n","Epoch 22/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.1804 - accuracy: 0.6905 - val_loss: 1.5068 - val_accuracy: 0.7078\n","Epoch 23/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.1473 - accuracy: 0.6952 - val_loss: 1.5062 - val_accuracy: 0.6959\n","Epoch 24/40\n","1846/1846 [==============================] - 12s 7ms/step - loss: 1.1210 - accuracy: 0.7007 - val_loss: 1.5152 - val_accuracy: 0.6788\n","Epoch 25/40\n","1846/1846 [==============================] - 15s 8ms/step - loss: 1.0955 - accuracy: 0.7143 - val_loss: 1.4983 - val_accuracy: 0.6895\n","Epoch 26/40\n","1846/1846 [==============================] - 12s 7ms/step - loss: 1.0744 - accuracy: 0.7162 - val_loss: 1.4486 - val_accuracy: 0.7157\n","Epoch 27/40\n","1846/1846 [==============================] - 13s 7ms/step - loss: 1.0469 - accuracy: 0.7265 - val_loss: 1.4293 - val_accuracy: 0.7107\n","Epoch 28/40\n","1846/1846 [==============================] - 15s 8ms/step - loss: 1.0241 - accuracy: 0.7327 - val_loss: 1.3976 - val_accuracy: 0.7243\n","Epoch 29/40\n","1846/1846 [==============================] - 15s 8ms/step - loss: 1.0068 - accuracy: 0.7380 - val_loss: 1.4149 - val_accuracy: 0.7171\n","Epoch 30/40\n","1846/1846 [==============================] - 12s 6ms/step - loss: 0.9825 - accuracy: 0.7409 - val_loss: 1.4700 - val_accuracy: 0.6882\n","Epoch 31/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.9670 - accuracy: 0.7442 - val_loss: 1.3088 - val_accuracy: 0.7586\n","Epoch 32/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 0.9434 - accuracy: 0.7534 - val_loss: 1.3183 - val_accuracy: 0.7578\n","Epoch 33/40\n","1846/1846 [==============================] - 13s 7ms/step - loss: 0.9349 - accuracy: 0.7559 - val_loss: 1.3472 - val_accuracy: 0.7437\n","Epoch 34/40\n","1846/1846 [==============================] - 14s 7ms/step - loss: 0.9196 - accuracy: 0.7561 - val_loss: 1.2839 - val_accuracy: 0.7518\n","Epoch 35/40\n","1846/1846 [==============================] - 12s 6ms/step - loss: 0.8975 - accuracy: 0.7651 - val_loss: 1.3254 - val_accuracy: 0.7483\n","Epoch 36/40\n","1846/1846 [==============================] - 12s 6ms/step - loss: 0.8881 - accuracy: 0.7685 - val_loss: 1.2606 - val_accuracy: 0.7639\n","Epoch 37/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.8790 - accuracy: 0.7685 - val_loss: 1.2990 - val_accuracy: 0.7586\n","Epoch 38/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 0.8583 - accuracy: 0.7738 - val_loss: 1.2812 - val_accuracy: 0.7703\n","Epoch 39/40\n","1846/1846 [==============================] - 12s 7ms/step - loss: 0.8478 - accuracy: 0.7781 - val_loss: 1.2466 - val_accuracy: 0.7652\n","Epoch 40/40\n","1846/1846 [==============================] - 14s 7ms/step - loss: 0.8345 - accuracy: 0.7848 - val_loss: 1.2477 - val_accuracy: 0.7696\n","Fold: 2\n","Epoch 1/40\n","1846/1846 [==============================] - 12s 5ms/step - loss: 4.8762 - accuracy: 0.0542 - val_loss: 4.5983 - val_accuracy: 0.1109\n","Epoch 2/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 4.1460 - accuracy: 0.1173 - val_loss: 3.9682 - val_accuracy: 0.1516\n","Epoch 3/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.6717 - accuracy: 0.1788 - val_loss: 3.6956 - val_accuracy: 0.1949\n","Epoch 4/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 3.3933 - accuracy: 0.2107 - val_loss: 3.4477 - val_accuracy: 0.2277\n","Epoch 5/40\n","1846/1846 [==============================] - 12s 7ms/step - loss: 3.1403 - accuracy: 0.2504 - val_loss: 3.2138 - val_accuracy: 0.2669\n","Epoch 6/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.8518 - accuracy: 0.3174 - val_loss: 2.9364 - val_accuracy: 0.3549\n","Epoch 7/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 2.5593 - accuracy: 0.3829 - val_loss: 2.6534 - val_accuracy: 0.4194\n","Epoch 8/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 2.2766 - accuracy: 0.4432 - val_loss: 2.4001 - val_accuracy: 0.4783\n","Epoch 9/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.0264 - accuracy: 0.5084 - val_loss: 2.2405 - val_accuracy: 0.4876\n","Epoch 10/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.8296 - accuracy: 0.5485 - val_loss: 2.0377 - val_accuracy: 0.5751\n","Epoch 11/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.6749 - accuracy: 0.5844 - val_loss: 1.9494 - val_accuracy: 0.5767\n","Epoch 12/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.5545 - accuracy: 0.6205 - val_loss: 1.8528 - val_accuracy: 0.6321\n","Epoch 13/40\n","1846/1846 [==============================] - 13s 7ms/step - loss: 1.4541 - accuracy: 0.6376 - val_loss: 1.7413 - val_accuracy: 0.6504\n","Epoch 14/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.3753 - accuracy: 0.6515 - val_loss: 1.7025 - val_accuracy: 0.6552\n","Epoch 15/40\n","1846/1846 [==============================] - 12s 7ms/step - loss: 1.3073 - accuracy: 0.6650 - val_loss: 1.6046 - val_accuracy: 0.6730\n","Epoch 16/40\n","1846/1846 [==============================] - 12s 7ms/step - loss: 1.2466 - accuracy: 0.6836 - val_loss: 1.6185 - val_accuracy: 0.6761\n","Epoch 17/40\n","1846/1846 [==============================] - 12s 6ms/step - loss: 1.2038 - accuracy: 0.6887 - val_loss: 1.5377 - val_accuracy: 0.7133\n","Epoch 18/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.1616 - accuracy: 0.6976 - val_loss: 1.4965 - val_accuracy: 0.7058\n","Epoch 19/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.1278 - accuracy: 0.7060 - val_loss: 1.5364 - val_accuracy: 0.6768\n","Epoch 20/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.0956 - accuracy: 0.7117 - val_loss: 1.4668 - val_accuracy: 0.7113\n","Epoch 21/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.0662 - accuracy: 0.7189 - val_loss: 1.4589 - val_accuracy: 0.7019\n","Epoch 22/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.0442 - accuracy: 0.7227 - val_loss: 1.4171 - val_accuracy: 0.7256\n","Epoch 23/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.0158 - accuracy: 0.7306 - val_loss: 1.3521 - val_accuracy: 0.7362\n","Epoch 24/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.9945 - accuracy: 0.7366 - val_loss: 1.3923 - val_accuracy: 0.7360\n","Epoch 25/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.9768 - accuracy: 0.7448 - val_loss: 1.3429 - val_accuracy: 0.7419\n","Epoch 26/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.9616 - accuracy: 0.7444 - val_loss: 1.3491 - val_accuracy: 0.7496\n","Epoch 27/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.9394 - accuracy: 0.7562 - val_loss: 1.3139 - val_accuracy: 0.7490\n","Epoch 28/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.9247 - accuracy: 0.7578 - val_loss: 1.3009 - val_accuracy: 0.7487\n","Epoch 29/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.9109 - accuracy: 0.7583 - val_loss: 1.3134 - val_accuracy: 0.7690\n","Epoch 30/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.8969 - accuracy: 0.7612 - val_loss: 1.2781 - val_accuracy: 0.7498\n","Epoch 31/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.8710 - accuracy: 0.7719 - val_loss: 1.2357 - val_accuracy: 0.7639\n","Epoch 32/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.8676 - accuracy: 0.7685 - val_loss: 1.2358 - val_accuracy: 0.7707\n","Epoch 33/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.8482 - accuracy: 0.7800 - val_loss: 1.2596 - val_accuracy: 0.7630\n","Epoch 34/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.8452 - accuracy: 0.7744 - val_loss: 1.2111 - val_accuracy: 0.7732\n","Epoch 35/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.8192 - accuracy: 0.7851 - val_loss: 1.2498 - val_accuracy: 0.7549\n","Epoch 36/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.8142 - accuracy: 0.7876 - val_loss: 1.1698 - val_accuracy: 0.7806\n","Epoch 37/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.7963 - accuracy: 0.7935 - val_loss: 1.1350 - val_accuracy: 0.7993\n","Epoch 38/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.7847 - accuracy: 0.7998 - val_loss: 1.2192 - val_accuracy: 0.7613\n","Epoch 39/40\n","1846/1846 [==============================] - 17s 9ms/step - loss: 0.7809 - accuracy: 0.7947 - val_loss: 1.1861 - val_accuracy: 0.7745\n","Epoch 40/40\n","1846/1846 [==============================] - 13s 7ms/step - loss: 0.7578 - accuracy: 0.8012 - val_loss: 1.1206 - val_accuracy: 0.7949\n","Average Validation Accuracy: 0.7940317392349243\n","Average Validation Loss: 0.9054939150810242\n","Average Test Accuracy: 0.7927323579788208\n","Final Test Accuracy for each fold: 0.8062209486961365\n","Number of input features: 20\n","Fold: 1\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/40\n","1846/1846 [==============================] - 14s 5ms/step - loss: 4.9661 - accuracy: 0.0444 - val_loss: 4.7977 - val_accuracy: 0.0491\n","Epoch 2/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 4.6327 - accuracy: 0.0577 - val_loss: 4.5121 - val_accuracy: 0.0827\n","Epoch 3/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 4.0947 - accuracy: 0.1150 - val_loss: 3.9095 - val_accuracy: 0.1338\n","Epoch 4/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.5827 - accuracy: 0.1948 - val_loss: 3.5394 - val_accuracy: 0.2158\n","Epoch 5/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 3.2499 - accuracy: 0.2429 - val_loss: 3.2824 - val_accuracy: 0.2363\n","Epoch 6/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 2.9903 - accuracy: 0.2867 - val_loss: 3.0340 - val_accuracy: 0.3061\n","Epoch 7/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 2.7371 - accuracy: 0.3434 - val_loss: 2.8364 - val_accuracy: 0.3787\n","Epoch 8/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 2.4936 - accuracy: 0.3961 - val_loss: 2.6356 - val_accuracy: 0.4059\n","Epoch 9/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.2768 - accuracy: 0.4363 - val_loss: 2.4295 - val_accuracy: 0.4697\n","Epoch 10/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.0868 - accuracy: 0.4797 - val_loss: 2.2988 - val_accuracy: 0.5012\n","Epoch 11/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.9203 - accuracy: 0.5220 - val_loss: 2.1780 - val_accuracy: 0.5182\n","Epoch 12/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.7890 - accuracy: 0.5464 - val_loss: 2.1445 - val_accuracy: 0.5259\n","Epoch 13/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.6723 - accuracy: 0.5768 - val_loss: 1.9760 - val_accuracy: 0.5813\n","Epoch 14/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 1.5659 - accuracy: 0.6009 - val_loss: 1.9427 - val_accuracy: 0.5877\n","Epoch 15/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.4855 - accuracy: 0.6190 - val_loss: 1.8391 - val_accuracy: 0.6222\n","Epoch 16/40\n","1846/1846 [==============================] - 6s 4ms/step - loss: 1.3986 - accuracy: 0.6448 - val_loss: 1.7973 - val_accuracy: 0.6548\n","Epoch 17/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.3449 - accuracy: 0.6510 - val_loss: 1.7578 - val_accuracy: 0.6539\n","Epoch 18/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 1.2933 - accuracy: 0.6643 - val_loss: 1.7226 - val_accuracy: 0.6788\n","Epoch 19/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.2470 - accuracy: 0.6803 - val_loss: 1.6881 - val_accuracy: 0.6697\n","Epoch 20/40\n","1846/1846 [==============================] - 10s 6ms/step - loss: 1.2064 - accuracy: 0.6857 - val_loss: 1.6811 - val_accuracy: 0.6700\n","Epoch 21/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 1.1700 - accuracy: 0.7005 - val_loss: 1.6618 - val_accuracy: 0.6700\n","Epoch 22/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 1.1340 - accuracy: 0.7051 - val_loss: 1.5918 - val_accuracy: 0.6926\n","Epoch 23/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.1051 - accuracy: 0.7160 - val_loss: 1.5941 - val_accuracy: 0.6818\n","Epoch 24/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.0723 - accuracy: 0.7223 - val_loss: 1.5138 - val_accuracy: 0.7292\n","Epoch 25/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.0452 - accuracy: 0.7313 - val_loss: 1.5363 - val_accuracy: 0.7087\n","Epoch 26/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0297 - accuracy: 0.7317 - val_loss: 1.4956 - val_accuracy: 0.7252\n","Epoch 27/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.0074 - accuracy: 0.7356 - val_loss: 1.5079 - val_accuracy: 0.7248\n","Epoch 28/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9818 - accuracy: 0.7389 - val_loss: 1.4767 - val_accuracy: 0.7355\n","Epoch 29/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9662 - accuracy: 0.7419 - val_loss: 1.4722 - val_accuracy: 0.7457\n","Epoch 30/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9473 - accuracy: 0.7505 - val_loss: 1.4373 - val_accuracy: 0.7498\n","Epoch 31/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9262 - accuracy: 0.7559 - val_loss: 1.4303 - val_accuracy: 0.7439\n","Epoch 32/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9155 - accuracy: 0.7591 - val_loss: 1.4212 - val_accuracy: 0.7454\n","Epoch 33/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8912 - accuracy: 0.7672 - val_loss: 1.4298 - val_accuracy: 0.7474\n","Epoch 34/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8830 - accuracy: 0.7676 - val_loss: 1.4118 - val_accuracy: 0.7507\n","Epoch 35/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8734 - accuracy: 0.7686 - val_loss: 1.3757 - val_accuracy: 0.7635\n","Epoch 36/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.8582 - accuracy: 0.7734 - val_loss: 1.3934 - val_accuracy: 0.7450\n","Epoch 37/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.8474 - accuracy: 0.7777 - val_loss: 1.3387 - val_accuracy: 0.7681\n","Epoch 38/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.8323 - accuracy: 0.7773 - val_loss: 1.3196 - val_accuracy: 0.7839\n","Epoch 39/40\n","1846/1846 [==============================] - 8s 5ms/step - loss: 0.8169 - accuracy: 0.7842 - val_loss: 1.3539 - val_accuracy: 0.7633\n","Epoch 40/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 0.8110 - accuracy: 0.7824 - val_loss: 1.3908 - val_accuracy: 0.7395\n","Fold: 2\n","Epoch 1/40\n","1846/1846 [==============================] - 9s 4ms/step - loss: 4.8579 - accuracy: 0.0556 - val_loss: 4.5166 - val_accuracy: 0.0700\n","Epoch 2/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 4.0546 - accuracy: 0.1271 - val_loss: 3.9056 - val_accuracy: 0.1710\n","Epoch 3/40\n","1846/1846 [==============================] - 8s 4ms/step - loss: 3.6138 - accuracy: 0.1833 - val_loss: 3.6230 - val_accuracy: 0.1835\n","Epoch 4/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 3.3306 - accuracy: 0.2044 - val_loss: 3.3682 - val_accuracy: 0.2255\n","Epoch 5/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.0499 - accuracy: 0.2517 - val_loss: 3.0835 - val_accuracy: 0.3061\n","Epoch 6/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.7212 - accuracy: 0.3322 - val_loss: 2.7680 - val_accuracy: 0.3419\n","Epoch 7/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.3866 - accuracy: 0.4140 - val_loss: 2.4509 - val_accuracy: 0.4513\n","Epoch 8/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.0873 - accuracy: 0.4863 - val_loss: 2.2093 - val_accuracy: 0.5065\n","Epoch 9/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.8536 - accuracy: 0.5417 - val_loss: 2.0497 - val_accuracy: 0.5520\n","Epoch 10/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.6830 - accuracy: 0.5767 - val_loss: 1.9523 - val_accuracy: 0.5815\n","Epoch 11/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.5483 - accuracy: 0.6109 - val_loss: 1.8761 - val_accuracy: 0.6128\n","Epoch 12/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.4417 - accuracy: 0.6337 - val_loss: 1.7222 - val_accuracy: 0.6570\n","Epoch 13/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.3671 - accuracy: 0.6559 - val_loss: 1.6854 - val_accuracy: 0.6535\n","Epoch 14/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.2918 - accuracy: 0.6719 - val_loss: 1.7090 - val_accuracy: 0.6264\n","Epoch 15/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.2517 - accuracy: 0.6736 - val_loss: 1.5645 - val_accuracy: 0.6706\n","Epoch 16/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.1984 - accuracy: 0.6941 - val_loss: 1.5251 - val_accuracy: 0.6484\n","Epoch 17/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.1587 - accuracy: 0.6993 - val_loss: 1.4903 - val_accuracy: 0.6836\n","Epoch 18/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.1218 - accuracy: 0.7105 - val_loss: 1.4599 - val_accuracy: 0.6959\n","Epoch 19/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.0891 - accuracy: 0.7162 - val_loss: 1.3812 - val_accuracy: 0.7188\n","Epoch 20/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0539 - accuracy: 0.7236 - val_loss: 1.4041 - val_accuracy: 0.7010\n","Epoch 21/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0260 - accuracy: 0.7338 - val_loss: 1.3595 - val_accuracy: 0.7322\n","Epoch 22/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9987 - accuracy: 0.7373 - val_loss: 1.3352 - val_accuracy: 0.7292\n","Epoch 23/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9752 - accuracy: 0.7451 - val_loss: 1.3332 - val_accuracy: 0.7426\n","Epoch 24/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9535 - accuracy: 0.7495 - val_loss: 1.3046 - val_accuracy: 0.7470\n","Epoch 25/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9291 - accuracy: 0.7569 - val_loss: 1.2929 - val_accuracy: 0.7402\n","Epoch 26/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9078 - accuracy: 0.7619 - val_loss: 1.2456 - val_accuracy: 0.7672\n","Epoch 27/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8952 - accuracy: 0.7667 - val_loss: 1.2211 - val_accuracy: 0.7679\n","Epoch 28/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8811 - accuracy: 0.7673 - val_loss: 1.2439 - val_accuracy: 0.7507\n","Epoch 29/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8645 - accuracy: 0.7709 - val_loss: 1.1971 - val_accuracy: 0.7732\n","Epoch 30/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8511 - accuracy: 0.7750 - val_loss: 1.2109 - val_accuracy: 0.7573\n","Epoch 31/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8373 - accuracy: 0.7790 - val_loss: 1.1588 - val_accuracy: 0.7782\n","Epoch 32/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8169 - accuracy: 0.7855 - val_loss: 1.1877 - val_accuracy: 0.7727\n","Epoch 33/40\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8127 - accuracy: 0.7843 - val_loss: 1.1709 - val_accuracy: 0.7641\n","Epoch 34/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7938 - accuracy: 0.7921 - val_loss: 1.1763 - val_accuracy: 0.7820\n","Epoch 35/40\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7846 - accuracy: 0.7941 - val_loss: 1.1502 - val_accuracy: 0.7756\n","Epoch 36/40\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7736 - accuracy: 0.7931 - val_loss: 1.1361 - val_accuracy: 0.7963\n","Epoch 37/40\n","1846/1846 [==============================] - 9s 5ms/step - loss: 0.7578 - accuracy: 0.7978 - val_loss: 1.1462 - val_accuracy: 0.7712\n","Epoch 38/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.7456 - accuracy: 0.8055 - val_loss: 1.1205 - val_accuracy: 0.7879\n","Epoch 39/40\n","1846/1846 [==============================] - 11s 6ms/step - loss: 0.7391 - accuracy: 0.8060 - val_loss: 1.1670 - val_accuracy: 0.7762\n","Epoch 40/40\n","1846/1846 [==============================] - 10s 5ms/step - loss: 0.7290 - accuracy: 0.8072 - val_loss: 1.1279 - val_accuracy: 0.7905\n","Average Validation Accuracy: 0.7794369161128998\n","Average Validation Loss: 0.952432245016098\n","Average Test Accuracy: 0.7772167921066284\n","Final Test Accuracy for each fold: 0.8088744878768921\n"]}],"source":["# Define the number of folds for k-fold cross-validation\n","k = 2\n","\n","# Define the cross-validation method\n","cv_method = StratifiedKFold(n_splits=k)\n","\n","# Initialize the list to store the history, train & validation(accuracy & loss) for each model\n","models = []\n","model_history = []\n","model_accuracy = []\n","model_train_acc = []\n","model_train_loss = []\n","model_val_acc = []\n","model_val_loss = []\n","\n","\n","for i in range(1,21):\n","\n","    models_fold = []\n","    hist = []\n","    train_accuracy = []\n","    train_loss = []\n","    val_accuracy = []\n","    val_loss = []\n","    test_accuracy = []\n","\n","    print(\"Number of input features:\",i)\n","\n","    # Select the input features from the input data\n","    X_train_selected = X_train[:, :i]\n","    X_test_selected = X_test[:, :i]\n","\n","    # Loop over the folds\n","    for fold, (train_index, val_index) in enumerate(cv_method.split(X_train_selected, y_train)):\n","\n","        print(\"Fold:\", fold+1)\n","\n","        # Split the data into train and validation sets using the current fold index\n","        X_train_fold  = X_train_selected[train_index]\n","        y_train_fold  = y_train[train_index]\n","        X_val_fold = X_train_selected[val_index]\n","        y_val_fold = y_train[val_index]\n","\n","        # Prepare the target data\n","        y_train_fold_enc, y_val_fold_enc = prepare_targets(y_train_fold, y_val_fold)\n","\n","        # build the model\n","        model = RBFN_model(i)\n","\n","        # Fit the model to the training data for the current fold\n","        history = model.fit(X_train_fold, to_categorical(y_train_fold_enc, num_classes=373), epochs=40, batch_size=5, verbose=1, validation_split = 0.33)\n","    \n","        # Evaluate the model on the validation data for the current fold\n","        val_scores = model.evaluate(X_val_fold, to_categorical(y_val_fold_enc, num_classes=373), verbose=0)\n","        val_accuracy.append(val_scores[1])\n","        val_loss.append(val_scores[0])\n","\n","        # Evaluate the model on the test data for the current fold\n","        test_scores = model.evaluate(X_test_selected, to_categorical(y_test_enc, num_classes=373), verbose=0)\n","        test_accuracy.append(test_scores[1])\n","\n","        # add the model to the list of models\n","        models_fold.append(model)\n","        hist.append(history)\n","\n","        # store the training accuracy and loss for each fold\n","        train_accuracy.append(history.history['accuracy'])\n","        train_loss.append(history.history['loss'])\n","        \n","    # Calculate the average test and validation accuracy and loss across all folds\n","    avg_test_acc = sum(test_accuracy) / len(test_accuracy)\n","    avg_val_acc = sum(val_accuracy) / len(val_accuracy)\n","    avg_val_loss = sum(val_loss) / len(val_loss)\n","\n","    # Print the average validation and test accuracy and loss\n","    print(\"Average Validation Accuracy:\", avg_val_acc)\n","    print(\"Average Validation Loss:\",avg_val_loss)\n","    print(\"Average Test Accuracy:\", avg_test_acc)\n","\n","    best_fold_index = test_accuracy.index(max(test_accuracy))\n","    model_accuracy.append(test_accuracy[best_fold_index])\n","    models.append(models_fold[best_fold_index])\n","    model_history.append(hist[best_fold_index])\n","    model_train_acc.append(train_accuracy[best_fold_index])\n","    model_train_loss.append(train_loss[best_fold_index])\n","    model_val_acc.append(val_accuracy[best_fold_index])\n","    model_val_loss.append(val_loss[best_fold_index])\n","\n","\n","    print(\"Final Test Accuracy for each fold:\", test_accuracy[best_fold_index])"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"xzn9CBYTQsYy"},"outputs":[],"source":["#to show the no of input features and its corresponding model accuracy\n","model_list = []\n","\n","#Iterate through each model's accuracy \n","for i in range (len(model_accuracy)):\n","    #get the number of input features for the current model\n","    no_features = i + 1\n","\n","    #round the model accuries to 3 d.p.\n","    rounded_model_acc = round(model_accuracy[i], 3)\n","    \n","    model_list.append([no_features, rounded_model_acc])\n","\n","models_df = pd.DataFrame(model_list, columns=[\"No of input features\", \"Model accuracy\"])"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":72,"status":"ok","timestamp":1682274337709,"user":{"displayName":"Good Notes UM","userId":"12493674840996884929"},"user_tz":-480},"id":"WAjs7sLtQsYz","outputId":"42b8ee3f-cf37-459f-e38e-c3b79cbd580a"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 2 Axes>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Final Test Accuracy: 0.8435173630714417\n"]}],"source":["# Select the final model based on the max test accuracy across all models\n","\n","best_model_index = model_accuracy.index(max(model_accuracy))\n","\n","best_model = models[best_model_index]\n","best_model_history = model_history[best_model_index]\n","best_model_train_acc = model_train_acc[best_model_index]\n","best_model_train_loss = model_train_loss[best_model_index]\n","best_model_val_acc = model_val_acc[best_model_index]\n","best_model_val_loss = model_val_loss[best_model_index]\n","\n","# summarize history for accuracy  \n","plt.subplot(211)  \n","plt.plot(best_model_history.history['accuracy'])  \n","plt.plot(best_model_history.history['val_accuracy'])  \n","plt.title('model accuracy')  \n","plt.ylabel('accuracy')  \n","plt.xlabel('epoch')  \n","plt.legend(['train', 'test'], loc='lower right')  \n","\n","# summarize history for loss\n","plt.subplot(212)  \n","plt.plot(best_model_history.history['loss'])  \n","plt.plot(best_model_history.history['val_loss'])  \n","plt.title('model loss')  \n","plt.ylabel('loss')  \n","plt.xlabel('epoch')  \n","plt.legend(['train', 'test'], loc='upper right')  \n","plt.subplots_adjust(hspace=0.7)\n","\n","plt.show() \n","\n","print(\"Final Test Accuracy:\", model_accuracy[best_model_index])"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1862,"status":"ok","timestamp":1682274339506,"user":{"displayName":"Good Notes UM","userId":"12493674840996884929"},"user_tz":-480},"id":"KfbFjY7JQsY1","outputId":"892dc1e7-c7ee-4897-cf17-ad1d9811caf4"},"outputs":[{"name":"stdout","output_type":"stream","text":["424/424 [==============================] - 2s 2ms/step\n","Classification report:\n","               precision    recall  f1-score   support\n","\n","           0       0.91      1.00      0.95       591\n","           1       1.00      1.00      1.00       430\n","           2       1.00      1.00      1.00       419\n","           3       1.00      1.00      1.00       384\n","           4       1.00      1.00      1.00       339\n","           5       0.99      1.00      0.99       342\n","           6       1.00      1.00      1.00       310\n","           7       0.97      1.00      0.98       325\n","           8       0.71      0.99      0.83       294\n","           9       0.92      1.00      0.96       269\n","          10       0.90      1.00      0.95       296\n","          11       0.85      1.00      0.92       258\n","          12       0.98      1.00      0.99       247\n","          13       0.95      1.00      0.97       237\n","          14       0.93      1.00      0.96       239\n","          15       0.98      1.00      0.99       235\n","          16       1.00      1.00      1.00       213\n","          17       1.00      1.00      1.00       202\n","          18       0.98      1.00      0.99       196\n","          19       1.00      1.00      1.00       181\n","          20       1.00      1.00      1.00       177\n","          21       0.92      1.00      0.96       177\n","          22       0.92      1.00      0.96       155\n","          23       1.00      1.00      1.00       155\n","          24       1.00      0.98      0.99       144\n","          25       0.87      1.00      0.93       126\n","          26       0.99      1.00      1.00       108\n","          27       0.83      0.77      0.80       121\n","          28       0.95      1.00      0.97        95\n","          29       0.75      0.96      0.84       106\n","          30       0.93      1.00      0.96       102\n","          31       0.92      1.00      0.96        86\n","          32       0.84      1.00      0.92       108\n","          33       1.00      0.92      0.96        88\n","          34       0.82      1.00      0.90       102\n","          35       1.00      0.98      0.99        88\n","          36       0.99      1.00      0.99        83\n","          37       0.93      0.99      0.96        93\n","          38       0.99      1.00      0.99        76\n","          39       0.92      1.00      0.96        85\n","          40       1.00      1.00      1.00        86\n","          41       0.63      1.00      0.78        85\n","          42       0.91      1.00      0.95        68\n","          43       0.70      1.00      0.82        75\n","          44       0.85      1.00      0.92        71\n","          45       0.34      0.81      0.47        58\n","          46       0.88      1.00      0.93        71\n","          47       0.78      0.12      0.21        57\n","          48       1.00      1.00      1.00        67\n","          49       0.58      0.66      0.62        47\n","          50       1.00      1.00      1.00        48\n","          51       0.00      0.00      0.00        47\n","          52       0.73      1.00      0.84        43\n","          53       0.96      1.00      0.98        51\n","          54       0.72      1.00      0.84        44\n","          55       0.88      0.82      0.85        51\n","          56       1.00      0.29      0.45        45\n","          57       1.00      1.00      1.00        44\n","          58       0.80      0.68      0.74        41\n","          59       0.31      1.00      0.47        41\n","          60       1.00      0.12      0.21        52\n","          61       1.00      0.81      0.90        43\n","          62       0.67      1.00      0.80        37\n","          63       0.93      0.65      0.77        43\n","          64       0.93      1.00      0.97        42\n","          65       0.86      0.80      0.83        46\n","          66       1.00      0.23      0.38        43\n","          67       0.34      1.00      0.51        40\n","          68       0.92      1.00      0.96        44\n","          69       0.62      1.00      0.77        43\n","          70       0.92      0.89      0.91        38\n","          71       1.00      1.00      1.00        33\n","          72       0.90      1.00      0.95        45\n","          73       0.68      0.95      0.79        38\n","          74       0.50      1.00      0.67        42\n","          75       0.81      1.00      0.90        39\n","          76       0.39      0.57      0.46        30\n","          77       0.88      1.00      0.93        28\n","          78       0.00      0.00      0.00        28\n","          79       0.68      1.00      0.81        32\n","          80       1.00      1.00      1.00        33\n","          81       1.00      1.00      1.00        31\n","          82       0.76      1.00      0.86        35\n","          83       0.91      1.00      0.95        39\n","          84       1.00      0.93      0.96        27\n","          85       0.80      1.00      0.89        36\n","          86       0.76      1.00      0.86        31\n","          87       0.50      1.00      0.67        28\n","          88       0.00      0.00      0.00        20\n","          89       0.94      0.48      0.64        33\n","          90       0.38      1.00      0.55        24\n","          91       0.92      1.00      0.96        22\n","          92       0.62      1.00      0.76        26\n","          93       0.69      1.00      0.81        35\n","          94       0.15      0.19      0.16        27\n","          95       0.82      1.00      0.90        23\n","          96       0.92      0.41      0.56        27\n","          97       1.00      0.89      0.94        28\n","          98       0.86      0.38      0.52        16\n","          99       1.00      0.46      0.63        35\n","         100       1.00      0.93      0.96        28\n","         101       0.00      0.00      0.00        25\n","         102       0.14      0.19      0.16        26\n","         103       1.00      0.70      0.82        33\n","         104       0.75      0.35      0.47        26\n","         105       0.75      0.50      0.60        24\n","         106       0.00      0.00      0.00        22\n","         107       1.00      1.00      1.00        26\n","         108       0.06      0.12      0.08        25\n","         109       0.00      0.00      0.00        16\n","         110       0.76      0.80      0.78        20\n","         111       1.00      1.00      1.00        26\n","         112       1.00      1.00      1.00        18\n","         113       0.88      1.00      0.94        23\n","         114       1.00      1.00      1.00        25\n","         115       1.00      1.00      1.00        18\n","         116       0.88      0.74      0.80        19\n","         117       0.00      0.00      0.00        16\n","         118       0.00      0.00      0.00        26\n","         119       0.00      0.00      0.00        22\n","         120       0.38      0.82      0.52        17\n","         121       0.83      0.33      0.48        15\n","         122       0.78      0.78      0.78        18\n","         123       0.94      0.85      0.89        20\n","         124       0.00      0.00      0.00        14\n","         125       0.92      1.00      0.96        22\n","         126       1.00      1.00      1.00        19\n","         127       0.90      1.00      0.95        27\n","         128       0.00      0.00      0.00        26\n","         129       0.51      1.00      0.68        21\n","         130       0.00      0.00      0.00        18\n","         131       0.46      1.00      0.63        18\n","         132       0.00      0.00      0.00        20\n","         133       0.78      1.00      0.88        14\n","         134       0.69      0.95      0.80        19\n","         135       0.64      1.00      0.78        16\n","         136       0.00      0.00      0.00        23\n","         137       1.00      0.27      0.43        11\n","         138       1.00      1.00      1.00        14\n","         139       0.31      1.00      0.48        20\n","         140       0.96      1.00      0.98        23\n","         141       0.00      0.00      0.00        14\n","         142       1.00      0.08      0.14        13\n","         143       0.00      0.00      0.00        23\n","         144       1.00      0.06      0.11        17\n","         145       1.00      0.50      0.67        24\n","         146       0.00      0.00      0.00        16\n","         147       0.56      0.53      0.54        19\n","         148       1.00      0.82      0.90        22\n","         149       0.00      0.00      0.00        15\n","         150       1.00      0.36      0.53        11\n","         151       0.59      1.00      0.75        19\n","         152       1.00      0.75      0.86        20\n","         153       0.51      1.00      0.68        24\n","         154       0.22      1.00      0.36        11\n","         155       0.20      1.00      0.34        17\n","         156       1.00      0.89      0.94        18\n","         157       0.35      1.00      0.52        12\n","         158       0.00      0.00      0.00        18\n","         159       1.00      0.75      0.86        20\n","         160       0.00      0.00      0.00        20\n","         161       1.00      0.94      0.97        16\n","         162       0.00      0.00      0.00        15\n","         163       0.00      0.00      0.00        15\n","         164       1.00      1.00      1.00        13\n","         165       0.00      0.00      0.00        19\n","         166       0.00      0.00      0.00        11\n","         167       1.00      1.00      1.00         9\n","         168       1.00      0.91      0.95        11\n","         169       0.90      0.86      0.88        21\n","         170       1.00      0.87      0.93        15\n","         171       0.64      1.00      0.78        18\n","         172       0.00      0.00      0.00        11\n","         173       0.92      0.69      0.79        16\n","         174       0.77      1.00      0.87        10\n","         175       0.00      0.00      0.00        11\n","         176       1.00      0.80      0.89        10\n","         177       0.00      0.00      0.00        15\n","         178       0.00      0.00      0.00        11\n","         179       0.79      1.00      0.88        15\n","         180       0.00      0.00      0.00        13\n","         181       1.00      1.00      1.00        15\n","         182       0.53      0.89      0.67         9\n","         183       0.00      0.00      0.00        16\n","         184       1.00      0.38      0.55         8\n","         185       0.00      0.00      0.00        12\n","         186       0.00      0.00      0.00        15\n","         187       0.00      0.00      0.00        15\n","         188       0.00      0.00      0.00        13\n","         189       0.00      0.00      0.00        14\n","         190       0.43      0.55      0.48        11\n","         191       0.28      1.00      0.43        10\n","         192       0.73      0.94      0.82        17\n","         193       0.00      0.00      0.00         9\n","         194       1.00      1.00      1.00         9\n","         195       0.00      0.00      0.00         4\n","         196       0.78      1.00      0.88         7\n","         197       1.00      1.00      1.00         7\n","         198       0.00      0.00      0.00        12\n","         199       0.93      1.00      0.97        14\n","         200       0.00      0.00      0.00         6\n","         201       0.60      1.00      0.75         9\n","         202       0.50      1.00      0.67         7\n","         203       0.00      0.00      0.00         6\n","         204       0.00      0.00      0.00        11\n","         205       0.00      0.00      0.00        14\n","         206       1.00      0.67      0.80        12\n","         207       0.00      0.00      0.00        14\n","         208       0.38      1.00      0.55        12\n","         209       0.47      1.00      0.64         7\n","         210       0.00      0.00      0.00        19\n","         211       0.80      0.57      0.67         7\n","         212       0.00      0.00      0.00        11\n","         213       0.00      0.00      0.00         9\n","         214       0.29      0.86      0.43         7\n","         215       0.10      0.17      0.12         6\n","         216       0.00      0.00      0.00        12\n","         217       0.60      0.25      0.35        12\n","         218       0.00      0.00      0.00         9\n","         219       0.67      0.67      0.67         6\n","         220       0.00      0.00      0.00         8\n","         221       0.08      1.00      0.16         5\n","         222       0.00      0.00      0.00         4\n","         223       0.00      0.00      0.00        14\n","         224       1.00      0.08      0.14        13\n","         225       0.00      0.00      0.00         4\n","         226       0.17      0.40      0.24        10\n","         227       1.00      0.83      0.91        12\n","         228       0.00      0.00      0.00        13\n","         229       0.69      1.00      0.81        11\n","         230       0.00      0.00      0.00         5\n","         231       0.00      0.00      0.00         6\n","         232       0.00      0.00      0.00         8\n","         233       1.00      1.00      1.00        10\n","         234       0.00      0.00      0.00         4\n","         235       0.00      0.00      0.00         8\n","         236       0.00      0.00      0.00         9\n","         237       0.00      0.00      0.00         8\n","         238       0.83      1.00      0.91        10\n","         239       0.00      0.00      0.00         9\n","         240       0.00      0.00      0.00         7\n","         241       0.00      0.00      0.00         8\n","         242       1.00      0.17      0.29         6\n","         243       0.25      1.00      0.40         7\n","         244       0.00      0.00      0.00         9\n","         245       0.00      0.00      0.00         7\n","         246       1.00      0.44      0.62         9\n","         247       0.00      0.00      0.00         7\n","         248       0.00      0.00      0.00        10\n","         249       1.00      0.10      0.18        10\n","         250       0.00      0.00      0.00         7\n","         251       0.00      0.00      0.00         6\n","         252       0.00      0.00      0.00        11\n","         253       0.00      0.00      0.00         7\n","         254       0.00      0.00      0.00         8\n","         255       0.00      0.00      0.00         5\n","         256       0.00      0.00      0.00         7\n","         257       0.90      1.00      0.95         9\n","         258       0.18      0.33      0.24         6\n","         259       0.00      0.00      0.00         3\n","         260       0.44      1.00      0.62         4\n","         261       0.15      1.00      0.27         2\n","         262       0.00      0.00      0.00         5\n","         263       1.00      0.17      0.29        12\n","         264       0.31      1.00      0.48         5\n","         265       0.00      0.00      0.00         7\n","         266       1.00      0.90      0.95        10\n","         267       0.00      0.00      0.00         8\n","         268       0.00      0.00      0.00         9\n","         269       0.50      1.00      0.67         6\n","         270       0.00      0.00      0.00         4\n","         271       0.00      0.00      0.00         7\n","         272       0.00      0.00      0.00        10\n","         273       0.00      0.00      0.00         3\n","         274       0.00      0.00      0.00         9\n","         275       1.00      1.00      1.00         6\n","         276       0.00      0.00      0.00         5\n","         277       0.00      0.00      0.00         4\n","         278       0.00      0.00      0.00         3\n","         279       0.80      1.00      0.89         4\n","         280       1.00      0.20      0.33         5\n","         281       1.00      0.17      0.29         6\n","         282       0.00      0.00      0.00        11\n","         283       0.00      0.00      0.00         6\n","         284       0.00      0.00      0.00         2\n","         285       0.27      1.00      0.42         4\n","         286       0.00      0.00      0.00         7\n","         287       1.00      1.00      1.00         9\n","         288       0.00      0.00      0.00         7\n","         289       0.00      0.00      0.00         7\n","         290       0.00      0.00      0.00         6\n","         291       0.00      0.00      0.00         5\n","         292       0.55      1.00      0.71         6\n","         293       0.00      0.00      0.00         8\n","         294       0.29      1.00      0.45         7\n","         295       0.00      0.00      0.00         3\n","         296       0.00      0.00      0.00         6\n","         297       0.00      0.00      0.00         8\n","         298       0.00      0.00      0.00         4\n","         299       0.00      0.00      0.00         6\n","         300       0.00      0.00      0.00         5\n","         301       0.00      0.00      0.00         5\n","         302       0.00      0.00      0.00         5\n","         303       0.00      0.00      0.00         2\n","         304       0.00      0.00      0.00         8\n","         305       0.00      0.00      0.00         3\n","         306       0.00      0.00      0.00         6\n","         307       0.00      0.00      0.00         7\n","         308       0.00      0.00      0.00         4\n","         309       0.00      0.00      0.00         4\n","         310       0.00      0.00      0.00         9\n","         311       0.00      0.00      0.00         7\n","         312       0.00      0.00      0.00         6\n","         313       0.86      1.00      0.92         6\n","         314       0.00      0.00      0.00         8\n","         315       1.00      1.00      1.00         7\n","         316       0.00      0.00      0.00         3\n","         317       0.32      1.00      0.48         6\n","         318       0.00      0.00      0.00        10\n","         319       0.00      0.00      0.00         6\n","         320       0.00      0.00      0.00         2\n","         321       0.00      0.00      0.00         8\n","         322       0.00      0.00      0.00         4\n","         323       0.00      0.00      0.00         4\n","         324       0.00      0.00      0.00         8\n","         325       0.00      0.00      0.00         4\n","         326       0.00      0.00      0.00         7\n","         327       0.00      0.00      0.00         4\n","         328       0.00      0.00      0.00         6\n","         329       0.00      0.00      0.00         4\n","         330       0.00      0.00      0.00         3\n","         331       0.00      0.00      0.00         8\n","         332       1.00      1.00      1.00         1\n","         333       0.00      0.00      0.00         3\n","         334       0.00      0.00      0.00         4\n","         335       0.00      0.00      0.00         3\n","         336       0.00      0.00      0.00         6\n","         337       0.00      0.00      0.00         2\n","         338       0.00      0.00      0.00         7\n","         339       0.00      0.00      0.00         4\n","         340       0.00      0.00      0.00         6\n","         341       0.70      1.00      0.82         7\n","         342       1.00      1.00      1.00         2\n","         343       0.00      0.00      0.00         5\n","         344       0.00      0.00      0.00         4\n","         345       0.00      0.00      0.00         1\n","         346       0.00      0.00      0.00         2\n","         347       0.00      0.00      0.00         4\n","         348       1.00      0.29      0.44         7\n","         349       0.00      0.00      0.00         4\n","         350       0.00      0.00      0.00         6\n","         351       0.00      0.00      0.00         4\n","         352       0.00      0.00      0.00         5\n","         353       1.00      0.25      0.40         4\n","         354       0.00      0.00      0.00         3\n","         355       0.00      0.00      0.00         1\n","         356       0.00      0.00      0.00         4\n","         357       0.14      1.00      0.25         1\n","         358       0.00      0.00      0.00         3\n","         359       1.00      0.25      0.40         4\n","         360       0.00      0.00      0.00         3\n","         361       0.00      0.00      0.00         3\n","         362       0.00      0.00      0.00         3\n","         363       0.00      0.00      0.00         2\n","         364       0.00      0.00      0.00         3\n","         365       0.00      0.00      0.00         3\n","         366       0.00      0.00      0.00         2\n","         367       0.00      0.00      0.00         3\n","         368       0.00      0.00      0.00         1\n","         369       0.00      0.00      0.00         2\n","         370       0.00      0.00      0.00         2\n","         371       0.00      0.00      0.00         0\n","         372       0.00      0.00      0.00         4\n","\n","    accuracy                           0.84     13567\n","   macro avg       0.45      0.47      0.43     13567\n","weighted avg       0.80      0.84      0.81     13567\n","\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["#Select only the optimal number of input features for X_test\n","X_test = X_test[:,:(best_model_index+1)]\n","\n","# Evaluate the best model on the test data\n","y_pred = best_model.predict(X_test)\n","\n","# np.argmax() is used to convert the one-hot encoded predictions and test labels to class labels.\n","y_pred_label = np.argmax(y_pred, axis = 1)\n","\n","# Print the classification report\n","print(\"Classification report:\\n\", classification_report(y_test_enc, y_pred_label))"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1682274341951,"user":{"displayName":"Good Notes UM","userId":"12493674840996884929"},"user_tz":-480},"id":"zbyINa_nQsY4","outputId":"32acb779-1c44-4b3e-e550-95feded30db2"},"outputs":[{"name":"stdout","output_type":"stream","text":["           OsID  True Class  Predicted Class  True/False\n","0  Os04g0475500         328              294       False\n","1  Os04g0659100         161              161        True\n","2  Os01g0686800          17               17        True\n","3  Os04g0107900          34               34        True\n","4  Os04g0517100           7                7        True\n"]}],"source":["# extract class labels from test data\n","class_test = y_test_enc\n","\n","# Invert OsID_labels dictionary\n","inv_OsID_labels = {v: k for k, v in OsID_labels.items()}\n","\n","# map OsID values to the class labels\n","OsID_test = [inv_OsID_labels.get(value, 'Unknown') for value in class_test]\n","\n","# create dataframe with OsID, true class, predicted class, and true/false columns\n","results = pd.DataFrame({\n","    'OsID': OsID_test,\n","    'True Class': y_test_enc,\n","    'Predicted Class': y_pred_label,\n","    'True/False': class_test == y_pred_label\n","})\n","\n","# display dataframe\n","print(results.head())\n","\n","# save results_df to a CSV file\n","results.to_csv('MLP_gene classification.csv', index=False)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":676},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1682274341952,"user":{"displayName":"Good Notes UM","userId":"12493674840996884929"},"user_tz":-480},"id":"l2lvuFxIN3QQ","outputId":"33989bc3-f928-4279-954e-b86af3fc24da"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>No of input features</th>\n","      <th>Model accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0.045</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>0.060</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>0.087</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0.480</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>0.631</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>6</td>\n","      <td>0.782</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>7</td>\n","      <td>0.807</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>8</td>\n","      <td>0.843</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>9</td>\n","      <td>0.817</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>10</td>\n","      <td>0.844</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>11</td>\n","      <td>0.814</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>12</td>\n","      <td>0.797</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>13</td>\n","      <td>0.808</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>14</td>\n","      <td>0.800</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>15</td>\n","      <td>0.825</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>16</td>\n","      <td>0.785</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>17</td>\n","      <td>0.785</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>18</td>\n","      <td>0.754</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>19</td>\n","      <td>0.806</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>20</td>\n","      <td>0.809</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    No of input features  Model accuracy\n","0                      1           0.045\n","1                      2           0.060\n","2                      3           0.087\n","3                      4           0.480\n","4                      5           0.631\n","5                      6           0.782\n","6                      7           0.807\n","7                      8           0.843\n","8                      9           0.817\n","9                     10           0.844\n","10                    11           0.814\n","11                    12           0.797\n","12                    13           0.808\n","13                    14           0.800\n","14                    15           0.825\n","15                    16           0.785\n","16                    17           0.785\n","17                    18           0.754\n","18                    19           0.806\n","19                    20           0.809"]},"metadata":{},"output_type":"display_data"}],"source":["display(models_df)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"},"vscode":{"interpreter":{"hash":"54645332476aec3a1589d49135d9c8280fdb5d7db877f5b7af7a1b58b8f996bc"}}},"nbformat":4,"nbformat_minor":0}

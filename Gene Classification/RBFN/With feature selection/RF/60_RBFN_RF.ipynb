{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"G3AagWU9QsYg","executionInfo":{"status":"ok","timestamp":1682315411090,"user_tz":-480,"elapsed":5893,"user":{"displayName":"chong yoongsim","userId":"18145963220657887961"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd \n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","\n","from keras.models import Sequential\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","#for RBN\n","from keras.layers import Layer, Flatten, Dense\n","from keras import backend as K\n","from sklearn.metrics import classification_report\n","\n","\n","from sklearn.model_selection import StratifiedKFold\n","from keras import regularizers\n","from tensorflow.keras.utils import to_categorical\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"TAShwIegQsYj","executionInfo":{"status":"ok","timestamp":1682315411093,"user_tz":-480,"elapsed":47,"user":{"displayName":"chong yoongsim","userId":"18145963220657887961"}}},"outputs":[],"source":["#fixed random seed for reproducibility \n","np.random.seed(0)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"GgUSY9c4QsYl","executionInfo":{"status":"ok","timestamp":1682315645568,"user_tz":-480,"elapsed":234516,"user":{"displayName":"chong yoongsim","userId":"18145963220657887961"}},"outputId":"9be2d19f-2435-4dee-aa44-e4cc99727012"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-868b90d5-4149-430f-8670-1010c14ded56\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-868b90d5-4149-430f-8670-1010c14ded56\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving TrainingData.csv to TrainingData.csv\n","Summary of dataGene:\n","        log_2FoldChange            ET  CoExpression           PCC  \\\n","count     41110.000000  41110.000000  41110.000000  41110.000000   \n","mean         -0.037332      1.407395      0.991997     -0.361737   \n","std           0.391444      0.784327      0.089101      0.463979   \n","min          -1.000000      0.000000      0.000000     -1.000000   \n","25%          -0.251534      1.000000      1.000000     -0.747963   \n","50%           0.030675      2.000000      1.000000     -0.449089   \n","75%           0.251534      2.000000      1.000000     -0.051646   \n","max           1.000000      2.000000      1.000000      1.000000   \n","\n","                PPI  Root10DaysSeedling  Root14DaysSeedling  \\\n","count  41110.000000        41110.000000        41110.000000   \n","mean       0.914668           -0.522040           -0.646982   \n","std        0.279379            0.498568            0.393549   \n","min        0.000000           -1.000000           -1.000000   \n","25%        1.000000           -0.901371           -0.965084   \n","50%        1.000000           -0.663664           -0.680003   \n","75%        1.000000           -0.378497           -0.559627   \n","max        1.000000            1.000000            1.000000   \n","\n","       Root17DaysSeedling  Root21DaysSeedling  Root24DaysSeedling  ...  \\\n","count        41110.000000        41110.000000        41110.000000  ...   \n","mean            -0.700869           -0.669349           -0.670048  ...   \n","std              0.378219            0.405860            0.390751  ...   \n","min             -1.000000           -1.000000           -1.000000  ...   \n","25%             -0.980226           -1.000000           -0.982003  ...   \n","50%             -0.795609           -0.726665           -0.708584  ...   \n","75%             -0.601266           -0.543621           -0.482133  ...   \n","max              1.000000            1.000000            1.000000  ...   \n","\n","       Root52DaysSeedling  Shoot3DaysSeedling  Shoot10DaysSeedling  \\\n","count        41110.000000        41110.000000         41110.000000   \n","mean            -0.670345           -0.590806            -0.545055   \n","std              0.478222            0.443552             0.477438   \n","min             -1.000000           -1.000000            -1.000000   \n","25%             -1.000000           -1.000000            -0.906055   \n","50%             -0.853382           -0.676286            -0.698864   \n","75%             -0.542371           -0.409775            -0.250588   \n","max              1.000000            0.955179             1.000000   \n","\n","       Shoot14DaysSeedling  Shoot17DaysSeedling  Shoot21DaysSeedling  \\\n","count         41110.000000         41110.000000         41110.000000   \n","mean             -0.734141            -0.680810            -0.659443   \n","std               0.413716             0.478189             0.463838   \n","min              -1.000000            -1.000000            -1.000000   \n","25%              -1.000000            -1.000000            -1.000000   \n","50%              -0.924976            -0.954040            -0.874080   \n","75%              -0.513759            -0.420386            -0.440577   \n","max               0.997390             1.000000             1.000000   \n","\n","       Shoot35DaysSeedling  Leaf21DaysSeedling  Leaf45DaysOldPlant  \\\n","count         41110.000000        41110.000000        41110.000000   \n","mean             -0.558906           -0.828778           -0.585144   \n","std               0.506423            0.327542            0.399046   \n","min              -1.000000           -1.000000           -1.000000   \n","25%              -0.962199           -1.000000           -0.901444   \n","50%              -0.699035           -0.951894           -0.643376   \n","75%              -0.352995           -0.883755           -0.451900   \n","max               0.993958            1.000000            1.000000   \n","\n","              class  \n","count  41110.000000  \n","mean      60.092703  \n","std       77.624892  \n","min        1.000000  \n","25%        9.000000  \n","50%       26.000000  \n","75%       78.000000  \n","max      373.000000  \n","\n","[8 rows x 21 columns]\n"]}],"source":["from google.colab import files\n","uploaded = files.upload()\n","\n","# load dataset (input variables = X, output variables = Y)\n","df = pd.read_csv(\"TrainingData.csv\")\n","\n","#count the number of occurances for each osID\n","OsID_counts = df['OsID'].value_counts()\n","\n","#filter for osIDs that have 10 or more occurances\n","OsID_counts_filtered = OsID_counts[OsID_counts >= 10]\n","\n","#assign a label for each osID \n","OsID_labels = {}\n","class_no = 1\n","for osID in OsID_counts_filtered.index:\n","    OsID_labels[osID] = class_no\n","    class_no +=1\n","\n","#filter the dataset with osID that contain 10 or more occurances\n","dataGene = df[df['OsID'].isin(OsID_counts_filtered.index)]\n","\n","dataGene = dataGene.drop(['Class', 'Trait'],axis=1)\n","\n","# Add a new column 'class' to the filtered dataset\n","dataGene['class'] = dataGene['OsID'].map(OsID_labels)\n","\n","print(\"Summary of dataGene:\\n\",dataGene.describe())\n"]},{"cell_type":"code","source":["X = dataGene.drop(['class','OsID'], axis=1) # exclude class & OsID column\n","Y = dataGene['class']\n","\n","#input feature names in order of descending importance scores in RF feature selection method\n","feature_names = ['Root10DaysSeedling', 'Leaf45DaysOldPlant', 'Shoot10DaysSeedling', 'Shoot35DaysSeedling', 'Root35DaysSeedling', \n","                 'Leaf21DaysSeedling', 'Root14DaysSeedling', 'Shoot3DaysSeedling', 'Root24DaysSeedling', 'Root52DaysSeedling', \n","                 'Root17DaysSeedling', 'Root21DaysSeedling', 'Shoot14DaysSeedling', 'Shoot21DaysSeedling', 'Shoot17DaysSeedling',\n","                  'ET', 'PCC', 'log_2FoldChange', 'PPI', 'CoExpression' ]\n","\n","X_fs = X.reindex(columns=feature_names)\n","\n","print(\"Shape of X:\\n\",X_fs.shape)\n","print(\"Shape of Y:\\n\",Y.shape)\n","\n","# Statistical summary of the variables\n","print(\"Summary of X:\\n\",X_fs.describe())\n","print(\"Summary of Y:\\n\",Y.describe())\n","\n","# Check for class imbalance\n","print(df.groupby(Y).size())\n","\n","# change both input and target variables datatype to ndarray\n","X_fs = X_fs.values # 2-D array\n","\n","# select target variable \n","\n","Y = Y.values #1-D array\n","Y = Y.reshape(-1, 1)"],"metadata":{"id":"ZQjM56LwvG4i","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682315645576,"user_tz":-480,"elapsed":46,"user":{"displayName":"chong yoongsim","userId":"18145963220657887961"}},"outputId":"23c3b6e9-7a7f-4956-bd3b-1209c4d3e478"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of X:\n"," (41110, 20)\n","Shape of Y:\n"," (41110,)\n","Summary of X:\n","        Root10DaysSeedling  Leaf45DaysOldPlant  Shoot10DaysSeedling  \\\n","count        41110.000000        41110.000000         41110.000000   \n","mean            -0.522040           -0.585144            -0.545055   \n","std              0.498568            0.399046             0.477438   \n","min             -1.000000           -1.000000            -1.000000   \n","25%             -0.901371           -0.901444            -0.906055   \n","50%             -0.663664           -0.643376            -0.698864   \n","75%             -0.378497           -0.451900            -0.250588   \n","max              1.000000            1.000000             1.000000   \n","\n","       Shoot35DaysSeedling  Root35DaysSeedling  Leaf21DaysSeedling  \\\n","count         41110.000000        41110.000000        41110.000000   \n","mean             -0.558906           -0.596196           -0.828778   \n","std               0.506423            0.461679            0.327542   \n","min              -1.000000           -1.000000           -1.000000   \n","25%              -0.962199           -0.937286           -1.000000   \n","50%              -0.699035           -0.769184           -0.951894   \n","75%              -0.352995           -0.323664           -0.883755   \n","max               0.993958            1.000000            1.000000   \n","\n","       Root14DaysSeedling  Shoot3DaysSeedling  Root24DaysSeedling  \\\n","count        41110.000000        41110.000000        41110.000000   \n","mean            -0.646982           -0.590806           -0.670048   \n","std              0.393549            0.443552            0.390751   \n","min             -1.000000           -1.000000           -1.000000   \n","25%             -0.965084           -1.000000           -0.982003   \n","50%             -0.680003           -0.676286           -0.708584   \n","75%             -0.559627           -0.409775           -0.482133   \n","max              1.000000            0.955179            1.000000   \n","\n","       Root52DaysSeedling  Root17DaysSeedling  Root21DaysSeedling  \\\n","count        41110.000000        41110.000000        41110.000000   \n","mean            -0.670345           -0.700869           -0.669349   \n","std              0.478222            0.378219            0.405860   \n","min             -1.000000           -1.000000           -1.000000   \n","25%             -1.000000           -0.980226           -1.000000   \n","50%             -0.853382           -0.795609           -0.726665   \n","75%             -0.542371           -0.601266           -0.543621   \n","max              1.000000            1.000000            1.000000   \n","\n","       Shoot14DaysSeedling  Shoot21DaysSeedling  Shoot17DaysSeedling  \\\n","count         41110.000000         41110.000000         41110.000000   \n","mean             -0.734141            -0.659443            -0.680810   \n","std               0.413716             0.463838             0.478189   \n","min              -1.000000            -1.000000            -1.000000   \n","25%              -1.000000            -1.000000            -1.000000   \n","50%              -0.924976            -0.874080            -0.954040   \n","75%              -0.513759            -0.440577            -0.420386   \n","max               0.997390             1.000000             1.000000   \n","\n","                 ET           PCC  log_2FoldChange           PPI  CoExpression  \n","count  41110.000000  41110.000000     41110.000000  41110.000000  41110.000000  \n","mean       1.407395     -0.361737        -0.037332      0.914668      0.991997  \n","std        0.784327      0.463979         0.391444      0.279379      0.089101  \n","min        0.000000     -1.000000        -1.000000      0.000000      0.000000  \n","25%        1.000000     -0.747963        -0.251534      1.000000      1.000000  \n","50%        2.000000     -0.449089         0.030675      1.000000      1.000000  \n","75%        2.000000     -0.051646         0.251534      1.000000      1.000000  \n","max        2.000000      1.000000         1.000000      1.000000      1.000000  \n","Summary of Y:\n"," count    41110.000000\n","mean        60.092703\n","std         77.624892\n","min          1.000000\n","25%          9.000000\n","50%         26.000000\n","75%         78.000000\n","max        373.000000\n","Name: class, dtype: float64\n","class\n","1.0      1800\n","2.0      1296\n","3.0      1260\n","4.0      1218\n","5.0      1026\n","         ... \n","369.0      10\n","370.0      10\n","371.0      10\n","372.0      10\n","373.0      10\n","Length: 373, dtype: int64\n"]}]},{"cell_type":"code","source":["y_labelenc = LabelEncoder().fit_transform(Y)\n","\n","from collections import Counter\n","# summarize distribution\n","counter = Counter(y_labelenc)\n","\n","# sort counter by keys\n","counter = dict(sorted(counter.items()))\n","\n","\n","for k,v in counter.items():\n"," per = v / len(y_labelenc) * 100\n"," print('Class=%d, n=%d (%.3f%%)' % (k, v, per))\n","# plot the distribution\n","plt.bar( counter.keys(), counter.values())\n","\n","plt.ylabel('No of gene samples')\n","plt.xlabel('Class Index of OsID')\n","plt.title('Distribution of Target Classes (OsID)')\n","plt.show()"],"metadata":{"id":"F2UQyOKPvMXF","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1682315646420,"user_tz":-480,"elapsed":876,"user":{"displayName":"chong yoongsim","userId":"18145963220657887961"}},"outputId":"5d0486db-995c-4d3d-867c-be1239cc2891"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Class=0, n=1800 (4.378%)\n","Class=1, n=1296 (3.153%)\n","Class=2, n=1260 (3.065%)\n","Class=3, n=1218 (2.963%)\n","Class=4, n=1026 (2.496%)\n","Class=5, n=1008 (2.452%)\n","Class=6, n=930 (2.262%)\n","Class=7, n=912 (2.218%)\n","Class=8, n=880 (2.141%)\n","Class=9, n=798 (1.941%)\n","Class=10, n=792 (1.927%)\n","Class=11, n=759 (1.846%)\n","Class=12, n=729 (1.773%)\n","Class=13, n=720 (1.751%)\n","Class=14, n=702 (1.708%)\n","Class=15, n=693 (1.686%)\n","Class=16, n=672 (1.635%)\n","Class=17, n=640 (1.557%)\n","Class=18, n=625 (1.520%)\n","Class=19, n=570 (1.387%)\n","Class=20, n=546 (1.328%)\n","Class=21, n=506 (1.231%)\n","Class=22, n=483 (1.175%)\n","Class=23, n=448 (1.090%)\n","Class=24, n=432 (1.051%)\n","Class=25, n=384 (0.934%)\n","Class=26, n=360 (0.876%)\n","Class=27, n=360 (0.876%)\n","Class=28, n=320 (0.778%)\n","Class=29, n=312 (0.759%)\n","Class=30, n=312 (0.759%)\n","Class=31, n=306 (0.744%)\n","Class=32, n=304 (0.739%)\n","Class=33, n=299 (0.727%)\n","Class=34, n=297 (0.722%)\n","Class=35, n=296 (0.720%)\n","Class=36, n=280 (0.681%)\n","Class=37, n=264 (0.642%)\n","Class=38, n=260 (0.632%)\n","Class=39, n=253 (0.615%)\n","Class=40, n=252 (0.613%)\n","Class=41, n=248 (0.603%)\n","Class=42, n=242 (0.589%)\n","Class=43, n=228 (0.555%)\n","Class=44, n=216 (0.525%)\n","Class=45, n=210 (0.511%)\n","Class=46, n=200 (0.486%)\n","Class=47, n=192 (0.467%)\n","Class=48, n=180 (0.438%)\n","Class=49, n=171 (0.416%)\n","Class=50, n=168 (0.409%)\n","Class=51, n=168 (0.409%)\n","Class=52, n=162 (0.394%)\n","Class=53, n=150 (0.365%)\n","Class=54, n=148 (0.360%)\n","Class=55, n=138 (0.336%)\n","Class=56, n=135 (0.328%)\n","Class=57, n=135 (0.328%)\n","Class=58, n=133 (0.324%)\n","Class=59, n=132 (0.321%)\n","Class=60, n=132 (0.321%)\n","Class=61, n=130 (0.316%)\n","Class=62, n=130 (0.316%)\n","Class=63, n=130 (0.316%)\n","Class=64, n=128 (0.311%)\n","Class=65, n=128 (0.311%)\n","Class=66, n=126 (0.306%)\n","Class=67, n=124 (0.302%)\n","Class=68, n=124 (0.302%)\n","Class=69, n=124 (0.302%)\n","Class=70, n=120 (0.292%)\n","Class=71, n=120 (0.292%)\n","Class=72, n=118 (0.287%)\n","Class=73, n=116 (0.282%)\n","Class=74, n=114 (0.277%)\n","Class=75, n=105 (0.255%)\n","Class=76, n=104 (0.253%)\n","Class=77, n=102 (0.248%)\n","Class=78, n=99 (0.241%)\n","Class=79, n=98 (0.238%)\n","Class=80, n=98 (0.238%)\n","Class=81, n=98 (0.238%)\n","Class=82, n=98 (0.238%)\n","Class=83, n=96 (0.234%)\n","Class=84, n=96 (0.234%)\n","Class=85, n=96 (0.234%)\n","Class=86, n=93 (0.226%)\n","Class=87, n=92 (0.224%)\n","Class=88, n=92 (0.224%)\n","Class=89, n=91 (0.221%)\n","Class=90, n=88 (0.214%)\n","Class=91, n=88 (0.214%)\n","Class=92, n=86 (0.209%)\n","Class=93, n=86 (0.209%)\n","Class=94, n=84 (0.204%)\n","Class=95, n=84 (0.204%)\n","Class=96, n=84 (0.204%)\n","Class=97, n=78 (0.190%)\n","Class=98, n=78 (0.190%)\n","Class=99, n=76 (0.185%)\n","Class=100, n=75 (0.182%)\n","Class=101, n=75 (0.182%)\n","Class=102, n=73 (0.178%)\n","Class=103, n=72 (0.175%)\n","Class=104, n=72 (0.175%)\n","Class=105, n=70 (0.170%)\n","Class=106, n=69 (0.168%)\n","Class=107, n=68 (0.165%)\n","Class=108, n=67 (0.163%)\n","Class=109, n=66 (0.161%)\n","Class=110, n=66 (0.161%)\n","Class=111, n=66 (0.161%)\n","Class=112, n=66 (0.161%)\n","Class=113, n=66 (0.161%)\n","Class=114, n=65 (0.158%)\n","Class=115, n=64 (0.156%)\n","Class=116, n=63 (0.153%)\n","Class=117, n=63 (0.153%)\n","Class=118, n=62 (0.151%)\n","Class=119, n=61 (0.148%)\n","Class=120, n=60 (0.146%)\n","Class=121, n=60 (0.146%)\n","Class=122, n=60 (0.146%)\n","Class=123, n=60 (0.146%)\n","Class=124, n=60 (0.146%)\n","Class=125, n=60 (0.146%)\n","Class=126, n=60 (0.146%)\n","Class=127, n=60 (0.146%)\n","Class=128, n=60 (0.146%)\n","Class=129, n=60 (0.146%)\n","Class=130, n=59 (0.144%)\n","Class=131, n=59 (0.144%)\n","Class=132, n=58 (0.141%)\n","Class=133, n=56 (0.136%)\n","Class=134, n=56 (0.136%)\n","Class=135, n=56 (0.136%)\n","Class=136, n=56 (0.136%)\n","Class=137, n=56 (0.136%)\n","Class=138, n=56 (0.136%)\n","Class=139, n=56 (0.136%)\n","Class=140, n=56 (0.136%)\n","Class=141, n=56 (0.136%)\n","Class=142, n=55 (0.134%)\n","Class=143, n=55 (0.134%)\n","Class=144, n=54 (0.131%)\n","Class=145, n=54 (0.131%)\n","Class=146, n=54 (0.131%)\n","Class=147, n=54 (0.131%)\n","Class=148, n=54 (0.131%)\n","Class=149, n=53 (0.129%)\n","Class=150, n=52 (0.126%)\n","Class=151, n=52 (0.126%)\n","Class=152, n=52 (0.126%)\n","Class=153, n=52 (0.126%)\n","Class=154, n=50 (0.122%)\n","Class=155, n=50 (0.122%)\n","Class=156, n=49 (0.119%)\n","Class=157, n=49 (0.119%)\n","Class=158, n=48 (0.117%)\n","Class=159, n=48 (0.117%)\n","Class=160, n=48 (0.117%)\n","Class=161, n=46 (0.112%)\n","Class=162, n=45 (0.109%)\n","Class=163, n=44 (0.107%)\n","Class=164, n=44 (0.107%)\n","Class=165, n=44 (0.107%)\n","Class=166, n=42 (0.102%)\n","Class=167, n=42 (0.102%)\n","Class=168, n=42 (0.102%)\n","Class=169, n=42 (0.102%)\n","Class=170, n=42 (0.102%)\n","Class=171, n=42 (0.102%)\n","Class=172, n=42 (0.102%)\n","Class=173, n=41 (0.100%)\n","Class=174, n=41 (0.100%)\n","Class=175, n=40 (0.097%)\n","Class=176, n=40 (0.097%)\n","Class=177, n=39 (0.095%)\n","Class=178, n=39 (0.095%)\n","Class=179, n=38 (0.092%)\n","Class=180, n=37 (0.090%)\n","Class=181, n=36 (0.088%)\n","Class=182, n=35 (0.085%)\n","Class=183, n=35 (0.085%)\n","Class=184, n=35 (0.085%)\n","Class=185, n=35 (0.085%)\n","Class=186, n=34 (0.083%)\n","Class=187, n=34 (0.083%)\n","Class=188, n=34 (0.083%)\n","Class=189, n=34 (0.083%)\n","Class=190, n=32 (0.078%)\n","Class=191, n=32 (0.078%)\n","Class=192, n=32 (0.078%)\n","Class=193, n=32 (0.078%)\n","Class=194, n=32 (0.078%)\n","Class=195, n=32 (0.078%)\n","Class=196, n=31 (0.075%)\n","Class=197, n=31 (0.075%)\n","Class=198, n=31 (0.075%)\n","Class=199, n=31 (0.075%)\n","Class=200, n=30 (0.073%)\n","Class=201, n=30 (0.073%)\n","Class=202, n=30 (0.073%)\n","Class=203, n=30 (0.073%)\n","Class=204, n=30 (0.073%)\n","Class=205, n=30 (0.073%)\n","Class=206, n=30 (0.073%)\n","Class=207, n=30 (0.073%)\n","Class=208, n=30 (0.073%)\n","Class=209, n=29 (0.071%)\n","Class=210, n=29 (0.071%)\n","Class=211, n=28 (0.068%)\n","Class=212, n=28 (0.068%)\n","Class=213, n=28 (0.068%)\n","Class=214, n=28 (0.068%)\n","Class=215, n=28 (0.068%)\n","Class=216, n=28 (0.068%)\n","Class=217, n=27 (0.066%)\n","Class=218, n=27 (0.066%)\n","Class=219, n=27 (0.066%)\n","Class=220, n=27 (0.066%)\n","Class=221, n=27 (0.066%)\n","Class=222, n=27 (0.066%)\n","Class=223, n=26 (0.063%)\n","Class=224, n=26 (0.063%)\n","Class=225, n=26 (0.063%)\n","Class=226, n=26 (0.063%)\n","Class=227, n=26 (0.063%)\n","Class=228, n=25 (0.061%)\n","Class=229, n=25 (0.061%)\n","Class=230, n=25 (0.061%)\n","Class=231, n=25 (0.061%)\n","Class=232, n=24 (0.058%)\n","Class=233, n=24 (0.058%)\n","Class=234, n=24 (0.058%)\n","Class=235, n=24 (0.058%)\n","Class=236, n=24 (0.058%)\n","Class=237, n=24 (0.058%)\n","Class=238, n=24 (0.058%)\n","Class=239, n=24 (0.058%)\n","Class=240, n=24 (0.058%)\n","Class=241, n=24 (0.058%)\n","Class=242, n=24 (0.058%)\n","Class=243, n=24 (0.058%)\n","Class=244, n=23 (0.056%)\n","Class=245, n=23 (0.056%)\n","Class=246, n=22 (0.054%)\n","Class=247, n=22 (0.054%)\n","Class=248, n=22 (0.054%)\n","Class=249, n=22 (0.054%)\n","Class=250, n=22 (0.054%)\n","Class=251, n=22 (0.054%)\n","Class=252, n=22 (0.054%)\n","Class=253, n=22 (0.054%)\n","Class=254, n=22 (0.054%)\n","Class=255, n=22 (0.054%)\n","Class=256, n=22 (0.054%)\n","Class=257, n=22 (0.054%)\n","Class=258, n=22 (0.054%)\n","Class=259, n=22 (0.054%)\n","Class=260, n=22 (0.054%)\n","Class=261, n=22 (0.054%)\n","Class=262, n=22 (0.054%)\n","Class=263, n=22 (0.054%)\n","Class=264, n=21 (0.051%)\n","Class=265, n=21 (0.051%)\n","Class=266, n=21 (0.051%)\n","Class=267, n=21 (0.051%)\n","Class=268, n=21 (0.051%)\n","Class=269, n=20 (0.049%)\n","Class=270, n=20 (0.049%)\n","Class=271, n=20 (0.049%)\n","Class=272, n=20 (0.049%)\n","Class=273, n=20 (0.049%)\n","Class=274, n=20 (0.049%)\n","Class=275, n=20 (0.049%)\n","Class=276, n=20 (0.049%)\n","Class=277, n=20 (0.049%)\n","Class=278, n=20 (0.049%)\n","Class=279, n=20 (0.049%)\n","Class=280, n=19 (0.046%)\n","Class=281, n=19 (0.046%)\n","Class=282, n=19 (0.046%)\n","Class=283, n=18 (0.044%)\n","Class=284, n=18 (0.044%)\n","Class=285, n=18 (0.044%)\n","Class=286, n=18 (0.044%)\n","Class=287, n=18 (0.044%)\n","Class=288, n=18 (0.044%)\n","Class=289, n=18 (0.044%)\n","Class=290, n=18 (0.044%)\n","Class=291, n=18 (0.044%)\n","Class=292, n=17 (0.041%)\n","Class=293, n=17 (0.041%)\n","Class=294, n=17 (0.041%)\n","Class=295, n=17 (0.041%)\n","Class=296, n=17 (0.041%)\n","Class=297, n=17 (0.041%)\n","Class=298, n=16 (0.039%)\n","Class=299, n=16 (0.039%)\n","Class=300, n=16 (0.039%)\n","Class=301, n=16 (0.039%)\n","Class=302, n=16 (0.039%)\n","Class=303, n=16 (0.039%)\n","Class=304, n=16 (0.039%)\n","Class=305, n=16 (0.039%)\n","Class=306, n=15 (0.036%)\n","Class=307, n=15 (0.036%)\n","Class=308, n=15 (0.036%)\n","Class=309, n=15 (0.036%)\n","Class=310, n=15 (0.036%)\n","Class=311, n=14 (0.034%)\n","Class=312, n=14 (0.034%)\n","Class=313, n=14 (0.034%)\n","Class=314, n=14 (0.034%)\n","Class=315, n=14 (0.034%)\n","Class=316, n=14 (0.034%)\n","Class=317, n=14 (0.034%)\n","Class=318, n=14 (0.034%)\n","Class=319, n=14 (0.034%)\n","Class=320, n=14 (0.034%)\n","Class=321, n=14 (0.034%)\n","Class=322, n=14 (0.034%)\n","Class=323, n=14 (0.034%)\n","Class=324, n=14 (0.034%)\n","Class=325, n=14 (0.034%)\n","Class=326, n=14 (0.034%)\n","Class=327, n=14 (0.034%)\n","Class=328, n=13 (0.032%)\n","Class=329, n=13 (0.032%)\n","Class=330, n=13 (0.032%)\n","Class=331, n=13 (0.032%)\n","Class=332, n=13 (0.032%)\n","Class=333, n=13 (0.032%)\n","Class=334, n=13 (0.032%)\n","Class=335, n=13 (0.032%)\n","Class=336, n=13 (0.032%)\n","Class=337, n=12 (0.029%)\n","Class=338, n=12 (0.029%)\n","Class=339, n=12 (0.029%)\n","Class=340, n=12 (0.029%)\n","Class=341, n=12 (0.029%)\n","Class=342, n=12 (0.029%)\n","Class=343, n=12 (0.029%)\n","Class=344, n=12 (0.029%)\n","Class=345, n=12 (0.029%)\n","Class=346, n=12 (0.029%)\n","Class=347, n=12 (0.029%)\n","Class=348, n=12 (0.029%)\n","Class=349, n=12 (0.029%)\n","Class=350, n=12 (0.029%)\n","Class=351, n=12 (0.029%)\n","Class=352, n=12 (0.029%)\n","Class=353, n=12 (0.029%)\n","Class=354, n=12 (0.029%)\n","Class=355, n=11 (0.027%)\n","Class=356, n=11 (0.027%)\n","Class=357, n=11 (0.027%)\n","Class=358, n=11 (0.027%)\n","Class=359, n=11 (0.027%)\n","Class=360, n=11 (0.027%)\n","Class=361, n=10 (0.024%)\n","Class=362, n=10 (0.024%)\n","Class=363, n=10 (0.024%)\n","Class=364, n=10 (0.024%)\n","Class=365, n=10 (0.024%)\n","Class=366, n=10 (0.024%)\n","Class=367, n=10 (0.024%)\n","Class=368, n=10 (0.024%)\n","Class=369, n=10 (0.024%)\n","Class=370, n=10 (0.024%)\n","Class=371, n=10 (0.024%)\n","Class=372, n=10 (0.024%)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["# prepare target data\n","def prepare_targets(y_train, y_test):\n","\tle = LabelEncoder()\n","\t\n","\t#fit the encoders only to the training data and then transform both train and test data\n","\ty_train_enc = le.fit_transform(y_train)\n","\ty_test_enc = le.transform(y_test)\n","\n","\treturn y_train_enc, y_test_enc"],"metadata":{"id":"SyCZMH9wvSyC","executionInfo":{"status":"ok","timestamp":1682315646425,"user_tz":-480,"elapsed":28,"user":{"displayName":"chong yoongsim","userId":"18145963220657887961"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","execution_count":7,"metadata":{"id":"wZNqq1dZQsYs","executionInfo":{"status":"ok","timestamp":1682315646781,"user_tz":-480,"elapsed":382,"user":{"displayName":"chong yoongsim","userId":"18145963220657887961"}}},"outputs":[],"source":["# define the RBFLayer layer as a custom layer\n","class RBFLayer(Layer):\n","    def __init__(self, units, gamma, **kwargs):\n","        super(RBFLayer, self).__init__(**kwargs)\n","        self.units = units\n","        self.gamma = K.cast_to_floatx(gamma)\n","\n","    def build(self, input_shape):\n","        self.mu = self.add_weight(name='mu',\n","                                  shape=(int(input_shape[1]), self.units),\n","                                  initializer='uniform',\n","                                  trainable=True)\n","        super(RBFLayer, self).build(input_shape)\n","\n","    def call(self, inputs):\n","        diff = K.expand_dims(inputs) - self.mu\n","        l2 = K.sum(K.pow(diff, 2), axis=1)\n","        res = K.exp(-1 * self.gamma * l2)\n","        return res\n","\n","    def compute_output_shape(self, input_shape):\n","        return (input_shape[0], self.units)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"0AqXG7_jQsYu","executionInfo":{"status":"ok","timestamp":1682315646782,"user_tz":-480,"elapsed":28,"user":{"displayName":"chong yoongsim","userId":"18145963220657887961"}}},"outputs":[],"source":["# prepare target data\n","def prepare_targets(y_train, y_test):\n","\tle = LabelEncoder()\n","\t\n","\t#fit the encoders only to the training data and then transform both train and test data\n","\ty_train_enc = le.fit_transform(y_train)\n","\ty_test_enc = le.transform(y_test)\n","\n","\treturn y_train_enc, y_test_enc"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"Nh4EHT34QsYv","executionInfo":{"status":"ok","timestamp":1682315646783,"user_tz":-480,"elapsed":22,"user":{"displayName":"chong yoongsim","userId":"18145963220657887961"}}},"outputs":[],"source":["# define baseline model (RBFN)\n","def RBFN_model(input_dim):\n","\n","    model = Sequential()\n","    model.add(Flatten(input_shape=(input_dim,)))\n","    #add the RBF layer\n","    model.add(RBFLayer(10, 0.5))\n","    \n","    model.add(Dense(60, input_dim=input_dim, activation='relu',bias_initializer='normal',kernel_regularizer=regularizers.l2(0.0001)))\n","    model.add(Dense(20,bias_initializer='normal',activation='relu',kernel_regularizer=regularizers.l2(0.0001)))\n","    model.add(Dense(20,bias_initializer='normal',activation='relu',kernel_regularizer=regularizers.l2(0.0001)))\n","    model.add(Dense(373,kernel_initializer='normal',activation='softmax'))\n","\n","    # Compile model\n","    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    \n","    return model"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"qGxBoDFmQsYw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682315646785,"user_tz":-480,"elapsed":22,"user":{"displayName":"chong yoongsim","userId":"18145963220657887961"}},"outputId":"ac9bc952-212d-4074-befe-83e3e1eb76e1"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"]}],"source":["# split into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X_fs, Y, test_size=0.33, random_state=1)\n","\n","# prepare output data\n","y_train_enc, y_test_enc = prepare_targets(y_train, y_test)\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"21D3eb-NQsYx","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d66477b3-28f8-4052-84f6-ffd3c89777ac","executionInfo":{"status":"ok","timestamp":1682329568747,"user_tz":-480,"elapsed":13921981,"user":{"displayName":"chong yoongsim","userId":"18145963220657887961"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"]},{"output_type":"stream","name":"stdout","text":["Number of input features: 1\n","Fold: 1\n","Epoch 1/60\n","1846/1846 [==============================] - 7s 3ms/step - loss: 5.0109 - accuracy: 0.0382 - val_loss: 4.9468 - val_accuracy: 0.0436\n","Epoch 2/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 4.5126 - accuracy: 0.0959 - val_loss: 4.2355 - val_accuracy: 0.1096\n","Epoch 3/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.9320 - accuracy: 0.1353 - val_loss: 3.8736 - val_accuracy: 0.1626\n","Epoch 4/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 3.5982 - accuracy: 0.1785 - val_loss: 3.6164 - val_accuracy: 0.1743\n","Epoch 5/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 3.3409 - accuracy: 0.2234 - val_loss: 3.4487 - val_accuracy: 0.2099\n","Epoch 6/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.1369 - accuracy: 0.2484 - val_loss: 3.2595 - val_accuracy: 0.2286\n","Epoch 7/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.9883 - accuracy: 0.2677 - val_loss: 3.1300 - val_accuracy: 0.2539\n","Epoch 8/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.8654 - accuracy: 0.2954 - val_loss: 3.0838 - val_accuracy: 0.2574\n","Epoch 9/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.7677 - accuracy: 0.3140 - val_loss: 2.9709 - val_accuracy: 0.2572\n","Epoch 10/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.6997 - accuracy: 0.3170 - val_loss: 2.9086 - val_accuracy: 0.3487\n","Epoch 11/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.6296 - accuracy: 0.3322 - val_loss: 2.8300 - val_accuracy: 0.3210\n","Epoch 12/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.5819 - accuracy: 0.3371 - val_loss: 2.8535 - val_accuracy: 0.2955\n","Epoch 13/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.5388 - accuracy: 0.3540 - val_loss: 2.7624 - val_accuracy: 0.4103\n","Epoch 14/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 2.5090 - accuracy: 0.3591 - val_loss: 2.8355 - val_accuracy: 0.3193\n","Epoch 15/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.4789 - accuracy: 0.3604 - val_loss: 2.7352 - val_accuracy: 0.3437\n","Epoch 16/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 2.4535 - accuracy: 0.3721 - val_loss: 2.7803 - val_accuracy: 0.3168\n","Epoch 17/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 2.4413 - accuracy: 0.3711 - val_loss: 2.7011 - val_accuracy: 0.3272\n","Epoch 18/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.4159 - accuracy: 0.3777 - val_loss: 2.6605 - val_accuracy: 0.3732\n","Epoch 19/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 2.3900 - accuracy: 0.3822 - val_loss: 2.6900 - val_accuracy: 0.3369\n","Epoch 20/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.3673 - accuracy: 0.3860 - val_loss: 2.5960 - val_accuracy: 0.4095\n","Epoch 21/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.3567 - accuracy: 0.3938 - val_loss: 2.6051 - val_accuracy: 0.3382\n","Epoch 22/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 2.3511 - accuracy: 0.3912 - val_loss: 2.5256 - val_accuracy: 0.4341\n","Epoch 23/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.3239 - accuracy: 0.3979 - val_loss: 2.5715 - val_accuracy: 0.3602\n","Epoch 24/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.3204 - accuracy: 0.3877 - val_loss: 2.5874 - val_accuracy: 0.3639\n","Epoch 25/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.3083 - accuracy: 0.3964 - val_loss: 2.7018 - val_accuracy: 0.3384\n","Epoch 26/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.2965 - accuracy: 0.3998 - val_loss: 2.5266 - val_accuracy: 0.4015\n","Epoch 27/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.2710 - accuracy: 0.4058 - val_loss: 2.5090 - val_accuracy: 0.3727\n","Epoch 28/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.2711 - accuracy: 0.4105 - val_loss: 2.4521 - val_accuracy: 0.4354\n","Epoch 29/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 2.2524 - accuracy: 0.4015 - val_loss: 2.5189 - val_accuracy: 0.4145\n","Epoch 30/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.2640 - accuracy: 0.3983 - val_loss: 2.5407 - val_accuracy: 0.3989\n","Epoch 31/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.2392 - accuracy: 0.4062 - val_loss: 2.4782 - val_accuracy: 0.4130\n","Epoch 32/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 2.2289 - accuracy: 0.4146 - val_loss: 2.5361 - val_accuracy: 0.4218\n","Epoch 33/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.2284 - accuracy: 0.4136 - val_loss: 2.4970 - val_accuracy: 0.3824\n","Epoch 34/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.2191 - accuracy: 0.4112 - val_loss: 2.4563 - val_accuracy: 0.4530\n","Epoch 35/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.2037 - accuracy: 0.4169 - val_loss: 2.4971 - val_accuracy: 0.3881\n","Epoch 36/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.2071 - accuracy: 0.4160 - val_loss: 2.6029 - val_accuracy: 0.3523\n","Epoch 37/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.1960 - accuracy: 0.4217 - val_loss: 2.4203 - val_accuracy: 0.4117\n","Epoch 38/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.1871 - accuracy: 0.4189 - val_loss: 2.5149 - val_accuracy: 0.4090\n","Epoch 39/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.1821 - accuracy: 0.4219 - val_loss: 2.3708 - val_accuracy: 0.4255\n","Epoch 40/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.1639 - accuracy: 0.4256 - val_loss: 2.3700 - val_accuracy: 0.4253\n","Epoch 41/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 2.1581 - accuracy: 0.4267 - val_loss: 2.3938 - val_accuracy: 0.4152\n","Epoch 42/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.1475 - accuracy: 0.4274 - val_loss: 2.4569 - val_accuracy: 0.3529\n","Epoch 43/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 2.1509 - accuracy: 0.4271 - val_loss: 2.3609 - val_accuracy: 0.4194\n","Epoch 44/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.1484 - accuracy: 0.4265 - val_loss: 2.3584 - val_accuracy: 0.4013\n","Epoch 45/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.1292 - accuracy: 0.4291 - val_loss: 2.5411 - val_accuracy: 0.3492\n","Epoch 46/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.1400 - accuracy: 0.4293 - val_loss: 2.4495 - val_accuracy: 0.3798\n","Epoch 47/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.1436 - accuracy: 0.4277 - val_loss: 2.3625 - val_accuracy: 0.4108\n","Epoch 48/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.1306 - accuracy: 0.4307 - val_loss: 2.3209 - val_accuracy: 0.4260\n","Epoch 49/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.1328 - accuracy: 0.4303 - val_loss: 2.4363 - val_accuracy: 0.3835\n","Epoch 50/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.1142 - accuracy: 0.4286 - val_loss: 2.3250 - val_accuracy: 0.4706\n","Epoch 51/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 2.1048 - accuracy: 0.4358 - val_loss: 2.3756 - val_accuracy: 0.3974\n","Epoch 52/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.1108 - accuracy: 0.4367 - val_loss: 2.3468 - val_accuracy: 0.4343\n","Epoch 53/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.1013 - accuracy: 0.4352 - val_loss: 2.2965 - val_accuracy: 0.4257\n","Epoch 54/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.1021 - accuracy: 0.4352 - val_loss: 2.5792 - val_accuracy: 0.3663\n","Epoch 55/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.0872 - accuracy: 0.4380 - val_loss: 2.3945 - val_accuracy: 0.4081\n","Epoch 56/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.0922 - accuracy: 0.4341 - val_loss: 2.3163 - val_accuracy: 0.4563\n","Epoch 57/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.0705 - accuracy: 0.4416 - val_loss: 2.3052 - val_accuracy: 0.4240\n","Epoch 58/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.0646 - accuracy: 0.4428 - val_loss: 2.3183 - val_accuracy: 0.4521\n","Epoch 59/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.0793 - accuracy: 0.4404 - val_loss: 2.3707 - val_accuracy: 0.4618\n","Epoch 60/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.0505 - accuracy: 0.4475 - val_loss: 2.2858 - val_accuracy: 0.4400\n","Fold: 2\n","Epoch 1/60\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"]},{"output_type":"stream","name":"stdout","text":["1846/1846 [==============================] - 7s 3ms/step - loss: 5.0209 - accuracy: 0.0421 - val_loss: 4.9583 - val_accuracy: 0.0328\n","Epoch 2/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 4.4363 - accuracy: 0.0943 - val_loss: 4.2140 - val_accuracy: 0.1386\n","Epoch 3/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 3.9174 - accuracy: 0.1385 - val_loss: 3.9366 - val_accuracy: 0.1767\n","Epoch 4/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 3.6196 - accuracy: 0.1870 - val_loss: 3.7058 - val_accuracy: 0.2370\n","Epoch 5/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.3548 - accuracy: 0.2298 - val_loss: 3.5336 - val_accuracy: 0.1886\n","Epoch 6/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 3.1349 - accuracy: 0.2599 - val_loss: 3.3884 - val_accuracy: 0.2521\n","Epoch 7/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.9649 - accuracy: 0.2809 - val_loss: 3.2636 - val_accuracy: 0.3639\n","Epoch 8/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.8234 - accuracy: 0.3075 - val_loss: 3.1843 - val_accuracy: 0.3243\n","Epoch 9/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.7236 - accuracy: 0.3253 - val_loss: 3.1220 - val_accuracy: 0.2609\n","Epoch 10/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.6345 - accuracy: 0.3417 - val_loss: 3.0299 - val_accuracy: 0.3448\n","Epoch 11/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.5615 - accuracy: 0.3548 - val_loss: 3.0735 - val_accuracy: 0.3408\n","Epoch 12/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 2.5077 - accuracy: 0.3656 - val_loss: 2.9273 - val_accuracy: 0.3769\n","Epoch 13/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.4624 - accuracy: 0.3681 - val_loss: 2.8294 - val_accuracy: 0.3712\n","Epoch 14/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.4060 - accuracy: 0.3819 - val_loss: 2.7821 - val_accuracy: 0.3910\n","Epoch 15/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.3709 - accuracy: 0.3838 - val_loss: 2.7472 - val_accuracy: 0.3813\n","Epoch 16/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.3387 - accuracy: 0.3910 - val_loss: 2.7046 - val_accuracy: 0.3982\n","Epoch 17/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.3018 - accuracy: 0.3947 - val_loss: 2.6767 - val_accuracy: 0.4000\n","Epoch 18/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.2710 - accuracy: 0.4022 - val_loss: 2.6841 - val_accuracy: 0.3628\n","Epoch 19/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.2585 - accuracy: 0.4028 - val_loss: 2.5916 - val_accuracy: 0.4286\n","Epoch 20/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.2282 - accuracy: 0.4100 - val_loss: 2.6056 - val_accuracy: 0.4112\n","Epoch 21/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 2.2104 - accuracy: 0.4139 - val_loss: 2.5588 - val_accuracy: 0.3795\n","Epoch 22/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.1877 - accuracy: 0.4184 - val_loss: 2.5647 - val_accuracy: 0.3987\n","Epoch 23/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.1800 - accuracy: 0.4143 - val_loss: 2.4867 - val_accuracy: 0.4317\n","Epoch 24/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.1673 - accuracy: 0.4208 - val_loss: 2.4365 - val_accuracy: 0.4612\n","Epoch 25/60\n","1846/1846 [==============================] - 6s 4ms/step - loss: 2.1435 - accuracy: 0.4299 - val_loss: 2.4427 - val_accuracy: 0.4455\n","Epoch 26/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.1413 - accuracy: 0.4270 - val_loss: 2.4288 - val_accuracy: 0.4504\n","Epoch 27/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.1160 - accuracy: 0.4315 - val_loss: 2.4132 - val_accuracy: 0.4255\n","Epoch 28/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.1150 - accuracy: 0.4289 - val_loss: 2.4980 - val_accuracy: 0.4154\n","Epoch 29/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 2.0836 - accuracy: 0.4413 - val_loss: 2.4609 - val_accuracy: 0.4466\n","Epoch 30/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.0951 - accuracy: 0.4384 - val_loss: 2.4190 - val_accuracy: 0.4158\n","Epoch 31/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.0825 - accuracy: 0.4389 - val_loss: 2.4317 - val_accuracy: 0.3804\n","Epoch 32/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.0647 - accuracy: 0.4490 - val_loss: 2.4172 - val_accuracy: 0.4196\n","Epoch 33/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.0647 - accuracy: 0.4395 - val_loss: 2.4225 - val_accuracy: 0.4290\n","Epoch 34/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.0639 - accuracy: 0.4475 - val_loss: 2.3689 - val_accuracy: 0.4312\n","Epoch 35/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.0354 - accuracy: 0.4563 - val_loss: 2.3942 - val_accuracy: 0.4392\n","Epoch 36/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.0423 - accuracy: 0.4459 - val_loss: 2.3419 - val_accuracy: 0.4603\n","Epoch 37/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.0213 - accuracy: 0.4594 - val_loss: 2.4256 - val_accuracy: 0.4319\n","Epoch 38/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.0277 - accuracy: 0.4504 - val_loss: 2.3497 - val_accuracy: 0.4693\n","Epoch 39/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.0147 - accuracy: 0.4543 - val_loss: 2.3429 - val_accuracy: 0.4675\n","Epoch 40/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.0137 - accuracy: 0.4524 - val_loss: 2.3182 - val_accuracy: 0.4843\n","Epoch 41/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.0046 - accuracy: 0.4600 - val_loss: 2.3071 - val_accuracy: 0.4955\n","Epoch 42/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.0020 - accuracy: 0.4562 - val_loss: 2.3220 - val_accuracy: 0.4821\n","Epoch 43/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.9969 - accuracy: 0.4591 - val_loss: 2.3115 - val_accuracy: 0.4429\n","Epoch 44/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.9852 - accuracy: 0.4630 - val_loss: 2.2797 - val_accuracy: 0.4396\n","Epoch 45/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 1.9827 - accuracy: 0.4607 - val_loss: 2.3083 - val_accuracy: 0.4697\n","Epoch 46/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 1.9867 - accuracy: 0.4645 - val_loss: 2.3321 - val_accuracy: 0.4062\n","Epoch 47/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.9772 - accuracy: 0.4667 - val_loss: 2.3268 - val_accuracy: 0.4400\n","Epoch 48/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 1.9696 - accuracy: 0.4659 - val_loss: 2.2517 - val_accuracy: 0.4429\n","Epoch 49/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.9681 - accuracy: 0.4670 - val_loss: 2.3728 - val_accuracy: 0.4235\n","Epoch 50/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.9583 - accuracy: 0.4644 - val_loss: 2.2651 - val_accuracy: 0.4763\n","Epoch 51/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 1.9592 - accuracy: 0.4684 - val_loss: 2.2281 - val_accuracy: 0.4869\n","Epoch 52/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.9499 - accuracy: 0.4681 - val_loss: 2.2746 - val_accuracy: 0.4587\n","Epoch 53/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.9431 - accuracy: 0.4670 - val_loss: 2.2801 - val_accuracy: 0.4394\n","Epoch 54/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.9516 - accuracy: 0.4730 - val_loss: 2.2251 - val_accuracy: 0.4568\n","Epoch 55/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.9305 - accuracy: 0.4694 - val_loss: 2.2590 - val_accuracy: 0.4746\n","Epoch 56/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.9412 - accuracy: 0.4700 - val_loss: 2.2354 - val_accuracy: 0.4924\n","Epoch 57/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.9280 - accuracy: 0.4722 - val_loss: 2.2532 - val_accuracy: 0.4689\n","Epoch 58/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 1.9268 - accuracy: 0.4706 - val_loss: 2.3168 - val_accuracy: 0.4537\n","Epoch 59/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.9203 - accuracy: 0.4739 - val_loss: 2.2115 - val_accuracy: 0.4988\n","Epoch 60/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.9354 - accuracy: 0.4735 - val_loss: 2.2583 - val_accuracy: 0.4719\n","Average Validation Accuracy: 0.4666890799999237\n","Average Validation Loss: 2.0420172214508057\n","Average Test Accuracy: 0.46532027423381805\n","Final Test Accuracy for each fold: 0.4833787977695465\n","Number of input features: 2\n","Fold: 1\n","Epoch 1/60\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"]},{"output_type":"stream","name":"stdout","text":["1846/1846 [==============================] - 7s 3ms/step - loss: 4.9632 - accuracy: 0.0483 - val_loss: 4.5988 - val_accuracy: 0.0906\n","Epoch 2/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 4.0209 - accuracy: 0.1561 - val_loss: 3.6392 - val_accuracy: 0.3162\n","Epoch 3/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.0947 - accuracy: 0.3642 - val_loss: 2.8941 - val_accuracy: 0.4158\n","Epoch 4/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.4649 - accuracy: 0.4524 - val_loss: 2.4116 - val_accuracy: 0.4832\n","Epoch 5/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.0255 - accuracy: 0.5222 - val_loss: 2.0664 - val_accuracy: 0.5494\n","Epoch 6/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.7111 - accuracy: 0.5745 - val_loss: 1.8780 - val_accuracy: 0.5591\n","Epoch 7/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.5145 - accuracy: 0.6100 - val_loss: 1.6950 - val_accuracy: 0.6605\n","Epoch 8/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 1.3807 - accuracy: 0.6388 - val_loss: 1.5966 - val_accuracy: 0.6370\n","Epoch 9/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.2756 - accuracy: 0.6572 - val_loss: 1.7236 - val_accuracy: 0.6011\n","Epoch 10/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.2166 - accuracy: 0.6706 - val_loss: 1.4485 - val_accuracy: 0.6684\n","Epoch 11/60\n","1846/1846 [==============================] - 6s 4ms/step - loss: 1.1635 - accuracy: 0.6842 - val_loss: 1.3981 - val_accuracy: 0.6631\n","Epoch 12/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.1171 - accuracy: 0.6869 - val_loss: 1.4173 - val_accuracy: 0.6057\n","Epoch 13/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.0946 - accuracy: 0.6940 - val_loss: 1.3041 - val_accuracy: 0.7314\n","Epoch 14/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0527 - accuracy: 0.7067 - val_loss: 1.2807 - val_accuracy: 0.7146\n","Epoch 15/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 1.0384 - accuracy: 0.7104 - val_loss: 1.2348 - val_accuracy: 0.7135\n","Epoch 16/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9984 - accuracy: 0.7192 - val_loss: 1.2531 - val_accuracy: 0.7230\n","Epoch 17/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9884 - accuracy: 0.7194 - val_loss: 1.2175 - val_accuracy: 0.7270\n","Epoch 18/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9715 - accuracy: 0.7226 - val_loss: 1.2392 - val_accuracy: 0.7138\n","Epoch 19/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9548 - accuracy: 0.7327 - val_loss: 1.2219 - val_accuracy: 0.7195\n","Epoch 20/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.9291 - accuracy: 0.7328 - val_loss: 1.2373 - val_accuracy: 0.7030\n","Epoch 21/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9273 - accuracy: 0.7324 - val_loss: 1.2591 - val_accuracy: 0.7032\n","Epoch 22/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.8969 - accuracy: 0.7427 - val_loss: 1.1280 - val_accuracy: 0.7452\n","Epoch 23/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8879 - accuracy: 0.7455 - val_loss: 1.1041 - val_accuracy: 0.7586\n","Epoch 24/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 0.8760 - accuracy: 0.7496 - val_loss: 1.1375 - val_accuracy: 0.7573\n","Epoch 25/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.8823 - accuracy: 0.7456 - val_loss: 1.0964 - val_accuracy: 0.7397\n","Epoch 26/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8539 - accuracy: 0.7523 - val_loss: 1.0297 - val_accuracy: 0.7619\n","Epoch 27/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8482 - accuracy: 0.7530 - val_loss: 1.0836 - val_accuracy: 0.7531\n","Epoch 28/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.8463 - accuracy: 0.7524 - val_loss: 1.0528 - val_accuracy: 0.7604\n","Epoch 29/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8163 - accuracy: 0.7651 - val_loss: 1.0834 - val_accuracy: 0.7575\n","Epoch 30/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8297 - accuracy: 0.7620 - val_loss: 0.9796 - val_accuracy: 0.7817\n","Epoch 31/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8108 - accuracy: 0.7627 - val_loss: 1.1591 - val_accuracy: 0.7050\n","Epoch 32/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7970 - accuracy: 0.7697 - val_loss: 0.9557 - val_accuracy: 0.8147\n","Epoch 33/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8069 - accuracy: 0.7635 - val_loss: 0.9949 - val_accuracy: 0.7606\n","Epoch 34/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7847 - accuracy: 0.7710 - val_loss: 1.0470 - val_accuracy: 0.7382\n","Epoch 35/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7855 - accuracy: 0.7701 - val_loss: 0.9955 - val_accuracy: 0.7712\n","Epoch 36/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7676 - accuracy: 0.7762 - val_loss: 0.9799 - val_accuracy: 0.7901\n","Epoch 37/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7743 - accuracy: 0.7744 - val_loss: 0.9497 - val_accuracy: 0.7877\n","Epoch 38/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7563 - accuracy: 0.7808 - val_loss: 0.9385 - val_accuracy: 0.7996\n","Epoch 39/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7502 - accuracy: 0.7841 - val_loss: 0.9735 - val_accuracy: 0.7745\n","Epoch 40/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7522 - accuracy: 0.7796 - val_loss: 0.9370 - val_accuracy: 0.7745\n","Epoch 41/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7664 - accuracy: 0.7776 - val_loss: 0.8996 - val_accuracy: 0.8009\n","Epoch 42/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7378 - accuracy: 0.7814 - val_loss: 0.9318 - val_accuracy: 0.8042\n","Epoch 43/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7267 - accuracy: 0.7882 - val_loss: 0.9524 - val_accuracy: 0.7809\n","Epoch 44/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7308 - accuracy: 0.7826 - val_loss: 0.9874 - val_accuracy: 0.7749\n","Epoch 45/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7289 - accuracy: 0.7827 - val_loss: 0.9265 - val_accuracy: 0.7679\n","Epoch 46/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7145 - accuracy: 0.7878 - val_loss: 0.8895 - val_accuracy: 0.8026\n","Epoch 47/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7201 - accuracy: 0.7863 - val_loss: 0.8659 - val_accuracy: 0.7901\n","Epoch 48/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7240 - accuracy: 0.7827 - val_loss: 0.8605 - val_accuracy: 0.7842\n","Epoch 49/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6950 - accuracy: 0.7907 - val_loss: 1.0620 - val_accuracy: 0.7523\n","Epoch 50/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7081 - accuracy: 0.7923 - val_loss: 0.9001 - val_accuracy: 0.7945\n","Epoch 51/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7023 - accuracy: 0.7905 - val_loss: 0.8885 - val_accuracy: 0.7870\n","Epoch 52/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6884 - accuracy: 0.7979 - val_loss: 0.9131 - val_accuracy: 0.7806\n","Epoch 53/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6944 - accuracy: 0.7908 - val_loss: 0.8316 - val_accuracy: 0.8165\n","Epoch 54/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6791 - accuracy: 0.7976 - val_loss: 0.7940 - val_accuracy: 0.8211\n","Epoch 55/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6783 - accuracy: 0.8008 - val_loss: 0.8174 - val_accuracy: 0.8114\n","Epoch 56/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6842 - accuracy: 0.7966 - val_loss: 0.8647 - val_accuracy: 0.7723\n","Epoch 57/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.6596 - accuracy: 0.8031 - val_loss: 0.8332 - val_accuracy: 0.7960\n","Epoch 58/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6761 - accuracy: 0.8003 - val_loss: 0.8308 - val_accuracy: 0.8359\n","Epoch 59/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6733 - accuracy: 0.7990 - val_loss: 0.8071 - val_accuracy: 0.8262\n","Epoch 60/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.6586 - accuracy: 0.8039 - val_loss: 0.8636 - val_accuracy: 0.7982\n","Fold: 2\n","Epoch 1/60\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"]},{"output_type":"stream","name":"stdout","text":["1846/1846 [==============================] - 6s 3ms/step - loss: 5.0133 - accuracy: 0.0407 - val_loss: 4.9033 - val_accuracy: 0.0440\n","Epoch 2/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 4.4645 - accuracy: 0.1030 - val_loss: 4.3014 - val_accuracy: 0.1560\n","Epoch 3/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 4.0257 - accuracy: 0.1571 - val_loss: 4.0553 - val_accuracy: 0.1886\n","Epoch 4/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 3.8070 - accuracy: 0.1699 - val_loss: 3.9376 - val_accuracy: 0.1848\n","Epoch 5/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.6436 - accuracy: 0.1902 - val_loss: 3.7853 - val_accuracy: 0.2295\n","Epoch 6/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.4781 - accuracy: 0.2079 - val_loss: 3.6935 - val_accuracy: 0.2407\n","Epoch 7/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.3069 - accuracy: 0.2341 - val_loss: 3.5364 - val_accuracy: 0.2748\n","Epoch 8/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.1588 - accuracy: 0.2578 - val_loss: 3.4661 - val_accuracy: 0.3041\n","Epoch 9/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 3.0121 - accuracy: 0.2867 - val_loss: 3.3447 - val_accuracy: 0.2744\n","Epoch 10/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 2.8847 - accuracy: 0.3161 - val_loss: 3.1947 - val_accuracy: 0.3512\n","Epoch 11/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.7808 - accuracy: 0.3289 - val_loss: 3.1863 - val_accuracy: 0.3419\n","Epoch 12/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.6936 - accuracy: 0.3411 - val_loss: 3.1429 - val_accuracy: 0.2869\n","Epoch 13/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.6108 - accuracy: 0.3568 - val_loss: 3.0172 - val_accuracy: 0.3463\n","Epoch 14/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.5362 - accuracy: 0.3702 - val_loss: 2.9486 - val_accuracy: 0.3624\n","Epoch 15/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 2.4840 - accuracy: 0.3787 - val_loss: 2.9309 - val_accuracy: 0.3872\n","Epoch 16/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 2.4241 - accuracy: 0.3907 - val_loss: 2.8976 - val_accuracy: 0.4295\n","Epoch 17/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.3862 - accuracy: 0.3971 - val_loss: 2.8737 - val_accuracy: 0.4220\n","Epoch 18/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.3482 - accuracy: 0.4055 - val_loss: 2.9415 - val_accuracy: 0.3881\n","Epoch 19/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.3093 - accuracy: 0.4124 - val_loss: 2.7672 - val_accuracy: 0.4304\n","Epoch 20/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 2.2717 - accuracy: 0.4308 - val_loss: 2.7283 - val_accuracy: 0.4722\n","Epoch 21/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 2.2354 - accuracy: 0.4400 - val_loss: 2.9256 - val_accuracy: 0.4163\n","Epoch 22/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.2087 - accuracy: 0.4488 - val_loss: 2.7407 - val_accuracy: 0.3978\n","Epoch 23/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 2.1659 - accuracy: 0.4587 - val_loss: 2.6607 - val_accuracy: 0.4592\n","Epoch 24/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.1377 - accuracy: 0.4671 - val_loss: 2.6212 - val_accuracy: 0.4946\n","Epoch 25/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.1071 - accuracy: 0.4735 - val_loss: 2.6060 - val_accuracy: 0.4587\n","Epoch 26/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 2.0871 - accuracy: 0.4730 - val_loss: 2.6747 - val_accuracy: 0.4508\n","Epoch 27/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.0615 - accuracy: 0.4824 - val_loss: 2.6158 - val_accuracy: 0.4768\n","Epoch 28/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 2.0314 - accuracy: 0.4944 - val_loss: 2.5362 - val_accuracy: 0.5109\n","Epoch 29/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.0008 - accuracy: 0.4951 - val_loss: 2.6810 - val_accuracy: 0.4339\n","Epoch 30/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 1.9612 - accuracy: 0.5096 - val_loss: 2.5218 - val_accuracy: 0.5025\n","Epoch 31/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.9550 - accuracy: 0.5059 - val_loss: 2.4392 - val_accuracy: 0.5122\n","Epoch 32/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.9359 - accuracy: 0.5174 - val_loss: 2.4162 - val_accuracy: 0.5375\n","Epoch 33/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.9143 - accuracy: 0.5200 - val_loss: 2.3668 - val_accuracy: 0.5342\n","Epoch 34/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.8788 - accuracy: 0.5307 - val_loss: 2.3672 - val_accuracy: 0.5710\n","Epoch 35/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.8353 - accuracy: 0.5428 - val_loss: 2.4734 - val_accuracy: 0.5149\n","Epoch 36/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.8057 - accuracy: 0.5422 - val_loss: 2.4136 - val_accuracy: 0.5289\n","Epoch 37/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.8100 - accuracy: 0.5460 - val_loss: 2.2481 - val_accuracy: 0.5492\n","Epoch 38/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.7497 - accuracy: 0.5601 - val_loss: 2.3319 - val_accuracy: 0.5318\n","Epoch 39/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.7379 - accuracy: 0.5550 - val_loss: 2.1837 - val_accuracy: 0.5520\n","Epoch 40/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.6922 - accuracy: 0.5714 - val_loss: 2.2112 - val_accuracy: 0.5782\n","Epoch 41/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.7007 - accuracy: 0.5651 - val_loss: 2.1436 - val_accuracy: 0.5327\n","Epoch 42/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.6355 - accuracy: 0.5818 - val_loss: 2.1057 - val_accuracy: 0.5681\n","Epoch 43/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.6346 - accuracy: 0.5857 - val_loss: 2.1359 - val_accuracy: 0.5824\n","Epoch 44/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.6004 - accuracy: 0.5926 - val_loss: 2.0929 - val_accuracy: 0.5886\n","Epoch 45/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 1.5627 - accuracy: 0.6016 - val_loss: 2.0036 - val_accuracy: 0.6257\n","Epoch 46/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.5531 - accuracy: 0.6041 - val_loss: 2.0089 - val_accuracy: 0.6198\n","Epoch 47/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.4889 - accuracy: 0.6211 - val_loss: 1.9909 - val_accuracy: 0.6176\n","Epoch 48/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 1.4851 - accuracy: 0.6212 - val_loss: 2.0457 - val_accuracy: 0.6374\n","Epoch 49/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.4444 - accuracy: 0.6315 - val_loss: 1.9102 - val_accuracy: 0.6330\n","Epoch 50/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 1.4318 - accuracy: 0.6334 - val_loss: 1.9561 - val_accuracy: 0.6354\n","Epoch 51/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.4200 - accuracy: 0.6340 - val_loss: 1.9449 - val_accuracy: 0.6213\n","Epoch 52/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.3990 - accuracy: 0.6379 - val_loss: 1.9396 - val_accuracy: 0.6064\n","Epoch 53/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 1.3819 - accuracy: 0.6396 - val_loss: 1.8560 - val_accuracy: 0.6983\n","Epoch 54/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.3457 - accuracy: 0.6511 - val_loss: 1.8575 - val_accuracy: 0.6904\n","Epoch 55/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 1.3521 - accuracy: 0.6556 - val_loss: 1.8022 - val_accuracy: 0.6757\n","Epoch 56/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.3059 - accuracy: 0.6596 - val_loss: 1.7927 - val_accuracy: 0.6953\n","Epoch 57/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.3254 - accuracy: 0.6628 - val_loss: 1.8096 - val_accuracy: 0.6972\n","Epoch 58/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.2949 - accuracy: 0.6638 - val_loss: 3.1730 - val_accuracy: 0.4535\n","Epoch 59/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.2592 - accuracy: 0.6737 - val_loss: 2.2709 - val_accuracy: 0.6229\n","Epoch 60/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.2605 - accuracy: 0.6723 - val_loss: 1.7689 - val_accuracy: 0.6629\n","Average Validation Accuracy: 0.7343399524688721\n","Average Validation Loss: 1.0462139248847961\n","Average Test Accuracy: 0.7301540374755859\n","Final Test Accuracy for each fold: 0.8077688217163086\n","Number of input features: 3\n","Fold: 1\n","Epoch 1/60\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"]},{"output_type":"stream","name":"stdout","text":["1846/1846 [==============================] - 6s 3ms/step - loss: 5.0101 - accuracy: 0.0421 - val_loss: 4.7930 - val_accuracy: 0.0750\n","Epoch 2/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 4.3751 - accuracy: 0.0968 - val_loss: 4.2096 - val_accuracy: 0.1232\n","Epoch 3/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.6588 - accuracy: 0.2497 - val_loss: 3.3142 - val_accuracy: 0.3408\n","Epoch 4/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.7606 - accuracy: 0.4340 - val_loss: 2.6723 - val_accuracy: 0.4730\n","Epoch 5/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 2.2047 - accuracy: 0.4998 - val_loss: 2.3142 - val_accuracy: 0.5173\n","Epoch 6/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 1.8576 - accuracy: 0.5502 - val_loss: 2.0681 - val_accuracy: 0.5738\n","Epoch 7/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.6487 - accuracy: 0.5883 - val_loss: 1.9102 - val_accuracy: 0.5828\n","Epoch 8/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.4997 - accuracy: 0.6151 - val_loss: 1.8394 - val_accuracy: 0.6174\n","Epoch 9/60\n","1846/1846 [==============================] - 6s 4ms/step - loss: 1.4033 - accuracy: 0.6360 - val_loss: 1.7330 - val_accuracy: 0.6425\n","Epoch 10/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.3208 - accuracy: 0.6569 - val_loss: 1.6401 - val_accuracy: 0.6988\n","Epoch 11/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.2558 - accuracy: 0.6731 - val_loss: 1.5868 - val_accuracy: 0.6854\n","Epoch 12/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.2042 - accuracy: 0.6856 - val_loss: 1.5336 - val_accuracy: 0.6752\n","Epoch 13/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.1599 - accuracy: 0.6923 - val_loss: 1.4764 - val_accuracy: 0.7292\n","Epoch 14/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.1221 - accuracy: 0.7045 - val_loss: 1.4771 - val_accuracy: 0.7142\n","Epoch 15/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0823 - accuracy: 0.7131 - val_loss: 1.4177 - val_accuracy: 0.7050\n","Epoch 16/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.0495 - accuracy: 0.7185 - val_loss: 1.4912 - val_accuracy: 0.6865\n","Epoch 17/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 1.0308 - accuracy: 0.7235 - val_loss: 1.3948 - val_accuracy: 0.7085\n","Epoch 18/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0096 - accuracy: 0.7252 - val_loss: 1.3502 - val_accuracy: 0.7254\n","Epoch 19/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9911 - accuracy: 0.7238 - val_loss: 1.3305 - val_accuracy: 0.7309\n","Epoch 20/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 0.9680 - accuracy: 0.7348 - val_loss: 1.3201 - val_accuracy: 0.7364\n","Epoch 21/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9449 - accuracy: 0.7393 - val_loss: 1.3106 - val_accuracy: 0.7402\n","Epoch 22/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9297 - accuracy: 0.7439 - val_loss: 1.3053 - val_accuracy: 0.7146\n","Epoch 23/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.9197 - accuracy: 0.7438 - val_loss: 1.2615 - val_accuracy: 0.7410\n","Epoch 24/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9097 - accuracy: 0.7426 - val_loss: 1.2826 - val_accuracy: 0.7496\n","Epoch 25/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8973 - accuracy: 0.7517 - val_loss: 1.2251 - val_accuracy: 0.7809\n","Epoch 26/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8755 - accuracy: 0.7560 - val_loss: 1.1945 - val_accuracy: 0.7547\n","Epoch 27/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8683 - accuracy: 0.7571 - val_loss: 1.2109 - val_accuracy: 0.7696\n","Epoch 28/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.8670 - accuracy: 0.7524 - val_loss: 1.1857 - val_accuracy: 0.7657\n","Epoch 29/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8392 - accuracy: 0.7624 - val_loss: 1.1844 - val_accuracy: 0.7549\n","Epoch 30/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8318 - accuracy: 0.7654 - val_loss: 1.2372 - val_accuracy: 0.7657\n","Epoch 31/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.8259 - accuracy: 0.7702 - val_loss: 1.1501 - val_accuracy: 0.7925\n","Epoch 32/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8410 - accuracy: 0.7624 - val_loss: 1.1686 - val_accuracy: 0.7523\n","Epoch 33/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8038 - accuracy: 0.7731 - val_loss: 1.1261 - val_accuracy: 0.7591\n","Epoch 34/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8042 - accuracy: 0.7679 - val_loss: 1.1165 - val_accuracy: 0.7648\n","Epoch 35/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.7858 - accuracy: 0.7809 - val_loss: 1.1575 - val_accuracy: 0.7241\n","Epoch 36/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8014 - accuracy: 0.7692 - val_loss: 1.1076 - val_accuracy: 0.7767\n","Epoch 37/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7664 - accuracy: 0.7796 - val_loss: 1.1602 - val_accuracy: 0.7765\n","Epoch 38/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7728 - accuracy: 0.7765 - val_loss: 1.1832 - val_accuracy: 0.7595\n","Epoch 39/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7748 - accuracy: 0.7734 - val_loss: 1.1310 - val_accuracy: 0.7934\n","Epoch 40/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7684 - accuracy: 0.7786 - val_loss: 1.3460 - val_accuracy: 0.7010\n","Epoch 41/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7532 - accuracy: 0.7846 - val_loss: 1.1308 - val_accuracy: 0.7472\n","Epoch 42/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7369 - accuracy: 0.7826 - val_loss: 1.0861 - val_accuracy: 0.7650\n","Epoch 43/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7405 - accuracy: 0.7872 - val_loss: 1.0868 - val_accuracy: 0.7853\n","Epoch 44/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.7348 - accuracy: 0.7915 - val_loss: 1.0592 - val_accuracy: 0.7971\n","Epoch 45/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7317 - accuracy: 0.7871 - val_loss: 1.4764 - val_accuracy: 0.6847\n","Epoch 46/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7296 - accuracy: 0.7863 - val_loss: 1.0548 - val_accuracy: 0.7853\n","Epoch 47/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7249 - accuracy: 0.7914 - val_loss: 1.2043 - val_accuracy: 0.7718\n","Epoch 48/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7103 - accuracy: 0.7937 - val_loss: 1.0461 - val_accuracy: 0.7888\n","Epoch 49/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.7123 - accuracy: 0.7934 - val_loss: 1.0450 - val_accuracy: 0.7835\n","Epoch 50/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.7007 - accuracy: 0.7954 - val_loss: 1.0738 - val_accuracy: 0.7531\n","Epoch 51/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6933 - accuracy: 0.7951 - val_loss: 1.0153 - val_accuracy: 0.7925\n","Epoch 52/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6907 - accuracy: 0.8031 - val_loss: 1.0664 - val_accuracy: 0.7760\n","Epoch 53/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6817 - accuracy: 0.7994 - val_loss: 1.1710 - val_accuracy: 0.7452\n","Epoch 54/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6965 - accuracy: 0.7933 - val_loss: 0.9950 - val_accuracy: 0.7985\n","Epoch 55/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6719 - accuracy: 0.8019 - val_loss: 1.0034 - val_accuracy: 0.8110\n","Epoch 56/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6710 - accuracy: 0.7989 - val_loss: 1.0074 - val_accuracy: 0.8099\n","Epoch 57/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6608 - accuracy: 0.8038 - val_loss: 1.0319 - val_accuracy: 0.7822\n","Epoch 58/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6707 - accuracy: 0.8013 - val_loss: 1.0480 - val_accuracy: 0.7916\n","Epoch 59/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6581 - accuracy: 0.8067 - val_loss: 0.9419 - val_accuracy: 0.8262\n","Epoch 60/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6565 - accuracy: 0.8090 - val_loss: 1.0141 - val_accuracy: 0.7958\n","Fold: 2\n","Epoch 1/60\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"]},{"output_type":"stream","name":"stdout","text":["1846/1846 [==============================] - 6s 3ms/step - loss: 4.9905 - accuracy: 0.0452 - val_loss: 4.6848 - val_accuracy: 0.0717\n","Epoch 2/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 4.3068 - accuracy: 0.1195 - val_loss: 4.2107 - val_accuracy: 0.1228\n","Epoch 3/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.7166 - accuracy: 0.2324 - val_loss: 3.4585 - val_accuracy: 0.3210\n","Epoch 4/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 2.8741 - accuracy: 0.3892 - val_loss: 2.8723 - val_accuracy: 0.4304\n","Epoch 5/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.3404 - accuracy: 0.4663 - val_loss: 2.5098 - val_accuracy: 0.5105\n","Epoch 6/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 1.9789 - accuracy: 0.5270 - val_loss: 2.3837 - val_accuracy: 0.5155\n","Epoch 7/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.7428 - accuracy: 0.5786 - val_loss: 2.1681 - val_accuracy: 0.5872\n","Epoch 8/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.5813 - accuracy: 0.6086 - val_loss: 2.0734 - val_accuracy: 0.5892\n","Epoch 9/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.4732 - accuracy: 0.6276 - val_loss: 1.9193 - val_accuracy: 0.6568\n","Epoch 10/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.3894 - accuracy: 0.6439 - val_loss: 1.8790 - val_accuracy: 0.6064\n","Epoch 11/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.3089 - accuracy: 0.6610 - val_loss: 1.7627 - val_accuracy: 0.6990\n","Epoch 12/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.2671 - accuracy: 0.6698 - val_loss: 1.7159 - val_accuracy: 0.7067\n","Epoch 13/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 1.2078 - accuracy: 0.6865 - val_loss: 1.7159 - val_accuracy: 0.6282\n","Epoch 14/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 1.1902 - accuracy: 0.6835 - val_loss: 1.6356 - val_accuracy: 0.6882\n","Epoch 15/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.1370 - accuracy: 0.6988 - val_loss: 1.6142 - val_accuracy: 0.7058\n","Epoch 16/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0981 - accuracy: 0.7075 - val_loss: 1.5516 - val_accuracy: 0.6497\n","Epoch 17/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.0914 - accuracy: 0.7081 - val_loss: 1.4996 - val_accuracy: 0.7028\n","Epoch 18/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0555 - accuracy: 0.7149 - val_loss: 1.4395 - val_accuracy: 0.7386\n","Epoch 19/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0255 - accuracy: 0.7267 - val_loss: 1.4352 - val_accuracy: 0.7292\n","Epoch 20/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0156 - accuracy: 0.7236 - val_loss: 1.4158 - val_accuracy: 0.7322\n","Epoch 21/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9886 - accuracy: 0.7313 - val_loss: 1.4285 - val_accuracy: 0.7243\n","Epoch 22/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9816 - accuracy: 0.7309 - val_loss: 1.4523 - val_accuracy: 0.7010\n","Epoch 23/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9574 - accuracy: 0.7388 - val_loss: 1.4044 - val_accuracy: 0.7349\n","Epoch 24/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9438 - accuracy: 0.7397 - val_loss: 1.4247 - val_accuracy: 0.7259\n","Epoch 25/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9487 - accuracy: 0.7360 - val_loss: 1.4136 - val_accuracy: 0.7327\n","Epoch 26/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9399 - accuracy: 0.7401 - val_loss: 1.6342 - val_accuracy: 0.6502\n","Epoch 27/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9137 - accuracy: 0.7465 - val_loss: 1.4014 - val_accuracy: 0.7307\n","Epoch 28/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8939 - accuracy: 0.7546 - val_loss: 1.2959 - val_accuracy: 0.7674\n","Epoch 29/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.8948 - accuracy: 0.7505 - val_loss: 1.2639 - val_accuracy: 0.7567\n","Epoch 30/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8727 - accuracy: 0.7513 - val_loss: 1.2827 - val_accuracy: 0.7820\n","Epoch 31/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8687 - accuracy: 0.7568 - val_loss: 1.2950 - val_accuracy: 0.7582\n","Epoch 32/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8764 - accuracy: 0.7542 - val_loss: 1.2506 - val_accuracy: 0.7494\n","Epoch 33/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 0.8407 - accuracy: 0.7628 - val_loss: 1.2520 - val_accuracy: 0.7688\n","Epoch 34/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8519 - accuracy: 0.7601 - val_loss: 1.1969 - val_accuracy: 0.7930\n","Epoch 35/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 0.8328 - accuracy: 0.7635 - val_loss: 1.2290 - val_accuracy: 0.7729\n","Epoch 36/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8258 - accuracy: 0.7642 - val_loss: 1.4400 - val_accuracy: 0.7230\n","Epoch 37/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8282 - accuracy: 0.7655 - val_loss: 1.2426 - val_accuracy: 0.7527\n","Epoch 38/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8193 - accuracy: 0.7710 - val_loss: 1.2295 - val_accuracy: 0.7351\n","Epoch 39/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8083 - accuracy: 0.7714 - val_loss: 1.2593 - val_accuracy: 0.7584\n","Epoch 40/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.8066 - accuracy: 0.7732 - val_loss: 1.2700 - val_accuracy: 0.7351\n","Epoch 41/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7988 - accuracy: 0.7752 - val_loss: 1.2516 - val_accuracy: 0.7485\n","Epoch 42/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8063 - accuracy: 0.7706 - val_loss: 1.1714 - val_accuracy: 0.7855\n","Epoch 43/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 0.7846 - accuracy: 0.7737 - val_loss: 1.2036 - val_accuracy: 0.7839\n","Epoch 44/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7843 - accuracy: 0.7753 - val_loss: 1.1991 - val_accuracy: 0.7501\n","Epoch 45/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7773 - accuracy: 0.7797 - val_loss: 1.1875 - val_accuracy: 0.7751\n","Epoch 46/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7667 - accuracy: 0.7814 - val_loss: 1.1849 - val_accuracy: 0.7637\n","Epoch 47/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7595 - accuracy: 0.7822 - val_loss: 1.1734 - val_accuracy: 0.7930\n","Epoch 48/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7626 - accuracy: 0.7806 - val_loss: 1.2552 - val_accuracy: 0.7534\n","Epoch 49/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7530 - accuracy: 0.7834 - val_loss: 1.1058 - val_accuracy: 0.7824\n","Epoch 50/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7613 - accuracy: 0.7767 - val_loss: 1.1177 - val_accuracy: 0.7897\n","Epoch 51/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7432 - accuracy: 0.7860 - val_loss: 1.1362 - val_accuracy: 0.7738\n","Epoch 52/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7513 - accuracy: 0.7802 - val_loss: 1.1774 - val_accuracy: 0.7795\n","Epoch 53/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7426 - accuracy: 0.7853 - val_loss: 1.1284 - val_accuracy: 0.7622\n","Epoch 54/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7384 - accuracy: 0.7866 - val_loss: 1.0923 - val_accuracy: 0.7945\n","Epoch 55/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 0.7143 - accuracy: 0.7933 - val_loss: 1.1228 - val_accuracy: 0.7958\n","Epoch 56/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.7427 - accuracy: 0.7832 - val_loss: 1.1088 - val_accuracy: 0.7853\n","Epoch 57/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7188 - accuracy: 0.7922 - val_loss: 1.1381 - val_accuracy: 0.7892\n","Epoch 58/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7177 - accuracy: 0.7905 - val_loss: 1.1396 - val_accuracy: 0.7760\n","Epoch 59/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7166 - accuracy: 0.7927 - val_loss: 1.1518 - val_accuracy: 0.7655\n","Epoch 60/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6943 - accuracy: 0.7993 - val_loss: 1.0498 - val_accuracy: 0.7914\n","Average Validation Accuracy: 0.8023454248905182\n","Average Validation Loss: 0.7728502750396729\n","Average Test Accuracy: 0.7977814078330994\n","Final Test Accuracy for each fold: 0.800987720489502\n","Number of input features: 4\n","Fold: 1\n","Epoch 1/60\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"]},{"output_type":"stream","name":"stdout","text":["1846/1846 [==============================] - 7s 3ms/step - loss: 4.9669 - accuracy: 0.0577 - val_loss: 4.6161 - val_accuracy: 0.1199\n","Epoch 2/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 4.0036 - accuracy: 0.2051 - val_loss: 3.6728 - val_accuracy: 0.3102\n","Epoch 3/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.2431 - accuracy: 0.3283 - val_loss: 3.1705 - val_accuracy: 0.3558\n","Epoch 4/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.7997 - accuracy: 0.3958 - val_loss: 2.8413 - val_accuracy: 0.4748\n","Epoch 5/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.4562 - accuracy: 0.4831 - val_loss: 2.5724 - val_accuracy: 0.5333\n","Epoch 6/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.1370 - accuracy: 0.5556 - val_loss: 2.2987 - val_accuracy: 0.5765\n","Epoch 7/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.8594 - accuracy: 0.5933 - val_loss: 2.1680 - val_accuracy: 0.5828\n","Epoch 8/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.6723 - accuracy: 0.6183 - val_loss: 2.0074 - val_accuracy: 0.6370\n","Epoch 9/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.5235 - accuracy: 0.6439 - val_loss: 1.8829 - val_accuracy: 0.6596\n","Epoch 10/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.4108 - accuracy: 0.6623 - val_loss: 1.8780 - val_accuracy: 0.6477\n","Epoch 11/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.3223 - accuracy: 0.6788 - val_loss: 1.7257 - val_accuracy: 0.6746\n","Epoch 12/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.2490 - accuracy: 0.6931 - val_loss: 1.6630 - val_accuracy: 0.7067\n","Epoch 13/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 1.1874 - accuracy: 0.7098 - val_loss: 1.6295 - val_accuracy: 0.7052\n","Epoch 14/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.1478 - accuracy: 0.7155 - val_loss: 1.5711 - val_accuracy: 0.7230\n","Epoch 15/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0980 - accuracy: 0.7237 - val_loss: 1.6130 - val_accuracy: 0.6788\n","Epoch 16/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0588 - accuracy: 0.7326 - val_loss: 1.5775 - val_accuracy: 0.7074\n","Epoch 17/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0279 - accuracy: 0.7389 - val_loss: 1.4149 - val_accuracy: 0.7292\n","Epoch 18/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9886 - accuracy: 0.7460 - val_loss: 1.4374 - val_accuracy: 0.7296\n","Epoch 19/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9641 - accuracy: 0.7522 - val_loss: 1.4127 - val_accuracy: 0.7551\n","Epoch 20/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.9525 - accuracy: 0.7553 - val_loss: 1.3672 - val_accuracy: 0.7498\n","Epoch 21/60\n","1846/1846 [==============================] - 6s 4ms/step - loss: 0.9223 - accuracy: 0.7597 - val_loss: 1.5043 - val_accuracy: 0.7135\n","Epoch 22/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.9154 - accuracy: 0.7619 - val_loss: 1.3115 - val_accuracy: 0.7773\n","Epoch 23/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 0.8959 - accuracy: 0.7633 - val_loss: 1.3164 - val_accuracy: 0.7459\n","Epoch 24/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8719 - accuracy: 0.7684 - val_loss: 1.4380 - val_accuracy: 0.7336\n","Epoch 25/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8632 - accuracy: 0.7682 - val_loss: 1.3301 - val_accuracy: 0.7487\n","Epoch 26/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8459 - accuracy: 0.7737 - val_loss: 1.2809 - val_accuracy: 0.7800\n","Epoch 27/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8212 - accuracy: 0.7768 - val_loss: 1.3135 - val_accuracy: 0.7685\n","Epoch 28/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8111 - accuracy: 0.7827 - val_loss: 1.2672 - val_accuracy: 0.7560\n","Epoch 29/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8080 - accuracy: 0.7839 - val_loss: 1.2397 - val_accuracy: 0.7894\n","Epoch 30/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.7966 - accuracy: 0.7881 - val_loss: 1.2226 - val_accuracy: 0.7861\n","Epoch 31/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7833 - accuracy: 0.7895 - val_loss: 1.1939 - val_accuracy: 0.8007\n","Epoch 32/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.7656 - accuracy: 0.7927 - val_loss: 1.2917 - val_accuracy: 0.7622\n","Epoch 33/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7578 - accuracy: 0.7940 - val_loss: 1.1993 - val_accuracy: 0.7798\n","Epoch 34/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7554 - accuracy: 0.7953 - val_loss: 1.1605 - val_accuracy: 0.7980\n","Epoch 35/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.7490 - accuracy: 0.7955 - val_loss: 1.2240 - val_accuracy: 0.7888\n","Epoch 36/60\n","1846/1846 [==============================] - 6s 4ms/step - loss: 0.7322 - accuracy: 0.8013 - val_loss: 1.2162 - val_accuracy: 0.8002\n","Epoch 37/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7276 - accuracy: 0.8008 - val_loss: 1.2074 - val_accuracy: 0.7899\n","Epoch 38/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7206 - accuracy: 0.8078 - val_loss: 1.1552 - val_accuracy: 0.7921\n","Epoch 39/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7085 - accuracy: 0.8123 - val_loss: 1.2788 - val_accuracy: 0.7283\n","Epoch 40/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7080 - accuracy: 0.8076 - val_loss: 1.1212 - val_accuracy: 0.7996\n","Epoch 41/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7000 - accuracy: 0.8074 - val_loss: 1.2201 - val_accuracy: 0.7558\n","Epoch 42/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6871 - accuracy: 0.8119 - val_loss: 1.0852 - val_accuracy: 0.8356\n","Epoch 43/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6734 - accuracy: 0.8190 - val_loss: 1.0938 - val_accuracy: 0.8200\n","Epoch 44/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6661 - accuracy: 0.8179 - val_loss: 1.1555 - val_accuracy: 0.7901\n","Epoch 45/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6638 - accuracy: 0.8195 - val_loss: 1.1051 - val_accuracy: 0.7938\n","Epoch 46/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.6501 - accuracy: 0.8234 - val_loss: 1.1340 - val_accuracy: 0.8073\n","Epoch 47/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.6728 - accuracy: 0.8165 - val_loss: 1.0415 - val_accuracy: 0.8251\n","Epoch 48/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6497 - accuracy: 0.8265 - val_loss: 1.2689 - val_accuracy: 0.7954\n","Epoch 49/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6498 - accuracy: 0.8210 - val_loss: 1.0556 - val_accuracy: 0.8057\n","Epoch 50/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.6487 - accuracy: 0.8246 - val_loss: 1.0689 - val_accuracy: 0.8246\n","Epoch 51/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6235 - accuracy: 0.8292 - val_loss: 1.0163 - val_accuracy: 0.8502\n","Epoch 52/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6151 - accuracy: 0.8291 - val_loss: 1.0209 - val_accuracy: 0.8315\n","Epoch 53/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.6175 - accuracy: 0.8299 - val_loss: 1.1227 - val_accuracy: 0.7877\n","Epoch 54/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6135 - accuracy: 0.8313 - val_loss: 1.0136 - val_accuracy: 0.8246\n","Epoch 55/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6077 - accuracy: 0.8290 - val_loss: 1.0517 - val_accuracy: 0.8196\n","Epoch 56/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6063 - accuracy: 0.8324 - val_loss: 0.9619 - val_accuracy: 0.8581\n","Epoch 57/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5953 - accuracy: 0.8372 - val_loss: 1.0448 - val_accuracy: 0.8128\n","Epoch 58/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5966 - accuracy: 0.8371 - val_loss: 0.9899 - val_accuracy: 0.8425\n","Epoch 59/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5949 - accuracy: 0.8364 - val_loss: 1.0342 - val_accuracy: 0.8026\n","Epoch 60/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5747 - accuracy: 0.8438 - val_loss: 0.9582 - val_accuracy: 0.8405\n","Fold: 2\n","Epoch 1/60\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"]},{"output_type":"stream","name":"stdout","text":["1846/1846 [==============================] - 6s 3ms/step - loss: 4.7621 - accuracy: 0.0697 - val_loss: 4.2599 - val_accuracy: 0.1327\n","Epoch 2/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.6197 - accuracy: 0.2744 - val_loss: 3.3093 - val_accuracy: 0.4304\n","Epoch 3/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.6440 - accuracy: 0.4795 - val_loss: 2.6688 - val_accuracy: 0.5032\n","Epoch 4/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 1.9986 - accuracy: 0.5703 - val_loss: 2.2882 - val_accuracy: 0.6090\n","Epoch 5/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.6282 - accuracy: 0.6205 - val_loss: 2.1029 - val_accuracy: 0.6220\n","Epoch 6/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.3944 - accuracy: 0.6677 - val_loss: 1.9054 - val_accuracy: 0.6455\n","Epoch 7/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.2346 - accuracy: 0.6975 - val_loss: 1.7482 - val_accuracy: 0.7036\n","Epoch 8/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 1.1240 - accuracy: 0.7142 - val_loss: 1.6458 - val_accuracy: 0.7259\n","Epoch 9/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0440 - accuracy: 0.7308 - val_loss: 1.6306 - val_accuracy: 0.6928\n","Epoch 10/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9829 - accuracy: 0.7439 - val_loss: 1.4388 - val_accuracy: 0.7540\n","Epoch 11/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9479 - accuracy: 0.7481 - val_loss: 1.4033 - val_accuracy: 0.7529\n","Epoch 12/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9025 - accuracy: 0.7554 - val_loss: 1.3099 - val_accuracy: 0.7837\n","Epoch 13/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8622 - accuracy: 0.7663 - val_loss: 1.3663 - val_accuracy: 0.7184\n","Epoch 14/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8392 - accuracy: 0.7666 - val_loss: 1.3331 - val_accuracy: 0.7666\n","Epoch 15/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8160 - accuracy: 0.7748 - val_loss: 1.2517 - val_accuracy: 0.7806\n","Epoch 16/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7978 - accuracy: 0.7744 - val_loss: 1.2820 - val_accuracy: 0.7435\n","Epoch 17/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7762 - accuracy: 0.7777 - val_loss: 1.2247 - val_accuracy: 0.7868\n","Epoch 18/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7679 - accuracy: 0.7863 - val_loss: 1.1775 - val_accuracy: 0.7681\n","Epoch 19/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 0.7395 - accuracy: 0.7917 - val_loss: 1.1813 - val_accuracy: 0.7619\n","Epoch 20/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7327 - accuracy: 0.7890 - val_loss: 1.2025 - val_accuracy: 0.7804\n","Epoch 21/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7107 - accuracy: 0.7987 - val_loss: 1.1326 - val_accuracy: 0.7938\n","Epoch 22/60\n","1846/1846 [==============================] - 6s 4ms/step - loss: 0.7058 - accuracy: 0.8002 - val_loss: 1.2162 - val_accuracy: 0.7606\n","Epoch 23/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6807 - accuracy: 0.8028 - val_loss: 1.0904 - val_accuracy: 0.7767\n","Epoch 24/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6752 - accuracy: 0.8015 - val_loss: 1.1124 - val_accuracy: 0.8037\n","Epoch 25/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6655 - accuracy: 0.8067 - val_loss: 1.0872 - val_accuracy: 0.8031\n","Epoch 26/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6626 - accuracy: 0.8115 - val_loss: 1.0406 - val_accuracy: 0.7916\n","Epoch 27/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6416 - accuracy: 0.8154 - val_loss: 1.1221 - val_accuracy: 0.7820\n","Epoch 28/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6280 - accuracy: 0.8228 - val_loss: 1.0749 - val_accuracy: 0.8086\n","Epoch 29/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6238 - accuracy: 0.8167 - val_loss: 1.0706 - val_accuracy: 0.8231\n","Epoch 30/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6172 - accuracy: 0.8211 - val_loss: 1.1572 - val_accuracy: 0.7648\n","Epoch 31/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.6159 - accuracy: 0.8222 - val_loss: 1.0572 - val_accuracy: 0.8004\n","Epoch 32/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.5949 - accuracy: 0.8283 - val_loss: 1.0293 - val_accuracy: 0.8125\n","Epoch 33/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 0.5926 - accuracy: 0.8290 - val_loss: 0.9236 - val_accuracy: 0.8427\n","Epoch 34/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5733 - accuracy: 0.8357 - val_loss: 1.0307 - val_accuracy: 0.8077\n","Epoch 35/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 0.5774 - accuracy: 0.8302 - val_loss: 0.9117 - val_accuracy: 0.8596\n","Epoch 36/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5629 - accuracy: 0.8366 - val_loss: 1.0476 - val_accuracy: 0.8213\n","Epoch 37/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5484 - accuracy: 0.8383 - val_loss: 0.9086 - val_accuracy: 0.8528\n","Epoch 38/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 0.5521 - accuracy: 0.8420 - val_loss: 0.8941 - val_accuracy: 0.8341\n","Epoch 39/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5477 - accuracy: 0.8407 - val_loss: 0.9221 - val_accuracy: 0.8535\n","Epoch 40/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.5423 - accuracy: 0.8433 - val_loss: 1.0134 - val_accuracy: 0.7980\n","Epoch 41/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.5249 - accuracy: 0.8453 - val_loss: 0.8860 - val_accuracy: 0.8462\n","Epoch 42/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5224 - accuracy: 0.8491 - val_loss: 0.9032 - val_accuracy: 0.8433\n","Epoch 43/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5143 - accuracy: 0.8529 - val_loss: 0.8759 - val_accuracy: 0.8466\n","Epoch 44/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5096 - accuracy: 0.8552 - val_loss: 0.8566 - val_accuracy: 0.8640\n","Epoch 45/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5012 - accuracy: 0.8550 - val_loss: 0.8807 - val_accuracy: 0.8495\n","Epoch 46/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4854 - accuracy: 0.8601 - val_loss: 0.8763 - val_accuracy: 0.8284\n","Epoch 47/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4913 - accuracy: 0.8611 - val_loss: 0.8745 - val_accuracy: 0.8440\n","Epoch 48/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 0.4742 - accuracy: 0.8649 - val_loss: 0.9001 - val_accuracy: 0.8330\n","Epoch 49/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4706 - accuracy: 0.8707 - val_loss: 0.8695 - val_accuracy: 0.8572\n","Epoch 50/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 0.4598 - accuracy: 0.8712 - val_loss: 0.8230 - val_accuracy: 0.8581\n","Epoch 51/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4680 - accuracy: 0.8646 - val_loss: 0.7919 - val_accuracy: 0.8689\n","Epoch 52/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4458 - accuracy: 0.8722 - val_loss: 0.7645 - val_accuracy: 0.8805\n","Epoch 53/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4421 - accuracy: 0.8723 - val_loss: 0.7596 - val_accuracy: 0.8708\n","Epoch 54/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4405 - accuracy: 0.8740 - val_loss: 0.7743 - val_accuracy: 0.8783\n","Epoch 55/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.4394 - accuracy: 0.8766 - val_loss: 0.7526 - val_accuracy: 0.8675\n","Epoch 56/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4248 - accuracy: 0.8811 - val_loss: 0.9315 - val_accuracy: 0.8383\n","Epoch 57/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4359 - accuracy: 0.8773 - val_loss: 0.7549 - val_accuracy: 0.8737\n","Epoch 58/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4159 - accuracy: 0.8820 - val_loss: 0.8118 - val_accuracy: 0.8612\n","Epoch 59/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4128 - accuracy: 0.8844 - val_loss: 0.8681 - val_accuracy: 0.8174\n","Epoch 60/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4215 - accuracy: 0.8849 - val_loss: 0.6967 - val_accuracy: 0.8990\n","Average Validation Accuracy: 0.8765213191509247\n","Average Validation Loss: 0.571096882224083\n","Average Test Accuracy: 0.8742168247699738\n","Final Test Accuracy for each fold: 0.8990196585655212\n","Number of input features: 5\n","Fold: 1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 4.7035 - accuracy: 0.0747 - val_loss: 4.1522 - val_accuracy: 0.1127\n","Epoch 2/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 3.6211 - accuracy: 0.2607 - val_loss: 3.3385 - val_accuracy: 0.4231\n","Epoch 3/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.7981 - accuracy: 0.4302 - val_loss: 2.7237 - val_accuracy: 0.4614\n","Epoch 4/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.1967 - accuracy: 0.5256 - val_loss: 2.3000 - val_accuracy: 0.5600\n","Epoch 5/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.7920 - accuracy: 0.6005 - val_loss: 2.0541 - val_accuracy: 0.5921\n","Epoch 6/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.5474 - accuracy: 0.6369 - val_loss: 1.8672 - val_accuracy: 0.6101\n","Epoch 7/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.3738 - accuracy: 0.6750 - val_loss: 1.7470 - val_accuracy: 0.6768\n","Epoch 8/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.2636 - accuracy: 0.6949 - val_loss: 1.7102 - val_accuracy: 0.6713\n","Epoch 9/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.1797 - accuracy: 0.7105 - val_loss: 1.6498 - val_accuracy: 0.7127\n","Epoch 10/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.1131 - accuracy: 0.7188 - val_loss: 1.4984 - val_accuracy: 0.7221\n","Epoch 11/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 1.0465 - accuracy: 0.7369 - val_loss: 1.4342 - val_accuracy: 0.7483\n","Epoch 12/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 1.0011 - accuracy: 0.7365 - val_loss: 1.3874 - val_accuracy: 0.7492\n","Epoch 13/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9626 - accuracy: 0.7502 - val_loss: 1.4541 - val_accuracy: 0.7050\n","Epoch 14/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.9218 - accuracy: 0.7533 - val_loss: 1.2759 - val_accuracy: 0.7393\n","Epoch 15/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8899 - accuracy: 0.7598 - val_loss: 1.2577 - val_accuracy: 0.7606\n","Epoch 16/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8630 - accuracy: 0.7639 - val_loss: 1.2322 - val_accuracy: 0.7672\n","Epoch 17/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8314 - accuracy: 0.7777 - val_loss: 1.2702 - val_accuracy: 0.7602\n","Epoch 18/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8199 - accuracy: 0.7769 - val_loss: 1.2058 - val_accuracy: 0.7820\n","Epoch 19/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 0.8025 - accuracy: 0.7800 - val_loss: 1.3261 - val_accuracy: 0.7450\n","Epoch 20/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7808 - accuracy: 0.7834 - val_loss: 1.2474 - val_accuracy: 0.7457\n","Epoch 21/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 0.7701 - accuracy: 0.7869 - val_loss: 1.2315 - val_accuracy: 0.7661\n","Epoch 22/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7599 - accuracy: 0.7884 - val_loss: 1.1484 - val_accuracy: 0.7875\n","Epoch 23/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7468 - accuracy: 0.7899 - val_loss: 1.0911 - val_accuracy: 0.7888\n","Epoch 24/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7239 - accuracy: 0.7961 - val_loss: 1.1334 - val_accuracy: 0.8015\n","Epoch 25/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7183 - accuracy: 0.7971 - val_loss: 1.0971 - val_accuracy: 0.8013\n","Epoch 26/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7116 - accuracy: 0.8000 - val_loss: 1.1033 - val_accuracy: 0.7877\n","Epoch 27/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7006 - accuracy: 0.8045 - val_loss: 1.0995 - val_accuracy: 0.7956\n","Epoch 28/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6912 - accuracy: 0.8076 - val_loss: 1.1885 - val_accuracy: 0.7624\n","Epoch 29/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6846 - accuracy: 0.8065 - val_loss: 1.0361 - val_accuracy: 0.8216\n","Epoch 30/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6701 - accuracy: 0.8116 - val_loss: 1.0760 - val_accuracy: 0.7949\n","Epoch 31/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6551 - accuracy: 0.8170 - val_loss: 1.0567 - val_accuracy: 0.8084\n","Epoch 32/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6633 - accuracy: 0.8154 - val_loss: 1.0056 - val_accuracy: 0.8174\n","Epoch 33/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6514 - accuracy: 0.8183 - val_loss: 1.0281 - val_accuracy: 0.8088\n","Epoch 34/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6341 - accuracy: 0.8225 - val_loss: 0.9969 - val_accuracy: 0.8257\n","Epoch 35/60\n","1846/1846 [==============================] - 6s 4ms/step - loss: 0.6571 - accuracy: 0.8175 - val_loss: 1.0020 - val_accuracy: 0.8136\n","Epoch 36/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6386 - accuracy: 0.8186 - val_loss: 1.0098 - val_accuracy: 0.8044\n","Epoch 37/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6216 - accuracy: 0.8257 - val_loss: 1.0010 - val_accuracy: 0.8299\n","Epoch 38/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6166 - accuracy: 0.8258 - val_loss: 1.0295 - val_accuracy: 0.7985\n","Epoch 39/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6237 - accuracy: 0.8223 - val_loss: 1.0161 - val_accuracy: 0.8143\n","Epoch 40/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6187 - accuracy: 0.8253 - val_loss: 1.0189 - val_accuracy: 0.8169\n","Epoch 41/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5895 - accuracy: 0.8323 - val_loss: 0.9942 - val_accuracy: 0.8315\n","Epoch 42/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 0.6010 - accuracy: 0.8290 - val_loss: 0.9788 - val_accuracy: 0.8231\n","Epoch 43/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6118 - accuracy: 0.8289 - val_loss: 0.9241 - val_accuracy: 0.8244\n","Epoch 44/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 0.5818 - accuracy: 0.8365 - val_loss: 0.9780 - val_accuracy: 0.7974\n","Epoch 45/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5776 - accuracy: 0.8358 - val_loss: 0.9499 - val_accuracy: 0.8273\n","Epoch 46/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5714 - accuracy: 0.8398 - val_loss: 0.9260 - val_accuracy: 0.8436\n","Epoch 47/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5694 - accuracy: 0.8381 - val_loss: 0.9964 - val_accuracy: 0.8101\n","Epoch 48/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5616 - accuracy: 0.8385 - val_loss: 0.9013 - val_accuracy: 0.8416\n","Epoch 49/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 0.5652 - accuracy: 0.8372 - val_loss: 0.8651 - val_accuracy: 0.8508\n","Epoch 50/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5481 - accuracy: 0.8421 - val_loss: 0.9036 - val_accuracy: 0.8383\n","Epoch 51/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5487 - accuracy: 0.8438 - val_loss: 0.9761 - val_accuracy: 0.8246\n","Epoch 52/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5501 - accuracy: 0.8432 - val_loss: 0.9032 - val_accuracy: 0.8273\n","Epoch 53/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5327 - accuracy: 0.8462 - val_loss: 0.8902 - val_accuracy: 0.8414\n","Epoch 54/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 0.5387 - accuracy: 0.8463 - val_loss: 0.8801 - val_accuracy: 0.8337\n","Epoch 55/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5224 - accuracy: 0.8494 - val_loss: 0.8571 - val_accuracy: 0.8477\n","Epoch 56/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5443 - accuracy: 0.8461 - val_loss: 0.8590 - val_accuracy: 0.8350\n","Epoch 57/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5078 - accuracy: 0.8566 - val_loss: 0.8039 - val_accuracy: 0.8554\n","Epoch 58/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.4990 - accuracy: 0.8571 - val_loss: 0.8271 - val_accuracy: 0.8574\n","Epoch 59/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5186 - accuracy: 0.8522 - val_loss: 0.8313 - val_accuracy: 0.8508\n","Epoch 60/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5048 - accuracy: 0.8567 - val_loss: 0.8417 - val_accuracy: 0.8539\n","Fold: 2\n","Epoch 1/60\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"]},{"output_type":"stream","name":"stdout","text":["1846/1846 [==============================] - 7s 3ms/step - loss: 4.6546 - accuracy: 0.0781 - val_loss: 4.0738 - val_accuracy: 0.1534\n","Epoch 2/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 3.4929 - accuracy: 0.2874 - val_loss: 3.2827 - val_accuracy: 0.4222\n","Epoch 3/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.7183 - accuracy: 0.4393 - val_loss: 2.7913 - val_accuracy: 0.4482\n","Epoch 4/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.2009 - accuracy: 0.5043 - val_loss: 2.4212 - val_accuracy: 0.5404\n","Epoch 5/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.8114 - accuracy: 0.5773 - val_loss: 2.1371 - val_accuracy: 0.6240\n","Epoch 6/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.5521 - accuracy: 0.6273 - val_loss: 2.0157 - val_accuracy: 0.5945\n","Epoch 7/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.3779 - accuracy: 0.6637 - val_loss: 1.8996 - val_accuracy: 0.6640\n","Epoch 8/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 1.2585 - accuracy: 0.6903 - val_loss: 1.7725 - val_accuracy: 0.7017\n","Epoch 9/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.1641 - accuracy: 0.7074 - val_loss: 1.6861 - val_accuracy: 0.6906\n","Epoch 10/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0881 - accuracy: 0.7195 - val_loss: 1.6697 - val_accuracy: 0.7201\n","Epoch 11/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0408 - accuracy: 0.7331 - val_loss: 1.5518 - val_accuracy: 0.7105\n","Epoch 12/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9845 - accuracy: 0.7436 - val_loss: 1.5154 - val_accuracy: 0.6981\n","Epoch 13/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.9577 - accuracy: 0.7430 - val_loss: 1.4146 - val_accuracy: 0.7562\n","Epoch 14/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9274 - accuracy: 0.7550 - val_loss: 1.4567 - val_accuracy: 0.7430\n","Epoch 15/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 0.8868 - accuracy: 0.7621 - val_loss: 1.4238 - val_accuracy: 0.7778\n","Epoch 16/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8758 - accuracy: 0.7669 - val_loss: 1.3083 - val_accuracy: 0.7795\n","Epoch 17/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8356 - accuracy: 0.7742 - val_loss: 1.3201 - val_accuracy: 0.7696\n","Epoch 18/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 0.8326 - accuracy: 0.7713 - val_loss: 1.2906 - val_accuracy: 0.7729\n","Epoch 19/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8016 - accuracy: 0.7814 - val_loss: 1.2694 - val_accuracy: 0.7855\n","Epoch 20/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.8053 - accuracy: 0.7787 - val_loss: 1.2372 - val_accuracy: 0.7897\n","Epoch 21/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.7841 - accuracy: 0.7836 - val_loss: 1.2970 - val_accuracy: 0.7496\n","Epoch 22/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7649 - accuracy: 0.7902 - val_loss: 1.2659 - val_accuracy: 0.7608\n","Epoch 23/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 0.7545 - accuracy: 0.7949 - val_loss: 1.2225 - val_accuracy: 0.7767\n","Epoch 24/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7316 - accuracy: 0.8000 - val_loss: 1.2696 - val_accuracy: 0.7800\n","Epoch 25/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7243 - accuracy: 0.8006 - val_loss: 1.1756 - val_accuracy: 0.7910\n","Epoch 26/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7234 - accuracy: 0.8016 - val_loss: 1.1777 - val_accuracy: 0.7901\n","Epoch 27/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6943 - accuracy: 0.8088 - val_loss: 1.1516 - val_accuracy: 0.8011\n","Epoch 28/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6948 - accuracy: 0.8086 - val_loss: 1.0880 - val_accuracy: 0.8339\n","Epoch 29/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.6829 - accuracy: 0.8111 - val_loss: 1.0771 - val_accuracy: 0.8345\n","Epoch 30/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6842 - accuracy: 0.8080 - val_loss: 1.1858 - val_accuracy: 0.7765\n","Epoch 31/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6567 - accuracy: 0.8174 - val_loss: 1.1296 - val_accuracy: 0.8099\n","Epoch 32/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6512 - accuracy: 0.8207 - val_loss: 1.1235 - val_accuracy: 0.7806\n","Epoch 33/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 0.6432 - accuracy: 0.8235 - val_loss: 1.0934 - val_accuracy: 0.8174\n","Epoch 34/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6313 - accuracy: 0.8264 - val_loss: 1.0337 - val_accuracy: 0.8420\n","Epoch 35/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 0.6527 - accuracy: 0.8190 - val_loss: 1.1092 - val_accuracy: 0.8029\n","Epoch 36/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6059 - accuracy: 0.8353 - val_loss: 1.1860 - val_accuracy: 0.8152\n","Epoch 37/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6114 - accuracy: 0.8324 - val_loss: 1.0265 - val_accuracy: 0.8147\n","Epoch 38/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 0.6136 - accuracy: 0.8284 - val_loss: 1.1249 - val_accuracy: 0.7802\n","Epoch 39/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6212 - accuracy: 0.8279 - val_loss: 0.9658 - val_accuracy: 0.8484\n","Epoch 40/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.5822 - accuracy: 0.8346 - val_loss: 1.0568 - val_accuracy: 0.8257\n","Epoch 41/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5784 - accuracy: 0.8409 - val_loss: 0.9937 - val_accuracy: 0.8396\n","Epoch 42/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5832 - accuracy: 0.8350 - val_loss: 0.9773 - val_accuracy: 0.8475\n","Epoch 43/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 0.5725 - accuracy: 0.8424 - val_loss: 1.0119 - val_accuracy: 0.8290\n","Epoch 44/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5794 - accuracy: 0.8405 - val_loss: 0.9277 - val_accuracy: 0.8447\n","Epoch 45/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 0.5678 - accuracy: 0.8421 - val_loss: 1.0476 - val_accuracy: 0.8323\n","Epoch 46/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5595 - accuracy: 0.8445 - val_loss: 0.9711 - val_accuracy: 0.8273\n","Epoch 47/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5552 - accuracy: 0.8508 - val_loss: 0.9305 - val_accuracy: 0.8561\n","Epoch 48/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5377 - accuracy: 0.8499 - val_loss: 0.9125 - val_accuracy: 0.8546\n","Epoch 49/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5407 - accuracy: 0.8501 - val_loss: 0.9622 - val_accuracy: 0.8499\n","Epoch 50/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5651 - accuracy: 0.8420 - val_loss: 0.9508 - val_accuracy: 0.8323\n","Epoch 51/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5404 - accuracy: 0.8513 - val_loss: 0.8946 - val_accuracy: 0.8631\n","Epoch 52/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.5362 - accuracy: 0.8536 - val_loss: 0.9399 - val_accuracy: 0.8416\n","Epoch 53/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.5317 - accuracy: 0.8521 - val_loss: 0.9348 - val_accuracy: 0.8473\n","Epoch 54/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5128 - accuracy: 0.8578 - val_loss: 0.8648 - val_accuracy: 0.8475\n","Epoch 55/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5213 - accuracy: 0.8549 - val_loss: 0.9307 - val_accuracy: 0.8416\n","Epoch 56/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5026 - accuracy: 0.8586 - val_loss: 0.9167 - val_accuracy: 0.8400\n","Epoch 57/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.5030 - accuracy: 0.8626 - val_loss: 0.9013 - val_accuracy: 0.8502\n","Epoch 58/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5085 - accuracy: 0.8604 - val_loss: 0.8808 - val_accuracy: 0.8506\n","Epoch 59/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5009 - accuracy: 0.8577 - val_loss: 0.9013 - val_accuracy: 0.8433\n","Epoch 60/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5095 - accuracy: 0.8591 - val_loss: 0.8558 - val_accuracy: 0.8464\n","Average Validation Accuracy: 0.859746515750885\n","Average Validation Loss: 0.590171217918396\n","Average Test Accuracy: 0.8587749898433685\n","Final Test Accuracy for each fold: 0.8633449077606201\n","Number of input features: 6\n","Fold: 1\n","Epoch 1/60\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"]},{"output_type":"stream","name":"stdout","text":["1846/1846 [==============================] - 6s 3ms/step - loss: 4.7664 - accuracy: 0.0668 - val_loss: 4.2496 - val_accuracy: 0.1078\n","Epoch 2/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.6978 - accuracy: 0.2434 - val_loss: 3.3956 - val_accuracy: 0.3305\n","Epoch 3/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 2.8500 - accuracy: 0.4221 - val_loss: 2.7298 - val_accuracy: 0.4794\n","Epoch 4/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.2343 - accuracy: 0.5188 - val_loss: 2.2723 - val_accuracy: 0.5520\n","Epoch 5/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.8078 - accuracy: 0.5985 - val_loss: 1.9836 - val_accuracy: 0.6167\n","Epoch 6/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.5147 - accuracy: 0.6491 - val_loss: 1.7586 - val_accuracy: 0.6878\n","Epoch 7/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.3166 - accuracy: 0.6818 - val_loss: 1.6197 - val_accuracy: 0.6939\n","Epoch 8/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.1851 - accuracy: 0.7094 - val_loss: 1.5683 - val_accuracy: 0.7215\n","Epoch 9/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.0780 - accuracy: 0.7336 - val_loss: 1.5561 - val_accuracy: 0.7344\n","Epoch 10/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0096 - accuracy: 0.7459 - val_loss: 1.4215 - val_accuracy: 0.7470\n","Epoch 11/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9642 - accuracy: 0.7493 - val_loss: 1.5221 - val_accuracy: 0.7036\n","Epoch 12/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9144 - accuracy: 0.7641 - val_loss: 1.3502 - val_accuracy: 0.7776\n","Epoch 13/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8815 - accuracy: 0.7661 - val_loss: 1.3557 - val_accuracy: 0.7571\n","Epoch 14/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8560 - accuracy: 0.7759 - val_loss: 1.3704 - val_accuracy: 0.7659\n","Epoch 15/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8151 - accuracy: 0.7818 - val_loss: 1.2906 - val_accuracy: 0.7859\n","Epoch 16/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.8000 - accuracy: 0.7880 - val_loss: 1.2913 - val_accuracy: 0.7751\n","Epoch 17/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 0.7679 - accuracy: 0.7923 - val_loss: 1.2095 - val_accuracy: 0.7960\n","Epoch 18/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7649 - accuracy: 0.7932 - val_loss: 1.2284 - val_accuracy: 0.7793\n","Epoch 19/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7336 - accuracy: 0.8006 - val_loss: 1.2407 - val_accuracy: 0.7729\n","Epoch 20/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7300 - accuracy: 0.8012 - val_loss: 1.1606 - val_accuracy: 0.8057\n","Epoch 21/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7012 - accuracy: 0.8108 - val_loss: 1.1942 - val_accuracy: 0.7762\n","Epoch 22/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6927 - accuracy: 0.8105 - val_loss: 1.1442 - val_accuracy: 0.7963\n","Epoch 23/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 0.6770 - accuracy: 0.8137 - val_loss: 1.0881 - val_accuracy: 0.8145\n","Epoch 24/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6726 - accuracy: 0.8145 - val_loss: 1.2274 - val_accuracy: 0.7430\n","Epoch 25/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6599 - accuracy: 0.8168 - val_loss: 1.0919 - val_accuracy: 0.8037\n","Epoch 26/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6588 - accuracy: 0.8209 - val_loss: 1.1189 - val_accuracy: 0.7890\n","Epoch 27/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6583 - accuracy: 0.8163 - val_loss: 1.0707 - val_accuracy: 0.8068\n","Epoch 28/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.6418 - accuracy: 0.8231 - val_loss: 1.0956 - val_accuracy: 0.8191\n","Epoch 29/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6269 - accuracy: 0.8271 - val_loss: 1.0977 - val_accuracy: 0.8123\n","Epoch 30/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6072 - accuracy: 0.8263 - val_loss: 1.0489 - val_accuracy: 0.8216\n","Epoch 31/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6312 - accuracy: 0.8256 - val_loss: 1.1801 - val_accuracy: 0.7861\n","Epoch 32/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.6032 - accuracy: 0.8296 - val_loss: 1.0062 - val_accuracy: 0.8460\n","Epoch 33/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6019 - accuracy: 0.8348 - val_loss: 1.1612 - val_accuracy: 0.7954\n","Epoch 34/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5824 - accuracy: 0.8380 - val_loss: 1.0317 - val_accuracy: 0.8425\n","Epoch 35/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 0.5907 - accuracy: 0.8346 - val_loss: 1.0299 - val_accuracy: 0.8444\n","Epoch 36/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5701 - accuracy: 0.8410 - val_loss: 1.0403 - val_accuracy: 0.8224\n","Epoch 37/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.6027 - accuracy: 0.8342 - val_loss: 0.9909 - val_accuracy: 0.8469\n","Epoch 38/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5682 - accuracy: 0.8432 - val_loss: 0.9714 - val_accuracy: 0.8561\n","Epoch 39/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5536 - accuracy: 0.8464 - val_loss: 1.0879 - val_accuracy: 0.8330\n","Epoch 40/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.5467 - accuracy: 0.8476 - val_loss: 1.0074 - val_accuracy: 0.8288\n","Epoch 41/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.5549 - accuracy: 0.8462 - val_loss: 0.9505 - val_accuracy: 0.8524\n","Epoch 42/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5319 - accuracy: 0.8529 - val_loss: 1.0643 - val_accuracy: 0.8079\n","Epoch 43/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5504 - accuracy: 0.8479 - val_loss: 0.9257 - val_accuracy: 0.8554\n","Epoch 44/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5149 - accuracy: 0.8573 - val_loss: 0.9608 - val_accuracy: 0.8493\n","Epoch 45/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.5352 - accuracy: 0.8564 - val_loss: 0.9620 - val_accuracy: 0.8598\n","Epoch 46/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5390 - accuracy: 0.8523 - val_loss: 0.9000 - val_accuracy: 0.8656\n","Epoch 47/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.5184 - accuracy: 0.8581 - val_loss: 0.9215 - val_accuracy: 0.8568\n","Epoch 48/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5359 - accuracy: 0.8530 - val_loss: 0.8887 - val_accuracy: 0.8647\n","Epoch 49/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5002 - accuracy: 0.8618 - val_loss: 0.9048 - val_accuracy: 0.8565\n","Epoch 50/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5106 - accuracy: 0.8613 - val_loss: 0.9460 - val_accuracy: 0.8572\n","Epoch 51/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4978 - accuracy: 0.8659 - val_loss: 0.9214 - val_accuracy: 0.8506\n","Epoch 52/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5004 - accuracy: 0.8630 - val_loss: 0.9236 - val_accuracy: 0.8638\n","Epoch 53/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4891 - accuracy: 0.8629 - val_loss: 0.9773 - val_accuracy: 0.8330\n","Epoch 54/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.4983 - accuracy: 0.8638 - val_loss: 0.8841 - val_accuracy: 0.8689\n","Epoch 55/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4844 - accuracy: 0.8655 - val_loss: 0.8920 - val_accuracy: 0.8598\n","Epoch 56/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4850 - accuracy: 0.8660 - val_loss: 0.8686 - val_accuracy: 0.8640\n","Epoch 57/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.4831 - accuracy: 0.8658 - val_loss: 0.9472 - val_accuracy: 0.8530\n","Epoch 58/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4906 - accuracy: 0.8641 - val_loss: 0.9345 - val_accuracy: 0.8543\n","Epoch 59/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.4729 - accuracy: 0.8699 - val_loss: 0.8700 - val_accuracy: 0.8658\n","Epoch 60/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4958 - accuracy: 0.8653 - val_loss: 0.8992 - val_accuracy: 0.8484\n","Fold: 2\n","Epoch 1/60\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"]},{"output_type":"stream","name":"stdout","text":["1846/1846 [==============================] - 6s 2ms/step - loss: 4.6960 - accuracy: 0.0657 - val_loss: 4.1532 - val_accuracy: 0.1549\n","Epoch 2/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 3.6693 - accuracy: 0.2418 - val_loss: 3.5190 - val_accuracy: 0.3512\n","Epoch 3/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.0061 - accuracy: 0.4084 - val_loss: 3.0359 - val_accuracy: 0.4339\n","Epoch 4/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 2.4939 - accuracy: 0.4622 - val_loss: 2.7366 - val_accuracy: 0.4920\n","Epoch 5/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.1264 - accuracy: 0.5189 - val_loss: 2.4875 - val_accuracy: 0.5030\n","Epoch 6/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 1.8499 - accuracy: 0.5694 - val_loss: 2.3098 - val_accuracy: 0.5780\n","Epoch 7/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.6331 - accuracy: 0.6111 - val_loss: 2.1928 - val_accuracy: 0.6257\n","Epoch 8/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 1.4669 - accuracy: 0.6545 - val_loss: 2.1291 - val_accuracy: 0.6572\n","Epoch 9/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 1.3549 - accuracy: 0.6846 - val_loss: 2.0065 - val_accuracy: 0.7094\n","Epoch 10/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.2607 - accuracy: 0.7055 - val_loss: 1.9666 - val_accuracy: 0.7201\n","Epoch 11/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.1866 - accuracy: 0.7236 - val_loss: 1.9138 - val_accuracy: 0.7230\n","Epoch 12/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.1227 - accuracy: 0.7311 - val_loss: 1.8386 - val_accuracy: 0.7179\n","Epoch 13/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 1.0739 - accuracy: 0.7378 - val_loss: 1.8072 - val_accuracy: 0.7494\n","Epoch 14/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 1.0405 - accuracy: 0.7457 - val_loss: 1.7615 - val_accuracy: 0.7503\n","Epoch 15/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.0010 - accuracy: 0.7516 - val_loss: 1.7423 - val_accuracy: 0.7498\n","Epoch 16/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.9674 - accuracy: 0.7547 - val_loss: 1.7346 - val_accuracy: 0.7320\n","Epoch 17/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9349 - accuracy: 0.7604 - val_loss: 1.6735 - val_accuracy: 0.7602\n","Epoch 18/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9137 - accuracy: 0.7677 - val_loss: 1.6387 - val_accuracy: 0.7696\n","Epoch 19/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8926 - accuracy: 0.7714 - val_loss: 1.6428 - val_accuracy: 0.7531\n","Epoch 20/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8692 - accuracy: 0.7725 - val_loss: 1.5981 - val_accuracy: 0.7397\n","Epoch 21/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8533 - accuracy: 0.7758 - val_loss: 1.5613 - val_accuracy: 0.7776\n","Epoch 22/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8503 - accuracy: 0.7758 - val_loss: 1.5469 - val_accuracy: 0.7813\n","Epoch 23/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8244 - accuracy: 0.7796 - val_loss: 1.5327 - val_accuracy: 0.7767\n","Epoch 24/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8211 - accuracy: 0.7813 - val_loss: 1.5327 - val_accuracy: 0.7804\n","Epoch 25/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7952 - accuracy: 0.7871 - val_loss: 1.5628 - val_accuracy: 0.7776\n","Epoch 26/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7842 - accuracy: 0.7882 - val_loss: 1.5304 - val_accuracy: 0.7839\n","Epoch 27/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7777 - accuracy: 0.7899 - val_loss: 1.5948 - val_accuracy: 0.7419\n","Epoch 28/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.7873 - accuracy: 0.7874 - val_loss: 1.4713 - val_accuracy: 0.7875\n","Epoch 29/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7622 - accuracy: 0.7932 - val_loss: 1.4693 - val_accuracy: 0.7879\n","Epoch 30/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7497 - accuracy: 0.7943 - val_loss: 1.4656 - val_accuracy: 0.8018\n","Epoch 31/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.7501 - accuracy: 0.7958 - val_loss: 1.5304 - val_accuracy: 0.7661\n","Epoch 32/60\n","1846/1846 [==============================] - 6s 4ms/step - loss: 0.7266 - accuracy: 0.8000 - val_loss: 1.4350 - val_accuracy: 0.8108\n","Epoch 33/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7278 - accuracy: 0.8043 - val_loss: 1.4465 - val_accuracy: 0.7806\n","Epoch 34/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.7210 - accuracy: 0.8025 - val_loss: 1.4460 - val_accuracy: 0.7487\n","Epoch 35/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7332 - accuracy: 0.7981 - val_loss: 1.3819 - val_accuracy: 0.8255\n","Epoch 36/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6820 - accuracy: 0.8128 - val_loss: 1.4139 - val_accuracy: 0.8169\n","Epoch 37/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7077 - accuracy: 0.8065 - val_loss: 1.4150 - val_accuracy: 0.7864\n","Epoch 38/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6904 - accuracy: 0.8106 - val_loss: 1.4937 - val_accuracy: 0.7516\n","Epoch 39/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6902 - accuracy: 0.8124 - val_loss: 1.3589 - val_accuracy: 0.8218\n","Epoch 40/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6826 - accuracy: 0.8088 - val_loss: 1.4419 - val_accuracy: 0.7910\n","Epoch 41/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6672 - accuracy: 0.8181 - val_loss: 1.4303 - val_accuracy: 0.7934\n","Epoch 42/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6765 - accuracy: 0.8148 - val_loss: 1.3500 - val_accuracy: 0.8253\n","Epoch 43/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6713 - accuracy: 0.8145 - val_loss: 1.3577 - val_accuracy: 0.8163\n","Epoch 44/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6609 - accuracy: 0.8160 - val_loss: 1.3860 - val_accuracy: 0.8013\n","Epoch 45/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6574 - accuracy: 0.8194 - val_loss: 1.3800 - val_accuracy: 0.8002\n","Epoch 46/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6497 - accuracy: 0.8201 - val_loss: 1.3786 - val_accuracy: 0.8134\n","Epoch 47/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.6623 - accuracy: 0.8146 - val_loss: 1.4164 - val_accuracy: 0.7980\n","Epoch 48/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6428 - accuracy: 0.8218 - val_loss: 1.3080 - val_accuracy: 0.8211\n","Epoch 49/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6434 - accuracy: 0.8204 - val_loss: 1.3688 - val_accuracy: 0.7976\n","Epoch 50/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.6455 - accuracy: 0.8226 - val_loss: 1.3104 - val_accuracy: 0.8128\n","Epoch 51/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.6538 - accuracy: 0.8186 - val_loss: 1.3985 - val_accuracy: 0.7745\n","Epoch 52/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6270 - accuracy: 0.8278 - val_loss: 1.2944 - val_accuracy: 0.7936\n","Epoch 53/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.6300 - accuracy: 0.8259 - val_loss: 1.3154 - val_accuracy: 0.8134\n","Epoch 54/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6281 - accuracy: 0.8271 - val_loss: 1.2368 - val_accuracy: 0.8251\n","Epoch 55/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6173 - accuracy: 0.8267 - val_loss: 1.2584 - val_accuracy: 0.8299\n","Epoch 56/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.6241 - accuracy: 0.8263 - val_loss: 1.3615 - val_accuracy: 0.8029\n","Epoch 57/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6079 - accuracy: 0.8324 - val_loss: 1.3154 - val_accuracy: 0.7945\n","Epoch 58/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6150 - accuracy: 0.8271 - val_loss: 1.2876 - val_accuracy: 0.8290\n","Epoch 59/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6294 - accuracy: 0.8268 - val_loss: 1.2553 - val_accuracy: 0.8442\n","Epoch 60/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5856 - accuracy: 0.8392 - val_loss: 1.2789 - val_accuracy: 0.8218\n","Average Validation Accuracy: 0.8474379777908325\n","Average Validation Loss: 0.711078941822052\n","Average Test Accuracy: 0.842264324426651\n","Final Test Accuracy for each fold: 0.8562688827514648\n","Number of input features: 7\n","Fold: 1\n","Epoch 1/60\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"]},{"output_type":"stream","name":"stdout","text":["1846/1846 [==============================] - 7s 3ms/step - loss: 4.4924 - accuracy: 0.0787 - val_loss: 3.9943 - val_accuracy: 0.1199\n","Epoch 2/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.4951 - accuracy: 0.2351 - val_loss: 3.2470 - val_accuracy: 0.4194\n","Epoch 3/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.6552 - accuracy: 0.4597 - val_loss: 2.5503 - val_accuracy: 0.5250\n","Epoch 4/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.0515 - accuracy: 0.5641 - val_loss: 2.1205 - val_accuracy: 0.5971\n","Epoch 5/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.6670 - accuracy: 0.6287 - val_loss: 1.8741 - val_accuracy: 0.6255\n","Epoch 6/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.3880 - accuracy: 0.6852 - val_loss: 1.6280 - val_accuracy: 0.7065\n","Epoch 7/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 1.1864 - accuracy: 0.7270 - val_loss: 1.5107 - val_accuracy: 0.7426\n","Epoch 8/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.0427 - accuracy: 0.7527 - val_loss: 1.4419 - val_accuracy: 0.7485\n","Epoch 9/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9317 - accuracy: 0.7701 - val_loss: 1.3361 - val_accuracy: 0.7648\n","Epoch 10/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8575 - accuracy: 0.7861 - val_loss: 1.3922 - val_accuracy: 0.7472\n","Epoch 11/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8002 - accuracy: 0.7963 - val_loss: 1.2350 - val_accuracy: 0.7901\n","Epoch 12/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7522 - accuracy: 0.8050 - val_loss: 1.2644 - val_accuracy: 0.7837\n","Epoch 13/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7164 - accuracy: 0.8138 - val_loss: 1.1951 - val_accuracy: 0.8075\n","Epoch 14/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.6767 - accuracy: 0.8229 - val_loss: 1.2063 - val_accuracy: 0.8123\n","Epoch 15/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.6602 - accuracy: 0.8247 - val_loss: 1.1468 - val_accuracy: 0.8189\n","Epoch 16/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6299 - accuracy: 0.8325 - val_loss: 1.1551 - val_accuracy: 0.8161\n","Epoch 17/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.6117 - accuracy: 0.8381 - val_loss: 1.1117 - val_accuracy: 0.8328\n","Epoch 18/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.6081 - accuracy: 0.8361 - val_loss: 1.1013 - val_accuracy: 0.8308\n","Epoch 19/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5737 - accuracy: 0.8446 - val_loss: 1.0728 - val_accuracy: 0.8189\n","Epoch 20/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5523 - accuracy: 0.8519 - val_loss: 1.0205 - val_accuracy: 0.8484\n","Epoch 21/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5452 - accuracy: 0.8527 - val_loss: 1.0067 - val_accuracy: 0.8425\n","Epoch 22/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5354 - accuracy: 0.8553 - val_loss: 0.9791 - val_accuracy: 0.8458\n","Epoch 23/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5272 - accuracy: 0.8571 - val_loss: 1.0620 - val_accuracy: 0.8323\n","Epoch 24/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5190 - accuracy: 0.8566 - val_loss: 1.0455 - val_accuracy: 0.8326\n","Epoch 25/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4907 - accuracy: 0.8638 - val_loss: 0.9869 - val_accuracy: 0.8517\n","Epoch 26/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.4885 - accuracy: 0.8668 - val_loss: 0.9830 - val_accuracy: 0.8442\n","Epoch 27/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4949 - accuracy: 0.8633 - val_loss: 0.9460 - val_accuracy: 0.8535\n","Epoch 28/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4835 - accuracy: 0.8692 - val_loss: 0.9592 - val_accuracy: 0.8660\n","Epoch 29/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4630 - accuracy: 0.8723 - val_loss: 0.9025 - val_accuracy: 0.8790\n","Epoch 30/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.4620 - accuracy: 0.8750 - val_loss: 0.9564 - val_accuracy: 0.8594\n","Epoch 31/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.4518 - accuracy: 0.8812 - val_loss: 0.9076 - val_accuracy: 0.8601\n","Epoch 32/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4535 - accuracy: 0.8747 - val_loss: 0.8444 - val_accuracy: 0.8770\n","Epoch 33/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4364 - accuracy: 0.8815 - val_loss: 0.8823 - val_accuracy: 0.8858\n","Epoch 34/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.4493 - accuracy: 0.8786 - val_loss: 0.8583 - val_accuracy: 0.8821\n","Epoch 35/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4097 - accuracy: 0.8886 - val_loss: 0.9066 - val_accuracy: 0.8792\n","Epoch 36/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4360 - accuracy: 0.8806 - val_loss: 0.9866 - val_accuracy: 0.8339\n","Epoch 37/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4212 - accuracy: 0.8851 - val_loss: 0.8890 - val_accuracy: 0.8607\n","Epoch 38/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4161 - accuracy: 0.8846 - val_loss: 0.8693 - val_accuracy: 0.8581\n","Epoch 39/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3981 - accuracy: 0.8900 - val_loss: 0.8731 - val_accuracy: 0.8704\n","Epoch 40/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4072 - accuracy: 0.8911 - val_loss: 0.8433 - val_accuracy: 0.8750\n","Epoch 41/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.3960 - accuracy: 0.8874 - val_loss: 0.8303 - val_accuracy: 0.8783\n","Epoch 42/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.4178 - accuracy: 0.8847 - val_loss: 0.8237 - val_accuracy: 0.8911\n","Epoch 43/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3889 - accuracy: 0.8922 - val_loss: 0.8053 - val_accuracy: 0.9023\n","Epoch 44/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3916 - accuracy: 0.8927 - val_loss: 0.8829 - val_accuracy: 0.8821\n","Epoch 45/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.3861 - accuracy: 0.8939 - val_loss: 0.8412 - val_accuracy: 0.8803\n","Epoch 46/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3873 - accuracy: 0.8906 - val_loss: 0.8001 - val_accuracy: 0.8887\n","Epoch 47/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3765 - accuracy: 0.8974 - val_loss: 0.7440 - val_accuracy: 0.9144\n","Epoch 48/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.3752 - accuracy: 0.8981 - val_loss: 0.8031 - val_accuracy: 0.8904\n","Epoch 49/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3661 - accuracy: 0.8991 - val_loss: 0.8529 - val_accuracy: 0.8711\n","Epoch 50/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3698 - accuracy: 0.8964 - val_loss: 0.7750 - val_accuracy: 0.9006\n","Epoch 51/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3573 - accuracy: 0.9010 - val_loss: 0.8713 - val_accuracy: 0.8488\n","Epoch 52/60\n","1846/1846 [==============================] - 6s 4ms/step - loss: 0.3587 - accuracy: 0.8979 - val_loss: 0.8012 - val_accuracy: 0.8836\n","Epoch 53/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.3453 - accuracy: 0.9031 - val_loss: 0.8182 - val_accuracy: 0.8750\n","Epoch 54/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.3523 - accuracy: 0.9017 - val_loss: 0.9785 - val_accuracy: 0.8451\n","Epoch 55/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3494 - accuracy: 0.9060 - val_loss: 0.7619 - val_accuracy: 0.8948\n","Epoch 56/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3521 - accuracy: 0.9006 - val_loss: 0.6933 - val_accuracy: 0.9206\n","Epoch 57/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3411 - accuracy: 0.9056 - val_loss: 0.7332 - val_accuracy: 0.9212\n","Epoch 58/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3459 - accuracy: 0.9026 - val_loss: 0.8116 - val_accuracy: 0.8739\n","Epoch 59/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.3213 - accuracy: 0.9106 - val_loss: 0.7609 - val_accuracy: 0.8935\n","Epoch 60/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3459 - accuracy: 0.9043 - val_loss: 0.6932 - val_accuracy: 0.9228\n","Fold: 2\n","Epoch 1/60\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"]},{"output_type":"stream","name":"stdout","text":["1846/1846 [==============================] - 6s 3ms/step - loss: 4.7533 - accuracy: 0.0636 - val_loss: 4.1236 - val_accuracy: 0.1188\n","Epoch 2/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.3866 - accuracy: 0.3079 - val_loss: 3.1232 - val_accuracy: 0.4216\n","Epoch 3/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 2.5114 - accuracy: 0.5016 - val_loss: 2.5051 - val_accuracy: 0.5659\n","Epoch 4/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.9511 - accuracy: 0.5853 - val_loss: 2.1790 - val_accuracy: 0.6070\n","Epoch 5/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 1.5956 - accuracy: 0.6402 - val_loss: 1.9708 - val_accuracy: 0.6541\n","Epoch 6/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.3762 - accuracy: 0.6779 - val_loss: 1.8149 - val_accuracy: 0.6774\n","Epoch 7/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.2223 - accuracy: 0.7027 - val_loss: 1.7749 - val_accuracy: 0.6453\n","Epoch 8/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.1160 - accuracy: 0.7273 - val_loss: 1.6307 - val_accuracy: 0.7102\n","Epoch 9/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.0391 - accuracy: 0.7401 - val_loss: 1.5071 - val_accuracy: 0.7347\n","Epoch 10/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9788 - accuracy: 0.7507 - val_loss: 1.4588 - val_accuracy: 0.7386\n","Epoch 11/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.9217 - accuracy: 0.7634 - val_loss: 1.4406 - val_accuracy: 0.7351\n","Epoch 12/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8732 - accuracy: 0.7749 - val_loss: 1.5338 - val_accuracy: 0.6840\n","Epoch 13/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.8554 - accuracy: 0.7753 - val_loss: 1.3555 - val_accuracy: 0.7760\n","Epoch 14/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8146 - accuracy: 0.7834 - val_loss: 1.3473 - val_accuracy: 0.7630\n","Epoch 15/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7926 - accuracy: 0.7891 - val_loss: 1.3102 - val_accuracy: 0.7479\n","Epoch 16/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.7674 - accuracy: 0.7899 - val_loss: 1.2807 - val_accuracy: 0.7604\n","Epoch 17/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7567 - accuracy: 0.7956 - val_loss: 1.2533 - val_accuracy: 0.7782\n","Epoch 18/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.7307 - accuracy: 0.7993 - val_loss: 1.2787 - val_accuracy: 0.7804\n","Epoch 19/60\n","1846/1846 [==============================] - 6s 4ms/step - loss: 0.7073 - accuracy: 0.8023 - val_loss: 1.1697 - val_accuracy: 0.7969\n","Epoch 20/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.6993 - accuracy: 0.8050 - val_loss: 1.1689 - val_accuracy: 0.7870\n","Epoch 21/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6799 - accuracy: 0.8112 - val_loss: 1.1927 - val_accuracy: 0.7919\n","Epoch 22/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6806 - accuracy: 0.8115 - val_loss: 1.1673 - val_accuracy: 0.7877\n","Epoch 23/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6567 - accuracy: 0.8168 - val_loss: 1.1347 - val_accuracy: 0.8141\n","Epoch 24/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6350 - accuracy: 0.8237 - val_loss: 1.0723 - val_accuracy: 0.8130\n","Epoch 25/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6329 - accuracy: 0.8210 - val_loss: 1.1083 - val_accuracy: 0.8081\n","Epoch 26/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 0.6380 - accuracy: 0.8238 - val_loss: 1.0856 - val_accuracy: 0.8095\n","Epoch 27/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6202 - accuracy: 0.8262 - val_loss: 1.0703 - val_accuracy: 0.8174\n","Epoch 28/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.5975 - accuracy: 0.8354 - val_loss: 1.0196 - val_accuracy: 0.8233\n","Epoch 29/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.5987 - accuracy: 0.8339 - val_loss: 1.0610 - val_accuracy: 0.8224\n","Epoch 30/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.5944 - accuracy: 0.8321 - val_loss: 1.1074 - val_accuracy: 0.8156\n","Epoch 31/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.5766 - accuracy: 0.8393 - val_loss: 1.0756 - val_accuracy: 0.8161\n","Epoch 32/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5816 - accuracy: 0.8336 - val_loss: 1.0062 - val_accuracy: 0.8308\n","Epoch 33/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5720 - accuracy: 0.8394 - val_loss: 1.0461 - val_accuracy: 0.8319\n","Epoch 34/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.5785 - accuracy: 0.8378 - val_loss: 1.0277 - val_accuracy: 0.8046\n","Epoch 35/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.5452 - accuracy: 0.8489 - val_loss: 0.9822 - val_accuracy: 0.8317\n","Epoch 36/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5458 - accuracy: 0.8470 - val_loss: 0.9597 - val_accuracy: 0.8211\n","Epoch 37/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5437 - accuracy: 0.8463 - val_loss: 0.9783 - val_accuracy: 0.8464\n","Epoch 38/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5420 - accuracy: 0.8527 - val_loss: 0.9590 - val_accuracy: 0.8436\n","Epoch 39/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5280 - accuracy: 0.8521 - val_loss: 0.9491 - val_accuracy: 0.8394\n","Epoch 40/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5290 - accuracy: 0.8520 - val_loss: 0.9758 - val_accuracy: 0.8345\n","Epoch 41/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5222 - accuracy: 0.8517 - val_loss: 0.9596 - val_accuracy: 0.8378\n","Epoch 42/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5101 - accuracy: 0.8561 - val_loss: 0.9564 - val_accuracy: 0.8383\n","Epoch 43/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5134 - accuracy: 0.8552 - val_loss: 0.9058 - val_accuracy: 0.8427\n","Epoch 44/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5074 - accuracy: 0.8581 - val_loss: 1.0181 - val_accuracy: 0.8015\n","Epoch 45/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.5223 - accuracy: 0.8515 - val_loss: 0.9409 - val_accuracy: 0.8416\n","Epoch 46/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4881 - accuracy: 0.8624 - val_loss: 0.9743 - val_accuracy: 0.8227\n","Epoch 47/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5083 - accuracy: 0.8508 - val_loss: 0.9153 - val_accuracy: 0.8499\n","Epoch 48/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4850 - accuracy: 0.8630 - val_loss: 0.8901 - val_accuracy: 0.8546\n","Epoch 49/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4832 - accuracy: 0.8615 - val_loss: 0.9235 - val_accuracy: 0.8418\n","Epoch 50/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.4756 - accuracy: 0.8675 - val_loss: 0.9237 - val_accuracy: 0.8541\n","Epoch 51/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.4903 - accuracy: 0.8623 - val_loss: 0.8618 - val_accuracy: 0.8673\n","Epoch 52/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.4820 - accuracy: 0.8656 - val_loss: 0.9100 - val_accuracy: 0.8409\n","Epoch 53/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4666 - accuracy: 0.8678 - val_loss: 0.9448 - val_accuracy: 0.8128\n","Epoch 54/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.4797 - accuracy: 0.8641 - val_loss: 0.9302 - val_accuracy: 0.8205\n","Epoch 55/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4656 - accuracy: 0.8667 - val_loss: 0.9485 - val_accuracy: 0.8554\n","Epoch 56/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4570 - accuracy: 0.8729 - val_loss: 0.8471 - val_accuracy: 0.8812\n","Epoch 57/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.4543 - accuracy: 0.8711 - val_loss: 0.8692 - val_accuracy: 0.8664\n","Epoch 58/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4461 - accuracy: 0.8742 - val_loss: 0.8713 - val_accuracy: 0.8660\n","Epoch 59/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4574 - accuracy: 0.8690 - val_loss: 0.8437 - val_accuracy: 0.8653\n","Epoch 60/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4487 - accuracy: 0.8750 - val_loss: 0.8709 - val_accuracy: 0.8568\n","Average Validation Accuracy: 0.9043304324150085\n","Average Validation Loss: 0.4918063133955002\n","Average Test Accuracy: 0.9001990258693695\n","Final Test Accuracy for each fold: 0.9273973703384399\n","Number of input features: 8\n","Fold: 1\n","Epoch 1/60\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"]},{"output_type":"stream","name":"stdout","text":["1846/1846 [==============================] - 6s 3ms/step - loss: 4.5483 - accuracy: 0.0840 - val_loss: 4.0805 - val_accuracy: 0.0911\n","Epoch 2/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.5276 - accuracy: 0.2349 - val_loss: 3.2320 - val_accuracy: 0.3287\n","Epoch 3/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.6619 - accuracy: 0.4353 - val_loss: 2.5896 - val_accuracy: 0.4876\n","Epoch 4/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.0383 - accuracy: 0.5791 - val_loss: 2.0761 - val_accuracy: 0.6040\n","Epoch 5/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.5741 - accuracy: 0.6581 - val_loss: 1.7343 - val_accuracy: 0.6759\n","Epoch 6/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.2614 - accuracy: 0.7131 - val_loss: 1.5703 - val_accuracy: 0.6955\n","Epoch 7/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0770 - accuracy: 0.7390 - val_loss: 1.4182 - val_accuracy: 0.7439\n","Epoch 8/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9525 - accuracy: 0.7589 - val_loss: 1.3033 - val_accuracy: 0.7622\n","Epoch 9/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.8729 - accuracy: 0.7765 - val_loss: 1.2691 - val_accuracy: 0.7549\n","Epoch 10/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7994 - accuracy: 0.7869 - val_loss: 1.2133 - val_accuracy: 0.7826\n","Epoch 11/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7597 - accuracy: 0.7957 - val_loss: 1.1795 - val_accuracy: 0.7870\n","Epoch 12/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7151 - accuracy: 0.8041 - val_loss: 1.1541 - val_accuracy: 0.7991\n","Epoch 13/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6761 - accuracy: 0.8127 - val_loss: 1.1300 - val_accuracy: 0.7958\n","Epoch 14/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6437 - accuracy: 0.8276 - val_loss: 1.0593 - val_accuracy: 0.8187\n","Epoch 15/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6217 - accuracy: 0.8286 - val_loss: 1.0374 - val_accuracy: 0.8359\n","Epoch 16/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.6053 - accuracy: 0.8331 - val_loss: 1.0855 - val_accuracy: 0.8009\n","Epoch 17/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5862 - accuracy: 0.8381 - val_loss: 1.0711 - val_accuracy: 0.8264\n","Epoch 18/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5740 - accuracy: 0.8418 - val_loss: 1.0004 - val_accuracy: 0.8447\n","Epoch 19/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.5562 - accuracy: 0.8470 - val_loss: 0.9784 - val_accuracy: 0.8510\n","Epoch 20/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5419 - accuracy: 0.8503 - val_loss: 1.0167 - val_accuracy: 0.8227\n","Epoch 21/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5223 - accuracy: 0.8560 - val_loss: 0.9691 - val_accuracy: 0.8519\n","Epoch 22/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5234 - accuracy: 0.8554 - val_loss: 0.9594 - val_accuracy: 0.8442\n","Epoch 23/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4957 - accuracy: 0.8666 - val_loss: 0.9211 - val_accuracy: 0.8656\n","Epoch 24/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 0.4970 - accuracy: 0.8635 - val_loss: 0.9993 - val_accuracy: 0.8475\n","Epoch 25/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4790 - accuracy: 0.8693 - val_loss: 0.9702 - val_accuracy: 0.8541\n","Epoch 26/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4774 - accuracy: 0.8741 - val_loss: 0.8643 - val_accuracy: 0.8609\n","Epoch 27/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4737 - accuracy: 0.8735 - val_loss: 0.8987 - val_accuracy: 0.8475\n","Epoch 28/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.4533 - accuracy: 0.8787 - val_loss: 0.8787 - val_accuracy: 0.8695\n","Epoch 29/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4690 - accuracy: 0.8769 - val_loss: 0.8847 - val_accuracy: 0.8684\n","Epoch 30/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4389 - accuracy: 0.8806 - val_loss: 0.8693 - val_accuracy: 0.8790\n","Epoch 31/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4381 - accuracy: 0.8783 - val_loss: 0.8685 - val_accuracy: 0.8746\n","Epoch 32/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.4326 - accuracy: 0.8854 - val_loss: 0.9640 - val_accuracy: 0.8669\n","Epoch 33/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 0.4396 - accuracy: 0.8804 - val_loss: 0.8674 - val_accuracy: 0.8774\n","Epoch 34/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4087 - accuracy: 0.8894 - val_loss: 0.8593 - val_accuracy: 0.8955\n","Epoch 35/60\n","1846/1846 [==============================] - 4s 2ms/step - loss: 0.4116 - accuracy: 0.8911 - val_loss: 0.8540 - val_accuracy: 0.8799\n","Epoch 36/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.4099 - accuracy: 0.8893 - val_loss: 0.8309 - val_accuracy: 0.8979\n","Epoch 37/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3987 - accuracy: 0.8953 - val_loss: 0.8270 - val_accuracy: 0.8832\n","Epoch 38/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3925 - accuracy: 0.8929 - val_loss: 0.8130 - val_accuracy: 0.8865\n","Epoch 39/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3869 - accuracy: 0.8966 - val_loss: 0.8041 - val_accuracy: 0.8988\n","Epoch 40/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3880 - accuracy: 0.8972 - val_loss: 0.9928 - val_accuracy: 0.8513\n","Epoch 41/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.3773 - accuracy: 0.8988 - val_loss: 0.8101 - val_accuracy: 0.9056\n","Epoch 42/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3663 - accuracy: 0.9031 - val_loss: 0.7621 - val_accuracy: 0.8992\n","Epoch 43/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3805 - accuracy: 0.8991 - val_loss: 0.7551 - val_accuracy: 0.8990\n","Epoch 44/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3647 - accuracy: 0.9051 - val_loss: 0.7602 - val_accuracy: 0.9122\n","Epoch 45/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3658 - accuracy: 0.9015 - val_loss: 0.8557 - val_accuracy: 0.8574\n","Epoch 46/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3582 - accuracy: 0.9062 - val_loss: 0.7387 - val_accuracy: 0.9010\n","Epoch 47/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3451 - accuracy: 0.9099 - val_loss: 0.7554 - val_accuracy: 0.9155\n","Epoch 48/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3507 - accuracy: 0.9068 - val_loss: 0.7219 - val_accuracy: 0.9072\n","Epoch 49/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.3388 - accuracy: 0.9118 - val_loss: 0.8211 - val_accuracy: 0.8821\n","Epoch 50/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3315 - accuracy: 0.9119 - val_loss: 0.6878 - val_accuracy: 0.9285\n","Epoch 51/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3581 - accuracy: 0.9075 - val_loss: 0.8534 - val_accuracy: 0.8684\n","Epoch 52/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.3210 - accuracy: 0.9157 - val_loss: 0.8051 - val_accuracy: 0.8862\n","Epoch 53/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3251 - accuracy: 0.9185 - val_loss: 0.7034 - val_accuracy: 0.9133\n","Epoch 54/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3171 - accuracy: 0.9160 - val_loss: 0.6902 - val_accuracy: 0.9259\n","Epoch 55/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3283 - accuracy: 0.9148 - val_loss: 0.6793 - val_accuracy: 0.9149\n","Epoch 56/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.3284 - accuracy: 0.9138 - val_loss: 0.7624 - val_accuracy: 0.9003\n","Epoch 57/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.3104 - accuracy: 0.9192 - val_loss: 0.7452 - val_accuracy: 0.9098\n","Epoch 58/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.2924 - accuracy: 0.9271 - val_loss: 0.6666 - val_accuracy: 0.9234\n","Epoch 59/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3062 - accuracy: 0.9182 - val_loss: 0.6980 - val_accuracy: 0.9201\n","Epoch 60/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.2956 - accuracy: 0.9250 - val_loss: 0.8333 - val_accuracy: 0.8546\n","Fold: 2\n","Epoch 1/60\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"]},{"output_type":"stream","name":"stdout","text":["1846/1846 [==============================] - 8s 4ms/step - loss: 4.5818 - accuracy: 0.0859 - val_loss: 3.9837 - val_accuracy: 0.1087\n","Epoch 2/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.4257 - accuracy: 0.2769 - val_loss: 3.2273 - val_accuracy: 0.3804\n","Epoch 3/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.5715 - accuracy: 0.4717 - val_loss: 2.5156 - val_accuracy: 0.5375\n","Epoch 4/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.9161 - accuracy: 0.5873 - val_loss: 2.1432 - val_accuracy: 0.5732\n","Epoch 5/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.5344 - accuracy: 0.6466 - val_loss: 1.9263 - val_accuracy: 0.6640\n","Epoch 6/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.2895 - accuracy: 0.6938 - val_loss: 1.7388 - val_accuracy: 0.7083\n","Epoch 7/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.1212 - accuracy: 0.7288 - val_loss: 1.6223 - val_accuracy: 0.7327\n","Epoch 8/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.0079 - accuracy: 0.7517 - val_loss: 1.5568 - val_accuracy: 0.7413\n","Epoch 9/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9304 - accuracy: 0.7684 - val_loss: 1.4252 - val_accuracy: 0.7705\n","Epoch 10/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8654 - accuracy: 0.7763 - val_loss: 1.4247 - val_accuracy: 0.7608\n","Epoch 11/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8232 - accuracy: 0.7850 - val_loss: 1.3015 - val_accuracy: 0.7978\n","Epoch 12/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7745 - accuracy: 0.7957 - val_loss: 1.2753 - val_accuracy: 0.7903\n","Epoch 13/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7377 - accuracy: 0.8005 - val_loss: 1.2314 - val_accuracy: 0.7954\n","Epoch 14/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7087 - accuracy: 0.8071 - val_loss: 1.1931 - val_accuracy: 0.8024\n","Epoch 15/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6781 - accuracy: 0.8130 - val_loss: 1.1378 - val_accuracy: 0.7925\n","Epoch 16/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6569 - accuracy: 0.8141 - val_loss: 1.1068 - val_accuracy: 0.7980\n","Epoch 17/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6382 - accuracy: 0.8211 - val_loss: 1.1248 - val_accuracy: 0.8187\n","Epoch 18/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6160 - accuracy: 0.8216 - val_loss: 1.1096 - val_accuracy: 0.8064\n","Epoch 19/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6078 - accuracy: 0.8269 - val_loss: 1.0756 - val_accuracy: 0.8128\n","Epoch 20/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5869 - accuracy: 0.8353 - val_loss: 1.0682 - val_accuracy: 0.8240\n","Epoch 21/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5757 - accuracy: 0.8355 - val_loss: 1.0202 - val_accuracy: 0.8475\n","Epoch 22/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5633 - accuracy: 0.8388 - val_loss: 1.0213 - val_accuracy: 0.8286\n","Epoch 23/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5551 - accuracy: 0.8398 - val_loss: 1.0261 - val_accuracy: 0.8339\n","Epoch 24/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5397 - accuracy: 0.8455 - val_loss: 1.0019 - val_accuracy: 0.8352\n","Epoch 25/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5277 - accuracy: 0.8477 - val_loss: 0.9795 - val_accuracy: 0.8405\n","Epoch 26/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5294 - accuracy: 0.8477 - val_loss: 0.9649 - val_accuracy: 0.8359\n","Epoch 27/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5037 - accuracy: 0.8551 - val_loss: 0.9918 - val_accuracy: 0.8436\n","Epoch 28/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5072 - accuracy: 0.8526 - val_loss: 0.9773 - val_accuracy: 0.8429\n","Epoch 29/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4987 - accuracy: 0.8568 - val_loss: 1.0153 - val_accuracy: 0.8416\n","Epoch 30/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4795 - accuracy: 0.8640 - val_loss: 0.9028 - val_accuracy: 0.8645\n","Epoch 31/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4792 - accuracy: 0.8611 - val_loss: 0.9325 - val_accuracy: 0.8530\n","Epoch 32/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4695 - accuracy: 0.8663 - val_loss: 0.9900 - val_accuracy: 0.8260\n","Epoch 33/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4714 - accuracy: 0.8611 - val_loss: 0.9059 - val_accuracy: 0.8581\n","Epoch 34/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4562 - accuracy: 0.8706 - val_loss: 0.9148 - val_accuracy: 0.8462\n","Epoch 35/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4495 - accuracy: 0.8746 - val_loss: 0.9085 - val_accuracy: 0.8471\n","Epoch 36/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4626 - accuracy: 0.8667 - val_loss: 0.8398 - val_accuracy: 0.8733\n","Epoch 37/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4369 - accuracy: 0.8702 - val_loss: 0.8941 - val_accuracy: 0.8495\n","Epoch 38/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4366 - accuracy: 0.8737 - val_loss: 0.8773 - val_accuracy: 0.8605\n","Epoch 39/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4311 - accuracy: 0.8751 - val_loss: 0.8414 - val_accuracy: 0.8664\n","Epoch 40/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4340 - accuracy: 0.8729 - val_loss: 0.8738 - val_accuracy: 0.8636\n","Epoch 41/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4118 - accuracy: 0.8798 - val_loss: 0.8728 - val_accuracy: 0.8519\n","Epoch 42/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4241 - accuracy: 0.8774 - val_loss: 0.8690 - val_accuracy: 0.8645\n","Epoch 43/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4120 - accuracy: 0.8840 - val_loss: 0.8782 - val_accuracy: 0.8548\n","Epoch 44/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4072 - accuracy: 0.8805 - val_loss: 0.8451 - val_accuracy: 0.8750\n","Epoch 45/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3912 - accuracy: 0.8885 - val_loss: 0.8239 - val_accuracy: 0.8933\n","Epoch 46/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.4023 - accuracy: 0.8857 - val_loss: 0.8255 - val_accuracy: 0.8752\n","Epoch 47/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4031 - accuracy: 0.8850 - val_loss: 0.8562 - val_accuracy: 0.8636\n","Epoch 48/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.3944 - accuracy: 0.8892 - val_loss: 0.9233 - val_accuracy: 0.8671\n","Epoch 49/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3857 - accuracy: 0.8890 - val_loss: 0.7953 - val_accuracy: 0.9034\n","Epoch 50/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3891 - accuracy: 0.8909 - val_loss: 0.8852 - val_accuracy: 0.8686\n","Epoch 51/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3785 - accuracy: 0.8950 - val_loss: 0.8096 - val_accuracy: 0.8827\n","Epoch 52/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3835 - accuracy: 0.8917 - val_loss: 0.7576 - val_accuracy: 0.8924\n","Epoch 53/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3726 - accuracy: 0.8968 - val_loss: 0.8745 - val_accuracy: 0.8689\n","Epoch 54/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3781 - accuracy: 0.8947 - val_loss: 0.7918 - val_accuracy: 0.8827\n","Epoch 55/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3656 - accuracy: 0.8983 - val_loss: 0.8144 - val_accuracy: 0.8851\n","Epoch 56/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3675 - accuracy: 0.8964 - val_loss: 0.7820 - val_accuracy: 0.8882\n","Epoch 57/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3620 - accuracy: 0.8991 - val_loss: 0.7649 - val_accuracy: 0.9063\n","Epoch 58/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3500 - accuracy: 0.9018 - val_loss: 0.8364 - val_accuracy: 0.8524\n","Epoch 59/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3520 - accuracy: 0.9014 - val_loss: 0.7843 - val_accuracy: 0.8827\n","Epoch 60/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3543 - accuracy: 0.9007 - val_loss: 0.7900 - val_accuracy: 0.8757\n","Average Validation Accuracy: 0.8833464682102203\n","Average Validation Loss: 0.514155775308609\n","Average Test Accuracy: 0.8772388994693756\n","Final Test Accuracy for each fold: 0.8926807641983032\n","Number of input features: 9\n","Fold: 1\n","Epoch 1/60\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"]},{"output_type":"stream","name":"stdout","text":["1846/1846 [==============================] - 6s 3ms/step - loss: 4.5267 - accuracy: 0.0926 - val_loss: 4.0622 - val_accuracy: 0.1353\n","Epoch 2/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 3.5877 - accuracy: 0.2196 - val_loss: 3.3832 - val_accuracy: 0.3776\n","Epoch 3/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.7876 - accuracy: 0.4100 - val_loss: 2.6170 - val_accuracy: 0.4922\n","Epoch 4/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.0493 - accuracy: 0.5558 - val_loss: 2.0906 - val_accuracy: 0.5459\n","Epoch 5/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 1.5910 - accuracy: 0.6432 - val_loss: 1.7529 - val_accuracy: 0.6631\n","Epoch 6/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.2978 - accuracy: 0.6956 - val_loss: 1.5350 - val_accuracy: 0.7039\n","Epoch 7/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.1185 - accuracy: 0.7268 - val_loss: 1.4325 - val_accuracy: 0.7188\n","Epoch 8/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.9942 - accuracy: 0.7517 - val_loss: 1.3484 - val_accuracy: 0.7402\n","Epoch 9/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9061 - accuracy: 0.7706 - val_loss: 1.2772 - val_accuracy: 0.7589\n","Epoch 10/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8401 - accuracy: 0.7844 - val_loss: 1.2043 - val_accuracy: 0.7833\n","Epoch 11/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7913 - accuracy: 0.7924 - val_loss: 1.1624 - val_accuracy: 0.7837\n","Epoch 12/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7488 - accuracy: 0.8031 - val_loss: 1.1497 - val_accuracy: 0.7978\n","Epoch 13/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.7001 - accuracy: 0.8148 - val_loss: 1.0804 - val_accuracy: 0.8191\n","Epoch 14/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.6686 - accuracy: 0.8257 - val_loss: 1.1136 - val_accuracy: 0.8018\n","Epoch 15/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6445 - accuracy: 0.8296 - val_loss: 1.0776 - val_accuracy: 0.8079\n","Epoch 16/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6201 - accuracy: 0.8325 - val_loss: 1.0543 - val_accuracy: 0.8227\n","Epoch 17/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5942 - accuracy: 0.8448 - val_loss: 0.9986 - val_accuracy: 0.8224\n","Epoch 18/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5917 - accuracy: 0.8451 - val_loss: 0.9455 - val_accuracy: 0.8506\n","Epoch 19/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5547 - accuracy: 0.8515 - val_loss: 0.9590 - val_accuracy: 0.8499\n","Epoch 20/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5437 - accuracy: 0.8571 - val_loss: 0.9758 - val_accuracy: 0.8101\n","Epoch 21/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5294 - accuracy: 0.8573 - val_loss: 0.9641 - val_accuracy: 0.8570\n","Epoch 22/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5202 - accuracy: 0.8596 - val_loss: 0.9022 - val_accuracy: 0.8495\n","Epoch 23/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4916 - accuracy: 0.8678 - val_loss: 0.9282 - val_accuracy: 0.8554\n","Epoch 24/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4906 - accuracy: 0.8681 - val_loss: 0.8961 - val_accuracy: 0.8682\n","Epoch 25/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4763 - accuracy: 0.8733 - val_loss: 0.8784 - val_accuracy: 0.8772\n","Epoch 26/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4655 - accuracy: 0.8772 - val_loss: 0.9388 - val_accuracy: 0.8367\n","Epoch 27/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4652 - accuracy: 0.8733 - val_loss: 0.8584 - val_accuracy: 0.8719\n","Epoch 28/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.4473 - accuracy: 0.8797 - val_loss: 0.8397 - val_accuracy: 0.8851\n","Epoch 29/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4345 - accuracy: 0.8820 - val_loss: 0.8039 - val_accuracy: 0.8942\n","Epoch 30/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.4196 - accuracy: 0.8855 - val_loss: 0.8412 - val_accuracy: 0.8570\n","Epoch 31/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4128 - accuracy: 0.8878 - val_loss: 0.8640 - val_accuracy: 0.8616\n","Epoch 32/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4132 - accuracy: 0.8885 - val_loss: 0.7722 - val_accuracy: 0.8900\n","Epoch 33/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3993 - accuracy: 0.8915 - val_loss: 0.7882 - val_accuracy: 0.9047\n","Epoch 34/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3907 - accuracy: 0.8957 - val_loss: 0.8101 - val_accuracy: 0.8770\n","Epoch 35/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3816 - accuracy: 0.8948 - val_loss: 0.8182 - val_accuracy: 0.8702\n","Epoch 36/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3839 - accuracy: 0.8944 - val_loss: 0.8218 - val_accuracy: 0.8748\n","Epoch 37/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.3664 - accuracy: 0.9002 - val_loss: 0.8110 - val_accuracy: 0.8583\n","Epoch 38/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3628 - accuracy: 0.9045 - val_loss: 0.7286 - val_accuracy: 0.9010\n","Epoch 39/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3563 - accuracy: 0.9068 - val_loss: 0.7170 - val_accuracy: 0.9118\n","Epoch 40/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3572 - accuracy: 0.9047 - val_loss: 0.7379 - val_accuracy: 0.9010\n","Epoch 41/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3457 - accuracy: 0.9103 - val_loss: 0.7314 - val_accuracy: 0.8999\n","Epoch 42/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3435 - accuracy: 0.9090 - val_loss: 0.6921 - val_accuracy: 0.9232\n","Epoch 43/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3290 - accuracy: 0.9101 - val_loss: 0.7349 - val_accuracy: 0.8986\n","Epoch 44/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3471 - accuracy: 0.9059 - val_loss: 0.7027 - val_accuracy: 0.9140\n","Epoch 45/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3150 - accuracy: 0.9170 - val_loss: 0.7202 - val_accuracy: 0.8972\n","Epoch 46/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3305 - accuracy: 0.9135 - val_loss: 0.7036 - val_accuracy: 0.9120\n","Epoch 47/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3078 - accuracy: 0.9207 - val_loss: 0.6723 - val_accuracy: 0.9138\n","Epoch 48/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3233 - accuracy: 0.9139 - val_loss: 0.6634 - val_accuracy: 0.9122\n","Epoch 49/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3077 - accuracy: 0.9227 - val_loss: 0.7166 - val_accuracy: 0.9116\n","Epoch 50/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3057 - accuracy: 0.9192 - val_loss: 0.6631 - val_accuracy: 0.9217\n","Epoch 51/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.2931 - accuracy: 0.9236 - val_loss: 0.6683 - val_accuracy: 0.9135\n","Epoch 52/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.2989 - accuracy: 0.9237 - val_loss: 0.7289 - val_accuracy: 0.8970\n","Epoch 53/60\n","1846/1846 [==============================] - 6s 4ms/step - loss: 0.2841 - accuracy: 0.9269 - val_loss: 0.6667 - val_accuracy: 0.9230\n","Epoch 54/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.2892 - accuracy: 0.9269 - val_loss: 0.6328 - val_accuracy: 0.9327\n","Epoch 55/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.2663 - accuracy: 0.9305 - val_loss: 0.6449 - val_accuracy: 0.9245\n","Epoch 56/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.2848 - accuracy: 0.9260 - val_loss: 0.6174 - val_accuracy: 0.9410\n","Epoch 57/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.2741 - accuracy: 0.9288 - val_loss: 0.7273 - val_accuracy: 0.8928\n","Epoch 58/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.2700 - accuracy: 0.9297 - val_loss: 0.6572 - val_accuracy: 0.9226\n","Epoch 59/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.2619 - accuracy: 0.9320 - val_loss: 0.6388 - val_accuracy: 0.9292\n","Epoch 60/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.2629 - accuracy: 0.9339 - val_loss: 0.6464 - val_accuracy: 0.9318\n","Fold: 2\n","Epoch 1/60\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"]},{"output_type":"stream","name":"stdout","text":["1846/1846 [==============================] - 6s 3ms/step - loss: 4.5553 - accuracy: 0.0866 - val_loss: 4.0224 - val_accuracy: 0.1292\n","Epoch 2/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 3.4571 - accuracy: 0.2683 - val_loss: 3.1148 - val_accuracy: 0.4145\n","Epoch 3/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.5017 - accuracy: 0.4887 - val_loss: 2.4333 - val_accuracy: 0.5443\n","Epoch 4/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.8929 - accuracy: 0.5948 - val_loss: 2.0128 - val_accuracy: 0.6378\n","Epoch 5/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.5141 - accuracy: 0.6655 - val_loss: 1.7568 - val_accuracy: 0.6920\n","Epoch 6/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.2754 - accuracy: 0.6971 - val_loss: 1.5922 - val_accuracy: 0.7160\n","Epoch 7/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.1162 - accuracy: 0.7292 - val_loss: 1.4740 - val_accuracy: 0.7204\n","Epoch 8/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0043 - accuracy: 0.7525 - val_loss: 1.3694 - val_accuracy: 0.7417\n","Epoch 9/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9182 - accuracy: 0.7709 - val_loss: 1.2526 - val_accuracy: 0.7870\n","Epoch 10/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8496 - accuracy: 0.7800 - val_loss: 1.2028 - val_accuracy: 0.8002\n","Epoch 11/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7946 - accuracy: 0.7934 - val_loss: 1.2238 - val_accuracy: 0.7901\n","Epoch 12/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7519 - accuracy: 0.8019 - val_loss: 1.1516 - val_accuracy: 0.7745\n","Epoch 13/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7164 - accuracy: 0.8071 - val_loss: 1.0794 - val_accuracy: 0.7976\n","Epoch 14/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6857 - accuracy: 0.8152 - val_loss: 1.0580 - val_accuracy: 0.7857\n","Epoch 15/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6549 - accuracy: 0.8251 - val_loss: 1.0408 - val_accuracy: 0.8161\n","Epoch 16/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.6345 - accuracy: 0.8289 - val_loss: 0.9616 - val_accuracy: 0.8414\n","Epoch 17/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6116 - accuracy: 0.8324 - val_loss: 0.9979 - val_accuracy: 0.8218\n","Epoch 18/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5835 - accuracy: 0.8380 - val_loss: 0.9414 - val_accuracy: 0.8493\n","Epoch 19/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5736 - accuracy: 0.8443 - val_loss: 0.9304 - val_accuracy: 0.8425\n","Epoch 20/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5482 - accuracy: 0.8491 - val_loss: 0.9127 - val_accuracy: 0.8350\n","Epoch 21/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5320 - accuracy: 0.8527 - val_loss: 0.9017 - val_accuracy: 0.8321\n","Epoch 22/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5215 - accuracy: 0.8582 - val_loss: 0.8714 - val_accuracy: 0.8365\n","Epoch 23/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.5067 - accuracy: 0.8611 - val_loss: 0.9036 - val_accuracy: 0.8590\n","Epoch 24/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5025 - accuracy: 0.8626 - val_loss: 0.8365 - val_accuracy: 0.8680\n","Epoch 25/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4812 - accuracy: 0.8685 - val_loss: 0.8218 - val_accuracy: 0.8491\n","Epoch 26/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4739 - accuracy: 0.8714 - val_loss: 0.8110 - val_accuracy: 0.8777\n","Epoch 27/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4702 - accuracy: 0.8736 - val_loss: 0.8449 - val_accuracy: 0.8568\n","Epoch 28/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4550 - accuracy: 0.8730 - val_loss: 0.7898 - val_accuracy: 0.8601\n","Epoch 29/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4410 - accuracy: 0.8818 - val_loss: 0.8900 - val_accuracy: 0.8552\n","Epoch 30/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4379 - accuracy: 0.8811 - val_loss: 0.7744 - val_accuracy: 0.8814\n","Epoch 31/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4288 - accuracy: 0.8837 - val_loss: 0.8369 - val_accuracy: 0.8587\n","Epoch 32/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4162 - accuracy: 0.8895 - val_loss: 0.7385 - val_accuracy: 0.8814\n","Epoch 33/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4111 - accuracy: 0.8887 - val_loss: 0.7539 - val_accuracy: 0.8752\n","Epoch 34/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4002 - accuracy: 0.8908 - val_loss: 0.8145 - val_accuracy: 0.8605\n","Epoch 35/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3949 - accuracy: 0.8948 - val_loss: 0.8866 - val_accuracy: 0.8680\n","Epoch 36/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3944 - accuracy: 0.8912 - val_loss: 0.7374 - val_accuracy: 0.8937\n","Epoch 37/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3711 - accuracy: 0.9007 - val_loss: 0.7262 - val_accuracy: 0.8812\n","Epoch 38/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3851 - accuracy: 0.8936 - val_loss: 0.6894 - val_accuracy: 0.9045\n","Epoch 39/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3708 - accuracy: 0.8978 - val_loss: 0.6748 - val_accuracy: 0.9012\n","Epoch 40/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3636 - accuracy: 0.9031 - val_loss: 0.7315 - val_accuracy: 0.8750\n","Epoch 41/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3509 - accuracy: 0.9072 - val_loss: 0.7530 - val_accuracy: 0.8926\n","Epoch 42/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3683 - accuracy: 0.9009 - val_loss: 0.6749 - val_accuracy: 0.8928\n","Epoch 43/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3497 - accuracy: 0.9063 - val_loss: 0.6564 - val_accuracy: 0.9149\n","Epoch 44/60\n","1846/1846 [==============================] - 6s 4ms/step - loss: 0.3582 - accuracy: 0.9058 - val_loss: 0.6838 - val_accuracy: 0.8948\n","Epoch 45/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3306 - accuracy: 0.9118 - val_loss: 0.6990 - val_accuracy: 0.9008\n","Epoch 46/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3336 - accuracy: 0.9108 - val_loss: 0.6821 - val_accuracy: 0.9001\n","Epoch 47/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3395 - accuracy: 0.9091 - val_loss: 0.6435 - val_accuracy: 0.9058\n","Epoch 48/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3257 - accuracy: 0.9117 - val_loss: 0.6538 - val_accuracy: 0.8964\n","Epoch 49/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3271 - accuracy: 0.9134 - val_loss: 0.6388 - val_accuracy: 0.9001\n","Epoch 50/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3123 - accuracy: 0.9173 - val_loss: 0.6899 - val_accuracy: 0.8772\n","Epoch 51/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3089 - accuracy: 0.9161 - val_loss: 0.6505 - val_accuracy: 0.9102\n","Epoch 52/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3276 - accuracy: 0.9142 - val_loss: 0.6817 - val_accuracy: 0.8975\n","Epoch 53/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.2974 - accuracy: 0.9171 - val_loss: 0.6052 - val_accuracy: 0.9199\n","Epoch 54/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3030 - accuracy: 0.9209 - val_loss: 0.6074 - val_accuracy: 0.9155\n","Epoch 55/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.2960 - accuracy: 0.9227 - val_loss: 0.7157 - val_accuracy: 0.8851\n","Epoch 56/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.2999 - accuracy: 0.9185 - val_loss: 0.6030 - val_accuracy: 0.9296\n","Epoch 57/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.2914 - accuracy: 0.9237 - val_loss: 0.6106 - val_accuracy: 0.9201\n","Epoch 58/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.2843 - accuracy: 0.9246 - val_loss: 0.6363 - val_accuracy: 0.9041\n","Epoch 59/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.2909 - accuracy: 0.9226 - val_loss: 0.6248 - val_accuracy: 0.9091\n","Epoch 60/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.2755 - accuracy: 0.9273 - val_loss: 0.8435 - val_accuracy: 0.8629\n","Average Validation Accuracy: 0.9064723551273346\n","Average Validation Loss: 0.49939392507076263\n","Average Test Accuracy: 0.9052480161190033\n","Final Test Accuracy for each fold: 0.9338836669921875\n","Number of input features: 10\n","Fold: 1\n","Epoch 1/60\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"]},{"output_type":"stream","name":"stdout","text":["1846/1846 [==============================] - 6s 3ms/step - loss: 4.5272 - accuracy: 0.0819 - val_loss: 4.0794 - val_accuracy: 0.1188\n","Epoch 2/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 3.5848 - accuracy: 0.1946 - val_loss: 3.3948 - val_accuracy: 0.2405\n","Epoch 3/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.7806 - accuracy: 0.4226 - val_loss: 2.6840 - val_accuracy: 0.4757\n","Epoch 4/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.0973 - accuracy: 0.5615 - val_loss: 2.1755 - val_accuracy: 0.5844\n","Epoch 5/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.6565 - accuracy: 0.6259 - val_loss: 1.8247 - val_accuracy: 0.6438\n","Epoch 6/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.3692 - accuracy: 0.6809 - val_loss: 1.6121 - val_accuracy: 0.6942\n","Epoch 7/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.1621 - accuracy: 0.7262 - val_loss: 1.4724 - val_accuracy: 0.7230\n","Epoch 8/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0232 - accuracy: 0.7530 - val_loss: 1.3712 - val_accuracy: 0.7580\n","Epoch 9/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.9167 - accuracy: 0.7692 - val_loss: 1.3066 - val_accuracy: 0.7833\n","Epoch 10/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8442 - accuracy: 0.7850 - val_loss: 1.2656 - val_accuracy: 0.7714\n","Epoch 11/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7889 - accuracy: 0.7988 - val_loss: 1.2057 - val_accuracy: 0.7800\n","Epoch 12/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7393 - accuracy: 0.8087 - val_loss: 1.1328 - val_accuracy: 0.7974\n","Epoch 13/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7067 - accuracy: 0.8156 - val_loss: 1.2000 - val_accuracy: 0.7749\n","Epoch 14/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6762 - accuracy: 0.8199 - val_loss: 1.0982 - val_accuracy: 0.8130\n","Epoch 15/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6523 - accuracy: 0.8212 - val_loss: 1.1275 - val_accuracy: 0.8147\n","Epoch 16/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.6309 - accuracy: 0.8285 - val_loss: 1.0604 - val_accuracy: 0.8154\n","Epoch 17/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.6122 - accuracy: 0.8360 - val_loss: 1.0363 - val_accuracy: 0.8315\n","Epoch 18/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5914 - accuracy: 0.8387 - val_loss: 1.0463 - val_accuracy: 0.8266\n","Epoch 19/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5825 - accuracy: 0.8423 - val_loss: 1.0378 - val_accuracy: 0.8321\n","Epoch 20/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.5586 - accuracy: 0.8491 - val_loss: 1.0495 - val_accuracy: 0.8244\n","Epoch 21/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.5540 - accuracy: 0.8490 - val_loss: 1.0239 - val_accuracy: 0.8312\n","Epoch 22/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5359 - accuracy: 0.8531 - val_loss: 0.9735 - val_accuracy: 0.8394\n","Epoch 23/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5248 - accuracy: 0.8576 - val_loss: 0.9604 - val_accuracy: 0.8574\n","Epoch 24/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5198 - accuracy: 0.8583 - val_loss: 0.9409 - val_accuracy: 0.8493\n","Epoch 25/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4974 - accuracy: 0.8638 - val_loss: 0.9616 - val_accuracy: 0.8455\n","Epoch 26/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4973 - accuracy: 0.8643 - val_loss: 0.9527 - val_accuracy: 0.8436\n","Epoch 27/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4952 - accuracy: 0.8651 - val_loss: 0.9592 - val_accuracy: 0.8167\n","Epoch 28/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4821 - accuracy: 0.8662 - val_loss: 0.9167 - val_accuracy: 0.8552\n","Epoch 29/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4679 - accuracy: 0.8733 - val_loss: 0.8821 - val_accuracy: 0.8616\n","Epoch 30/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4649 - accuracy: 0.8751 - val_loss: 0.8800 - val_accuracy: 0.8693\n","Epoch 31/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4491 - accuracy: 0.8791 - val_loss: 0.9741 - val_accuracy: 0.8398\n","Epoch 32/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4542 - accuracy: 0.8759 - val_loss: 0.8582 - val_accuracy: 0.8673\n","Epoch 33/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4448 - accuracy: 0.8799 - val_loss: 0.8694 - val_accuracy: 0.8746\n","Epoch 34/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4358 - accuracy: 0.8803 - val_loss: 0.9241 - val_accuracy: 0.8686\n","Epoch 35/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4401 - accuracy: 0.8810 - val_loss: 0.8507 - val_accuracy: 0.8741\n","Epoch 36/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4210 - accuracy: 0.8866 - val_loss: 0.8627 - val_accuracy: 0.8601\n","Epoch 37/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4204 - accuracy: 0.8851 - val_loss: 0.8480 - val_accuracy: 0.8722\n","Epoch 38/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4133 - accuracy: 0.8841 - val_loss: 0.8426 - val_accuracy: 0.8774\n","Epoch 39/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4153 - accuracy: 0.8835 - val_loss: 0.8499 - val_accuracy: 0.8724\n","Epoch 40/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4066 - accuracy: 0.8869 - val_loss: 0.8550 - val_accuracy: 0.8612\n","Epoch 41/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4078 - accuracy: 0.8854 - val_loss: 0.8386 - val_accuracy: 0.8735\n","Epoch 42/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3980 - accuracy: 0.8872 - val_loss: 0.7979 - val_accuracy: 0.8772\n","Epoch 43/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3919 - accuracy: 0.8909 - val_loss: 0.8134 - val_accuracy: 0.8713\n","Epoch 44/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3897 - accuracy: 0.8917 - val_loss: 0.8199 - val_accuracy: 0.8671\n","Epoch 45/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3882 - accuracy: 0.8903 - val_loss: 0.7763 - val_accuracy: 0.8997\n","Epoch 46/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3776 - accuracy: 0.8945 - val_loss: 0.7716 - val_accuracy: 0.8997\n","Epoch 47/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3770 - accuracy: 0.8974 - val_loss: 0.7721 - val_accuracy: 0.8825\n","Epoch 48/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3694 - accuracy: 0.8991 - val_loss: 0.7635 - val_accuracy: 0.8801\n","Epoch 49/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3638 - accuracy: 0.8993 - val_loss: 0.8127 - val_accuracy: 0.8744\n","Epoch 50/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3689 - accuracy: 0.8978 - val_loss: 0.7262 - val_accuracy: 0.9094\n","Epoch 51/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3462 - accuracy: 0.9077 - val_loss: 0.7481 - val_accuracy: 0.9054\n","Epoch 52/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3629 - accuracy: 0.9040 - val_loss: 0.7501 - val_accuracy: 0.9058\n","Epoch 53/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3567 - accuracy: 0.9019 - val_loss: 0.7651 - val_accuracy: 0.8913\n","Epoch 54/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3497 - accuracy: 0.9028 - val_loss: 0.7335 - val_accuracy: 0.9094\n","Epoch 55/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3485 - accuracy: 0.9038 - val_loss: 0.7439 - val_accuracy: 0.9039\n","Epoch 56/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3408 - accuracy: 0.9047 - val_loss: 0.7932 - val_accuracy: 0.8783\n","Epoch 57/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3448 - accuracy: 0.9077 - val_loss: 0.7177 - val_accuracy: 0.8997\n","Epoch 58/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3406 - accuracy: 0.9081 - val_loss: 0.7489 - val_accuracy: 0.8893\n","Epoch 59/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3300 - accuracy: 0.9125 - val_loss: 0.7812 - val_accuracy: 0.8803\n","Epoch 60/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3284 - accuracy: 0.9108 - val_loss: 0.7885 - val_accuracy: 0.8970\n","Fold: 2\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 4.6895 - accuracy: 0.0668 - val_loss: 4.2263 - val_accuracy: 0.0821\n","Epoch 2/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 3.7624 - accuracy: 0.1789 - val_loss: 3.6260 - val_accuracy: 0.2583\n","Epoch 3/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.1041 - accuracy: 0.3431 - val_loss: 3.0455 - val_accuracy: 0.4139\n","Epoch 4/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.5142 - accuracy: 0.4699 - val_loss: 2.6114 - val_accuracy: 0.5373\n","Epoch 5/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.0514 - accuracy: 0.5592 - val_loss: 2.3170 - val_accuracy: 0.6101\n","Epoch 6/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.7070 - accuracy: 0.6169 - val_loss: 2.1257 - val_accuracy: 0.6103\n","Epoch 7/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.4743 - accuracy: 0.6538 - val_loss: 1.9558 - val_accuracy: 0.6596\n","Epoch 8/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.3079 - accuracy: 0.6873 - val_loss: 1.8681 - val_accuracy: 0.6950\n","Epoch 9/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.1871 - accuracy: 0.7086 - val_loss: 1.7120 - val_accuracy: 0.7228\n","Epoch 10/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0883 - accuracy: 0.7343 - val_loss: 1.6714 - val_accuracy: 0.7142\n","Epoch 11/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0167 - accuracy: 0.7491 - val_loss: 1.6450 - val_accuracy: 0.7373\n","Epoch 12/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9501 - accuracy: 0.7628 - val_loss: 1.5585 - val_accuracy: 0.7626\n","Epoch 13/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8997 - accuracy: 0.7744 - val_loss: 1.5146 - val_accuracy: 0.7648\n","Epoch 14/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8688 - accuracy: 0.7808 - val_loss: 1.4737 - val_accuracy: 0.7848\n","Epoch 15/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8260 - accuracy: 0.7900 - val_loss: 1.4772 - val_accuracy: 0.7952\n","Epoch 16/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8006 - accuracy: 0.7976 - val_loss: 1.4022 - val_accuracy: 0.8037\n","Epoch 17/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7724 - accuracy: 0.7982 - val_loss: 1.3962 - val_accuracy: 0.7894\n","Epoch 18/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7492 - accuracy: 0.8075 - val_loss: 1.3888 - val_accuracy: 0.7890\n","Epoch 19/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7284 - accuracy: 0.8119 - val_loss: 1.3536 - val_accuracy: 0.8013\n","Epoch 20/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7147 - accuracy: 0.8175 - val_loss: 1.3491 - val_accuracy: 0.8095\n","Epoch 21/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6845 - accuracy: 0.8222 - val_loss: 1.3846 - val_accuracy: 0.8073\n","Epoch 22/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6742 - accuracy: 0.8254 - val_loss: 1.2847 - val_accuracy: 0.8427\n","Epoch 23/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6691 - accuracy: 0.8262 - val_loss: 1.3828 - val_accuracy: 0.7613\n","Epoch 24/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6576 - accuracy: 0.8294 - val_loss: 1.3121 - val_accuracy: 0.8018\n","Epoch 25/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6330 - accuracy: 0.8380 - val_loss: 1.2628 - val_accuracy: 0.8268\n","Epoch 26/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6186 - accuracy: 0.8369 - val_loss: 1.2863 - val_accuracy: 0.8136\n","Epoch 27/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6089 - accuracy: 0.8404 - val_loss: 1.2314 - val_accuracy: 0.8392\n","Epoch 28/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6059 - accuracy: 0.8370 - val_loss: 1.2900 - val_accuracy: 0.8002\n","Epoch 29/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5912 - accuracy: 0.8433 - val_loss: 1.2566 - val_accuracy: 0.8218\n","Epoch 30/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5865 - accuracy: 0.8459 - val_loss: 1.2656 - val_accuracy: 0.8238\n","Epoch 31/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5741 - accuracy: 0.8473 - val_loss: 1.2008 - val_accuracy: 0.8372\n","Epoch 32/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5632 - accuracy: 0.8524 - val_loss: 1.2025 - val_accuracy: 0.8154\n","Epoch 33/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5637 - accuracy: 0.8500 - val_loss: 1.2297 - val_accuracy: 0.8407\n","Epoch 34/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5499 - accuracy: 0.8501 - val_loss: 1.2078 - val_accuracy: 0.8464\n","Epoch 35/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5528 - accuracy: 0.8523 - val_loss: 1.2042 - val_accuracy: 0.8453\n","Epoch 36/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5488 - accuracy: 0.8553 - val_loss: 1.1583 - val_accuracy: 0.8466\n","Epoch 37/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5339 - accuracy: 0.8579 - val_loss: 1.1503 - val_accuracy: 0.8396\n","Epoch 38/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5336 - accuracy: 0.8563 - val_loss: 1.1418 - val_accuracy: 0.8433\n","Epoch 39/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5365 - accuracy: 0.8580 - val_loss: 1.1225 - val_accuracy: 0.8576\n","Epoch 40/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5095 - accuracy: 0.8639 - val_loss: 1.1978 - val_accuracy: 0.8385\n","Epoch 41/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5226 - accuracy: 0.8561 - val_loss: 1.1591 - val_accuracy: 0.8464\n","Epoch 42/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.5079 - accuracy: 0.8672 - val_loss: 1.1226 - val_accuracy: 0.8603\n","Epoch 43/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5063 - accuracy: 0.8649 - val_loss: 1.0954 - val_accuracy: 0.8598\n","Epoch 44/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5073 - accuracy: 0.8640 - val_loss: 1.1427 - val_accuracy: 0.8372\n","Epoch 45/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4897 - accuracy: 0.8708 - val_loss: 1.0847 - val_accuracy: 0.8726\n","Epoch 46/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5086 - accuracy: 0.8629 - val_loss: 1.2403 - val_accuracy: 0.8081\n","Epoch 47/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5013 - accuracy: 0.8678 - val_loss: 1.1024 - val_accuracy: 0.8616\n","Epoch 48/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4849 - accuracy: 0.8725 - val_loss: 1.0596 - val_accuracy: 0.8752\n","Epoch 49/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4919 - accuracy: 0.8691 - val_loss: 1.0801 - val_accuracy: 0.8651\n","Epoch 50/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4810 - accuracy: 0.8724 - val_loss: 1.1340 - val_accuracy: 0.8517\n","Epoch 51/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4878 - accuracy: 0.8703 - val_loss: 1.0958 - val_accuracy: 0.8645\n","Epoch 52/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4734 - accuracy: 0.8737 - val_loss: 1.1807 - val_accuracy: 0.8231\n","Epoch 53/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4570 - accuracy: 0.8808 - val_loss: 1.0622 - val_accuracy: 0.8653\n","Epoch 54/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4583 - accuracy: 0.8794 - val_loss: 1.0478 - val_accuracy: 0.8902\n","Epoch 55/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4635 - accuracy: 0.8825 - val_loss: 1.0639 - val_accuracy: 0.8673\n","Epoch 56/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4493 - accuracy: 0.8812 - val_loss: 1.1062 - val_accuracy: 0.8598\n","Epoch 57/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4555 - accuracy: 0.8791 - val_loss: 1.0772 - val_accuracy: 0.8664\n","Epoch 58/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.4614 - accuracy: 0.8779 - val_loss: 1.0335 - val_accuracy: 0.8603\n","Epoch 59/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4641 - accuracy: 0.8761 - val_loss: 1.0612 - val_accuracy: 0.8565\n","Epoch 60/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4391 - accuracy: 0.8858 - val_loss: 1.0247 - val_accuracy: 0.8821\n","Average Validation Accuracy: 0.899865448474884\n","Average Validation Loss: 0.5562391579151154\n","Average Test Accuracy: 0.8932704329490662\n","Final Test Accuracy for each fold: 0.9000515937805176\n","Number of input features: 11\n","Fold: 1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/60\n","1846/1846 [==============================] - 7s 3ms/step - loss: 4.6539 - accuracy: 0.0749 - val_loss: 4.2366 - val_accuracy: 0.1142\n","Epoch 2/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.8358 - accuracy: 0.1639 - val_loss: 3.6929 - val_accuracy: 0.2207\n","Epoch 3/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 3.2449 - accuracy: 0.2695 - val_loss: 3.1269 - val_accuracy: 0.3974\n","Epoch 4/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.6186 - accuracy: 0.4387 - val_loss: 2.6040 - val_accuracy: 0.5193\n","Epoch 5/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.1346 - accuracy: 0.5474 - val_loss: 2.2686 - val_accuracy: 0.5688\n","Epoch 6/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.7800 - accuracy: 0.6073 - val_loss: 1.9782 - val_accuracy: 0.5855\n","Epoch 7/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.5077 - accuracy: 0.6547 - val_loss: 1.7399 - val_accuracy: 0.6803\n","Epoch 8/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.2986 - accuracy: 0.6965 - val_loss: 1.6331 - val_accuracy: 0.7036\n","Epoch 9/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.1501 - accuracy: 0.7250 - val_loss: 1.5636 - val_accuracy: 0.7023\n","Epoch 10/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.0495 - accuracy: 0.7443 - val_loss: 1.4117 - val_accuracy: 0.7437\n","Epoch 11/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9708 - accuracy: 0.7621 - val_loss: 1.3975 - val_accuracy: 0.7413\n","Epoch 12/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9057 - accuracy: 0.7756 - val_loss: 1.3715 - val_accuracy: 0.7534\n","Epoch 13/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8650 - accuracy: 0.7827 - val_loss: 1.3905 - val_accuracy: 0.7358\n","Epoch 14/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8223 - accuracy: 0.7924 - val_loss: 1.2344 - val_accuracy: 0.7864\n","Epoch 15/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7848 - accuracy: 0.8007 - val_loss: 1.2079 - val_accuracy: 0.7828\n","Epoch 16/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7524 - accuracy: 0.8065 - val_loss: 1.1686 - val_accuracy: 0.7883\n","Epoch 17/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7356 - accuracy: 0.8075 - val_loss: 1.1955 - val_accuracy: 0.7901\n","Epoch 18/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7143 - accuracy: 0.8093 - val_loss: 1.1667 - val_accuracy: 0.8011\n","Epoch 19/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6901 - accuracy: 0.8210 - val_loss: 1.1567 - val_accuracy: 0.8026\n","Epoch 20/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6793 - accuracy: 0.8170 - val_loss: 1.1276 - val_accuracy: 0.8176\n","Epoch 21/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6578 - accuracy: 0.8241 - val_loss: 1.1210 - val_accuracy: 0.8086\n","Epoch 22/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6487 - accuracy: 0.8246 - val_loss: 1.1177 - val_accuracy: 0.8084\n","Epoch 23/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6316 - accuracy: 0.8325 - val_loss: 1.0962 - val_accuracy: 0.8020\n","Epoch 24/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6253 - accuracy: 0.8283 - val_loss: 1.0549 - val_accuracy: 0.8372\n","Epoch 25/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6032 - accuracy: 0.8355 - val_loss: 1.0546 - val_accuracy: 0.8389\n","Epoch 26/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6122 - accuracy: 0.8328 - val_loss: 1.0863 - val_accuracy: 0.8381\n","Epoch 27/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5923 - accuracy: 0.8386 - val_loss: 1.0575 - val_accuracy: 0.8273\n","Epoch 28/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5847 - accuracy: 0.8403 - val_loss: 1.0796 - val_accuracy: 0.8189\n","Epoch 29/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5815 - accuracy: 0.8408 - val_loss: 0.9965 - val_accuracy: 0.8383\n","Epoch 30/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5722 - accuracy: 0.8455 - val_loss: 0.9876 - val_accuracy: 0.8365\n","Epoch 31/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5565 - accuracy: 0.8501 - val_loss: 1.0100 - val_accuracy: 0.8310\n","Epoch 32/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5488 - accuracy: 0.8536 - val_loss: 1.0801 - val_accuracy: 0.8035\n","Epoch 33/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5431 - accuracy: 0.8529 - val_loss: 0.9653 - val_accuracy: 0.8565\n","Epoch 34/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5364 - accuracy: 0.8568 - val_loss: 0.9886 - val_accuracy: 0.8609\n","Epoch 35/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.5270 - accuracy: 0.8599 - val_loss: 1.0107 - val_accuracy: 0.8429\n","Epoch 36/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5241 - accuracy: 0.8567 - val_loss: 0.9914 - val_accuracy: 0.8427\n","Epoch 37/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5171 - accuracy: 0.8603 - val_loss: 1.0551 - val_accuracy: 0.8288\n","Epoch 38/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5115 - accuracy: 0.8630 - val_loss: 0.9315 - val_accuracy: 0.8700\n","Epoch 39/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5212 - accuracy: 0.8614 - val_loss: 0.9392 - val_accuracy: 0.8625\n","Epoch 40/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4999 - accuracy: 0.8673 - val_loss: 0.9607 - val_accuracy: 0.8634\n","Epoch 41/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.4997 - accuracy: 0.8664 - val_loss: 0.9523 - val_accuracy: 0.8634\n","Epoch 42/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4987 - accuracy: 0.8668 - val_loss: 0.9247 - val_accuracy: 0.8713\n","Epoch 43/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4967 - accuracy: 0.8675 - val_loss: 0.9196 - val_accuracy: 0.8777\n","Epoch 44/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4720 - accuracy: 0.8720 - val_loss: 0.9353 - val_accuracy: 0.8645\n","Epoch 45/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4920 - accuracy: 0.8659 - val_loss: 0.8956 - val_accuracy: 0.8785\n","Epoch 46/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4734 - accuracy: 0.8741 - val_loss: 1.0957 - val_accuracy: 0.8042\n","Epoch 47/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4693 - accuracy: 0.8743 - val_loss: 0.9327 - val_accuracy: 0.8647\n","Epoch 48/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.4776 - accuracy: 0.8731 - val_loss: 0.9110 - val_accuracy: 0.8741\n","Epoch 49/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4589 - accuracy: 0.8774 - val_loss: 0.9156 - val_accuracy: 0.8587\n","Epoch 50/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4503 - accuracy: 0.8790 - val_loss: 0.9257 - val_accuracy: 0.8671\n","Epoch 51/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.4668 - accuracy: 0.8746 - val_loss: 0.9002 - val_accuracy: 0.8689\n","Epoch 52/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4534 - accuracy: 0.8810 - val_loss: 0.9035 - val_accuracy: 0.8724\n","Epoch 53/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4471 - accuracy: 0.8847 - val_loss: 0.8791 - val_accuracy: 0.8726\n","Epoch 54/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4305 - accuracy: 0.8900 - val_loss: 0.9083 - val_accuracy: 0.8680\n","Epoch 55/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4548 - accuracy: 0.8801 - val_loss: 0.8645 - val_accuracy: 0.8757\n","Epoch 56/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4339 - accuracy: 0.8858 - val_loss: 0.8598 - val_accuracy: 0.8752\n","Epoch 57/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4315 - accuracy: 0.8867 - val_loss: 0.8906 - val_accuracy: 0.8790\n","Epoch 58/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.4330 - accuracy: 0.8847 - val_loss: 0.8832 - val_accuracy: 0.8777\n","Epoch 59/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4205 - accuracy: 0.8863 - val_loss: 0.8625 - val_accuracy: 0.8928\n","Epoch 60/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4345 - accuracy: 0.8853 - val_loss: 0.8251 - val_accuracy: 0.8904\n","Fold: 2\n","Epoch 1/60\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"]},{"output_type":"stream","name":"stdout","text":["1846/1846 [==============================] - 7s 3ms/step - loss: 4.5774 - accuracy: 0.0745 - val_loss: 4.1603 - val_accuracy: 0.1146\n","Epoch 2/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.6839 - accuracy: 0.1610 - val_loss: 3.5508 - val_accuracy: 0.2046\n","Epoch 3/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.0026 - accuracy: 0.3379 - val_loss: 2.9090 - val_accuracy: 0.4471\n","Epoch 4/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.3713 - accuracy: 0.5051 - val_loss: 2.4915 - val_accuracy: 0.5402\n","Epoch 5/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.9143 - accuracy: 0.5887 - val_loss: 2.1851 - val_accuracy: 0.5934\n","Epoch 6/60\n","1846/1846 [==============================] - 6s 4ms/step - loss: 1.6086 - accuracy: 0.6401 - val_loss: 2.0173 - val_accuracy: 0.6429\n","Epoch 7/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 1.3973 - accuracy: 0.6805 - val_loss: 1.8626 - val_accuracy: 0.6739\n","Epoch 8/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.2553 - accuracy: 0.7068 - val_loss: 1.7539 - val_accuracy: 0.7142\n","Epoch 9/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.1385 - accuracy: 0.7284 - val_loss: 1.6295 - val_accuracy: 0.7342\n","Epoch 10/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0425 - accuracy: 0.7488 - val_loss: 1.6158 - val_accuracy: 0.7208\n","Epoch 11/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.9753 - accuracy: 0.7631 - val_loss: 1.5215 - val_accuracy: 0.7619\n","Epoch 12/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.9128 - accuracy: 0.7728 - val_loss: 1.4526 - val_accuracy: 0.7648\n","Epoch 13/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8643 - accuracy: 0.7816 - val_loss: 1.4393 - val_accuracy: 0.7826\n","Epoch 14/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8093 - accuracy: 0.7966 - val_loss: 1.3872 - val_accuracy: 0.7822\n","Epoch 15/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7768 - accuracy: 0.8048 - val_loss: 1.3657 - val_accuracy: 0.7850\n","Epoch 16/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7493 - accuracy: 0.8111 - val_loss: 1.3510 - val_accuracy: 0.7989\n","Epoch 17/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7218 - accuracy: 0.8137 - val_loss: 1.2659 - val_accuracy: 0.8062\n","Epoch 18/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.6979 - accuracy: 0.8210 - val_loss: 1.3215 - val_accuracy: 0.7991\n","Epoch 19/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6823 - accuracy: 0.8235 - val_loss: 1.2185 - val_accuracy: 0.8381\n","Epoch 20/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6642 - accuracy: 0.8252 - val_loss: 1.3249 - val_accuracy: 0.8046\n","Epoch 21/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6474 - accuracy: 0.8323 - val_loss: 1.1895 - val_accuracy: 0.8341\n","Epoch 22/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6255 - accuracy: 0.8385 - val_loss: 1.3165 - val_accuracy: 0.7859\n","Epoch 23/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6197 - accuracy: 0.8361 - val_loss: 1.2514 - val_accuracy: 0.7967\n","Epoch 24/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6113 - accuracy: 0.8384 - val_loss: 1.1778 - val_accuracy: 0.8328\n","Epoch 25/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5971 - accuracy: 0.8447 - val_loss: 1.2066 - val_accuracy: 0.8040\n","Epoch 26/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5880 - accuracy: 0.8448 - val_loss: 1.1788 - val_accuracy: 0.8213\n","Epoch 27/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5792 - accuracy: 0.8471 - val_loss: 1.1812 - val_accuracy: 0.8150\n","Epoch 28/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5555 - accuracy: 0.8527 - val_loss: 1.1348 - val_accuracy: 0.8370\n","Epoch 29/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5650 - accuracy: 0.8462 - val_loss: 1.1245 - val_accuracy: 0.8361\n","Epoch 30/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.5447 - accuracy: 0.8542 - val_loss: 1.1236 - val_accuracy: 0.8466\n","Epoch 31/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5370 - accuracy: 0.8561 - val_loss: 1.1381 - val_accuracy: 0.8460\n","Epoch 32/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5410 - accuracy: 0.8578 - val_loss: 1.1001 - val_accuracy: 0.8524\n","Epoch 33/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5228 - accuracy: 0.8587 - val_loss: 1.0615 - val_accuracy: 0.8592\n","Epoch 34/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5209 - accuracy: 0.8581 - val_loss: 1.0679 - val_accuracy: 0.8559\n","Epoch 35/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5237 - accuracy: 0.8607 - val_loss: 1.1944 - val_accuracy: 0.7835\n","Epoch 36/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5072 - accuracy: 0.8624 - val_loss: 1.0668 - val_accuracy: 0.8469\n","Epoch 37/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4997 - accuracy: 0.8659 - val_loss: 1.0577 - val_accuracy: 0.8552\n","Epoch 38/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4809 - accuracy: 0.8724 - val_loss: 1.0468 - val_accuracy: 0.8607\n","Epoch 39/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.4832 - accuracy: 0.8693 - val_loss: 1.1129 - val_accuracy: 0.8378\n","Epoch 40/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4988 - accuracy: 0.8675 - val_loss: 1.0526 - val_accuracy: 0.8295\n","Epoch 41/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4811 - accuracy: 0.8693 - val_loss: 1.0081 - val_accuracy: 0.8614\n","Epoch 42/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4720 - accuracy: 0.8753 - val_loss: 0.9988 - val_accuracy: 0.8741\n","Epoch 43/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4766 - accuracy: 0.8729 - val_loss: 1.0237 - val_accuracy: 0.8585\n","Epoch 44/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4850 - accuracy: 0.8659 - val_loss: 1.0242 - val_accuracy: 0.8603\n","Epoch 45/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4531 - accuracy: 0.8788 - val_loss: 1.0365 - val_accuracy: 0.8488\n","Epoch 46/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4538 - accuracy: 0.8763 - val_loss: 0.9909 - val_accuracy: 0.8735\n","Epoch 47/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4639 - accuracy: 0.8795 - val_loss: 0.9772 - val_accuracy: 0.8609\n","Epoch 48/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4571 - accuracy: 0.8744 - val_loss: 1.0188 - val_accuracy: 0.8462\n","Epoch 49/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4382 - accuracy: 0.8801 - val_loss: 1.0617 - val_accuracy: 0.8279\n","Epoch 50/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4384 - accuracy: 0.8847 - val_loss: 1.1561 - val_accuracy: 0.8229\n","Epoch 51/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4311 - accuracy: 0.8856 - val_loss: 0.9421 - val_accuracy: 0.8728\n","Epoch 52/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4694 - accuracy: 0.8706 - val_loss: 0.9500 - val_accuracy: 0.8733\n","Epoch 53/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4293 - accuracy: 0.8861 - val_loss: 0.9444 - val_accuracy: 0.8832\n","Epoch 54/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4279 - accuracy: 0.8872 - val_loss: 0.9631 - val_accuracy: 0.8519\n","Epoch 55/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4304 - accuracy: 0.8840 - val_loss: 0.9019 - val_accuracy: 0.8781\n","Epoch 56/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4226 - accuracy: 0.8897 - val_loss: 0.8804 - val_accuracy: 0.8968\n","Epoch 57/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4181 - accuracy: 0.8906 - val_loss: 0.9217 - val_accuracy: 0.8788\n","Epoch 58/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4222 - accuracy: 0.8886 - val_loss: 1.0072 - val_accuracy: 0.8517\n","Epoch 59/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4047 - accuracy: 0.8930 - val_loss: 0.9037 - val_accuracy: 0.8755\n","Epoch 60/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.4043 - accuracy: 0.8926 - val_loss: 0.9716 - val_accuracy: 0.8675\n","Average Validation Accuracy: 0.8907521367073059\n","Average Validation Loss: 0.5646872818470001\n","Average Test Accuracy: 0.885973334312439\n","Final Test Accuracy for each fold: 0.8976192474365234\n","Number of input features: 12\n","Fold: 1\n","Epoch 1/60\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"]},{"output_type":"stream","name":"stdout","text":["1846/1846 [==============================] - 6s 3ms/step - loss: 4.4350 - accuracy: 0.1015 - val_loss: 4.0072 - val_accuracy: 0.1430\n","Epoch 2/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 3.5674 - accuracy: 0.1922 - val_loss: 3.3921 - val_accuracy: 0.2497\n","Epoch 3/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.8036 - accuracy: 0.3621 - val_loss: 2.5762 - val_accuracy: 0.4565\n","Epoch 4/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.0114 - accuracy: 0.5327 - val_loss: 2.0110 - val_accuracy: 0.5536\n","Epoch 5/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.5205 - accuracy: 0.6356 - val_loss: 1.6548 - val_accuracy: 0.6464\n","Epoch 6/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.2277 - accuracy: 0.6963 - val_loss: 1.4128 - val_accuracy: 0.7146\n","Epoch 7/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0412 - accuracy: 0.7337 - val_loss: 1.2837 - val_accuracy: 0.7331\n","Epoch 8/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9267 - accuracy: 0.7551 - val_loss: 1.1925 - val_accuracy: 0.7597\n","Epoch 9/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8345 - accuracy: 0.7813 - val_loss: 1.1194 - val_accuracy: 0.7655\n","Epoch 10/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7789 - accuracy: 0.7894 - val_loss: 1.0629 - val_accuracy: 0.7606\n","Epoch 11/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7239 - accuracy: 0.8054 - val_loss: 1.0242 - val_accuracy: 0.7793\n","Epoch 12/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6912 - accuracy: 0.8117 - val_loss: 1.0079 - val_accuracy: 0.7991\n","Epoch 13/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6544 - accuracy: 0.8217 - val_loss: 0.9263 - val_accuracy: 0.7976\n","Epoch 14/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6326 - accuracy: 0.8296 - val_loss: 0.9126 - val_accuracy: 0.8249\n","Epoch 15/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6203 - accuracy: 0.8276 - val_loss: 0.8769 - val_accuracy: 0.8191\n","Epoch 16/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5785 - accuracy: 0.8382 - val_loss: 0.9555 - val_accuracy: 0.7903\n","Epoch 17/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5736 - accuracy: 0.8352 - val_loss: 0.8015 - val_accuracy: 0.8460\n","Epoch 18/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5559 - accuracy: 0.8435 - val_loss: 0.8000 - val_accuracy: 0.8348\n","Epoch 19/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5425 - accuracy: 0.8475 - val_loss: 0.8075 - val_accuracy: 0.8374\n","Epoch 20/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5310 - accuracy: 0.8518 - val_loss: 0.8756 - val_accuracy: 0.8246\n","Epoch 21/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5078 - accuracy: 0.8596 - val_loss: 0.8192 - val_accuracy: 0.8279\n","Epoch 22/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5099 - accuracy: 0.8599 - val_loss: 0.7558 - val_accuracy: 0.8418\n","Epoch 23/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4971 - accuracy: 0.8617 - val_loss: 0.7932 - val_accuracy: 0.8464\n","Epoch 24/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4783 - accuracy: 0.8620 - val_loss: 0.7083 - val_accuracy: 0.8715\n","Epoch 25/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4734 - accuracy: 0.8654 - val_loss: 0.7385 - val_accuracy: 0.8607\n","Epoch 26/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4658 - accuracy: 0.8730 - val_loss: 0.7356 - val_accuracy: 0.8510\n","Epoch 27/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4595 - accuracy: 0.8712 - val_loss: 0.7715 - val_accuracy: 0.8453\n","Epoch 28/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4478 - accuracy: 0.8756 - val_loss: 0.7277 - val_accuracy: 0.8704\n","Epoch 29/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4535 - accuracy: 0.8715 - val_loss: 0.6703 - val_accuracy: 0.8629\n","Epoch 30/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4304 - accuracy: 0.8797 - val_loss: 0.7112 - val_accuracy: 0.8495\n","Epoch 31/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4358 - accuracy: 0.8745 - val_loss: 0.7115 - val_accuracy: 0.8519\n","Epoch 32/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4305 - accuracy: 0.8810 - val_loss: 0.8191 - val_accuracy: 0.8244\n","Epoch 33/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4129 - accuracy: 0.8832 - val_loss: 0.6472 - val_accuracy: 0.8739\n","Epoch 34/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4248 - accuracy: 0.8811 - val_loss: 0.6572 - val_accuracy: 0.8662\n","Epoch 35/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4052 - accuracy: 0.8865 - val_loss: 0.6283 - val_accuracy: 0.8825\n","Epoch 36/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4149 - accuracy: 0.8860 - val_loss: 0.6195 - val_accuracy: 0.8812\n","Epoch 37/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3921 - accuracy: 0.8920 - val_loss: 0.6347 - val_accuracy: 0.8603\n","Epoch 38/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3995 - accuracy: 0.8878 - val_loss: 0.6156 - val_accuracy: 0.8768\n","Epoch 39/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.3901 - accuracy: 0.8918 - val_loss: 0.7046 - val_accuracy: 0.8508\n","Epoch 40/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3958 - accuracy: 0.8919 - val_loss: 0.7612 - val_accuracy: 0.8306\n","Epoch 41/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3768 - accuracy: 0.8954 - val_loss: 0.6135 - val_accuracy: 0.8807\n","Epoch 42/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3826 - accuracy: 0.8928 - val_loss: 0.5776 - val_accuracy: 0.8915\n","Epoch 43/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3753 - accuracy: 0.8940 - val_loss: 0.6020 - val_accuracy: 0.8994\n","Epoch 44/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3754 - accuracy: 0.8981 - val_loss: 0.6347 - val_accuracy: 0.8528\n","Epoch 45/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3652 - accuracy: 0.8988 - val_loss: 0.7150 - val_accuracy: 0.8191\n","Epoch 46/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3829 - accuracy: 0.8955 - val_loss: 0.5744 - val_accuracy: 0.8909\n","Epoch 47/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3591 - accuracy: 0.8993 - val_loss: 0.5648 - val_accuracy: 0.9008\n","Epoch 48/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3690 - accuracy: 0.8978 - val_loss: 0.6055 - val_accuracy: 0.8653\n","Epoch 49/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3658 - accuracy: 0.8977 - val_loss: 0.6103 - val_accuracy: 0.8893\n","Epoch 50/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3552 - accuracy: 0.9041 - val_loss: 0.5549 - val_accuracy: 0.8928\n","Epoch 51/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3564 - accuracy: 0.8984 - val_loss: 0.5674 - val_accuracy: 0.9001\n","Epoch 52/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3506 - accuracy: 0.9030 - val_loss: 0.5727 - val_accuracy: 0.8856\n","Epoch 53/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3559 - accuracy: 0.9002 - val_loss: 0.5512 - val_accuracy: 0.9003\n","Epoch 54/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3511 - accuracy: 0.9043 - val_loss: 0.5300 - val_accuracy: 0.9065\n","Epoch 55/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3510 - accuracy: 0.9017 - val_loss: 0.6018 - val_accuracy: 0.8939\n","Epoch 56/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3309 - accuracy: 0.9105 - val_loss: 0.6054 - val_accuracy: 0.8761\n","Epoch 57/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3430 - accuracy: 0.9049 - val_loss: 0.5654 - val_accuracy: 0.8856\n","Epoch 58/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3497 - accuracy: 0.9014 - val_loss: 0.5283 - val_accuracy: 0.9030\n","Epoch 59/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3328 - accuracy: 0.9098 - val_loss: 0.5232 - val_accuracy: 0.8983\n","Epoch 60/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.3285 - accuracy: 0.9106 - val_loss: 0.5108 - val_accuracy: 0.9067\n","Fold: 2\n","Epoch 1/60\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"]},{"output_type":"stream","name":"stdout","text":["1846/1846 [==============================] - 7s 3ms/step - loss: 4.5224 - accuracy: 0.0842 - val_loss: 4.0652 - val_accuracy: 0.1729\n","Epoch 2/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.5724 - accuracy: 0.1932 - val_loss: 3.4375 - val_accuracy: 0.2233\n","Epoch 3/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.9871 - accuracy: 0.2968 - val_loss: 2.9420 - val_accuracy: 0.4436\n","Epoch 4/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.4117 - accuracy: 0.4683 - val_loss: 2.4680 - val_accuracy: 0.5030\n","Epoch 5/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.9477 - accuracy: 0.5770 - val_loss: 2.1830 - val_accuracy: 0.6103\n","Epoch 6/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.6012 - accuracy: 0.6459 - val_loss: 1.9470 - val_accuracy: 0.6684\n","Epoch 7/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.3431 - accuracy: 0.7001 - val_loss: 1.7480 - val_accuracy: 0.7109\n","Epoch 8/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.1577 - accuracy: 0.7309 - val_loss: 1.7283 - val_accuracy: 0.6730\n","Epoch 9/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0365 - accuracy: 0.7480 - val_loss: 1.5366 - val_accuracy: 0.7564\n","Epoch 10/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9444 - accuracy: 0.7648 - val_loss: 1.4720 - val_accuracy: 0.7457\n","Epoch 11/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8796 - accuracy: 0.7786 - val_loss: 1.3968 - val_accuracy: 0.7721\n","Epoch 12/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8145 - accuracy: 0.7941 - val_loss: 1.3768 - val_accuracy: 0.7789\n","Epoch 13/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7725 - accuracy: 0.8041 - val_loss: 1.3595 - val_accuracy: 0.7787\n","Epoch 14/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7308 - accuracy: 0.8134 - val_loss: 1.2776 - val_accuracy: 0.8119\n","Epoch 15/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7028 - accuracy: 0.8215 - val_loss: 1.2503 - val_accuracy: 0.8108\n","Epoch 16/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6669 - accuracy: 0.8281 - val_loss: 1.2035 - val_accuracy: 0.8161\n","Epoch 17/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6412 - accuracy: 0.8353 - val_loss: 1.2097 - val_accuracy: 0.8152\n","Epoch 18/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6174 - accuracy: 0.8395 - val_loss: 1.1604 - val_accuracy: 0.8381\n","Epoch 19/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6035 - accuracy: 0.8398 - val_loss: 1.1925 - val_accuracy: 0.8088\n","Epoch 20/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5787 - accuracy: 0.8468 - val_loss: 1.1274 - val_accuracy: 0.8317\n","Epoch 21/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5625 - accuracy: 0.8489 - val_loss: 1.2000 - val_accuracy: 0.8147\n","Epoch 22/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5468 - accuracy: 0.8538 - val_loss: 1.1100 - val_accuracy: 0.8279\n","Epoch 23/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5358 - accuracy: 0.8593 - val_loss: 1.1183 - val_accuracy: 0.8449\n","Epoch 24/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5274 - accuracy: 0.8633 - val_loss: 1.1579 - val_accuracy: 0.8222\n","Epoch 25/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5131 - accuracy: 0.8656 - val_loss: 1.1264 - val_accuracy: 0.8409\n","Epoch 26/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.5103 - accuracy: 0.8649 - val_loss: 1.1274 - val_accuracy: 0.8429\n","Epoch 27/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4908 - accuracy: 0.8720 - val_loss: 1.0494 - val_accuracy: 0.8455\n","Epoch 28/60\n","1846/1846 [==============================] - 6s 4ms/step - loss: 0.4877 - accuracy: 0.8723 - val_loss: 1.0305 - val_accuracy: 0.8526\n","Epoch 29/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4774 - accuracy: 0.8735 - val_loss: 1.0408 - val_accuracy: 0.8585\n","Epoch 30/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4685 - accuracy: 0.8784 - val_loss: 1.0021 - val_accuracy: 0.8697\n","Epoch 31/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4480 - accuracy: 0.8826 - val_loss: 0.9891 - val_accuracy: 0.8748\n","Epoch 32/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4703 - accuracy: 0.8755 - val_loss: 1.0614 - val_accuracy: 0.8495\n","Epoch 33/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4381 - accuracy: 0.8839 - val_loss: 1.0379 - val_accuracy: 0.8594\n","Epoch 34/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4470 - accuracy: 0.8836 - val_loss: 0.9695 - val_accuracy: 0.8810\n","Epoch 35/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4291 - accuracy: 0.8909 - val_loss: 0.9544 - val_accuracy: 0.8818\n","Epoch 36/60\n","1846/1846 [==============================] - 5s 2ms/step - loss: 0.4216 - accuracy: 0.8879 - val_loss: 0.9488 - val_accuracy: 0.8970\n","Epoch 37/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4267 - accuracy: 0.8876 - val_loss: 0.9723 - val_accuracy: 0.8671\n","Epoch 38/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4209 - accuracy: 0.8898 - val_loss: 0.9413 - val_accuracy: 0.8770\n","Epoch 39/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4037 - accuracy: 0.8951 - val_loss: 0.9345 - val_accuracy: 0.8906\n","Epoch 40/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4154 - accuracy: 0.8955 - val_loss: 0.9093 - val_accuracy: 0.8836\n","Epoch 41/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4080 - accuracy: 0.8953 - val_loss: 0.9271 - val_accuracy: 0.8948\n","Epoch 42/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3931 - accuracy: 0.8980 - val_loss: 0.9542 - val_accuracy: 0.8944\n","Epoch 43/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3934 - accuracy: 0.8981 - val_loss: 0.9560 - val_accuracy: 0.8803\n","Epoch 44/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3882 - accuracy: 0.8995 - val_loss: 0.9783 - val_accuracy: 0.8532\n","Epoch 45/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3743 - accuracy: 0.9042 - val_loss: 1.0085 - val_accuracy: 0.8510\n","Epoch 46/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3866 - accuracy: 0.8977 - val_loss: 0.8639 - val_accuracy: 0.8981\n","Epoch 47/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3796 - accuracy: 0.9027 - val_loss: 0.8648 - val_accuracy: 0.8920\n","Epoch 48/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3731 - accuracy: 0.9035 - val_loss: 0.9083 - val_accuracy: 0.8909\n","Epoch 49/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3684 - accuracy: 0.9072 - val_loss: 0.8288 - val_accuracy: 0.9078\n","Epoch 50/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3629 - accuracy: 0.9063 - val_loss: 0.9084 - val_accuracy: 0.8836\n","Epoch 51/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3717 - accuracy: 0.9029 - val_loss: 0.8410 - val_accuracy: 0.8935\n","Epoch 52/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3697 - accuracy: 0.9008 - val_loss: 0.8523 - val_accuracy: 0.8931\n","Epoch 53/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3347 - accuracy: 0.9160 - val_loss: 0.8393 - val_accuracy: 0.8988\n","Epoch 54/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3579 - accuracy: 0.9103 - val_loss: 0.8512 - val_accuracy: 0.9107\n","Epoch 55/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3593 - accuracy: 0.9071 - val_loss: 0.7880 - val_accuracy: 0.9122\n","Epoch 56/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3363 - accuracy: 0.9150 - val_loss: 0.8344 - val_accuracy: 0.8920\n","Epoch 57/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3490 - accuracy: 0.9093 - val_loss: 0.7887 - val_accuracy: 0.9102\n","Epoch 58/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3336 - accuracy: 0.9181 - val_loss: 0.8196 - val_accuracy: 0.8988\n","Epoch 59/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3284 - accuracy: 0.9186 - val_loss: 0.8484 - val_accuracy: 0.8992\n","Epoch 60/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3350 - accuracy: 0.9157 - val_loss: 0.7896 - val_accuracy: 0.9182\n","Average Validation Accuracy: 0.9256075024604797\n","Average Validation Loss: 0.4052015691995621\n","Average Test Accuracy: 0.9232328534126282\n","Final Test Accuracy for each fold: 0.9283555746078491\n","Number of input features: 13\n","Fold: 1\n","Epoch 1/60\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"]},{"output_type":"stream","name":"stdout","text":["1846/1846 [==============================] - 8s 4ms/step - loss: 4.5718 - accuracy: 0.0854 - val_loss: 4.1390 - val_accuracy: 0.1353\n","Epoch 2/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.6510 - accuracy: 0.1746 - val_loss: 3.4330 - val_accuracy: 0.2295\n","Epoch 3/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.9222 - accuracy: 0.3406 - val_loss: 2.7505 - val_accuracy: 0.4548\n","Epoch 4/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.2336 - accuracy: 0.5076 - val_loss: 2.2242 - val_accuracy: 0.5547\n","Epoch 5/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.7502 - accuracy: 0.6136 - val_loss: 1.8597 - val_accuracy: 0.6473\n","Epoch 6/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.4285 - accuracy: 0.6717 - val_loss: 1.6277 - val_accuracy: 0.6785\n","Epoch 7/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.2144 - accuracy: 0.7104 - val_loss: 1.5120 - val_accuracy: 0.7039\n","Epoch 8/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0898 - accuracy: 0.7346 - val_loss: 1.3501 - val_accuracy: 0.7448\n","Epoch 9/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9878 - accuracy: 0.7562 - val_loss: 1.3088 - val_accuracy: 0.7483\n","Epoch 10/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9108 - accuracy: 0.7718 - val_loss: 1.2028 - val_accuracy: 0.7617\n","Epoch 11/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8441 - accuracy: 0.7866 - val_loss: 1.1745 - val_accuracy: 0.7864\n","Epoch 12/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8057 - accuracy: 0.7925 - val_loss: 1.1243 - val_accuracy: 0.7879\n","Epoch 13/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7670 - accuracy: 0.7987 - val_loss: 1.1098 - val_accuracy: 0.7712\n","Epoch 14/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7509 - accuracy: 0.8024 - val_loss: 1.0709 - val_accuracy: 0.8035\n","Epoch 15/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7080 - accuracy: 0.8121 - val_loss: 1.1009 - val_accuracy: 0.7976\n","Epoch 16/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6817 - accuracy: 0.8205 - val_loss: 1.0526 - val_accuracy: 0.8108\n","Epoch 17/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6690 - accuracy: 0.8240 - val_loss: 1.0636 - val_accuracy: 0.8048\n","Epoch 18/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6459 - accuracy: 0.8287 - val_loss: 0.9754 - val_accuracy: 0.8306\n","Epoch 19/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6448 - accuracy: 0.8307 - val_loss: 0.9550 - val_accuracy: 0.8414\n","Epoch 20/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6208 - accuracy: 0.8359 - val_loss: 0.9420 - val_accuracy: 0.8348\n","Epoch 21/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6017 - accuracy: 0.8405 - val_loss: 0.9649 - val_accuracy: 0.8233\n","Epoch 22/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5896 - accuracy: 0.8464 - val_loss: 0.9488 - val_accuracy: 0.8455\n","Epoch 23/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5861 - accuracy: 0.8472 - val_loss: 0.9789 - val_accuracy: 0.8286\n","Epoch 24/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5700 - accuracy: 0.8533 - val_loss: 0.9689 - val_accuracy: 0.8218\n","Epoch 25/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5622 - accuracy: 0.8539 - val_loss: 0.9153 - val_accuracy: 0.8493\n","Epoch 26/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5429 - accuracy: 0.8568 - val_loss: 0.9311 - val_accuracy: 0.8385\n","Epoch 27/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5582 - accuracy: 0.8536 - val_loss: 0.9391 - val_accuracy: 0.8389\n","Epoch 28/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5338 - accuracy: 0.8608 - val_loss: 1.0549 - val_accuracy: 0.8015\n","Epoch 29/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5455 - accuracy: 0.8582 - val_loss: 0.8801 - val_accuracy: 0.8491\n","Epoch 30/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5179 - accuracy: 0.8655 - val_loss: 0.9072 - val_accuracy: 0.8477\n","Epoch 31/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5160 - accuracy: 0.8621 - val_loss: 0.9425 - val_accuracy: 0.8246\n","Epoch 32/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5150 - accuracy: 0.8632 - val_loss: 0.9050 - val_accuracy: 0.8636\n","Epoch 33/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5092 - accuracy: 0.8661 - val_loss: 0.9216 - val_accuracy: 0.8400\n","Epoch 34/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4905 - accuracy: 0.8716 - val_loss: 0.8377 - val_accuracy: 0.8510\n","Epoch 35/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4887 - accuracy: 0.8695 - val_loss: 0.9379 - val_accuracy: 0.8491\n","Epoch 36/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4953 - accuracy: 0.8674 - val_loss: 0.8953 - val_accuracy: 0.8431\n","Epoch 37/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4853 - accuracy: 0.8736 - val_loss: 0.7912 - val_accuracy: 0.8585\n","Epoch 38/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4853 - accuracy: 0.8745 - val_loss: 0.8158 - val_accuracy: 0.8724\n","Epoch 39/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4740 - accuracy: 0.8776 - val_loss: 0.9281 - val_accuracy: 0.8363\n","Epoch 40/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4755 - accuracy: 0.8752 - val_loss: 0.8101 - val_accuracy: 0.8711\n","Epoch 41/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4616 - accuracy: 0.8773 - val_loss: 0.7800 - val_accuracy: 0.8636\n","Epoch 42/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4508 - accuracy: 0.8802 - val_loss: 0.9053 - val_accuracy: 0.8647\n","Epoch 43/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4660 - accuracy: 0.8796 - val_loss: 0.9134 - val_accuracy: 0.8332\n","Epoch 44/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4424 - accuracy: 0.8834 - val_loss: 0.8031 - val_accuracy: 0.8741\n","Epoch 45/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4414 - accuracy: 0.8846 - val_loss: 0.8046 - val_accuracy: 0.8638\n","Epoch 46/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4330 - accuracy: 0.8853 - val_loss: 0.8109 - val_accuracy: 0.8634\n","Epoch 47/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4308 - accuracy: 0.8854 - val_loss: 0.8026 - val_accuracy: 0.8777\n","Epoch 48/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4286 - accuracy: 0.8910 - val_loss: 0.7867 - val_accuracy: 0.8788\n","Epoch 49/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4226 - accuracy: 0.8885 - val_loss: 0.8121 - val_accuracy: 0.8601\n","Epoch 50/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4196 - accuracy: 0.8892 - val_loss: 0.8027 - val_accuracy: 0.8539\n","Epoch 51/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4293 - accuracy: 0.8887 - val_loss: 0.7959 - val_accuracy: 0.8664\n","Epoch 52/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4168 - accuracy: 0.8928 - val_loss: 0.7510 - val_accuracy: 0.8911\n","Epoch 53/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.4059 - accuracy: 0.8975 - val_loss: 0.7238 - val_accuracy: 0.8847\n","Epoch 54/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4160 - accuracy: 0.8900 - val_loss: 0.7280 - val_accuracy: 0.8836\n","Epoch 55/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3965 - accuracy: 0.8974 - val_loss: 0.7512 - val_accuracy: 0.8856\n","Epoch 56/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4044 - accuracy: 0.8957 - val_loss: 0.7054 - val_accuracy: 0.9017\n","Epoch 57/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3861 - accuracy: 0.8990 - val_loss: 0.6999 - val_accuracy: 0.9010\n","Epoch 58/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3968 - accuracy: 0.8977 - val_loss: 0.6981 - val_accuracy: 0.9050\n","Epoch 59/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3932 - accuracy: 0.8993 - val_loss: 0.7265 - val_accuracy: 0.8785\n","Epoch 60/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3806 - accuracy: 0.9032 - val_loss: 0.7155 - val_accuracy: 0.8933\n","Fold: 2\n","Epoch 1/60\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"]},{"output_type":"stream","name":"stdout","text":["1846/1846 [==============================] - 6s 3ms/step - loss: 4.4906 - accuracy: 0.1000 - val_loss: 4.0747 - val_accuracy: 0.1340\n","Epoch 2/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 3.6019 - accuracy: 0.2045 - val_loss: 3.4628 - val_accuracy: 0.2697\n","Epoch 3/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.8661 - accuracy: 0.3948 - val_loss: 2.7444 - val_accuracy: 0.5105\n","Epoch 4/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.2127 - accuracy: 0.5354 - val_loss: 2.3248 - val_accuracy: 0.5441\n","Epoch 5/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.7690 - accuracy: 0.6135 - val_loss: 1.9793 - val_accuracy: 0.6308\n","Epoch 6/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.4644 - accuracy: 0.6680 - val_loss: 1.7557 - val_accuracy: 0.6801\n","Epoch 7/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.2639 - accuracy: 0.7028 - val_loss: 1.6328 - val_accuracy: 0.7023\n","Epoch 8/60\n","1846/1846 [==============================] - 6s 4ms/step - loss: 1.1300 - accuracy: 0.7259 - val_loss: 1.4955 - val_accuracy: 0.7296\n","Epoch 9/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0206 - accuracy: 0.7477 - val_loss: 1.4244 - val_accuracy: 0.7358\n","Epoch 10/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.9409 - accuracy: 0.7621 - val_loss: 1.3634 - val_accuracy: 0.7591\n","Epoch 11/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8782 - accuracy: 0.7766 - val_loss: 1.2971 - val_accuracy: 0.7582\n","Epoch 12/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8189 - accuracy: 0.7834 - val_loss: 1.2485 - val_accuracy: 0.7613\n","Epoch 13/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7773 - accuracy: 0.7945 - val_loss: 1.1463 - val_accuracy: 0.8066\n","Epoch 14/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7310 - accuracy: 0.8081 - val_loss: 1.1553 - val_accuracy: 0.7853\n","Epoch 15/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6990 - accuracy: 0.8152 - val_loss: 1.1109 - val_accuracy: 0.8108\n","Epoch 16/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6615 - accuracy: 0.8224 - val_loss: 1.0465 - val_accuracy: 0.8297\n","Epoch 17/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.6297 - accuracy: 0.8350 - val_loss: 1.0897 - val_accuracy: 0.8121\n","Epoch 18/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6044 - accuracy: 0.8408 - val_loss: 0.9881 - val_accuracy: 0.8420\n","Epoch 19/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5769 - accuracy: 0.8487 - val_loss: 1.1068 - val_accuracy: 0.8227\n","Epoch 20/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5571 - accuracy: 0.8509 - val_loss: 1.0246 - val_accuracy: 0.8367\n","Epoch 21/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5336 - accuracy: 0.8611 - val_loss: 0.9497 - val_accuracy: 0.8394\n","Epoch 22/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5131 - accuracy: 0.8672 - val_loss: 0.9326 - val_accuracy: 0.8550\n","Epoch 23/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5081 - accuracy: 0.8663 - val_loss: 0.8999 - val_accuracy: 0.8662\n","Epoch 24/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4789 - accuracy: 0.8773 - val_loss: 0.9593 - val_accuracy: 0.8513\n","Epoch 25/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4695 - accuracy: 0.8782 - val_loss: 0.8933 - val_accuracy: 0.8585\n","Epoch 26/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.4539 - accuracy: 0.8821 - val_loss: 0.8762 - val_accuracy: 0.8590\n","Epoch 27/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4497 - accuracy: 0.8839 - val_loss: 0.8585 - val_accuracy: 0.8821\n","Epoch 28/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4372 - accuracy: 0.8871 - val_loss: 0.9775 - val_accuracy: 0.8427\n","Epoch 29/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4244 - accuracy: 0.8938 - val_loss: 0.8067 - val_accuracy: 0.8851\n","Epoch 30/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4147 - accuracy: 0.8921 - val_loss: 0.8086 - val_accuracy: 0.8777\n","Epoch 31/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4089 - accuracy: 0.8977 - val_loss: 0.7637 - val_accuracy: 0.8937\n","Epoch 32/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3875 - accuracy: 0.9019 - val_loss: 0.8055 - val_accuracy: 0.8887\n","Epoch 33/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3861 - accuracy: 0.9028 - val_loss: 0.8098 - val_accuracy: 0.8735\n","Epoch 34/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3838 - accuracy: 0.9050 - val_loss: 0.7449 - val_accuracy: 0.9067\n","Epoch 35/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3727 - accuracy: 0.9043 - val_loss: 0.7308 - val_accuracy: 0.9098\n","Epoch 36/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3713 - accuracy: 0.9056 - val_loss: 0.7831 - val_accuracy: 0.8869\n","Epoch 37/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3459 - accuracy: 0.9139 - val_loss: 0.7113 - val_accuracy: 0.9063\n","Epoch 38/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3554 - accuracy: 0.9132 - val_loss: 0.6981 - val_accuracy: 0.9052\n","Epoch 39/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3482 - accuracy: 0.9139 - val_loss: 0.6951 - val_accuracy: 0.9140\n","Epoch 40/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3332 - accuracy: 0.9178 - val_loss: 0.7139 - val_accuracy: 0.9036\n","Epoch 41/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3474 - accuracy: 0.9159 - val_loss: 0.7336 - val_accuracy: 0.9008\n","Epoch 42/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3302 - accuracy: 0.9212 - val_loss: 0.6824 - val_accuracy: 0.9078\n","Epoch 43/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3341 - accuracy: 0.9155 - val_loss: 0.6755 - val_accuracy: 0.9107\n","Epoch 44/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3130 - accuracy: 0.9264 - val_loss: 0.7011 - val_accuracy: 0.9065\n","Epoch 45/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3073 - accuracy: 0.9245 - val_loss: 0.7372 - val_accuracy: 0.9043\n","Epoch 46/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3155 - accuracy: 0.9235 - val_loss: 0.6716 - val_accuracy: 0.9131\n","Epoch 47/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3131 - accuracy: 0.9263 - val_loss: 0.6398 - val_accuracy: 0.9190\n","Epoch 48/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.2940 - accuracy: 0.9294 - val_loss: 0.6487 - val_accuracy: 0.9254\n","Epoch 49/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3014 - accuracy: 0.9285 - val_loss: 0.6127 - val_accuracy: 0.9294\n","Epoch 50/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3003 - accuracy: 0.9280 - val_loss: 0.6523 - val_accuracy: 0.9135\n","Epoch 51/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.2877 - accuracy: 0.9313 - val_loss: 0.5945 - val_accuracy: 0.9307\n","Epoch 52/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.2912 - accuracy: 0.9304 - val_loss: 0.6167 - val_accuracy: 0.9254\n","Epoch 53/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.2893 - accuracy: 0.9293 - val_loss: 0.6489 - val_accuracy: 0.9072\n","Epoch 54/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.2747 - accuracy: 0.9350 - val_loss: 0.6314 - val_accuracy: 0.9223\n","Epoch 55/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.2773 - accuracy: 0.9362 - val_loss: 0.5785 - val_accuracy: 0.9351\n","Epoch 56/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.2738 - accuracy: 0.9335 - val_loss: 0.6119 - val_accuracy: 0.9096\n","Epoch 57/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.2624 - accuracy: 0.9384 - val_loss: 0.5731 - val_accuracy: 0.9397\n","Epoch 58/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.2850 - accuracy: 0.9298 - val_loss: 0.5734 - val_accuracy: 0.9285\n","Epoch 59/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.2733 - accuracy: 0.9350 - val_loss: 0.6087 - val_accuracy: 0.9188\n","Epoch 60/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.2676 - accuracy: 0.9349 - val_loss: 0.6079 - val_accuracy: 0.9212\n","Average Validation Accuracy: 0.9221221804618835\n","Average Validation Loss: 0.4288884997367859\n","Average Test Accuracy: 0.9189946055412292\n","Final Test Accuracy for each fold: 0.9306405186653137\n","Number of input features: 14\n","Fold: 1\n","Epoch 1/60\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"]},{"output_type":"stream","name":"stdout","text":["1846/1846 [==============================] - 7s 3ms/step - loss: 4.5096 - accuracy: 0.0862 - val_loss: 3.9626 - val_accuracy: 0.1340\n","Epoch 2/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.4812 - accuracy: 0.2077 - val_loss: 3.2889 - val_accuracy: 0.2477\n","Epoch 3/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.8428 - accuracy: 0.3349 - val_loss: 2.7256 - val_accuracy: 0.3883\n","Epoch 4/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.2492 - accuracy: 0.5119 - val_loss: 2.2293 - val_accuracy: 0.5844\n","Epoch 5/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.7868 - accuracy: 0.6132 - val_loss: 1.8660 - val_accuracy: 0.6328\n","Epoch 6/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.4816 - accuracy: 0.6599 - val_loss: 1.6568 - val_accuracy: 0.6587\n","Epoch 7/60\n","1846/1846 [==============================] - 6s 4ms/step - loss: 1.2756 - accuracy: 0.6949 - val_loss: 1.5038 - val_accuracy: 0.6981\n","Epoch 8/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.1327 - accuracy: 0.7196 - val_loss: 1.4200 - val_accuracy: 0.7371\n","Epoch 9/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0268 - accuracy: 0.7430 - val_loss: 1.3382 - val_accuracy: 0.7507\n","Epoch 10/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9463 - accuracy: 0.7683 - val_loss: 1.2610 - val_accuracy: 0.7608\n","Epoch 11/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8890 - accuracy: 0.7764 - val_loss: 1.2102 - val_accuracy: 0.7789\n","Epoch 12/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8496 - accuracy: 0.7827 - val_loss: 1.2154 - val_accuracy: 0.7804\n","Epoch 13/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8022 - accuracy: 0.7982 - val_loss: 1.2196 - val_accuracy: 0.7908\n","Epoch 14/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7745 - accuracy: 0.8041 - val_loss: 1.1638 - val_accuracy: 0.7842\n","Epoch 15/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7456 - accuracy: 0.8100 - val_loss: 1.0704 - val_accuracy: 0.8053\n","Epoch 16/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7114 - accuracy: 0.8219 - val_loss: 1.1169 - val_accuracy: 0.7890\n","Epoch 17/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6910 - accuracy: 0.8260 - val_loss: 1.0811 - val_accuracy: 0.8134\n","Epoch 18/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6781 - accuracy: 0.8273 - val_loss: 1.0503 - val_accuracy: 0.8222\n","Epoch 19/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6510 - accuracy: 0.8346 - val_loss: 1.0533 - val_accuracy: 0.8233\n","Epoch 20/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6380 - accuracy: 0.8398 - val_loss: 1.0190 - val_accuracy: 0.8240\n","Epoch 21/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6190 - accuracy: 0.8421 - val_loss: 1.0300 - val_accuracy: 0.8288\n","Epoch 22/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6101 - accuracy: 0.8432 - val_loss: 0.9908 - val_accuracy: 0.7985\n","Epoch 23/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.5919 - accuracy: 0.8515 - val_loss: 0.9799 - val_accuracy: 0.8392\n","Epoch 24/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5867 - accuracy: 0.8520 - val_loss: 0.9368 - val_accuracy: 0.8442\n","Epoch 25/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.5730 - accuracy: 0.8553 - val_loss: 0.9341 - val_accuracy: 0.8607\n","Epoch 26/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5574 - accuracy: 0.8600 - val_loss: 0.9378 - val_accuracy: 0.8297\n","Epoch 27/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5537 - accuracy: 0.8595 - val_loss: 0.9112 - val_accuracy: 0.8475\n","Epoch 28/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5401 - accuracy: 0.8623 - val_loss: 0.8968 - val_accuracy: 0.8559\n","Epoch 29/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5325 - accuracy: 0.8620 - val_loss: 0.9415 - val_accuracy: 0.8475\n","Epoch 30/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5256 - accuracy: 0.8697 - val_loss: 0.8735 - val_accuracy: 0.8550\n","Epoch 31/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5173 - accuracy: 0.8705 - val_loss: 0.8610 - val_accuracy: 0.8680\n","Epoch 32/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5087 - accuracy: 0.8726 - val_loss: 0.8546 - val_accuracy: 0.8640\n","Epoch 33/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5158 - accuracy: 0.8695 - val_loss: 0.8550 - val_accuracy: 0.8920\n","Epoch 34/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.4979 - accuracy: 0.8767 - val_loss: 0.8149 - val_accuracy: 0.8728\n","Epoch 35/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4795 - accuracy: 0.8800 - val_loss: 0.9704 - val_accuracy: 0.8002\n","Epoch 36/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4845 - accuracy: 0.8784 - val_loss: 0.8396 - val_accuracy: 0.8585\n","Epoch 37/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4726 - accuracy: 0.8826 - val_loss: 0.8736 - val_accuracy: 0.8607\n","Epoch 38/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4703 - accuracy: 0.8814 - val_loss: 0.8293 - val_accuracy: 0.8717\n","Epoch 39/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4524 - accuracy: 0.8880 - val_loss: 0.8027 - val_accuracy: 0.8719\n","Epoch 40/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4658 - accuracy: 0.8860 - val_loss: 0.7668 - val_accuracy: 0.8862\n","Epoch 41/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4503 - accuracy: 0.8887 - val_loss: 0.7733 - val_accuracy: 0.8887\n","Epoch 42/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4455 - accuracy: 0.8866 - val_loss: 0.7995 - val_accuracy: 0.8871\n","Epoch 43/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4408 - accuracy: 0.8907 - val_loss: 0.7724 - val_accuracy: 0.8827\n","Epoch 44/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4274 - accuracy: 0.8949 - val_loss: 0.7950 - val_accuracy: 0.8961\n","Epoch 45/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4371 - accuracy: 0.8885 - val_loss: 0.7619 - val_accuracy: 0.8821\n","Epoch 46/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4346 - accuracy: 0.8929 - val_loss: 0.8069 - val_accuracy: 0.8682\n","Epoch 47/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4165 - accuracy: 0.8988 - val_loss: 0.7645 - val_accuracy: 0.8744\n","Epoch 48/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4102 - accuracy: 0.8977 - val_loss: 0.7327 - val_accuracy: 0.8955\n","Epoch 49/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4177 - accuracy: 0.8970 - val_loss: 0.7317 - val_accuracy: 0.8977\n","Epoch 50/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4088 - accuracy: 0.8978 - val_loss: 0.7063 - val_accuracy: 0.8999\n","Epoch 51/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3923 - accuracy: 0.9056 - val_loss: 0.7523 - val_accuracy: 0.8686\n","Epoch 52/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4045 - accuracy: 0.9004 - val_loss: 0.7408 - val_accuracy: 0.8876\n","Epoch 53/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.3867 - accuracy: 0.9049 - val_loss: 0.7071 - val_accuracy: 0.8983\n","Epoch 54/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4008 - accuracy: 0.9006 - val_loss: 0.6944 - val_accuracy: 0.8983\n","Epoch 55/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3795 - accuracy: 0.9088 - val_loss: 0.6827 - val_accuracy: 0.8994\n","Epoch 56/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3995 - accuracy: 0.9034 - val_loss: 0.6850 - val_accuracy: 0.9039\n","Epoch 57/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3670 - accuracy: 0.9095 - val_loss: 0.6733 - val_accuracy: 0.8889\n","Epoch 58/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3805 - accuracy: 0.9084 - val_loss: 0.6792 - val_accuracy: 0.8948\n","Epoch 59/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3785 - accuracy: 0.9062 - val_loss: 0.6579 - val_accuracy: 0.9074\n","Epoch 60/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3676 - accuracy: 0.9107 - val_loss: 0.6444 - val_accuracy: 0.9162\n","Fold: 2\n","Epoch 1/60\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"]},{"output_type":"stream","name":"stdout","text":["1846/1846 [==============================] - 6s 3ms/step - loss: 4.4846 - accuracy: 0.0832 - val_loss: 3.9922 - val_accuracy: 0.1582\n","Epoch 2/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 3.5317 - accuracy: 0.1985 - val_loss: 3.3706 - val_accuracy: 0.2546\n","Epoch 3/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.9247 - accuracy: 0.3259 - val_loss: 2.8200 - val_accuracy: 0.4442\n","Epoch 4/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.2717 - accuracy: 0.4833 - val_loss: 2.2956 - val_accuracy: 0.5424\n","Epoch 5/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.7443 - accuracy: 0.6038 - val_loss: 1.9519 - val_accuracy: 0.6315\n","Epoch 6/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.4155 - accuracy: 0.6691 - val_loss: 1.7142 - val_accuracy: 0.6926\n","Epoch 7/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.1869 - accuracy: 0.7166 - val_loss: 1.5719 - val_accuracy: 0.7318\n","Epoch 8/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0416 - accuracy: 0.7451 - val_loss: 1.4020 - val_accuracy: 0.7384\n","Epoch 9/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9247 - accuracy: 0.7693 - val_loss: 1.3400 - val_accuracy: 0.7771\n","Epoch 10/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8544 - accuracy: 0.7809 - val_loss: 1.3249 - val_accuracy: 0.7296\n","Epoch 11/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7823 - accuracy: 0.7995 - val_loss: 1.1996 - val_accuracy: 0.8015\n","Epoch 12/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7486 - accuracy: 0.8032 - val_loss: 1.1738 - val_accuracy: 0.7877\n","Epoch 13/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6983 - accuracy: 0.8143 - val_loss: 1.0814 - val_accuracy: 0.8180\n","Epoch 14/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6675 - accuracy: 0.8227 - val_loss: 1.0870 - val_accuracy: 0.8180\n","Epoch 15/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6390 - accuracy: 0.8279 - val_loss: 1.0212 - val_accuracy: 0.8185\n","Epoch 16/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6053 - accuracy: 0.8408 - val_loss: 1.0165 - val_accuracy: 0.8167\n","Epoch 17/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5855 - accuracy: 0.8460 - val_loss: 0.9971 - val_accuracy: 0.8310\n","Epoch 18/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.5648 - accuracy: 0.8517 - val_loss: 0.9364 - val_accuracy: 0.8484\n","Epoch 19/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5361 - accuracy: 0.8584 - val_loss: 0.9348 - val_accuracy: 0.8623\n","Epoch 20/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5213 - accuracy: 0.8641 - val_loss: 0.9258 - val_accuracy: 0.8400\n","Epoch 21/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4919 - accuracy: 0.8721 - val_loss: 0.8503 - val_accuracy: 0.8711\n","Epoch 22/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4779 - accuracy: 0.8731 - val_loss: 0.8542 - val_accuracy: 0.8469\n","Epoch 23/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4676 - accuracy: 0.8776 - val_loss: 0.8945 - val_accuracy: 0.8453\n","Epoch 24/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4549 - accuracy: 0.8784 - val_loss: 0.8241 - val_accuracy: 0.8706\n","Epoch 25/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4524 - accuracy: 0.8793 - val_loss: 0.7857 - val_accuracy: 0.8781\n","Epoch 26/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4292 - accuracy: 0.8862 - val_loss: 0.7634 - val_accuracy: 0.8783\n","Epoch 27/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4172 - accuracy: 0.8895 - val_loss: 0.7681 - val_accuracy: 0.8801\n","Epoch 28/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4060 - accuracy: 0.8934 - val_loss: 0.7752 - val_accuracy: 0.8719\n","Epoch 29/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3947 - accuracy: 0.8965 - val_loss: 0.7533 - val_accuracy: 0.8799\n","Epoch 30/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3884 - accuracy: 0.8992 - val_loss: 0.8535 - val_accuracy: 0.8625\n","Epoch 31/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3858 - accuracy: 0.9003 - val_loss: 0.7248 - val_accuracy: 0.8981\n","Epoch 32/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3739 - accuracy: 0.9019 - val_loss: 0.7299 - val_accuracy: 0.9012\n","Epoch 33/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3641 - accuracy: 0.9065 - val_loss: 0.7191 - val_accuracy: 0.8917\n","Epoch 34/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.3672 - accuracy: 0.9055 - val_loss: 0.6876 - val_accuracy: 0.9072\n","Epoch 35/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3475 - accuracy: 0.9122 - val_loss: 0.6765 - val_accuracy: 0.9058\n","Epoch 36/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3552 - accuracy: 0.9097 - val_loss: 0.6911 - val_accuracy: 0.9032\n","Epoch 37/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3391 - accuracy: 0.9144 - val_loss: 0.6404 - val_accuracy: 0.8937\n","Epoch 38/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3358 - accuracy: 0.9149 - val_loss: 0.6244 - val_accuracy: 0.9091\n","Epoch 39/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3286 - accuracy: 0.9162 - val_loss: 0.6638 - val_accuracy: 0.9069\n","Epoch 40/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3268 - accuracy: 0.9175 - val_loss: 0.6687 - val_accuracy: 0.8917\n","Epoch 41/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.3306 - accuracy: 0.9182 - val_loss: 0.7180 - val_accuracy: 0.8902\n","Epoch 42/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3160 - accuracy: 0.9200 - val_loss: 0.6335 - val_accuracy: 0.9098\n","Epoch 43/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3071 - accuracy: 0.9252 - val_loss: 0.6352 - val_accuracy: 0.8990\n","Epoch 44/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.2977 - accuracy: 0.9262 - val_loss: 0.6755 - val_accuracy: 0.9034\n","Epoch 45/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.2995 - accuracy: 0.9290 - val_loss: 0.5888 - val_accuracy: 0.9265\n","Epoch 46/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.2868 - accuracy: 0.9328 - val_loss: 0.5817 - val_accuracy: 0.9175\n","Epoch 47/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.2950 - accuracy: 0.9279 - val_loss: 0.5851 - val_accuracy: 0.9292\n","Epoch 48/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.2897 - accuracy: 0.9296 - val_loss: 0.5543 - val_accuracy: 0.9305\n","Epoch 49/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.2843 - accuracy: 0.9328 - val_loss: 0.6094 - val_accuracy: 0.9241\n","Epoch 50/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.2685 - accuracy: 0.9377 - val_loss: 0.5543 - val_accuracy: 0.9274\n","Epoch 51/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.2814 - accuracy: 0.9322 - val_loss: 0.5222 - val_accuracy: 0.9399\n","Epoch 52/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.2727 - accuracy: 0.9353 - val_loss: 0.5252 - val_accuracy: 0.9278\n","Epoch 53/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.2644 - accuracy: 0.9401 - val_loss: 0.5602 - val_accuracy: 0.9318\n","Epoch 54/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.2748 - accuracy: 0.9356 - val_loss: 0.5271 - val_accuracy: 0.9402\n","Epoch 55/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.2685 - accuracy: 0.9374 - val_loss: 0.5335 - val_accuracy: 0.9274\n","Epoch 56/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.2506 - accuracy: 0.9426 - val_loss: 0.6105 - val_accuracy: 0.8854\n","Epoch 57/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.2587 - accuracy: 0.9369 - val_loss: 0.5393 - val_accuracy: 0.9307\n","Epoch 58/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.2510 - accuracy: 0.9434 - val_loss: 0.4879 - val_accuracy: 0.9342\n","Epoch 59/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.2627 - accuracy: 0.9391 - val_loss: 0.5157 - val_accuracy: 0.9353\n","Epoch 60/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.2474 - accuracy: 0.9449 - val_loss: 0.4911 - val_accuracy: 0.9399\n","Average Validation Accuracy: 0.9362455904483795\n","Average Validation Loss: 0.3714313805103302\n","Average Test Accuracy: 0.9370899796485901\n","Final Test Accuracy for each fold: 0.9499520659446716\n","Number of input features: 15\n","Fold: 1\n","Epoch 1/60\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"]},{"output_type":"stream","name":"stdout","text":["1846/1846 [==============================] - 7s 3ms/step - loss: 4.7086 - accuracy: 0.0713 - val_loss: 4.2095 - val_accuracy: 0.1025\n","Epoch 2/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.8074 - accuracy: 0.1553 - val_loss: 3.6742 - val_accuracy: 0.1692\n","Epoch 3/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 3.2786 - accuracy: 0.2339 - val_loss: 3.2396 - val_accuracy: 0.2647\n","Epoch 4/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.7415 - accuracy: 0.3938 - val_loss: 2.6914 - val_accuracy: 0.4706\n","Epoch 5/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.2083 - accuracy: 0.5219 - val_loss: 2.2990 - val_accuracy: 0.5776\n","Epoch 6/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.8430 - accuracy: 0.5926 - val_loss: 2.0334 - val_accuracy: 0.6207\n","Epoch 7/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.5888 - accuracy: 0.6364 - val_loss: 1.8356 - val_accuracy: 0.6418\n","Epoch 8/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.3949 - accuracy: 0.6666 - val_loss: 1.7371 - val_accuracy: 0.6860\n","Epoch 9/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.2553 - accuracy: 0.6940 - val_loss: 1.6284 - val_accuracy: 0.7085\n","Epoch 10/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.1553 - accuracy: 0.7173 - val_loss: 1.5438 - val_accuracy: 0.6997\n","Epoch 11/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0668 - accuracy: 0.7347 - val_loss: 1.4987 - val_accuracy: 0.7085\n","Epoch 12/60\n","1846/1846 [==============================] - 6s 4ms/step - loss: 0.9998 - accuracy: 0.7501 - val_loss: 1.4134 - val_accuracy: 0.7468\n","Epoch 13/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9497 - accuracy: 0.7605 - val_loss: 1.3976 - val_accuracy: 0.7369\n","Epoch 14/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9056 - accuracy: 0.7718 - val_loss: 1.3516 - val_accuracy: 0.7595\n","Epoch 15/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8520 - accuracy: 0.7847 - val_loss: 1.3274 - val_accuracy: 0.7872\n","Epoch 16/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8116 - accuracy: 0.7931 - val_loss: 1.2764 - val_accuracy: 0.7637\n","Epoch 17/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7885 - accuracy: 0.7966 - val_loss: 1.2317 - val_accuracy: 0.7932\n","Epoch 18/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7490 - accuracy: 0.8110 - val_loss: 1.2320 - val_accuracy: 0.8084\n","Epoch 19/60\n","1846/1846 [==============================] - 6s 4ms/step - loss: 0.7097 - accuracy: 0.8209 - val_loss: 1.2250 - val_accuracy: 0.8022\n","Epoch 20/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6922 - accuracy: 0.8217 - val_loss: 1.1396 - val_accuracy: 0.8282\n","Epoch 21/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6800 - accuracy: 0.8258 - val_loss: 1.1402 - val_accuracy: 0.8103\n","Epoch 22/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6501 - accuracy: 0.8328 - val_loss: 1.1010 - val_accuracy: 0.8337\n","Epoch 23/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6364 - accuracy: 0.8341 - val_loss: 1.0970 - val_accuracy: 0.8290\n","Epoch 24/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6080 - accuracy: 0.8423 - val_loss: 1.0694 - val_accuracy: 0.8321\n","Epoch 25/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6062 - accuracy: 0.8442 - val_loss: 1.1258 - val_accuracy: 0.8297\n","Epoch 26/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.5761 - accuracy: 0.8518 - val_loss: 1.0396 - val_accuracy: 0.8293\n","Epoch 27/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5653 - accuracy: 0.8528 - val_loss: 1.1236 - val_accuracy: 0.8189\n","Epoch 28/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5673 - accuracy: 0.8539 - val_loss: 1.0773 - val_accuracy: 0.8035\n","Epoch 29/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5518 - accuracy: 0.8564 - val_loss: 1.0205 - val_accuracy: 0.8308\n","Epoch 30/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5279 - accuracy: 0.8629 - val_loss: 0.9865 - val_accuracy: 0.8596\n","Epoch 31/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5139 - accuracy: 0.8648 - val_loss: 1.0287 - val_accuracy: 0.8495\n","Epoch 32/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5130 - accuracy: 0.8698 - val_loss: 0.9886 - val_accuracy: 0.8715\n","Epoch 33/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4975 - accuracy: 0.8738 - val_loss: 0.9626 - val_accuracy: 0.8759\n","Epoch 34/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4950 - accuracy: 0.8720 - val_loss: 0.9338 - val_accuracy: 0.8711\n","Epoch 35/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.4738 - accuracy: 0.8815 - val_loss: 0.9140 - val_accuracy: 0.8779\n","Epoch 36/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4700 - accuracy: 0.8809 - val_loss: 0.9353 - val_accuracy: 0.8790\n","Epoch 37/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4593 - accuracy: 0.8868 - val_loss: 0.9083 - val_accuracy: 0.8821\n","Epoch 38/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4487 - accuracy: 0.8893 - val_loss: 0.9064 - val_accuracy: 0.8739\n","Epoch 39/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4506 - accuracy: 0.8902 - val_loss: 0.8946 - val_accuracy: 0.8715\n","Epoch 40/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4563 - accuracy: 0.8839 - val_loss: 0.8894 - val_accuracy: 0.8849\n","Epoch 41/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4334 - accuracy: 0.8940 - val_loss: 0.8934 - val_accuracy: 0.8942\n","Epoch 42/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4311 - accuracy: 0.8955 - val_loss: 0.9634 - val_accuracy: 0.8557\n","Epoch 43/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4252 - accuracy: 0.8939 - val_loss: 0.8518 - val_accuracy: 0.8986\n","Epoch 44/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.4271 - accuracy: 0.8936 - val_loss: 0.8774 - val_accuracy: 0.8761\n","Epoch 45/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4060 - accuracy: 0.9020 - val_loss: 0.8339 - val_accuracy: 0.8906\n","Epoch 46/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.4234 - accuracy: 0.8937 - val_loss: 0.8515 - val_accuracy: 0.8867\n","Epoch 47/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4046 - accuracy: 0.8987 - val_loss: 0.8589 - val_accuracy: 0.8792\n","Epoch 48/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4043 - accuracy: 0.8987 - val_loss: 0.8106 - val_accuracy: 0.9017\n","Epoch 49/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3977 - accuracy: 0.9008 - val_loss: 0.8525 - val_accuracy: 0.8970\n","Epoch 50/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4036 - accuracy: 0.9003 - val_loss: 0.7866 - val_accuracy: 0.9054\n","Epoch 51/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3895 - accuracy: 0.9057 - val_loss: 0.8921 - val_accuracy: 0.8656\n","Epoch 52/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3928 - accuracy: 0.9028 - val_loss: 0.7674 - val_accuracy: 0.8999\n","Epoch 53/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3611 - accuracy: 0.9113 - val_loss: 0.8241 - val_accuracy: 0.8953\n","Epoch 54/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3847 - accuracy: 0.9061 - val_loss: 0.8316 - val_accuracy: 0.8849\n","Epoch 55/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3659 - accuracy: 0.9122 - val_loss: 0.8230 - val_accuracy: 0.8887\n","Epoch 56/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3696 - accuracy: 0.9107 - val_loss: 0.7570 - val_accuracy: 0.9133\n","Epoch 57/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3641 - accuracy: 0.9104 - val_loss: 0.7528 - val_accuracy: 0.8950\n","Epoch 58/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3662 - accuracy: 0.9134 - val_loss: 0.7630 - val_accuracy: 0.8968\n","Epoch 59/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3479 - accuracy: 0.9143 - val_loss: 0.7770 - val_accuracy: 0.8926\n","Epoch 60/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3541 - accuracy: 0.9174 - val_loss: 0.7511 - val_accuracy: 0.9052\n","Fold: 2\n","Epoch 1/60\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"]},{"output_type":"stream","name":"stdout","text":["1846/1846 [==============================] - 6s 3ms/step - loss: 4.6779 - accuracy: 0.0597 - val_loss: 4.2232 - val_accuracy: 0.0939\n","Epoch 2/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 3.7992 - accuracy: 0.1561 - val_loss: 3.6161 - val_accuracy: 0.2183\n","Epoch 3/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.1097 - accuracy: 0.3349 - val_loss: 3.0377 - val_accuracy: 0.4398\n","Epoch 4/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.4798 - accuracy: 0.5057 - val_loss: 2.5963 - val_accuracy: 0.5263\n","Epoch 5/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.0100 - accuracy: 0.5578 - val_loss: 2.2858 - val_accuracy: 0.5826\n","Epoch 6/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.6778 - accuracy: 0.6314 - val_loss: 2.1567 - val_accuracy: 0.6077\n","Epoch 7/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.4574 - accuracy: 0.6752 - val_loss: 1.9593 - val_accuracy: 0.6572\n","Epoch 8/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.2991 - accuracy: 0.6963 - val_loss: 1.8568 - val_accuracy: 0.6884\n","Epoch 9/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.1788 - accuracy: 0.7182 - val_loss: 1.7287 - val_accuracy: 0.7289\n","Epoch 10/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0853 - accuracy: 0.7363 - val_loss: 1.6885 - val_accuracy: 0.7424\n","Epoch 11/60\n","1846/1846 [==============================] - 6s 4ms/step - loss: 1.0180 - accuracy: 0.7494 - val_loss: 1.6304 - val_accuracy: 0.7560\n","Epoch 12/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9589 - accuracy: 0.7636 - val_loss: 1.6110 - val_accuracy: 0.7628\n","Epoch 13/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9110 - accuracy: 0.7746 - val_loss: 1.5670 - val_accuracy: 0.7518\n","Epoch 14/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8715 - accuracy: 0.7823 - val_loss: 1.5088 - val_accuracy: 0.7800\n","Epoch 15/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8375 - accuracy: 0.7905 - val_loss: 1.4699 - val_accuracy: 0.7963\n","Epoch 16/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8028 - accuracy: 0.7973 - val_loss: 1.4365 - val_accuracy: 0.8015\n","Epoch 17/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7748 - accuracy: 0.8045 - val_loss: 1.3744 - val_accuracy: 0.8180\n","Epoch 18/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7529 - accuracy: 0.8088 - val_loss: 1.4014 - val_accuracy: 0.7892\n","Epoch 19/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7360 - accuracy: 0.8111 - val_loss: 1.3893 - val_accuracy: 0.7945\n","Epoch 20/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7180 - accuracy: 0.8122 - val_loss: 1.3194 - val_accuracy: 0.8057\n","Epoch 21/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7012 - accuracy: 0.8189 - val_loss: 1.3817 - val_accuracy: 0.7897\n","Epoch 22/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6881 - accuracy: 0.8222 - val_loss: 1.2813 - val_accuracy: 0.8180\n","Epoch 23/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6600 - accuracy: 0.8294 - val_loss: 1.2677 - val_accuracy: 0.8345\n","Epoch 24/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6573 - accuracy: 0.8263 - val_loss: 1.2608 - val_accuracy: 0.8112\n","Epoch 25/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6346 - accuracy: 0.8331 - val_loss: 1.2282 - val_accuracy: 0.8092\n","Epoch 26/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6207 - accuracy: 0.8388 - val_loss: 1.1961 - val_accuracy: 0.8337\n","Epoch 27/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6102 - accuracy: 0.8421 - val_loss: 1.2226 - val_accuracy: 0.8356\n","Epoch 28/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6136 - accuracy: 0.8435 - val_loss: 1.1916 - val_accuracy: 0.8332\n","Epoch 29/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5865 - accuracy: 0.8483 - val_loss: 1.2185 - val_accuracy: 0.8132\n","Epoch 30/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5757 - accuracy: 0.8531 - val_loss: 1.1294 - val_accuracy: 0.8449\n","Epoch 31/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5724 - accuracy: 0.8535 - val_loss: 1.1773 - val_accuracy: 0.8218\n","Epoch 32/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5626 - accuracy: 0.8542 - val_loss: 1.1384 - val_accuracy: 0.8211\n","Epoch 33/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5569 - accuracy: 0.8594 - val_loss: 1.1242 - val_accuracy: 0.8315\n","Epoch 34/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5502 - accuracy: 0.8589 - val_loss: 1.1094 - val_accuracy: 0.8601\n","Epoch 35/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5334 - accuracy: 0.8633 - val_loss: 1.4830 - val_accuracy: 0.7483\n","Epoch 36/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5338 - accuracy: 0.8614 - val_loss: 1.1030 - val_accuracy: 0.8576\n","Epoch 37/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5227 - accuracy: 0.8681 - val_loss: 1.0573 - val_accuracy: 0.8719\n","Epoch 38/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5164 - accuracy: 0.8677 - val_loss: 1.1064 - val_accuracy: 0.8383\n","Epoch 39/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5307 - accuracy: 0.8656 - val_loss: 1.0941 - val_accuracy: 0.8574\n","Epoch 40/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4982 - accuracy: 0.8730 - val_loss: 1.0740 - val_accuracy: 0.8568\n","Epoch 41/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5045 - accuracy: 0.8716 - val_loss: 1.1068 - val_accuracy: 0.8407\n","Epoch 42/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5066 - accuracy: 0.8710 - val_loss: 1.0112 - val_accuracy: 0.8759\n","Epoch 43/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4954 - accuracy: 0.8745 - val_loss: 1.1186 - val_accuracy: 0.8374\n","Epoch 44/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4874 - accuracy: 0.8759 - val_loss: 1.0311 - val_accuracy: 0.8548\n","Epoch 45/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4827 - accuracy: 0.8775 - val_loss: 0.9710 - val_accuracy: 0.8768\n","Epoch 46/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4788 - accuracy: 0.8780 - val_loss: 0.9947 - val_accuracy: 0.8792\n","Epoch 47/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4917 - accuracy: 0.8733 - val_loss: 1.0106 - val_accuracy: 0.8581\n","Epoch 48/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4672 - accuracy: 0.8820 - val_loss: 0.9940 - val_accuracy: 0.8563\n","Epoch 49/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.4792 - accuracy: 0.8797 - val_loss: 1.0157 - val_accuracy: 0.8605\n","Epoch 50/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4648 - accuracy: 0.8824 - val_loss: 0.9911 - val_accuracy: 0.8647\n","Epoch 51/60\n","1846/1846 [==============================] - 6s 4ms/step - loss: 0.4594 - accuracy: 0.8811 - val_loss: 0.9719 - val_accuracy: 0.8550\n","Epoch 52/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4593 - accuracy: 0.8827 - val_loss: 0.9498 - val_accuracy: 0.8759\n","Epoch 53/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4434 - accuracy: 0.8875 - val_loss: 1.0301 - val_accuracy: 0.8535\n","Epoch 54/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4565 - accuracy: 0.8834 - val_loss: 0.9850 - val_accuracy: 0.8671\n","Epoch 55/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4462 - accuracy: 0.8877 - val_loss: 0.9529 - val_accuracy: 0.8658\n","Epoch 56/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4367 - accuracy: 0.8896 - val_loss: 0.9997 - val_accuracy: 0.8510\n","Epoch 57/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4435 - accuracy: 0.8875 - val_loss: 0.9286 - val_accuracy: 0.8689\n","Epoch 58/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4404 - accuracy: 0.8875 - val_loss: 0.9693 - val_accuracy: 0.8495\n","Epoch 59/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4325 - accuracy: 0.8914 - val_loss: 0.9120 - val_accuracy: 0.8865\n","Epoch 60/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4349 - accuracy: 0.8905 - val_loss: 0.9436 - val_accuracy: 0.8781\n","Average Validation Accuracy: 0.910612165927887\n","Average Validation Loss: 0.516994908452034\n","Average Test Accuracy: 0.9080120623111725\n","Final Test Accuracy for each fold: 0.9191420078277588\n","Number of input features: 16\n","Fold: 1\n","Epoch 1/60\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"]},{"output_type":"stream","name":"stdout","text":["1846/1846 [==============================] - 7s 3ms/step - loss: 4.7662 - accuracy: 0.0596 - val_loss: 4.3111 - val_accuracy: 0.1226\n","Epoch 2/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.9426 - accuracy: 0.1559 - val_loss: 3.7932 - val_accuracy: 0.1681\n","Epoch 3/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 3.5184 - accuracy: 0.1934 - val_loss: 3.4686 - val_accuracy: 0.1855\n","Epoch 4/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.1547 - accuracy: 0.2567 - val_loss: 3.1119 - val_accuracy: 0.2557\n","Epoch 5/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.7244 - accuracy: 0.3769 - val_loss: 2.7041 - val_accuracy: 0.4145\n","Epoch 6/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.3167 - accuracy: 0.4741 - val_loss: 2.3788 - val_accuracy: 0.4563\n","Epoch 7/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.0055 - accuracy: 0.5393 - val_loss: 2.1610 - val_accuracy: 0.5604\n","Epoch 8/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.7747 - accuracy: 0.5966 - val_loss: 1.9545 - val_accuracy: 0.6180\n","Epoch 9/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.6034 - accuracy: 0.6367 - val_loss: 1.8152 - val_accuracy: 0.6438\n","Epoch 10/60\n","1846/1846 [==============================] - 6s 4ms/step - loss: 1.4631 - accuracy: 0.6608 - val_loss: 1.7293 - val_accuracy: 0.6763\n","Epoch 11/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.3522 - accuracy: 0.6843 - val_loss: 1.6102 - val_accuracy: 0.6975\n","Epoch 12/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.2622 - accuracy: 0.6995 - val_loss: 1.5192 - val_accuracy: 0.7140\n","Epoch 13/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.1794 - accuracy: 0.7153 - val_loss: 1.5166 - val_accuracy: 0.7047\n","Epoch 14/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.1107 - accuracy: 0.7314 - val_loss: 1.5480 - val_accuracy: 0.6997\n","Epoch 15/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.0464 - accuracy: 0.7459 - val_loss: 1.3934 - val_accuracy: 0.7655\n","Epoch 16/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0005 - accuracy: 0.7591 - val_loss: 1.4137 - val_accuracy: 0.7452\n","Epoch 17/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9490 - accuracy: 0.7665 - val_loss: 1.3538 - val_accuracy: 0.7776\n","Epoch 18/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9257 - accuracy: 0.7723 - val_loss: 1.3294 - val_accuracy: 0.7608\n","Epoch 19/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8771 - accuracy: 0.7877 - val_loss: 1.2719 - val_accuracy: 0.7791\n","Epoch 20/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8520 - accuracy: 0.7865 - val_loss: 1.3132 - val_accuracy: 0.7628\n","Epoch 21/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8284 - accuracy: 0.7979 - val_loss: 1.2759 - val_accuracy: 0.7850\n","Epoch 22/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7952 - accuracy: 0.8027 - val_loss: 1.2291 - val_accuracy: 0.8141\n","Epoch 23/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7781 - accuracy: 0.8073 - val_loss: 1.2240 - val_accuracy: 0.7941\n","Epoch 24/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7535 - accuracy: 0.8123 - val_loss: 1.2838 - val_accuracy: 0.7699\n","Epoch 25/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7370 - accuracy: 0.8217 - val_loss: 1.2283 - val_accuracy: 0.7877\n","Epoch 26/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7182 - accuracy: 0.8221 - val_loss: 1.2087 - val_accuracy: 0.8251\n","Epoch 27/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6984 - accuracy: 0.8284 - val_loss: 1.2424 - val_accuracy: 0.8064\n","Epoch 28/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6883 - accuracy: 0.8279 - val_loss: 1.1541 - val_accuracy: 0.8238\n","Epoch 29/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.6630 - accuracy: 0.8348 - val_loss: 1.1855 - val_accuracy: 0.8218\n","Epoch 30/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6576 - accuracy: 0.8371 - val_loss: 1.2308 - val_accuracy: 0.7844\n","Epoch 31/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6367 - accuracy: 0.8398 - val_loss: 1.1533 - val_accuracy: 0.8200\n","Epoch 32/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6231 - accuracy: 0.8439 - val_loss: 1.1114 - val_accuracy: 0.8462\n","Epoch 33/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6205 - accuracy: 0.8450 - val_loss: 1.0958 - val_accuracy: 0.8508\n","Epoch 34/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6084 - accuracy: 0.8478 - val_loss: 1.1754 - val_accuracy: 0.8163\n","Epoch 35/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6099 - accuracy: 0.8427 - val_loss: 1.1121 - val_accuracy: 0.8594\n","Epoch 36/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.5939 - accuracy: 0.8543 - val_loss: 1.1587 - val_accuracy: 0.8306\n","Epoch 37/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5966 - accuracy: 0.8503 - val_loss: 1.1062 - val_accuracy: 0.8376\n","Epoch 38/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5655 - accuracy: 0.8595 - val_loss: 1.0717 - val_accuracy: 0.8546\n","Epoch 39/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5666 - accuracy: 0.8577 - val_loss: 1.1422 - val_accuracy: 0.8361\n","Epoch 40/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5554 - accuracy: 0.8615 - val_loss: 1.1054 - val_accuracy: 0.8275\n","Epoch 41/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5555 - accuracy: 0.8615 - val_loss: 1.1307 - val_accuracy: 0.8328\n","Epoch 42/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5481 - accuracy: 0.8636 - val_loss: 1.0626 - val_accuracy: 0.8416\n","Epoch 43/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5280 - accuracy: 0.8657 - val_loss: 1.1063 - val_accuracy: 0.8427\n","Epoch 44/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5344 - accuracy: 0.8660 - val_loss: 1.0604 - val_accuracy: 0.8548\n","Epoch 45/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.5164 - accuracy: 0.8736 - val_loss: 1.0165 - val_accuracy: 0.8521\n","Epoch 46/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5197 - accuracy: 0.8700 - val_loss: 1.0315 - val_accuracy: 0.8629\n","Epoch 47/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5169 - accuracy: 0.8670 - val_loss: 1.0278 - val_accuracy: 0.8572\n","Epoch 48/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5090 - accuracy: 0.8739 - val_loss: 1.0061 - val_accuracy: 0.8724\n","Epoch 49/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4919 - accuracy: 0.8774 - val_loss: 0.9838 - val_accuracy: 0.8766\n","Epoch 50/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5012 - accuracy: 0.8735 - val_loss: 0.9889 - val_accuracy: 0.8653\n","Epoch 51/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4855 - accuracy: 0.8788 - val_loss: 1.0261 - val_accuracy: 0.8440\n","Epoch 52/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4874 - accuracy: 0.8767 - val_loss: 1.0193 - val_accuracy: 0.8614\n","Epoch 53/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4773 - accuracy: 0.8812 - val_loss: 1.0116 - val_accuracy: 0.8667\n","Epoch 54/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4772 - accuracy: 0.8804 - val_loss: 1.0190 - val_accuracy: 0.8550\n","Epoch 55/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4601 - accuracy: 0.8851 - val_loss: 1.0269 - val_accuracy: 0.8726\n","Epoch 56/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4673 - accuracy: 0.8845 - val_loss: 1.0111 - val_accuracy: 0.8629\n","Epoch 57/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4638 - accuracy: 0.8863 - val_loss: 0.9920 - val_accuracy: 0.8715\n","Epoch 58/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4552 - accuracy: 0.8845 - val_loss: 0.9415 - val_accuracy: 0.8829\n","Epoch 59/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.4460 - accuracy: 0.8900 - val_loss: 0.9418 - val_accuracy: 0.8638\n","Epoch 60/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4482 - accuracy: 0.8883 - val_loss: 0.9675 - val_accuracy: 0.8711\n","Fold: 2\n","Epoch 1/60\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"]},{"output_type":"stream","name":"stdout","text":["1846/1846 [==============================] - 6s 3ms/step - loss: 4.7379 - accuracy: 0.0717 - val_loss: 4.3156 - val_accuracy: 0.1430\n","Epoch 2/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 3.8981 - accuracy: 0.1538 - val_loss: 3.7805 - val_accuracy: 0.2033\n","Epoch 3/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.4831 - accuracy: 0.2013 - val_loss: 3.4761 - val_accuracy: 0.2484\n","Epoch 4/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 3.1013 - accuracy: 0.2739 - val_loss: 3.0869 - val_accuracy: 0.3470\n","Epoch 5/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.6268 - accuracy: 0.3818 - val_loss: 2.6485 - val_accuracy: 0.4191\n","Epoch 6/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.1785 - accuracy: 0.4943 - val_loss: 2.3250 - val_accuracy: 0.4823\n","Epoch 7/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.8481 - accuracy: 0.5759 - val_loss: 2.0477 - val_accuracy: 0.6092\n","Epoch 8/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.5803 - accuracy: 0.6376 - val_loss: 1.8461 - val_accuracy: 0.6469\n","Epoch 9/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.3768 - accuracy: 0.6744 - val_loss: 1.6993 - val_accuracy: 0.6744\n","Epoch 10/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.2363 - accuracy: 0.6955 - val_loss: 1.5710 - val_accuracy: 0.6955\n","Epoch 11/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.1375 - accuracy: 0.7087 - val_loss: 1.5040 - val_accuracy: 0.6961\n","Epoch 12/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0590 - accuracy: 0.7309 - val_loss: 1.3978 - val_accuracy: 0.6898\n","Epoch 13/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9916 - accuracy: 0.7404 - val_loss: 1.3304 - val_accuracy: 0.7448\n","Epoch 14/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9304 - accuracy: 0.7580 - val_loss: 1.2527 - val_accuracy: 0.7461\n","Epoch 15/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8892 - accuracy: 0.7625 - val_loss: 1.2101 - val_accuracy: 0.7492\n","Epoch 16/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8448 - accuracy: 0.7745 - val_loss: 1.1516 - val_accuracy: 0.7578\n","Epoch 17/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8006 - accuracy: 0.7864 - val_loss: 1.1755 - val_accuracy: 0.7591\n","Epoch 18/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7718 - accuracy: 0.7968 - val_loss: 1.1382 - val_accuracy: 0.7718\n","Epoch 19/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7511 - accuracy: 0.7980 - val_loss: 1.1322 - val_accuracy: 0.7608\n","Epoch 20/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7150 - accuracy: 0.8109 - val_loss: 1.0304 - val_accuracy: 0.7872\n","Epoch 21/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6996 - accuracy: 0.8130 - val_loss: 1.0131 - val_accuracy: 0.7971\n","Epoch 22/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6768 - accuracy: 0.8235 - val_loss: 0.9399 - val_accuracy: 0.7901\n","Epoch 23/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6566 - accuracy: 0.8253 - val_loss: 0.9426 - val_accuracy: 0.7947\n","Epoch 24/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6413 - accuracy: 0.8327 - val_loss: 0.9470 - val_accuracy: 0.7974\n","Epoch 25/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6226 - accuracy: 0.8359 - val_loss: 0.9176 - val_accuracy: 0.8306\n","Epoch 26/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6166 - accuracy: 0.8340 - val_loss: 0.8970 - val_accuracy: 0.8165\n","Epoch 27/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5939 - accuracy: 0.8433 - val_loss: 0.8758 - val_accuracy: 0.8301\n","Epoch 28/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5867 - accuracy: 0.8438 - val_loss: 0.8491 - val_accuracy: 0.8224\n","Epoch 29/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5806 - accuracy: 0.8475 - val_loss: 0.8924 - val_accuracy: 0.8185\n","Epoch 30/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5641 - accuracy: 0.8491 - val_loss: 0.7910 - val_accuracy: 0.8392\n","Epoch 31/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5441 - accuracy: 0.8552 - val_loss: 0.8416 - val_accuracy: 0.8147\n","Epoch 32/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5416 - accuracy: 0.8539 - val_loss: 0.8051 - val_accuracy: 0.8462\n","Epoch 33/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5291 - accuracy: 0.8576 - val_loss: 0.8080 - val_accuracy: 0.8088\n","Epoch 34/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5231 - accuracy: 0.8566 - val_loss: 0.7608 - val_accuracy: 0.8343\n","Epoch 35/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5102 - accuracy: 0.8633 - val_loss: 0.7882 - val_accuracy: 0.8345\n","Epoch 36/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4958 - accuracy: 0.8672 - val_loss: 0.7413 - val_accuracy: 0.8609\n","Epoch 37/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.5088 - accuracy: 0.8607 - val_loss: 0.7908 - val_accuracy: 0.8341\n","Epoch 38/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4884 - accuracy: 0.8683 - val_loss: 0.7214 - val_accuracy: 0.8568\n","Epoch 39/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.4820 - accuracy: 0.8716 - val_loss: 0.6978 - val_accuracy: 0.8642\n","Epoch 40/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4859 - accuracy: 0.8659 - val_loss: 0.8716 - val_accuracy: 0.8231\n","Epoch 41/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4776 - accuracy: 0.8708 - val_loss: 0.7843 - val_accuracy: 0.8271\n","Epoch 42/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4605 - accuracy: 0.8736 - val_loss: 0.7381 - val_accuracy: 0.8323\n","Epoch 43/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4792 - accuracy: 0.8718 - val_loss: 0.6893 - val_accuracy: 0.8649\n","Epoch 44/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4487 - accuracy: 0.8780 - val_loss: 0.6874 - val_accuracy: 0.8581\n","Epoch 45/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4561 - accuracy: 0.8783 - val_loss: 0.6831 - val_accuracy: 0.8656\n","Epoch 46/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4392 - accuracy: 0.8822 - val_loss: 0.8317 - val_accuracy: 0.8097\n","Epoch 47/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4409 - accuracy: 0.8805 - val_loss: 0.9250 - val_accuracy: 0.8092\n","Epoch 48/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4332 - accuracy: 0.8850 - val_loss: 0.6637 - val_accuracy: 0.8706\n","Epoch 49/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4415 - accuracy: 0.8813 - val_loss: 0.6418 - val_accuracy: 0.8799\n","Epoch 50/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4250 - accuracy: 0.8853 - val_loss: 0.6692 - val_accuracy: 0.8554\n","Epoch 51/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4215 - accuracy: 0.8860 - val_loss: 0.6781 - val_accuracy: 0.8565\n","Epoch 52/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4328 - accuracy: 0.8836 - val_loss: 0.8092 - val_accuracy: 0.8301\n","Epoch 53/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4178 - accuracy: 0.8873 - val_loss: 0.6592 - val_accuracy: 0.8587\n","Epoch 54/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4079 - accuracy: 0.8915 - val_loss: 0.6178 - val_accuracy: 0.8829\n","Epoch 55/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4170 - accuracy: 0.8870 - val_loss: 0.6015 - val_accuracy: 0.8860\n","Epoch 56/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.4023 - accuracy: 0.8887 - val_loss: 0.6104 - val_accuracy: 0.8827\n","Epoch 57/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.4006 - accuracy: 0.8951 - val_loss: 0.6457 - val_accuracy: 0.8761\n","Epoch 58/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.3890 - accuracy: 0.8976 - val_loss: 0.7033 - val_accuracy: 0.8730\n","Epoch 59/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.3988 - accuracy: 0.8930 - val_loss: 0.6422 - val_accuracy: 0.8612\n","Epoch 60/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.3939 - accuracy: 0.8937 - val_loss: 0.6291 - val_accuracy: 0.8638\n","Average Validation Accuracy: 0.8834187388420105\n","Average Validation Loss: 0.5509486645460129\n","Average Test Accuracy: 0.8827301561832428\n","Final Test Accuracy for each fold: 0.8887742161750793\n","Number of input features: 17\n","Fold: 1\n","Epoch 1/60\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"]},{"output_type":"stream","name":"stdout","text":["1846/1846 [==============================] - 7s 3ms/step - loss: 4.7116 - accuracy: 0.0710 - val_loss: 4.2265 - val_accuracy: 0.1243\n","Epoch 2/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.8666 - accuracy: 0.1513 - val_loss: 3.7320 - val_accuracy: 0.1459\n","Epoch 3/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 3.4020 - accuracy: 0.2051 - val_loss: 3.3001 - val_accuracy: 0.2548\n","Epoch 4/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.8844 - accuracy: 0.3292 - val_loss: 2.7795 - val_accuracy: 0.3817\n","Epoch 5/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.3989 - accuracy: 0.4355 - val_loss: 2.4333 - val_accuracy: 0.4680\n","Epoch 6/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.0799 - accuracy: 0.5049 - val_loss: 2.1899 - val_accuracy: 0.5270\n","Epoch 7/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.8719 - accuracy: 0.5468 - val_loss: 2.0339 - val_accuracy: 0.5604\n","Epoch 8/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.7069 - accuracy: 0.5793 - val_loss: 1.9459 - val_accuracy: 0.5927\n","Epoch 9/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.5722 - accuracy: 0.6054 - val_loss: 1.8023 - val_accuracy: 0.6169\n","Epoch 10/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.4552 - accuracy: 0.6354 - val_loss: 1.6961 - val_accuracy: 0.6519\n","Epoch 11/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.3514 - accuracy: 0.6604 - val_loss: 1.6548 - val_accuracy: 0.6557\n","Epoch 12/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.2830 - accuracy: 0.6748 - val_loss: 1.6585 - val_accuracy: 0.6546\n","Epoch 13/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.2283 - accuracy: 0.6834 - val_loss: 1.6027 - val_accuracy: 0.6821\n","Epoch 14/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.1802 - accuracy: 0.6941 - val_loss: 1.5143 - val_accuracy: 0.6986\n","Epoch 15/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.1366 - accuracy: 0.7008 - val_loss: 1.4790 - val_accuracy: 0.6878\n","Epoch 16/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.1069 - accuracy: 0.7141 - val_loss: 1.4967 - val_accuracy: 0.6928\n","Epoch 17/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.0732 - accuracy: 0.7169 - val_loss: 1.4778 - val_accuracy: 0.6955\n","Epoch 18/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0404 - accuracy: 0.7241 - val_loss: 1.4709 - val_accuracy: 0.7021\n","Epoch 19/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0079 - accuracy: 0.7350 - val_loss: 1.3891 - val_accuracy: 0.7404\n","Epoch 20/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9858 - accuracy: 0.7456 - val_loss: 1.3805 - val_accuracy: 0.7111\n","Epoch 21/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9661 - accuracy: 0.7425 - val_loss: 1.3328 - val_accuracy: 0.7428\n","Epoch 22/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9432 - accuracy: 0.7491 - val_loss: 1.3224 - val_accuracy: 0.7512\n","Epoch 23/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9233 - accuracy: 0.7530 - val_loss: 1.3070 - val_accuracy: 0.7520\n","Epoch 24/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9008 - accuracy: 0.7572 - val_loss: 1.3320 - val_accuracy: 0.7485\n","Epoch 25/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8924 - accuracy: 0.7605 - val_loss: 1.2779 - val_accuracy: 0.7613\n","Epoch 26/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8649 - accuracy: 0.7680 - val_loss: 1.3030 - val_accuracy: 0.7470\n","Epoch 27/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8566 - accuracy: 0.7667 - val_loss: 1.3043 - val_accuracy: 0.7461\n","Epoch 28/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8394 - accuracy: 0.7732 - val_loss: 1.2528 - val_accuracy: 0.7690\n","Epoch 29/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8288 - accuracy: 0.7749 - val_loss: 1.2362 - val_accuracy: 0.7611\n","Epoch 30/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8166 - accuracy: 0.7820 - val_loss: 1.2323 - val_accuracy: 0.7648\n","Epoch 31/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8063 - accuracy: 0.7812 - val_loss: 1.2404 - val_accuracy: 0.7606\n","Epoch 32/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7938 - accuracy: 0.7870 - val_loss: 1.2276 - val_accuracy: 0.7707\n","Epoch 33/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7837 - accuracy: 0.7903 - val_loss: 1.2320 - val_accuracy: 0.7762\n","Epoch 34/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7821 - accuracy: 0.7873 - val_loss: 1.1908 - val_accuracy: 0.7798\n","Epoch 35/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7708 - accuracy: 0.7900 - val_loss: 1.2051 - val_accuracy: 0.7822\n","Epoch 36/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7567 - accuracy: 0.7958 - val_loss: 1.1687 - val_accuracy: 0.7982\n","Epoch 37/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7535 - accuracy: 0.7925 - val_loss: 1.1296 - val_accuracy: 0.7846\n","Epoch 38/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7480 - accuracy: 0.7953 - val_loss: 1.1681 - val_accuracy: 0.7835\n","Epoch 39/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7301 - accuracy: 0.8033 - val_loss: 1.1794 - val_accuracy: 0.7679\n","Epoch 40/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7229 - accuracy: 0.8036 - val_loss: 1.1874 - val_accuracy: 0.7784\n","Epoch 41/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7209 - accuracy: 0.8111 - val_loss: 1.1624 - val_accuracy: 0.7815\n","Epoch 42/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7259 - accuracy: 0.8023 - val_loss: 1.1462 - val_accuracy: 0.7844\n","Epoch 43/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7093 - accuracy: 0.8100 - val_loss: 1.1139 - val_accuracy: 0.8128\n","Epoch 44/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7103 - accuracy: 0.8100 - val_loss: 1.0793 - val_accuracy: 0.8167\n","Epoch 45/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6832 - accuracy: 0.8169 - val_loss: 1.1388 - val_accuracy: 0.7998\n","Epoch 46/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6915 - accuracy: 0.8123 - val_loss: 1.1287 - val_accuracy: 0.7901\n","Epoch 47/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.6880 - accuracy: 0.8150 - val_loss: 1.1077 - val_accuracy: 0.7963\n","Epoch 48/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6777 - accuracy: 0.8206 - val_loss: 1.1014 - val_accuracy: 0.8090\n","Epoch 49/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6704 - accuracy: 0.8213 - val_loss: 1.1095 - val_accuracy: 0.8066\n","Epoch 50/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6777 - accuracy: 0.8176 - val_loss: 1.0893 - val_accuracy: 0.8086\n","Epoch 51/60\n","1846/1846 [==============================] - 6s 4ms/step - loss: 0.6608 - accuracy: 0.8252 - val_loss: 1.0727 - val_accuracy: 0.8090\n","Epoch 52/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6615 - accuracy: 0.8255 - val_loss: 1.1914 - val_accuracy: 0.7800\n","Epoch 53/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6531 - accuracy: 0.8270 - val_loss: 1.0410 - val_accuracy: 0.8139\n","Epoch 54/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6427 - accuracy: 0.8263 - val_loss: 1.1575 - val_accuracy: 0.7916\n","Epoch 55/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6409 - accuracy: 0.8284 - val_loss: 1.0711 - val_accuracy: 0.8117\n","Epoch 56/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6324 - accuracy: 0.8307 - val_loss: 1.0689 - val_accuracy: 0.7985\n","Epoch 57/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6298 - accuracy: 0.8311 - val_loss: 1.0803 - val_accuracy: 0.8103\n","Epoch 58/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6222 - accuracy: 0.8363 - val_loss: 1.1885 - val_accuracy: 0.7655\n","Epoch 59/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6280 - accuracy: 0.8336 - val_loss: 1.1169 - val_accuracy: 0.7813\n","Epoch 60/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6255 - accuracy: 0.8334 - val_loss: 1.0502 - val_accuracy: 0.8114\n","Fold: 2\n","Epoch 1/60\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"]},{"output_type":"stream","name":"stdout","text":["1846/1846 [==============================] - 6s 3ms/step - loss: 4.9298 - accuracy: 0.0527 - val_loss: 4.6798 - val_accuracy: 0.0634\n","Epoch 2/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 4.1531 - accuracy: 0.1124 - val_loss: 3.9482 - val_accuracy: 0.1556\n","Epoch 3/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.5882 - accuracy: 0.1808 - val_loss: 3.5319 - val_accuracy: 0.2275\n","Epoch 4/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 3.1530 - accuracy: 0.2556 - val_loss: 3.1200 - val_accuracy: 0.3177\n","Epoch 5/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.6729 - accuracy: 0.3743 - val_loss: 2.6737 - val_accuracy: 0.4200\n","Epoch 6/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.2961 - accuracy: 0.4621 - val_loss: 2.4008 - val_accuracy: 0.4807\n","Epoch 7/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.0502 - accuracy: 0.5069 - val_loss: 2.2159 - val_accuracy: 0.5287\n","Epoch 8/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.8702 - accuracy: 0.5495 - val_loss: 2.0497 - val_accuracy: 0.5692\n","Epoch 9/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.7331 - accuracy: 0.5786 - val_loss: 1.9786 - val_accuracy: 0.5721\n","Epoch 10/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.6166 - accuracy: 0.6026 - val_loss: 1.8751 - val_accuracy: 0.5844\n","Epoch 11/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.5118 - accuracy: 0.6278 - val_loss: 1.7790 - val_accuracy: 0.6257\n","Epoch 12/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.4208 - accuracy: 0.6431 - val_loss: 1.7222 - val_accuracy: 0.6381\n","Epoch 13/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.3455 - accuracy: 0.6606 - val_loss: 1.7199 - val_accuracy: 0.6297\n","Epoch 14/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.2850 - accuracy: 0.6779 - val_loss: 1.6218 - val_accuracy: 0.6713\n","Epoch 15/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.2346 - accuracy: 0.6891 - val_loss: 1.5631 - val_accuracy: 0.6851\n","Epoch 16/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.1919 - accuracy: 0.7020 - val_loss: 1.6325 - val_accuracy: 0.6675\n","Epoch 17/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.1507 - accuracy: 0.7092 - val_loss: 1.4889 - val_accuracy: 0.7017\n","Epoch 18/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.1251 - accuracy: 0.7155 - val_loss: 1.4698 - val_accuracy: 0.7008\n","Epoch 19/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0927 - accuracy: 0.7241 - val_loss: 1.4477 - val_accuracy: 0.7122\n","Epoch 20/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.0644 - accuracy: 0.7345 - val_loss: 1.4180 - val_accuracy: 0.7248\n","Epoch 21/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0362 - accuracy: 0.7370 - val_loss: 1.3873 - val_accuracy: 0.7250\n","Epoch 22/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.0135 - accuracy: 0.7394 - val_loss: 1.4351 - val_accuracy: 0.7226\n","Epoch 23/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9942 - accuracy: 0.7450 - val_loss: 1.3633 - val_accuracy: 0.7287\n","Epoch 24/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.9740 - accuracy: 0.7493 - val_loss: 1.3286 - val_accuracy: 0.7360\n","Epoch 25/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9373 - accuracy: 0.7585 - val_loss: 1.3310 - val_accuracy: 0.7362\n","Epoch 26/60\n","1846/1846 [==============================] - 6s 4ms/step - loss: 0.9276 - accuracy: 0.7615 - val_loss: 1.2978 - val_accuracy: 0.7454\n","Epoch 27/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9106 - accuracy: 0.7632 - val_loss: 1.3752 - val_accuracy: 0.7410\n","Epoch 28/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.8934 - accuracy: 0.7660 - val_loss: 1.2683 - val_accuracy: 0.7531\n","Epoch 29/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8745 - accuracy: 0.7714 - val_loss: 1.2930 - val_accuracy: 0.7619\n","Epoch 30/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8532 - accuracy: 0.7772 - val_loss: 1.2482 - val_accuracy: 0.7573\n","Epoch 31/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8287 - accuracy: 0.7840 - val_loss: 1.2123 - val_accuracy: 0.7784\n","Epoch 32/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8280 - accuracy: 0.7827 - val_loss: 1.1951 - val_accuracy: 0.7771\n","Epoch 33/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8137 - accuracy: 0.7894 - val_loss: 1.1681 - val_accuracy: 0.7875\n","Epoch 34/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7937 - accuracy: 0.7942 - val_loss: 1.1444 - val_accuracy: 0.7947\n","Epoch 35/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7807 - accuracy: 0.8010 - val_loss: 1.1754 - val_accuracy: 0.7795\n","Epoch 36/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7734 - accuracy: 0.8012 - val_loss: 1.2375 - val_accuracy: 0.7714\n","Epoch 37/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7646 - accuracy: 0.7999 - val_loss: 1.1509 - val_accuracy: 0.7908\n","Epoch 38/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7474 - accuracy: 0.8107 - val_loss: 1.1370 - val_accuracy: 0.7960\n","Epoch 39/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7378 - accuracy: 0.8123 - val_loss: 1.1098 - val_accuracy: 0.7952\n","Epoch 40/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7282 - accuracy: 0.8120 - val_loss: 1.1180 - val_accuracy: 0.7848\n","Epoch 41/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7295 - accuracy: 0.8113 - val_loss: 1.1092 - val_accuracy: 0.7930\n","Epoch 42/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7145 - accuracy: 0.8147 - val_loss: 1.0782 - val_accuracy: 0.8112\n","Epoch 43/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7116 - accuracy: 0.8164 - val_loss: 1.0501 - val_accuracy: 0.8020\n","Epoch 44/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6962 - accuracy: 0.8184 - val_loss: 1.0204 - val_accuracy: 0.8112\n","Epoch 45/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6934 - accuracy: 0.8191 - val_loss: 1.1166 - val_accuracy: 0.7903\n","Epoch 46/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6846 - accuracy: 0.8275 - val_loss: 1.1385 - val_accuracy: 0.7778\n","Epoch 47/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6794 - accuracy: 0.8246 - val_loss: 1.1093 - val_accuracy: 0.8031\n","Epoch 48/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6700 - accuracy: 0.8274 - val_loss: 1.0401 - val_accuracy: 0.8097\n","Epoch 49/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6687 - accuracy: 0.8303 - val_loss: 1.0075 - val_accuracy: 0.8299\n","Epoch 50/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6619 - accuracy: 0.8315 - val_loss: 1.0865 - val_accuracy: 0.7932\n","Epoch 51/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6551 - accuracy: 0.8333 - val_loss: 1.0090 - val_accuracy: 0.8301\n","Epoch 52/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.6548 - accuracy: 0.8350 - val_loss: 1.0337 - val_accuracy: 0.8147\n","Epoch 53/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6477 - accuracy: 0.8361 - val_loss: 1.0180 - val_accuracy: 0.8143\n","Epoch 54/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6388 - accuracy: 0.8392 - val_loss: 1.0146 - val_accuracy: 0.8262\n","Epoch 55/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6337 - accuracy: 0.8387 - val_loss: 0.9902 - val_accuracy: 0.8238\n","Epoch 56/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6339 - accuracy: 0.8407 - val_loss: 0.9829 - val_accuracy: 0.8231\n","Epoch 57/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6243 - accuracy: 0.8409 - val_loss: 1.0141 - val_accuracy: 0.8266\n","Epoch 58/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6262 - accuracy: 0.8395 - val_loss: 0.9593 - val_accuracy: 0.8370\n","Epoch 59/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6123 - accuracy: 0.8414 - val_loss: 1.0155 - val_accuracy: 0.8169\n","Epoch 60/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6062 - accuracy: 0.8502 - val_loss: 0.9969 - val_accuracy: 0.8323\n","Average Validation Accuracy: 0.836982399225235\n","Average Validation Loss: 0.7498063147068024\n","Average Test Accuracy: 0.8376575410366058\n","Final Test Accuracy for each fold: 0.8435173630714417\n","Number of input features: 18\n","Fold: 1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/60\n","1846/1846 [==============================] - 7s 3ms/step - loss: 4.8264 - accuracy: 0.0589 - val_loss: 4.4389 - val_accuracy: 0.0671\n","Epoch 2/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 4.0190 - accuracy: 0.1409 - val_loss: 3.8723 - val_accuracy: 0.1701\n","Epoch 3/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 3.5480 - accuracy: 0.1950 - val_loss: 3.5023 - val_accuracy: 0.2440\n","Epoch 4/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.1108 - accuracy: 0.2729 - val_loss: 3.0090 - val_accuracy: 0.3105\n","Epoch 5/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.6082 - accuracy: 0.3891 - val_loss: 2.5916 - val_accuracy: 0.4130\n","Epoch 6/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.2558 - accuracy: 0.4557 - val_loss: 2.3393 - val_accuracy: 0.4708\n","Epoch 7/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.0084 - accuracy: 0.5046 - val_loss: 2.1539 - val_accuracy: 0.5195\n","Epoch 8/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.8215 - accuracy: 0.5458 - val_loss: 2.0545 - val_accuracy: 0.5322\n","Epoch 9/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.6774 - accuracy: 0.5746 - val_loss: 1.8800 - val_accuracy: 0.5899\n","Epoch 10/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.5563 - accuracy: 0.5992 - val_loss: 1.8160 - val_accuracy: 0.6216\n","Epoch 11/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.4634 - accuracy: 0.6271 - val_loss: 1.7455 - val_accuracy: 0.6343\n","Epoch 12/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.3919 - accuracy: 0.6408 - val_loss: 1.6774 - val_accuracy: 0.6431\n","Epoch 13/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.3294 - accuracy: 0.6543 - val_loss: 1.6667 - val_accuracy: 0.6328\n","Epoch 14/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.2738 - accuracy: 0.6619 - val_loss: 1.6303 - val_accuracy: 0.6530\n","Epoch 15/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.2365 - accuracy: 0.6749 - val_loss: 1.6035 - val_accuracy: 0.6653\n","Epoch 16/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.1946 - accuracy: 0.6862 - val_loss: 1.5614 - val_accuracy: 0.6695\n","Epoch 17/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.1570 - accuracy: 0.6878 - val_loss: 1.5024 - val_accuracy: 0.6950\n","Epoch 18/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.1277 - accuracy: 0.6962 - val_loss: 1.5179 - val_accuracy: 0.6829\n","Epoch 19/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0966 - accuracy: 0.7049 - val_loss: 1.5620 - val_accuracy: 0.6737\n","Epoch 20/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0738 - accuracy: 0.7049 - val_loss: 1.5034 - val_accuracy: 0.6909\n","Epoch 21/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.0452 - accuracy: 0.7210 - val_loss: 1.5502 - val_accuracy: 0.6647\n","Epoch 22/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0303 - accuracy: 0.7149 - val_loss: 1.3770 - val_accuracy: 0.7454\n","Epoch 23/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.0059 - accuracy: 0.7237 - val_loss: 1.4147 - val_accuracy: 0.7151\n","Epoch 24/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9882 - accuracy: 0.7328 - val_loss: 1.4647 - val_accuracy: 0.7008\n","Epoch 25/60\n","1846/1846 [==============================] - 6s 4ms/step - loss: 0.9701 - accuracy: 0.7362 - val_loss: 1.3504 - val_accuracy: 0.7309\n","Epoch 26/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9529 - accuracy: 0.7469 - val_loss: 1.4134 - val_accuracy: 0.6939\n","Epoch 27/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.9260 - accuracy: 0.7478 - val_loss: 1.3706 - val_accuracy: 0.7298\n","Epoch 28/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9193 - accuracy: 0.7541 - val_loss: 1.3419 - val_accuracy: 0.7074\n","Epoch 29/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9023 - accuracy: 0.7572 - val_loss: 1.3356 - val_accuracy: 0.7538\n","Epoch 30/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8867 - accuracy: 0.7611 - val_loss: 1.2649 - val_accuracy: 0.7509\n","Epoch 31/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8673 - accuracy: 0.7675 - val_loss: 1.2801 - val_accuracy: 0.7564\n","Epoch 32/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8553 - accuracy: 0.7724 - val_loss: 1.2836 - val_accuracy: 0.7558\n","Epoch 33/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8489 - accuracy: 0.7731 - val_loss: 1.2217 - val_accuracy: 0.7718\n","Epoch 34/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8307 - accuracy: 0.7787 - val_loss: 1.2581 - val_accuracy: 0.7630\n","Epoch 35/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8174 - accuracy: 0.7824 - val_loss: 1.2323 - val_accuracy: 0.7718\n","Epoch 36/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8166 - accuracy: 0.7834 - val_loss: 1.2229 - val_accuracy: 0.7701\n","Epoch 37/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7955 - accuracy: 0.7866 - val_loss: 1.1831 - val_accuracy: 0.7773\n","Epoch 38/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7894 - accuracy: 0.7894 - val_loss: 1.2109 - val_accuracy: 0.7630\n","Epoch 39/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7803 - accuracy: 0.7914 - val_loss: 1.1739 - val_accuracy: 0.7765\n","Epoch 40/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7652 - accuracy: 0.7954 - val_loss: 1.2051 - val_accuracy: 0.7771\n","Epoch 41/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7517 - accuracy: 0.7979 - val_loss: 1.2580 - val_accuracy: 0.7553\n","Epoch 42/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7506 - accuracy: 0.8008 - val_loss: 1.1452 - val_accuracy: 0.8095\n","Epoch 43/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7400 - accuracy: 0.8042 - val_loss: 1.1437 - val_accuracy: 0.7910\n","Epoch 44/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7303 - accuracy: 0.8034 - val_loss: 1.1675 - val_accuracy: 0.7890\n","Epoch 45/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7303 - accuracy: 0.8045 - val_loss: 1.1611 - val_accuracy: 0.7952\n","Epoch 46/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7040 - accuracy: 0.8165 - val_loss: 1.1825 - val_accuracy: 0.7892\n","Epoch 47/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7037 - accuracy: 0.8131 - val_loss: 1.1192 - val_accuracy: 0.8011\n","Epoch 48/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7006 - accuracy: 0.8137 - val_loss: 1.1894 - val_accuracy: 0.7916\n","Epoch 49/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6906 - accuracy: 0.8199 - val_loss: 1.1396 - val_accuracy: 0.7963\n","Epoch 50/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6814 - accuracy: 0.8203 - val_loss: 1.0981 - val_accuracy: 0.8176\n","Epoch 51/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6733 - accuracy: 0.8208 - val_loss: 1.1838 - val_accuracy: 0.7778\n","Epoch 52/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6716 - accuracy: 0.8204 - val_loss: 1.1117 - val_accuracy: 0.8022\n","Epoch 53/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6649 - accuracy: 0.8242 - val_loss: 1.1364 - val_accuracy: 0.7910\n","Epoch 54/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6571 - accuracy: 0.8307 - val_loss: 1.0339 - val_accuracy: 0.8191\n","Epoch 55/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6536 - accuracy: 0.8295 - val_loss: 1.1286 - val_accuracy: 0.7839\n","Epoch 56/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6416 - accuracy: 0.8315 - val_loss: 1.1005 - val_accuracy: 0.8117\n","Epoch 57/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6424 - accuracy: 0.8315 - val_loss: 1.0937 - val_accuracy: 0.8035\n","Epoch 58/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6281 - accuracy: 0.8309 - val_loss: 1.0554 - val_accuracy: 0.8260\n","Epoch 59/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6168 - accuracy: 0.8433 - val_loss: 1.0517 - val_accuracy: 0.8205\n","Epoch 60/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.6304 - accuracy: 0.8349 - val_loss: 1.1203 - val_accuracy: 0.7853\n","Fold: 2\n","Epoch 1/60\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"]},{"output_type":"stream","name":"stdout","text":["1846/1846 [==============================] - 7s 3ms/step - loss: 4.7606 - accuracy: 0.0659 - val_loss: 4.2906 - val_accuracy: 0.1138\n","Epoch 2/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 3.8954 - accuracy: 0.1500 - val_loss: 3.7792 - val_accuracy: 0.1663\n","Epoch 3/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.4084 - accuracy: 0.2060 - val_loss: 3.3116 - val_accuracy: 0.2579\n","Epoch 4/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.8636 - accuracy: 0.3549 - val_loss: 2.8491 - val_accuracy: 0.4106\n","Epoch 5/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.4183 - accuracy: 0.4412 - val_loss: 2.5873 - val_accuracy: 0.4257\n","Epoch 6/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.1429 - accuracy: 0.4881 - val_loss: 2.3542 - val_accuracy: 0.5113\n","Epoch 7/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.9391 - accuracy: 0.5333 - val_loss: 2.2012 - val_accuracy: 0.5397\n","Epoch 8/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.8001 - accuracy: 0.5578 - val_loss: 2.1790 - val_accuracy: 0.5171\n","Epoch 9/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.6716 - accuracy: 0.5826 - val_loss: 2.0392 - val_accuracy: 0.5868\n","Epoch 10/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.5682 - accuracy: 0.6002 - val_loss: 1.8818 - val_accuracy: 0.6119\n","Epoch 11/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.4847 - accuracy: 0.6168 - val_loss: 1.8158 - val_accuracy: 0.6185\n","Epoch 12/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.4013 - accuracy: 0.6370 - val_loss: 1.7413 - val_accuracy: 0.6266\n","Epoch 13/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.3355 - accuracy: 0.6505 - val_loss: 1.7185 - val_accuracy: 0.6337\n","Epoch 14/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.2760 - accuracy: 0.6670 - val_loss: 1.6627 - val_accuracy: 0.6620\n","Epoch 15/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.2174 - accuracy: 0.6783 - val_loss: 1.5856 - val_accuracy: 0.6724\n","Epoch 16/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.1637 - accuracy: 0.6888 - val_loss: 1.6041 - val_accuracy: 0.6887\n","Epoch 17/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.1064 - accuracy: 0.7090 - val_loss: 1.5419 - val_accuracy: 0.6887\n","Epoch 18/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.0548 - accuracy: 0.7256 - val_loss: 1.5074 - val_accuracy: 0.7206\n","Epoch 19/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.0217 - accuracy: 0.7291 - val_loss: 1.5070 - val_accuracy: 0.7182\n","Epoch 20/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.9850 - accuracy: 0.7439 - val_loss: 1.4183 - val_accuracy: 0.7490\n","Epoch 21/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9596 - accuracy: 0.7496 - val_loss: 1.3950 - val_accuracy: 0.7479\n","Epoch 22/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.9275 - accuracy: 0.7532 - val_loss: 1.3938 - val_accuracy: 0.7408\n","Epoch 23/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9039 - accuracy: 0.7642 - val_loss: 1.3813 - val_accuracy: 0.7549\n","Epoch 24/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.8760 - accuracy: 0.7705 - val_loss: 1.3368 - val_accuracy: 0.7496\n","Epoch 25/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8629 - accuracy: 0.7692 - val_loss: 1.3420 - val_accuracy: 0.7483\n","Epoch 26/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8386 - accuracy: 0.7792 - val_loss: 1.3172 - val_accuracy: 0.7465\n","Epoch 27/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8257 - accuracy: 0.7835 - val_loss: 1.3019 - val_accuracy: 0.7723\n","Epoch 28/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8130 - accuracy: 0.7806 - val_loss: 1.3417 - val_accuracy: 0.7527\n","Epoch 29/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7900 - accuracy: 0.7926 - val_loss: 1.2375 - val_accuracy: 0.7813\n","Epoch 30/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7766 - accuracy: 0.7960 - val_loss: 1.2179 - val_accuracy: 0.7925\n","Epoch 31/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7692 - accuracy: 0.7925 - val_loss: 1.2342 - val_accuracy: 0.7795\n","Epoch 32/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7491 - accuracy: 0.8012 - val_loss: 1.2112 - val_accuracy: 0.7945\n","Epoch 33/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7488 - accuracy: 0.7984 - val_loss: 1.1985 - val_accuracy: 0.7855\n","Epoch 34/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7397 - accuracy: 0.8002 - val_loss: 1.1856 - val_accuracy: 0.7960\n","Epoch 35/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7378 - accuracy: 0.8009 - val_loss: 1.1651 - val_accuracy: 0.7987\n","Epoch 36/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7182 - accuracy: 0.8065 - val_loss: 1.1875 - val_accuracy: 0.7749\n","Epoch 37/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7132 - accuracy: 0.8084 - val_loss: 1.1524 - val_accuracy: 0.7914\n","Epoch 38/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7088 - accuracy: 0.8119 - val_loss: 1.1572 - val_accuracy: 0.8007\n","Epoch 39/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6982 - accuracy: 0.8150 - val_loss: 1.2519 - val_accuracy: 0.7712\n","Epoch 40/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.6777 - accuracy: 0.8201 - val_loss: 1.1389 - val_accuracy: 0.7976\n","Epoch 41/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6815 - accuracy: 0.8192 - val_loss: 1.0944 - val_accuracy: 0.8139\n","Epoch 42/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.6651 - accuracy: 0.8245 - val_loss: 1.1225 - val_accuracy: 0.8095\n","Epoch 43/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6724 - accuracy: 0.8211 - val_loss: 1.1638 - val_accuracy: 0.7899\n","Epoch 44/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.6615 - accuracy: 0.8246 - val_loss: 1.2082 - val_accuracy: 0.7971\n","Epoch 45/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6539 - accuracy: 0.8258 - val_loss: 1.1207 - val_accuracy: 0.8044\n","Epoch 46/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6452 - accuracy: 0.8285 - val_loss: 1.0742 - val_accuracy: 0.8319\n","Epoch 47/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6396 - accuracy: 0.8317 - val_loss: 1.0725 - val_accuracy: 0.8207\n","Epoch 48/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.6305 - accuracy: 0.8337 - val_loss: 1.0517 - val_accuracy: 0.8284\n","Epoch 49/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6208 - accuracy: 0.8354 - val_loss: 1.0640 - val_accuracy: 0.8246\n","Epoch 50/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.6309 - accuracy: 0.8354 - val_loss: 1.0918 - val_accuracy: 0.8174\n","Epoch 51/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6302 - accuracy: 0.8343 - val_loss: 1.0820 - val_accuracy: 0.8253\n","Epoch 52/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.6146 - accuracy: 0.8395 - val_loss: 1.0959 - val_accuracy: 0.8167\n","Epoch 53/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6169 - accuracy: 0.8363 - val_loss: 1.0981 - val_accuracy: 0.7960\n","Epoch 54/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.6055 - accuracy: 0.8416 - val_loss: 1.0365 - val_accuracy: 0.8304\n","Epoch 55/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6063 - accuracy: 0.8377 - val_loss: 1.0558 - val_accuracy: 0.8312\n","Epoch 56/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6041 - accuracy: 0.8422 - val_loss: 1.0490 - val_accuracy: 0.8249\n","Epoch 57/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5940 - accuracy: 0.8443 - val_loss: 1.0294 - val_accuracy: 0.8222\n","Epoch 58/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.5929 - accuracy: 0.8466 - val_loss: 1.0761 - val_accuracy: 0.8169\n","Epoch 59/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5872 - accuracy: 0.8435 - val_loss: 1.0034 - val_accuracy: 0.8376\n","Epoch 60/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.5783 - accuracy: 0.8472 - val_loss: 0.9924 - val_accuracy: 0.8473\n","Average Validation Accuracy: 0.8376005589962006\n","Average Validation Loss: 0.7519713044166565\n","Average Test Accuracy: 0.8316871821880341\n","Final Test Accuracy for each fold: 0.8658509850502014\n","Number of input features: 19\n","Fold: 1\n","Epoch 1/60\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"]},{"output_type":"stream","name":"stdout","text":["1846/1846 [==============================] - 6s 3ms/step - loss: 4.7946 - accuracy: 0.0579 - val_loss: 4.2917 - val_accuracy: 0.0766\n","Epoch 2/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 3.9145 - accuracy: 0.1475 - val_loss: 3.7397 - val_accuracy: 0.1791\n","Epoch 3/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.3920 - accuracy: 0.2186 - val_loss: 3.2481 - val_accuracy: 0.2737\n","Epoch 4/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.8654 - accuracy: 0.3228 - val_loss: 2.7479 - val_accuracy: 0.3793\n","Epoch 5/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.4273 - accuracy: 0.4195 - val_loss: 2.4310 - val_accuracy: 0.4737\n","Epoch 6/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 2.1267 - accuracy: 0.4843 - val_loss: 2.1679 - val_accuracy: 0.5210\n","Epoch 7/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.8872 - accuracy: 0.5359 - val_loss: 1.9819 - val_accuracy: 0.5496\n","Epoch 8/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.6921 - accuracy: 0.5868 - val_loss: 1.8558 - val_accuracy: 0.5870\n","Epoch 9/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.5407 - accuracy: 0.6137 - val_loss: 1.7423 - val_accuracy: 0.6264\n","Epoch 10/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.4346 - accuracy: 0.6381 - val_loss: 1.6414 - val_accuracy: 0.6438\n","Epoch 11/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.3422 - accuracy: 0.6593 - val_loss: 1.5753 - val_accuracy: 0.6466\n","Epoch 12/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.2668 - accuracy: 0.6759 - val_loss: 1.5042 - val_accuracy: 0.6724\n","Epoch 13/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.1992 - accuracy: 0.6948 - val_loss: 1.4301 - val_accuracy: 0.7003\n","Epoch 14/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.1414 - accuracy: 0.7053 - val_loss: 1.3881 - val_accuracy: 0.7226\n","Epoch 15/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.0973 - accuracy: 0.7191 - val_loss: 1.4135 - val_accuracy: 0.6871\n","Epoch 16/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0550 - accuracy: 0.7231 - val_loss: 1.3860 - val_accuracy: 0.6730\n","Epoch 17/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.0167 - accuracy: 0.7339 - val_loss: 1.2889 - val_accuracy: 0.7501\n","Epoch 18/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9819 - accuracy: 0.7437 - val_loss: 1.3278 - val_accuracy: 0.7127\n","Epoch 19/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9562 - accuracy: 0.7491 - val_loss: 1.3087 - val_accuracy: 0.7325\n","Epoch 20/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9293 - accuracy: 0.7559 - val_loss: 1.3639 - val_accuracy: 0.6999\n","Epoch 21/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9070 - accuracy: 0.7547 - val_loss: 1.2284 - val_accuracy: 0.7527\n","Epoch 22/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8840 - accuracy: 0.7620 - val_loss: 1.2185 - val_accuracy: 0.7439\n","Epoch 23/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8575 - accuracy: 0.7686 - val_loss: 1.1570 - val_accuracy: 0.7661\n","Epoch 24/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8542 - accuracy: 0.7719 - val_loss: 1.1967 - val_accuracy: 0.7639\n","Epoch 25/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8397 - accuracy: 0.7757 - val_loss: 1.1199 - val_accuracy: 0.7751\n","Epoch 26/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.8128 - accuracy: 0.7813 - val_loss: 1.1288 - val_accuracy: 0.7740\n","Epoch 27/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8025 - accuracy: 0.7871 - val_loss: 1.1997 - val_accuracy: 0.7703\n","Epoch 28/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7914 - accuracy: 0.7870 - val_loss: 1.1289 - val_accuracy: 0.7760\n","Epoch 29/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7731 - accuracy: 0.7931 - val_loss: 1.1455 - val_accuracy: 0.7712\n","Epoch 30/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7715 - accuracy: 0.7933 - val_loss: 1.1005 - val_accuracy: 0.8022\n","Epoch 31/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7604 - accuracy: 0.7948 - val_loss: 1.1101 - val_accuracy: 0.7771\n","Epoch 32/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7477 - accuracy: 0.7923 - val_loss: 1.1733 - val_accuracy: 0.7679\n","Epoch 33/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7413 - accuracy: 0.7969 - val_loss: 1.1332 - val_accuracy: 0.7760\n","Epoch 34/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7327 - accuracy: 0.8011 - val_loss: 1.1307 - val_accuracy: 0.7743\n","Epoch 35/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7206 - accuracy: 0.8044 - val_loss: 1.0633 - val_accuracy: 0.7980\n","Epoch 36/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7219 - accuracy: 0.8029 - val_loss: 1.1162 - val_accuracy: 0.7958\n","Epoch 37/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7019 - accuracy: 0.8091 - val_loss: 1.0702 - val_accuracy: 0.7965\n","Epoch 38/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7057 - accuracy: 0.8091 - val_loss: 1.0694 - val_accuracy: 0.8040\n","Epoch 39/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6939 - accuracy: 0.8123 - val_loss: 1.0243 - val_accuracy: 0.8141\n","Epoch 40/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.6830 - accuracy: 0.8147 - val_loss: 1.0450 - val_accuracy: 0.7949\n","Epoch 41/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6813 - accuracy: 0.8162 - val_loss: 1.0463 - val_accuracy: 0.8033\n","Epoch 42/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6697 - accuracy: 0.8192 - val_loss: 1.0198 - val_accuracy: 0.8068\n","Epoch 43/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6648 - accuracy: 0.8189 - val_loss: 1.0442 - val_accuracy: 0.8046\n","Epoch 44/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6601 - accuracy: 0.8201 - val_loss: 1.0206 - val_accuracy: 0.8086\n","Epoch 45/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6525 - accuracy: 0.8242 - val_loss: 0.9960 - val_accuracy: 0.8011\n","Epoch 46/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6518 - accuracy: 0.8227 - val_loss: 0.9953 - val_accuracy: 0.8183\n","Epoch 47/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6495 - accuracy: 0.8241 - val_loss: 1.0128 - val_accuracy: 0.8167\n","Epoch 48/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6362 - accuracy: 0.8279 - val_loss: 0.9736 - val_accuracy: 0.8264\n","Epoch 49/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.6433 - accuracy: 0.8263 - val_loss: 0.9855 - val_accuracy: 0.8196\n","Epoch 50/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6250 - accuracy: 0.8324 - val_loss: 0.9971 - val_accuracy: 0.8119\n","Epoch 51/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6237 - accuracy: 0.8292 - val_loss: 0.9933 - val_accuracy: 0.8220\n","Epoch 52/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6174 - accuracy: 0.8342 - val_loss: 0.9649 - val_accuracy: 0.8301\n","Epoch 53/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6112 - accuracy: 0.8337 - val_loss: 0.9314 - val_accuracy: 0.8317\n","Epoch 54/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6061 - accuracy: 0.8378 - val_loss: 0.9698 - val_accuracy: 0.8194\n","Epoch 55/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5976 - accuracy: 0.8401 - val_loss: 0.9535 - val_accuracy: 0.8273\n","Epoch 56/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5919 - accuracy: 0.8410 - val_loss: 1.0133 - val_accuracy: 0.8081\n","Epoch 57/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5927 - accuracy: 0.8447 - val_loss: 0.9217 - val_accuracy: 0.8484\n","Epoch 58/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5947 - accuracy: 0.8349 - val_loss: 0.9975 - val_accuracy: 0.8165\n","Epoch 59/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5857 - accuracy: 0.8422 - val_loss: 0.9272 - val_accuracy: 0.8420\n","Epoch 60/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5781 - accuracy: 0.8424 - val_loss: 0.9552 - val_accuracy: 0.8330\n","Fold: 2\n","Epoch 1/60\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"]},{"output_type":"stream","name":"stdout","text":["1846/1846 [==============================] - 6s 3ms/step - loss: 4.8190 - accuracy: 0.0572 - val_loss: 4.4070 - val_accuracy: 0.0702\n","Epoch 2/60\n","1846/1846 [==============================] - 6s 4ms/step - loss: 3.9979 - accuracy: 0.1377 - val_loss: 3.8856 - val_accuracy: 0.1751\n","Epoch 3/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.5942 - accuracy: 0.1903 - val_loss: 3.6002 - val_accuracy: 0.1949\n","Epoch 4/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 3.2336 - accuracy: 0.2447 - val_loss: 3.2183 - val_accuracy: 0.2603\n","Epoch 5/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.8261 - accuracy: 0.3293 - val_loss: 2.8518 - val_accuracy: 0.3745\n","Epoch 6/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.4477 - accuracy: 0.4071 - val_loss: 2.5561 - val_accuracy: 0.4218\n","Epoch 7/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.1468 - accuracy: 0.4748 - val_loss: 2.2989 - val_accuracy: 0.4823\n","Epoch 8/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.9338 - accuracy: 0.5208 - val_loss: 2.1514 - val_accuracy: 0.5265\n","Epoch 9/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.7743 - accuracy: 0.5463 - val_loss: 2.0412 - val_accuracy: 0.5182\n","Epoch 10/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.6469 - accuracy: 0.5700 - val_loss: 1.8648 - val_accuracy: 0.5648\n","Epoch 11/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.5442 - accuracy: 0.5884 - val_loss: 1.8103 - val_accuracy: 0.5945\n","Epoch 12/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.4514 - accuracy: 0.6131 - val_loss: 1.7415 - val_accuracy: 0.5952\n","Epoch 13/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.3753 - accuracy: 0.6274 - val_loss: 1.7115 - val_accuracy: 0.6057\n","Epoch 14/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.3089 - accuracy: 0.6483 - val_loss: 1.5704 - val_accuracy: 0.6451\n","Epoch 15/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.2564 - accuracy: 0.6609 - val_loss: 1.5757 - val_accuracy: 0.6431\n","Epoch 16/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.2122 - accuracy: 0.6705 - val_loss: 1.5497 - val_accuracy: 0.6493\n","Epoch 17/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.1664 - accuracy: 0.6820 - val_loss: 1.5171 - val_accuracy: 0.6700\n","Epoch 18/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.1426 - accuracy: 0.6864 - val_loss: 1.4829 - val_accuracy: 0.6788\n","Epoch 19/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.1064 - accuracy: 0.6965 - val_loss: 1.4644 - val_accuracy: 0.6761\n","Epoch 20/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.0855 - accuracy: 0.6993 - val_loss: 1.4018 - val_accuracy: 0.6871\n","Epoch 21/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0562 - accuracy: 0.7085 - val_loss: 1.4065 - val_accuracy: 0.6955\n","Epoch 22/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.0291 - accuracy: 0.7200 - val_loss: 1.4519 - val_accuracy: 0.6805\n","Epoch 23/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0139 - accuracy: 0.7200 - val_loss: 1.4017 - val_accuracy: 0.6856\n","Epoch 24/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9952 - accuracy: 0.7252 - val_loss: 1.3347 - val_accuracy: 0.7259\n","Epoch 25/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9768 - accuracy: 0.7301 - val_loss: 1.3588 - val_accuracy: 0.7078\n","Epoch 26/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9592 - accuracy: 0.7376 - val_loss: 1.3023 - val_accuracy: 0.7199\n","Epoch 27/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9500 - accuracy: 0.7383 - val_loss: 1.2877 - val_accuracy: 0.7336\n","Epoch 28/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9227 - accuracy: 0.7476 - val_loss: 1.2670 - val_accuracy: 0.7316\n","Epoch 29/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9097 - accuracy: 0.7551 - val_loss: 1.2294 - val_accuracy: 0.7358\n","Epoch 30/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9011 - accuracy: 0.7512 - val_loss: 1.3303 - val_accuracy: 0.7028\n","Epoch 31/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8882 - accuracy: 0.7506 - val_loss: 1.2046 - val_accuracy: 0.7619\n","Epoch 32/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8745 - accuracy: 0.7584 - val_loss: 1.1827 - val_accuracy: 0.7476\n","Epoch 33/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8660 - accuracy: 0.7622 - val_loss: 1.2137 - val_accuracy: 0.7443\n","Epoch 34/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8462 - accuracy: 0.7675 - val_loss: 1.1702 - val_accuracy: 0.7661\n","Epoch 35/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8292 - accuracy: 0.7757 - val_loss: 1.1240 - val_accuracy: 0.7802\n","Epoch 36/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.8353 - accuracy: 0.7732 - val_loss: 1.1950 - val_accuracy: 0.7413\n","Epoch 37/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8134 - accuracy: 0.7808 - val_loss: 1.1429 - val_accuracy: 0.7738\n","Epoch 38/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8001 - accuracy: 0.7818 - val_loss: 1.1930 - val_accuracy: 0.7432\n","Epoch 39/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7817 - accuracy: 0.7905 - val_loss: 1.1508 - val_accuracy: 0.7622\n","Epoch 40/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7702 - accuracy: 0.7841 - val_loss: 1.1229 - val_accuracy: 0.7716\n","Epoch 41/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7702 - accuracy: 0.7918 - val_loss: 1.1664 - val_accuracy: 0.7586\n","Epoch 42/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7497 - accuracy: 0.8003 - val_loss: 1.0611 - val_accuracy: 0.7949\n","Epoch 43/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7410 - accuracy: 0.7980 - val_loss: 1.0862 - val_accuracy: 0.7925\n","Epoch 44/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7318 - accuracy: 0.8021 - val_loss: 1.0530 - val_accuracy: 0.7967\n","Epoch 45/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7196 - accuracy: 0.8054 - val_loss: 1.0654 - val_accuracy: 0.7888\n","Epoch 46/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7158 - accuracy: 0.8049 - val_loss: 1.0906 - val_accuracy: 0.7798\n","Epoch 47/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6944 - accuracy: 0.8161 - val_loss: 1.0682 - val_accuracy: 0.7905\n","Epoch 48/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6960 - accuracy: 0.8125 - val_loss: 1.0252 - val_accuracy: 0.8106\n","Epoch 49/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6989 - accuracy: 0.8117 - val_loss: 1.0091 - val_accuracy: 0.8117\n","Epoch 50/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6824 - accuracy: 0.8167 - val_loss: 1.0626 - val_accuracy: 0.7910\n","Epoch 51/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6676 - accuracy: 0.8256 - val_loss: 0.9911 - val_accuracy: 0.8145\n","Epoch 52/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6782 - accuracy: 0.8177 - val_loss: 1.0035 - val_accuracy: 0.8099\n","Epoch 53/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6568 - accuracy: 0.8262 - val_loss: 1.0062 - val_accuracy: 0.8090\n","Epoch 54/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6684 - accuracy: 0.8230 - val_loss: 1.0335 - val_accuracy: 0.7932\n","Epoch 55/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6560 - accuracy: 0.8277 - val_loss: 0.9704 - val_accuracy: 0.8086\n","Epoch 56/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6470 - accuracy: 0.8295 - val_loss: 1.1545 - val_accuracy: 0.7602\n","Epoch 57/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6412 - accuracy: 0.8294 - val_loss: 1.0075 - val_accuracy: 0.8059\n","Epoch 58/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6349 - accuracy: 0.8326 - val_loss: 0.9754 - val_accuracy: 0.8196\n","Epoch 59/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6342 - accuracy: 0.8334 - val_loss: 0.9850 - val_accuracy: 0.8117\n","Epoch 60/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6262 - accuracy: 0.8308 - val_loss: 0.9813 - val_accuracy: 0.8077\n","Average Validation Accuracy: 0.8380710482597351\n","Average Validation Loss: 0.7162236869335175\n","Average Test Accuracy: 0.8392422795295715\n","Final Test Accuracy for each fold: 0.8494139909744263\n","Number of input features: 20\n","Fold: 1\n","Epoch 1/60\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"]},{"output_type":"stream","name":"stdout","text":["1846/1846 [==============================] - 7s 3ms/step - loss: 4.7924 - accuracy: 0.0623 - val_loss: 4.3241 - val_accuracy: 0.1023\n","Epoch 2/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.9250 - accuracy: 0.1539 - val_loss: 3.7942 - val_accuracy: 0.1679\n","Epoch 3/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.4785 - accuracy: 0.2029 - val_loss: 3.4378 - val_accuracy: 0.2242\n","Epoch 4/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 3.0871 - accuracy: 0.2770 - val_loss: 3.0662 - val_accuracy: 0.3127\n","Epoch 5/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.6224 - accuracy: 0.3866 - val_loss: 2.6440 - val_accuracy: 0.4310\n","Epoch 6/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.2546 - accuracy: 0.4690 - val_loss: 2.3586 - val_accuracy: 0.4453\n","Epoch 7/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.0129 - accuracy: 0.5150 - val_loss: 2.1443 - val_accuracy: 0.5267\n","Epoch 8/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.8292 - accuracy: 0.5511 - val_loss: 2.0639 - val_accuracy: 0.5428\n","Epoch 9/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.6739 - accuracy: 0.5844 - val_loss: 1.9045 - val_accuracy: 0.5527\n","Epoch 10/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.5518 - accuracy: 0.6104 - val_loss: 1.8279 - val_accuracy: 0.6077\n","Epoch 11/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.4618 - accuracy: 0.6320 - val_loss: 1.7422 - val_accuracy: 0.6297\n","Epoch 12/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.3775 - accuracy: 0.6474 - val_loss: 1.7014 - val_accuracy: 0.6374\n","Epoch 13/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.3098 - accuracy: 0.6657 - val_loss: 1.6923 - val_accuracy: 0.6332\n","Epoch 14/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.2586 - accuracy: 0.6748 - val_loss: 1.6376 - val_accuracy: 0.6486\n","Epoch 15/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.2162 - accuracy: 0.6852 - val_loss: 1.5593 - val_accuracy: 0.6766\n","Epoch 16/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.1696 - accuracy: 0.6924 - val_loss: 1.5068 - val_accuracy: 0.6920\n","Epoch 17/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.1365 - accuracy: 0.7012 - val_loss: 1.5061 - val_accuracy: 0.6873\n","Epoch 18/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.1042 - accuracy: 0.7113 - val_loss: 1.4781 - val_accuracy: 0.7061\n","Epoch 19/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0728 - accuracy: 0.7149 - val_loss: 1.4506 - val_accuracy: 0.6880\n","Epoch 20/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.0483 - accuracy: 0.7224 - val_loss: 1.4685 - val_accuracy: 0.6825\n","Epoch 21/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0174 - accuracy: 0.7297 - val_loss: 1.3900 - val_accuracy: 0.7226\n","Epoch 22/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9912 - accuracy: 0.7308 - val_loss: 1.3327 - val_accuracy: 0.7331\n","Epoch 23/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9633 - accuracy: 0.7433 - val_loss: 1.3933 - val_accuracy: 0.7034\n","Epoch 24/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9419 - accuracy: 0.7466 - val_loss: 1.3429 - val_accuracy: 0.7230\n","Epoch 25/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9263 - accuracy: 0.7471 - val_loss: 1.2809 - val_accuracy: 0.7406\n","Epoch 26/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8992 - accuracy: 0.7586 - val_loss: 1.2674 - val_accuracy: 0.7503\n","Epoch 27/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.8749 - accuracy: 0.7643 - val_loss: 1.2552 - val_accuracy: 0.7556\n","Epoch 28/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8642 - accuracy: 0.7676 - val_loss: 1.2266 - val_accuracy: 0.7593\n","Epoch 29/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8538 - accuracy: 0.7675 - val_loss: 1.2615 - val_accuracy: 0.7472\n","Epoch 30/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8234 - accuracy: 0.7793 - val_loss: 1.2075 - val_accuracy: 0.7545\n","Epoch 31/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8088 - accuracy: 0.7841 - val_loss: 1.1926 - val_accuracy: 0.7613\n","Epoch 32/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7958 - accuracy: 0.7871 - val_loss: 1.1604 - val_accuracy: 0.7908\n","Epoch 33/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7829 - accuracy: 0.7911 - val_loss: 1.3281 - val_accuracy: 0.7415\n","Epoch 34/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7709 - accuracy: 0.7917 - val_loss: 1.1781 - val_accuracy: 0.7780\n","Epoch 35/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7659 - accuracy: 0.7908 - val_loss: 1.1491 - val_accuracy: 0.7820\n","Epoch 36/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7461 - accuracy: 0.8013 - val_loss: 1.1570 - val_accuracy: 0.7883\n","Epoch 37/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7424 - accuracy: 0.8015 - val_loss: 1.1508 - val_accuracy: 0.7938\n","Epoch 38/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7390 - accuracy: 0.8048 - val_loss: 1.1290 - val_accuracy: 0.8029\n","Epoch 39/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7177 - accuracy: 0.8098 - val_loss: 1.1891 - val_accuracy: 0.7454\n","Epoch 40/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7089 - accuracy: 0.8090 - val_loss: 1.1456 - val_accuracy: 0.7976\n","Epoch 41/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7080 - accuracy: 0.8099 - val_loss: 1.1361 - val_accuracy: 0.7879\n","Epoch 42/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6905 - accuracy: 0.8152 - val_loss: 1.1548 - val_accuracy: 0.7850\n","Epoch 43/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6933 - accuracy: 0.8168 - val_loss: 1.1290 - val_accuracy: 0.7897\n","Epoch 44/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6856 - accuracy: 0.8176 - val_loss: 1.0893 - val_accuracy: 0.8123\n","Epoch 45/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6788 - accuracy: 0.8123 - val_loss: 1.0823 - val_accuracy: 0.8090\n","Epoch 46/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6671 - accuracy: 0.8192 - val_loss: 1.2289 - val_accuracy: 0.7804\n","Epoch 47/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6608 - accuracy: 0.8250 - val_loss: 1.1006 - val_accuracy: 0.8092\n","Epoch 48/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6531 - accuracy: 0.8213 - val_loss: 1.0817 - val_accuracy: 0.8183\n","Epoch 49/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6463 - accuracy: 0.8268 - val_loss: 1.0857 - val_accuracy: 0.8176\n","Epoch 50/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6406 - accuracy: 0.8258 - val_loss: 1.0383 - val_accuracy: 0.8286\n","Epoch 51/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6364 - accuracy: 0.8246 - val_loss: 1.0882 - val_accuracy: 0.8123\n","Epoch 52/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6232 - accuracy: 0.8342 - val_loss: 1.0800 - val_accuracy: 0.8092\n","Epoch 53/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6193 - accuracy: 0.8360 - val_loss: 1.1179 - val_accuracy: 0.7954\n","Epoch 54/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6156 - accuracy: 0.8348 - val_loss: 1.0406 - val_accuracy: 0.8315\n","Epoch 55/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6075 - accuracy: 0.8395 - val_loss: 1.1287 - val_accuracy: 0.7989\n","Epoch 56/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.6064 - accuracy: 0.8411 - val_loss: 1.0984 - val_accuracy: 0.7985\n","Epoch 57/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6056 - accuracy: 0.8390 - val_loss: 1.0291 - val_accuracy: 0.8286\n","Epoch 58/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.5996 - accuracy: 0.8402 - val_loss: 1.0293 - val_accuracy: 0.8244\n","Epoch 59/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5835 - accuracy: 0.8481 - val_loss: 1.0157 - val_accuracy: 0.8304\n","Epoch 60/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.5899 - accuracy: 0.8424 - val_loss: 1.0533 - val_accuracy: 0.8183\n","Fold: 2\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.9/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 4.7972 - accuracy: 0.0613 - val_loss: 4.5088 - val_accuracy: 0.1100\n","Epoch 2/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 4.0920 - accuracy: 0.1264 - val_loss: 3.9460 - val_accuracy: 0.1784\n","Epoch 3/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.6352 - accuracy: 0.1769 - val_loss: 3.6695 - val_accuracy: 0.2048\n","Epoch 4/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.3755 - accuracy: 0.2045 - val_loss: 3.4868 - val_accuracy: 0.2183\n","Epoch 5/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 3.1849 - accuracy: 0.2415 - val_loss: 3.3275 - val_accuracy: 0.2158\n","Epoch 6/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 3.0056 - accuracy: 0.2636 - val_loss: 3.1159 - val_accuracy: 0.2519\n","Epoch 7/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.8307 - accuracy: 0.2922 - val_loss: 2.9636 - val_accuracy: 0.3021\n","Epoch 8/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.6692 - accuracy: 0.3198 - val_loss: 2.8176 - val_accuracy: 0.2900\n","Epoch 9/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.5093 - accuracy: 0.3505 - val_loss: 2.6735 - val_accuracy: 0.3705\n","Epoch 10/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.3494 - accuracy: 0.3821 - val_loss: 2.4784 - val_accuracy: 0.4037\n","Epoch 11/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 2.1821 - accuracy: 0.4236 - val_loss: 2.4214 - val_accuracy: 0.4224\n","Epoch 12/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 2.0052 - accuracy: 0.4732 - val_loss: 2.2087 - val_accuracy: 0.4812\n","Epoch 13/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.8598 - accuracy: 0.5090 - val_loss: 2.0741 - val_accuracy: 0.5281\n","Epoch 14/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.7320 - accuracy: 0.5435 - val_loss: 1.9825 - val_accuracy: 0.5179\n","Epoch 15/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.6295 - accuracy: 0.5606 - val_loss: 1.8863 - val_accuracy: 0.5457\n","Epoch 16/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.5542 - accuracy: 0.5859 - val_loss: 1.7909 - val_accuracy: 0.5879\n","Epoch 17/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.4839 - accuracy: 0.5974 - val_loss: 1.7277 - val_accuracy: 0.5859\n","Epoch 18/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.4190 - accuracy: 0.6196 - val_loss: 1.6971 - val_accuracy: 0.6081\n","Epoch 19/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.3650 - accuracy: 0.6342 - val_loss: 1.6641 - val_accuracy: 0.5969\n","Epoch 20/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.3268 - accuracy: 0.6464 - val_loss: 1.5740 - val_accuracy: 0.6451\n","Epoch 21/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.2726 - accuracy: 0.6620 - val_loss: 1.5799 - val_accuracy: 0.6561\n","Epoch 22/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.2371 - accuracy: 0.6684 - val_loss: 1.5124 - val_accuracy: 0.6735\n","Epoch 23/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 1.1996 - accuracy: 0.6833 - val_loss: 1.4497 - val_accuracy: 0.6774\n","Epoch 24/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.1637 - accuracy: 0.6903 - val_loss: 1.4214 - val_accuracy: 0.6869\n","Epoch 25/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.1309 - accuracy: 0.6991 - val_loss: 1.4607 - val_accuracy: 0.6634\n","Epoch 26/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.1075 - accuracy: 0.7062 - val_loss: 1.3608 - val_accuracy: 0.7028\n","Epoch 27/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0840 - accuracy: 0.7112 - val_loss: 1.3398 - val_accuracy: 0.7237\n","Epoch 28/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 1.0526 - accuracy: 0.7205 - val_loss: 1.3429 - val_accuracy: 0.7054\n","Epoch 29/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 1.0325 - accuracy: 0.7218 - val_loss: 1.2975 - val_accuracy: 0.7347\n","Epoch 30/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9965 - accuracy: 0.7427 - val_loss: 1.3517 - val_accuracy: 0.7063\n","Epoch 31/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9964 - accuracy: 0.7326 - val_loss: 1.2470 - val_accuracy: 0.7298\n","Epoch 32/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9674 - accuracy: 0.7452 - val_loss: 1.2934 - val_accuracy: 0.7184\n","Epoch 33/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9572 - accuracy: 0.7481 - val_loss: 1.2508 - val_accuracy: 0.7221\n","Epoch 34/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9377 - accuracy: 0.7509 - val_loss: 1.2448 - val_accuracy: 0.7509\n","Epoch 35/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.9165 - accuracy: 0.7534 - val_loss: 1.1890 - val_accuracy: 0.7556\n","Epoch 36/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.9069 - accuracy: 0.7556 - val_loss: 1.2057 - val_accuracy: 0.7360\n","Epoch 37/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8951 - accuracy: 0.7602 - val_loss: 1.2275 - val_accuracy: 0.7344\n","Epoch 38/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8862 - accuracy: 0.7612 - val_loss: 1.1224 - val_accuracy: 0.7558\n","Epoch 39/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8643 - accuracy: 0.7700 - val_loss: 1.1808 - val_accuracy: 0.7496\n","Epoch 40/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8567 - accuracy: 0.7689 - val_loss: 1.1531 - val_accuracy: 0.7454\n","Epoch 41/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8417 - accuracy: 0.7763 - val_loss: 1.1570 - val_accuracy: 0.7516\n","Epoch 42/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8381 - accuracy: 0.7720 - val_loss: 1.1049 - val_accuracy: 0.7745\n","Epoch 43/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8168 - accuracy: 0.7864 - val_loss: 1.1374 - val_accuracy: 0.7613\n","Epoch 44/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.8189 - accuracy: 0.7814 - val_loss: 1.0913 - val_accuracy: 0.7760\n","Epoch 45/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.8067 - accuracy: 0.7837 - val_loss: 1.1065 - val_accuracy: 0.7545\n","Epoch 46/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7911 - accuracy: 0.7862 - val_loss: 1.0652 - val_accuracy: 0.7930\n","Epoch 47/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7969 - accuracy: 0.7854 - val_loss: 1.0575 - val_accuracy: 0.7883\n","Epoch 48/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7833 - accuracy: 0.7905 - val_loss: 1.0866 - val_accuracy: 0.7593\n","Epoch 49/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7775 - accuracy: 0.7939 - val_loss: 1.0927 - val_accuracy: 0.7685\n","Epoch 50/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7676 - accuracy: 0.8025 - val_loss: 1.0325 - val_accuracy: 0.8046\n","Epoch 51/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7683 - accuracy: 0.8010 - val_loss: 1.0244 - val_accuracy: 0.7901\n","Epoch 52/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7504 - accuracy: 0.7993 - val_loss: 1.1077 - val_accuracy: 0.7646\n","Epoch 53/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7476 - accuracy: 0.8035 - val_loss: 1.0354 - val_accuracy: 0.7892\n","Epoch 54/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7464 - accuracy: 0.8028 - val_loss: 1.0231 - val_accuracy: 0.8024\n","Epoch 55/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7265 - accuracy: 0.8070 - val_loss: 1.0295 - val_accuracy: 0.7910\n","Epoch 56/60\n","1846/1846 [==============================] - 6s 3ms/step - loss: 0.7251 - accuracy: 0.8109 - val_loss: 0.9716 - val_accuracy: 0.7888\n","Epoch 57/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7176 - accuracy: 0.8113 - val_loss: 0.9970 - val_accuracy: 0.8081\n","Epoch 58/60\n","1846/1846 [==============================] - 7s 4ms/step - loss: 0.7104 - accuracy: 0.8139 - val_loss: 0.9586 - val_accuracy: 0.8051\n","Epoch 59/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.7099 - accuracy: 0.8117 - val_loss: 0.9799 - val_accuracy: 0.8026\n","Epoch 60/60\n","1846/1846 [==============================] - 5s 3ms/step - loss: 0.6972 - accuracy: 0.8178 - val_loss: 0.9780 - val_accuracy: 0.7989\n","Average Validation Accuracy: 0.823439359664917\n","Average Validation Loss: 0.7686502039432526\n","Average Test Accuracy: 0.8215523064136505\n","Final Test Accuracy for each fold: 0.8342301249504089\n"]}],"source":["# Define the number of folds for k-fold cross-validation\n","k = 2\n","\n","# Define the cross-validation method\n","cv_method = StratifiedKFold(n_splits=k)\n","\n","# Initialize the list to store the history, train & validation(accuracy & loss) for each model\n","models = []\n","model_history = []\n","model_accuracy = []\n","model_train_acc = []\n","model_train_loss = []\n","model_val_acc = []\n","model_val_loss = []\n","\n","\n","for i in range(1,21):\n","\n","    models_fold = []\n","    hist = []\n","    train_accuracy = []\n","    train_loss = []\n","    val_accuracy = []\n","    val_loss = []\n","    test_accuracy = []\n","\n","    print(\"Number of input features:\",i)\n","\n","    # Select the input features from the input data\n","    X_train_selected = X_train[:, :i]\n","    X_test_selected = X_test[:, :i]\n","\n","    # Loop over the folds\n","    for fold, (train_index, val_index) in enumerate(cv_method.split(X_train_selected, y_train)):\n","\n","        print(\"Fold:\", fold+1)\n","\n","        # Split the data into train and validation sets using the current fold index\n","        X_train_fold  = X_train_selected[train_index]\n","        y_train_fold  = y_train[train_index]\n","        X_val_fold = X_train_selected[val_index]\n","        y_val_fold = y_train[val_index]\n","\n","        # Prepare the target data\n","        y_train_fold_enc, y_val_fold_enc = prepare_targets(y_train_fold, y_val_fold)\n","\n","        # build the model\n","        model = RBFN_model(i)\n","\n","        # Fit the model to the training data for the current fold\n","        history = model.fit(X_train_fold, to_categorical(y_train_fold_enc, num_classes=373), epochs=60, batch_size=5, verbose=1, validation_split = 0.33)\n","    \n","        # Evaluate the model on the validation data for the current fold\n","        val_scores = model.evaluate(X_val_fold, to_categorical(y_val_fold_enc, num_classes=373), verbose=0)\n","        val_accuracy.append(val_scores[1])\n","        val_loss.append(val_scores[0])\n","\n","        # Evaluate the model on the test data for the current fold\n","        test_scores = model.evaluate(X_test_selected, to_categorical(y_test_enc, num_classes=373), verbose=0)\n","        test_accuracy.append(test_scores[1])\n","\n","        # add the model to the list of models\n","        models_fold.append(model)\n","        hist.append(history)\n","\n","        # store the training accuracy and loss for each fold\n","        train_accuracy.append(history.history['accuracy'])\n","        train_loss.append(history.history['loss'])\n","        \n","    # Calculate the average test and validation accuracy and loss across all folds\n","    avg_test_acc = sum(test_accuracy) / len(test_accuracy)\n","    avg_val_acc = sum(val_accuracy) / len(val_accuracy)\n","    avg_val_loss = sum(val_loss) / len(val_loss)\n","\n","    # Print the average validation and test accuracy and loss\n","    print(\"Average Validation Accuracy:\", avg_val_acc)\n","    print(\"Average Validation Loss:\",avg_val_loss)\n","    print(\"Average Test Accuracy:\", avg_test_acc)\n","\n","    best_fold_index = test_accuracy.index(max(test_accuracy))\n","    model_accuracy.append(test_accuracy[best_fold_index])\n","    models.append(models_fold[best_fold_index])\n","    model_history.append(hist[best_fold_index])\n","    model_train_acc.append(train_accuracy[best_fold_index])\n","    model_train_loss.append(train_loss[best_fold_index])\n","    model_val_acc.append(val_accuracy[best_fold_index])\n","    model_val_loss.append(val_loss[best_fold_index])\n","\n","\n","    print(\"Final Test Accuracy for each fold:\", test_accuracy[best_fold_index])"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"xzn9CBYTQsYy","executionInfo":{"status":"ok","timestamp":1682329568749,"user_tz":-480,"elapsed":27,"user":{"displayName":"chong yoongsim","userId":"18145963220657887961"}}},"outputs":[],"source":["#to show the no of input features and its corresponding model accuracy\n","model_list = []\n","\n","#Iterate through each model's accuracy \n","for i in range (len(model_accuracy)):\n","    #get the number of input features for the current model\n","    no_features = i + 1\n","\n","    #round the model accuries to 3 d.p.\n","    rounded_model_acc = round(model_accuracy[i], 3)\n","    \n","    model_list.append([no_features, rounded_model_acc])\n","\n","models_df = pd.DataFrame(model_list, columns=[\"No of input features\", \"Model accuracy\"])"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"WAjs7sLtQsYz","colab":{"base_uri":"https://localhost:8080/","height":489},"executionInfo":{"status":"ok","timestamp":1682329569868,"user_tz":-480,"elapsed":1141,"user":{"displayName":"chong yoongsim","userId":"18145963220657887961"}},"outputId":"36edf3cb-4104-498c-8538-8acd2ca1acbe"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 2 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Final Test Accuracy: 0.9499520659446716\n"]}],"source":["# Select the final model based on the max test accuracy across all models\n","\n","best_model_index = model_accuracy.index(max(model_accuracy))\n","\n","best_model = models[best_model_index]\n","best_model_history = model_history[best_model_index]\n","best_model_train_acc = model_train_acc[best_model_index]\n","best_model_train_loss = model_train_loss[best_model_index]\n","best_model_val_acc = model_val_acc[best_model_index]\n","best_model_val_loss = model_val_loss[best_model_index]\n","\n","# summarize history for accuracy  \n","plt.subplot(211)  \n","plt.plot(best_model_history.history['accuracy'])  \n","plt.plot(best_model_history.history['val_accuracy'])  \n","plt.title('model accuracy')  \n","plt.ylabel('accuracy')  \n","plt.xlabel('epoch')  \n","plt.legend(['train', 'test'], loc='lower right')  \n","\n","# summarize history for loss\n","plt.subplot(212)  \n","plt.plot(best_model_history.history['loss'])  \n","plt.plot(best_model_history.history['val_loss'])  \n","plt.title('model loss')  \n","plt.ylabel('loss')  \n","plt.xlabel('epoch')  \n","plt.legend(['train', 'test'], loc='upper right')  \n","plt.subplots_adjust(hspace=0.7)\n","\n","plt.show() \n","\n","print(\"Final Test Accuracy:\", model_accuracy[best_model_index])"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"KfbFjY7JQsY1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682329570397,"user_tz":-480,"elapsed":671,"user":{"displayName":"chong yoongsim","userId":"18145963220657887961"}},"outputId":"fec5b36d-9592-4a32-c3f4-f362a9cf3a91"},"outputs":[{"output_type":"stream","name":"stdout","text":["424/424 [==============================] - 1s 1ms/step\n","Classification report:\n","               precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00       591\n","           1       1.00      1.00      1.00       430\n","           2       1.00      1.00      1.00       419\n","           3       1.00      1.00      1.00       384\n","           4       1.00      1.00      1.00       339\n","           5       1.00      1.00      1.00       342\n","           6       1.00      1.00      1.00       310\n","           7       1.00      1.00      1.00       325\n","           8       1.00      1.00      1.00       294\n","           9       1.00      1.00      1.00       269\n","          10       1.00      1.00      1.00       296\n","          11       1.00      1.00      1.00       258\n","          12       1.00      1.00      1.00       247\n","          13       1.00      1.00      1.00       237\n","          14       1.00      1.00      1.00       239\n","          15       1.00      1.00      1.00       235\n","          16       1.00      1.00      1.00       213\n","          17       1.00      1.00      1.00       202\n","          18       1.00      1.00      1.00       196\n","          19       1.00      1.00      1.00       181\n","          20       1.00      1.00      1.00       177\n","          21       1.00      1.00      1.00       177\n","          22       1.00      1.00      1.00       155\n","          23       1.00      1.00      1.00       155\n","          24       1.00      1.00      1.00       144\n","          25       1.00      1.00      1.00       126\n","          26       1.00      1.00      1.00       108\n","          27       1.00      1.00      1.00       121\n","          28       1.00      1.00      1.00        95\n","          29       1.00      1.00      1.00       106\n","          30       1.00      1.00      1.00       102\n","          31       1.00      1.00      1.00        86\n","          32       0.96      1.00      0.98       108\n","          33       1.00      1.00      1.00        88\n","          34       1.00      1.00      1.00       102\n","          35       0.94      1.00      0.97        88\n","          36       1.00      1.00      1.00        83\n","          37       0.99      1.00      0.99        93\n","          38       1.00      1.00      1.00        76\n","          39       1.00      1.00      1.00        85\n","          40       1.00      1.00      1.00        86\n","          41       1.00      1.00      1.00        85\n","          42       0.74      1.00      0.85        68\n","          43       1.00      1.00      1.00        75\n","          44       0.74      1.00      0.85        71\n","          45       0.00      0.00      0.00        58\n","          46       1.00      1.00      1.00        71\n","          47       0.26      1.00      0.41        57\n","          48       1.00      1.00      1.00        67\n","          49       0.60      1.00      0.75        47\n","          50       1.00      1.00      1.00        48\n","          51       1.00      1.00      1.00        47\n","          52       1.00      1.00      1.00        43\n","          53       1.00      1.00      1.00        51\n","          54       1.00      1.00      1.00        44\n","          55       1.00      1.00      1.00        51\n","          56       1.00      1.00      1.00        45\n","          57       1.00      1.00      1.00        44\n","          58       0.84      1.00      0.91        41\n","          59       1.00      1.00      1.00        41\n","          60       1.00      1.00      1.00        52\n","          61       1.00      1.00      1.00        43\n","          62       1.00      1.00      1.00        37\n","          63       1.00      1.00      1.00        43\n","          64       1.00      1.00      1.00        42\n","          65       1.00      1.00      1.00        46\n","          66       0.88      1.00      0.93        43\n","          67       1.00      1.00      1.00        40\n","          68       1.00      1.00      1.00        44\n","          69       1.00      1.00      1.00        43\n","          70       1.00      1.00      1.00        38\n","          71       1.00      1.00      1.00        33\n","          72       1.00      1.00      1.00        45\n","          73       1.00      1.00      1.00        38\n","          74       1.00      1.00      1.00        42\n","          75       1.00      1.00      1.00        39\n","          76       0.00      0.00      0.00        30\n","          77       1.00      1.00      1.00        28\n","          78       1.00      1.00      1.00        28\n","          79       1.00      1.00      1.00        32\n","          80       1.00      1.00      1.00        33\n","          81       1.00      1.00      1.00        31\n","          82       1.00      1.00      1.00        35\n","          83       1.00      1.00      1.00        39\n","          84       1.00      1.00      1.00        27\n","          85       1.00      1.00      1.00        36\n","          86       1.00      1.00      1.00        31\n","          87       1.00      1.00      1.00        28\n","          88       1.00      1.00      1.00        20\n","          89       1.00      1.00      1.00        33\n","          90       1.00      1.00      1.00        24\n","          91       1.00      1.00      1.00        22\n","          92       1.00      1.00      1.00        26\n","          93       1.00      1.00      1.00        35\n","          94       1.00      1.00      1.00        27\n","          95       1.00      1.00      1.00        23\n","          96       1.00      1.00      1.00        27\n","          97       1.00      1.00      1.00        28\n","          98       0.00      0.00      0.00        16\n","          99       1.00      1.00      1.00        35\n","         100       1.00      1.00      1.00        28\n","         101       0.00      0.00      0.00        25\n","         102       1.00      1.00      1.00        26\n","         103       1.00      1.00      1.00        33\n","         104       1.00      1.00      1.00        26\n","         105       1.00      1.00      1.00        24\n","         106       0.00      0.00      0.00        22\n","         107       1.00      1.00      1.00        26\n","         108       1.00      1.00      1.00        25\n","         109       1.00      1.00      1.00        16\n","         110       1.00      1.00      1.00        20\n","         111       1.00      1.00      1.00        26\n","         112       1.00      1.00      1.00        18\n","         113       0.85      1.00      0.92        23\n","         114       0.81      1.00      0.89        25\n","         115       1.00      1.00      1.00        18\n","         116       1.00      1.00      1.00        19\n","         117       1.00      1.00      1.00        16\n","         118       1.00      1.00      1.00        26\n","         119       1.00      1.00      1.00        22\n","         120       1.00      1.00      1.00        17\n","         121       1.00      1.00      1.00        15\n","         122       1.00      1.00      1.00        18\n","         123       0.74      1.00      0.85        20\n","         124       1.00      1.00      1.00        14\n","         125       1.00      1.00      1.00        22\n","         126       1.00      1.00      1.00        19\n","         127       1.00      1.00      1.00        27\n","         128       1.00      1.00      1.00        26\n","         129       1.00      1.00      1.00        21\n","         130       1.00      1.00      1.00        18\n","         131       1.00      1.00      1.00        18\n","         132       1.00      1.00      1.00        20\n","         133       1.00      1.00      1.00        14\n","         134       1.00      1.00      1.00        19\n","         135       0.00      0.00      0.00        16\n","         136       0.59      1.00      0.74        23\n","         137       0.00      0.00      0.00        11\n","         138       1.00      1.00      1.00        14\n","         139       1.00      1.00      1.00        20\n","         140       1.00      1.00      1.00        23\n","         141       0.39      1.00      0.56        14\n","         142       0.00      0.00      0.00        13\n","         143       1.00      1.00      1.00        23\n","         144       0.00      0.00      0.00        17\n","         145       1.00      1.00      1.00        24\n","         146       0.48      1.00      0.65        16\n","         147       1.00      1.00      1.00        19\n","         148       1.00      1.00      1.00        22\n","         149       1.00      1.00      1.00        15\n","         150       1.00      1.00      1.00        11\n","         151       1.00      1.00      1.00        19\n","         152       0.67      1.00      0.80        20\n","         153       1.00      1.00      1.00        24\n","         154       1.00      1.00      1.00        11\n","         155       1.00      1.00      1.00        17\n","         156       1.00      1.00      1.00        18\n","         157       0.00      0.00      0.00        12\n","         158       1.00      1.00      1.00        18\n","         159       1.00      1.00      1.00        20\n","         160       1.00      1.00      1.00        20\n","         161       1.00      1.00      1.00        16\n","         162       1.00      1.00      1.00        15\n","         163       1.00      1.00      1.00        15\n","         164       1.00      1.00      1.00        13\n","         165       0.00      0.00      0.00        19\n","         166       0.00      0.00      0.00        11\n","         167       1.00      1.00      1.00         9\n","         168       0.50      1.00      0.67        11\n","         169       0.00      0.00      0.00        21\n","         170       1.00      1.00      1.00        15\n","         171       0.69      1.00      0.82        18\n","         172       1.00      1.00      1.00        11\n","         173       1.00      1.00      1.00        16\n","         174       1.00      1.00      1.00        10\n","         175       1.00      1.00      1.00        11\n","         176       1.00      1.00      1.00        10\n","         177       1.00      1.00      1.00        15\n","         178       1.00      1.00      1.00        11\n","         179       1.00      1.00      1.00        15\n","         180       1.00      1.00      1.00        13\n","         181       0.00      0.00      0.00        15\n","         182       1.00      1.00      1.00         9\n","         183       1.00      1.00      1.00        16\n","         184       1.00      1.00      1.00         8\n","         185       0.00      0.00      0.00        12\n","         186       1.00      1.00      1.00        15\n","         187       1.00      1.00      1.00        15\n","         188       0.00      0.00      0.00        13\n","         189       0.00      0.00      0.00        14\n","         190       1.00      1.00      1.00        11\n","         191       1.00      1.00      1.00        10\n","         192       1.00      1.00      1.00        17\n","         193       1.00      1.00      1.00         9\n","         194       1.00      1.00      1.00         9\n","         195       0.44      1.00      0.62         4\n","         196       1.00      1.00      1.00         7\n","         197       1.00      1.00      1.00         7\n","         198       1.00      1.00      1.00        12\n","         199       1.00      1.00      1.00        14\n","         200       1.00      1.00      1.00         6\n","         201       1.00      1.00      1.00         9\n","         202       0.00      0.00      0.00         7\n","         203       0.00      0.00      0.00         6\n","         204       0.55      1.00      0.71        11\n","         205       0.00      0.00      0.00        14\n","         206       1.00      1.00      1.00        12\n","         207       0.00      0.00      0.00        14\n","         208       1.00      1.00      1.00        12\n","         209       1.00      1.00      1.00         7\n","         210       1.00      1.00      1.00        19\n","         211       1.00      1.00      1.00         7\n","         212       1.00      1.00      1.00        11\n","         213       1.00      1.00      1.00         9\n","         214       1.00      1.00      1.00         7\n","         215       1.00      1.00      1.00         6\n","         216       1.00      1.00      1.00        12\n","         217       1.00      1.00      1.00        12\n","         218       0.00      0.00      0.00         9\n","         219       1.00      1.00      1.00         6\n","         220       1.00      1.00      1.00         8\n","         221       0.29      1.00      0.45         5\n","         222       0.21      1.00      0.35         4\n","         223       1.00      1.00      1.00        14\n","         224       1.00      1.00      1.00        13\n","         225       1.00      1.00      1.00         4\n","         226       0.00      0.00      0.00        10\n","         227       1.00      1.00      1.00        12\n","         228       0.00      0.00      0.00        13\n","         229       0.69      1.00      0.81        11\n","         230       1.00      1.00      1.00         5\n","         231       1.00      1.00      1.00         6\n","         232       0.00      0.00      0.00         8\n","         233       1.00      1.00      1.00        10\n","         234       1.00      1.00      1.00         4\n","         235       1.00      1.00      1.00         8\n","         236       1.00      1.00      1.00         9\n","         237       1.00      1.00      1.00         8\n","         238       0.00      0.00      0.00        10\n","         239       0.00      0.00      0.00         9\n","         240       1.00      1.00      1.00         7\n","         241       1.00      1.00      1.00         8\n","         242       0.50      1.00      0.67         6\n","         243       1.00      1.00      1.00         7\n","         244       0.00      0.00      0.00         9\n","         245       1.00      1.00      1.00         7\n","         246       1.00      1.00      1.00         9\n","         247       1.00      1.00      1.00         7\n","         248       0.71      1.00      0.83        10\n","         249       1.00      1.00      1.00        10\n","         250       1.00      1.00      1.00         7\n","         251       1.00      1.00      1.00         6\n","         252       0.00      0.00      0.00        11\n","         253       1.00      1.00      1.00         7\n","         254       0.00      0.00      0.00         8\n","         255       1.00      1.00      1.00         5\n","         256       1.00      1.00      1.00         7\n","         257       1.00      1.00      1.00         9\n","         258       0.00      0.00      0.00         6\n","         259       0.00      0.00      0.00         3\n","         260       0.00      0.00      0.00         4\n","         261       0.00      0.00      0.00         2\n","         262       0.42      1.00      0.59         5\n","         263       1.00      1.00      1.00        12\n","         264       0.33      1.00      0.50         5\n","         265       1.00      1.00      1.00         7\n","         266       1.00      1.00      1.00        10\n","         267       1.00      1.00      1.00         8\n","         268       0.00      0.00      0.00         9\n","         269       0.55      1.00      0.71         6\n","         270       0.00      0.00      0.00         4\n","         271       1.00      1.00      1.00         7\n","         272       0.71      1.00      0.83        10\n","         273       1.00      1.00      1.00         3\n","         274       0.26      1.00      0.41         9\n","         275       0.17      1.00      0.29         6\n","         276       0.00      0.00      0.00         5\n","         277       0.00      0.00      0.00         4\n","         278       1.00      1.00      1.00         3\n","         279       1.00      1.00      1.00         4\n","         280       0.09      1.00      0.16         5\n","         281       0.00      0.00      0.00         6\n","         282       1.00      1.00      1.00        11\n","         283       1.00      1.00      1.00         6\n","         284       1.00      1.00      1.00         2\n","         285       0.29      1.00      0.44         4\n","         286       1.00      1.00      1.00         7\n","         287       0.00      0.00      0.00         9\n","         288       1.00      1.00      1.00         7\n","         289       0.00      0.00      0.00         7\n","         290       1.00      1.00      1.00         6\n","         291       0.00      0.00      0.00         5\n","         292       0.00      0.00      0.00         6\n","         293       1.00      1.00      1.00         8\n","         294       1.00      1.00      1.00         7\n","         295       0.00      0.00      0.00         3\n","         296       0.46      1.00      0.63         6\n","         297       1.00      1.00      1.00         8\n","         298       0.50      1.00      0.67         4\n","         299       0.00      0.00      0.00         6\n","         300       0.00      0.00      0.00         5\n","         301       0.00      0.00      0.00         5\n","         302       1.00      1.00      1.00         5\n","         303       1.00      1.00      1.00         2\n","         304       1.00      1.00      1.00         8\n","         305       1.00      1.00      1.00         3\n","         306       0.40      1.00      0.57         6\n","         307       0.00      0.00      0.00         7\n","         308       0.15      1.00      0.27         4\n","         309       0.27      1.00      0.42         4\n","         310       1.00      1.00      1.00         9\n","         311       1.00      1.00      1.00         7\n","         312       0.00      0.00      0.00         6\n","         313       1.00      1.00      1.00         6\n","         314       1.00      1.00      1.00         8\n","         315       1.00      1.00      1.00         7\n","         316       1.00      1.00      1.00         3\n","         317       1.00      1.00      1.00         6\n","         318       0.00      0.00      0.00        10\n","         319       0.00      0.00      0.00         6\n","         320       1.00      1.00      1.00         2\n","         321       0.00      0.00      0.00         8\n","         322       1.00      1.00      1.00         4\n","         323       1.00      1.00      1.00         4\n","         324       0.00      0.00      0.00         8\n","         325       0.50      1.00      0.67         4\n","         326       0.00      0.00      0.00         7\n","         327       1.00      1.00      1.00         4\n","         328       0.00      0.00      0.00         6\n","         329       1.00      1.00      1.00         4\n","         330       0.00      0.00      0.00         3\n","         331       0.00      0.00      0.00         8\n","         332       0.00      0.00      0.00         1\n","         333       0.00      0.00      0.00         3\n","         334       1.00      1.00      1.00         4\n","         335       0.60      1.00      0.75         3\n","         336       0.00      0.00      0.00         6\n","         337       1.00      1.00      1.00         2\n","         338       1.00      1.00      1.00         7\n","         339       1.00      1.00      1.00         4\n","         340       1.00      1.00      1.00         6\n","         341       1.00      1.00      1.00         7\n","         342       0.00      0.00      0.00         2\n","         343       0.00      0.00      0.00         5\n","         344       0.00      0.00      0.00         4\n","         345       0.00      0.00      0.00         1\n","         346       1.00      1.00      1.00         2\n","         347       0.00      0.00      0.00         4\n","         348       1.00      1.00      1.00         7\n","         349       0.00      0.00      0.00         4\n","         350       0.00      0.00      0.00         6\n","         351       0.00      0.00      0.00         4\n","         352       0.00      0.00      0.00         5\n","         353       0.00      0.00      0.00         4\n","         354       1.00      1.00      1.00         3\n","         355       1.00      1.00      1.00         1\n","         356       0.00      0.00      0.00         4\n","         357       0.00      0.00      0.00         1\n","         358       1.00      1.00      1.00         3\n","         359       0.57      1.00      0.73         4\n","         360       1.00      1.00      1.00         3\n","         361       1.00      1.00      1.00         3\n","         362       0.00      0.00      0.00         3\n","         363       0.00      0.00      0.00         2\n","         364       0.00      0.00      0.00         3\n","         365       1.00      1.00      1.00         3\n","         366       0.00      0.00      0.00         2\n","         367       1.00      1.00      1.00         3\n","         368       0.25      1.00      0.40         1\n","         369       1.00      1.00      1.00         2\n","         370       1.00      1.00      1.00         2\n","         372       0.00      0.00      0.00         4\n","\n","    accuracy                           0.95     13567\n","   macro avg       0.74      0.79      0.76     13567\n","weighted avg       0.93      0.95      0.94     13567\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["#Select only the optimal number of input features for X_test\n","X_test = X_test[:,:(best_model_index+1)]\n","\n","# Evaluate the best model on the test data\n","y_pred = best_model.predict(X_test)\n","\n","# np.argmax() is used to convert the one-hot encoded predictions and test labels to class labels.\n","y_pred_label = np.argmax(y_pred, axis = 1)\n","\n","# Print the classification report\n","print(\"Classification report:\\n\", classification_report(y_test_enc, y_pred_label))"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"zbyINa_nQsY4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682329570398,"user_tz":-480,"elapsed":31,"user":{"displayName":"chong yoongsim","userId":"18145963220657887961"}},"outputId":"84a8de7a-8a38-495f-9e06-cd6700c65bd0"},"outputs":[{"output_type":"stream","name":"stdout","text":["           OsID  True Class  Predicted Class  True/False\n","0  Os04g0475500         328              280       False\n","1  Os04g0659100         161              161        True\n","2  Os01g0686800          17               17        True\n","3  Os04g0107900          34               34        True\n","4  Os04g0517100           7                7        True\n"]}],"source":["# extract class labels from test data\n","class_test = y_test_enc\n","\n","# Invert OsID_labels dictionary\n","inv_OsID_labels = {v: k for k, v in OsID_labels.items()}\n","\n","# map OsID values to the class labels\n","OsID_test = [inv_OsID_labels.get(value, 'Unknown') for value in class_test]\n","\n","# create dataframe with OsID, true class, predicted class, and true/false columns\n","results = pd.DataFrame({\n","    'OsID': OsID_test,\n","    'True Class': y_test_enc,\n","    'Predicted Class': y_pred_label,\n","    'True/False': class_test == y_pred_label\n","})\n","\n","# display dataframe\n","print(results.head())\n","\n","# save results_df to a CSV file\n","results.to_csv('MLP_gene classification.csv', index=False)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"l2lvuFxIN3QQ","colab":{"base_uri":"https://localhost:8080/","height":676},"executionInfo":{"status":"ok","timestamp":1682329570399,"user_tz":-480,"elapsed":17,"user":{"displayName":"chong yoongsim","userId":"18145963220657887961"}},"outputId":"5466e826-3540-40e2-8b94-40ea41d0d060"},"outputs":[{"output_type":"display_data","data":{"text/plain":["    No of input features  Model accuracy\n","0                      1           0.483\n","1                      2           0.808\n","2                      3           0.801\n","3                      4           0.899\n","4                      5           0.863\n","5                      6           0.856\n","6                      7           0.927\n","7                      8           0.893\n","8                      9           0.934\n","9                     10           0.900\n","10                    11           0.898\n","11                    12           0.928\n","12                    13           0.931\n","13                    14           0.950\n","14                    15           0.919\n","15                    16           0.889\n","16                    17           0.844\n","17                    18           0.866\n","18                    19           0.849\n","19                    20           0.834"],"text/html":["\n","  <div id=\"df-caed18db-958d-4946-89e5-bff372dac3ae\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>No of input features</th>\n","      <th>Model accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>0.483</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>0.808</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>0.801</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>0.899</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>0.863</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>6</td>\n","      <td>0.856</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>7</td>\n","      <td>0.927</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>8</td>\n","      <td>0.893</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>9</td>\n","      <td>0.934</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>10</td>\n","      <td>0.900</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>11</td>\n","      <td>0.898</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>12</td>\n","      <td>0.928</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>13</td>\n","      <td>0.931</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>14</td>\n","      <td>0.950</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>15</td>\n","      <td>0.919</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>16</td>\n","      <td>0.889</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>17</td>\n","      <td>0.844</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>18</td>\n","      <td>0.866</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>19</td>\n","      <td>0.849</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>20</td>\n","      <td>0.834</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-caed18db-958d-4946-89e5-bff372dac3ae')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-caed18db-958d-4946-89e5-bff372dac3ae button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-caed18db-958d-4946-89e5-bff372dac3ae');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}}],"source":["display(models_df)"]}],"metadata":{"accelerator":"TPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"},"vscode":{"interpreter":{"hash":"54645332476aec3a1589d49135d9c8280fdb5d7db877f5b7af7a1b58b8f996bc"}}},"nbformat":4,"nbformat_minor":0}
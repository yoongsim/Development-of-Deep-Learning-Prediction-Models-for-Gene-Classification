{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixed random seed for reproducibility \n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of dataGene:\n",
      "        log_2FoldChange            ET  CoExpression           PCC  \\\n",
      "count     41110.000000  41110.000000  41110.000000  41110.000000   \n",
      "mean         -0.037332      1.407395      0.991997     -0.361737   \n",
      "std           0.391444      0.784327      0.089101      0.463979   \n",
      "min          -1.000000      0.000000      0.000000     -1.000000   \n",
      "25%          -0.251534      1.000000      1.000000     -0.747963   \n",
      "50%           0.030675      2.000000      1.000000     -0.449089   \n",
      "75%           0.251534      2.000000      1.000000     -0.051646   \n",
      "max           1.000000      2.000000      1.000000      1.000000   \n",
      "\n",
      "                PPI  Root10DaysSeedling  Root14DaysSeedling  \\\n",
      "count  41110.000000        41110.000000        41110.000000   \n",
      "mean       0.914668           -0.522040           -0.646982   \n",
      "std        0.279379            0.498568            0.393549   \n",
      "min        0.000000           -1.000000           -1.000000   \n",
      "25%        1.000000           -0.901371           -0.965084   \n",
      "50%        1.000000           -0.663664           -0.680003   \n",
      "75%        1.000000           -0.378497           -0.559627   \n",
      "max        1.000000            1.000000            1.000000   \n",
      "\n",
      "       Root17DaysSeedling  Root21DaysSeedling  Root24DaysSeedling  ...  \\\n",
      "count        41110.000000        41110.000000        41110.000000  ...   \n",
      "mean            -0.700869           -0.669349           -0.670048  ...   \n",
      "std              0.378219            0.405860            0.390751  ...   \n",
      "min             -1.000000           -1.000000           -1.000000  ...   \n",
      "25%             -0.980226           -1.000000           -0.982003  ...   \n",
      "50%             -0.795609           -0.726665           -0.708584  ...   \n",
      "75%             -0.601266           -0.543621           -0.482133  ...   \n",
      "max              1.000000            1.000000            1.000000  ...   \n",
      "\n",
      "       Root52DaysSeedling  Shoot3DaysSeedling  Shoot10DaysSeedling  \\\n",
      "count        41110.000000        41110.000000         41110.000000   \n",
      "mean            -0.670345           -0.590806            -0.545055   \n",
      "std              0.478222            0.443552             0.477438   \n",
      "min             -1.000000           -1.000000            -1.000000   \n",
      "25%             -1.000000           -1.000000            -0.906055   \n",
      "50%             -0.853382           -0.676286            -0.698864   \n",
      "75%             -0.542371           -0.409775            -0.250588   \n",
      "max              1.000000            0.955179             1.000000   \n",
      "\n",
      "       Shoot14DaysSeedling  Shoot17DaysSeedling  Shoot21DaysSeedling  \\\n",
      "count         41110.000000         41110.000000         41110.000000   \n",
      "mean             -0.734141            -0.680810            -0.659443   \n",
      "std               0.413716             0.478189             0.463838   \n",
      "min              -1.000000            -1.000000            -1.000000   \n",
      "25%              -1.000000            -1.000000            -1.000000   \n",
      "50%              -0.924976            -0.954040            -0.874080   \n",
      "75%              -0.513759            -0.420386            -0.440577   \n",
      "max               0.997390             1.000000             1.000000   \n",
      "\n",
      "       Shoot35DaysSeedling  Leaf21DaysSeedling  Leaf45DaysOldPlant  \\\n",
      "count         41110.000000        41110.000000        41110.000000   \n",
      "mean             -0.558906           -0.828778           -0.585144   \n",
      "std               0.506423            0.327542            0.399046   \n",
      "min              -1.000000           -1.000000           -1.000000   \n",
      "25%              -0.962199           -1.000000           -0.901444   \n",
      "50%              -0.699035           -0.951894           -0.643376   \n",
      "75%              -0.352995           -0.883755           -0.451900   \n",
      "max               0.993958            1.000000            1.000000   \n",
      "\n",
      "              class  \n",
      "count  41110.000000  \n",
      "mean      59.092703  \n",
      "std       77.624892  \n",
      "min        0.000000  \n",
      "25%        8.000000  \n",
      "50%       25.000000  \n",
      "75%       77.000000  \n",
      "max      372.000000  \n",
      "\n",
      "[8 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# load dataset (input variables = X, output variables = Y)\n",
    "df = pd.read_csv(\"2.csv\")\n",
    "\n",
    "#count the number of occurances for each osID\n",
    "OsID_counts = df['OsID'].value_counts()\n",
    "\n",
    "#filter for osIDs that have 10 or more occurances\n",
    "OsID_counts_filtered = OsID_counts[OsID_counts >= 10]\n",
    "\n",
    "#assign a label for each osID \n",
    "OsID_labels = {}\n",
    "class_no = 0\n",
    "for osID in OsID_counts_filtered.index:\n",
    "    OsID_labels[osID] = class_no\n",
    "    class_no +=1\n",
    "\n",
    "#filter the dataset with osID that contain 10 or more occurances\n",
    "dataGene = df[df['OsID'].isin(OsID_counts_filtered.index)]\n",
    "\n",
    "dataGene = dataGene.drop(['Class', 'Trait'],axis=1)\n",
    "\n",
    "# Add a new column 'class' to the filtered dataset\n",
    "dataGene['class'] = dataGene['OsID'].map(OsID_labels)\n",
    "\n",
    "print(\"Summary of dataGene:\\n\",dataGene.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X:\n",
      " (41110, 20)\n",
      "Shape of Y:\n",
      " (41110,)\n",
      "Summary of X:\n",
      "        Root10DaysSeedling  Leaf45DaysOldPlant  Shoot10DaysSeedling  \\\n",
      "count        41110.000000        41110.000000         41110.000000   \n",
      "mean            -0.522040           -0.585144            -0.545055   \n",
      "std              0.498568            0.399046             0.477438   \n",
      "min             -1.000000           -1.000000            -1.000000   \n",
      "25%             -0.901371           -0.901444            -0.906055   \n",
      "50%             -0.663664           -0.643376            -0.698864   \n",
      "75%             -0.378497           -0.451900            -0.250588   \n",
      "max              1.000000            1.000000             1.000000   \n",
      "\n",
      "       Shoot35DaysSeedling  Root35DaysSeedling  Leaf21DaysSeedling  \\\n",
      "count         41110.000000        41110.000000        41110.000000   \n",
      "mean             -0.558906           -0.596196           -0.828778   \n",
      "std               0.506423            0.461679            0.327542   \n",
      "min              -1.000000           -1.000000           -1.000000   \n",
      "25%              -0.962199           -0.937286           -1.000000   \n",
      "50%              -0.699035           -0.769184           -0.951894   \n",
      "75%              -0.352995           -0.323664           -0.883755   \n",
      "max               0.993958            1.000000            1.000000   \n",
      "\n",
      "       Root14DaysSeedling  Shoot3DaysSeedling  Root24DaysSeedling  \\\n",
      "count        41110.000000        41110.000000        41110.000000   \n",
      "mean            -0.646982           -0.590806           -0.670048   \n",
      "std              0.393549            0.443552            0.390751   \n",
      "min             -1.000000           -1.000000           -1.000000   \n",
      "25%             -0.965084           -1.000000           -0.982003   \n",
      "50%             -0.680003           -0.676286           -0.708584   \n",
      "75%             -0.559627           -0.409775           -0.482133   \n",
      "max              1.000000            0.955179            1.000000   \n",
      "\n",
      "       Root52DaysSeedling  Root17DaysSeedling  Root21DaysSeedling  \\\n",
      "count        41110.000000        41110.000000        41110.000000   \n",
      "mean            -0.670345           -0.700869           -0.669349   \n",
      "std              0.478222            0.378219            0.405860   \n",
      "min             -1.000000           -1.000000           -1.000000   \n",
      "25%             -1.000000           -0.980226           -1.000000   \n",
      "50%             -0.853382           -0.795609           -0.726665   \n",
      "75%             -0.542371           -0.601266           -0.543621   \n",
      "max              1.000000            1.000000            1.000000   \n",
      "\n",
      "       Shoot14DaysSeedling  Shoot21DaysSeedling  Shoot17DaysSeedling  \\\n",
      "count         41110.000000         41110.000000         41110.000000   \n",
      "mean             -0.734141            -0.659443            -0.680810   \n",
      "std               0.413716             0.463838             0.478189   \n",
      "min              -1.000000            -1.000000            -1.000000   \n",
      "25%              -1.000000            -1.000000            -1.000000   \n",
      "50%              -0.924976            -0.874080            -0.954040   \n",
      "75%              -0.513759            -0.440577            -0.420386   \n",
      "max               0.997390             1.000000             1.000000   \n",
      "\n",
      "                 ET           PCC  log_2FoldChange           PPI  CoExpression  \n",
      "count  41110.000000  41110.000000     41110.000000  41110.000000  41110.000000  \n",
      "mean       1.407395     -0.361737        -0.037332      0.914668      0.991997  \n",
      "std        0.784327      0.463979         0.391444      0.279379      0.089101  \n",
      "min        0.000000     -1.000000        -1.000000      0.000000      0.000000  \n",
      "25%        1.000000     -0.747963        -0.251534      1.000000      1.000000  \n",
      "50%        2.000000     -0.449089         0.030675      1.000000      1.000000  \n",
      "75%        2.000000     -0.051646         0.251534      1.000000      1.000000  \n",
      "max        2.000000      1.000000         1.000000      1.000000      1.000000  \n",
      "Summary of Y:\n",
      " count    41110.000000\n",
      "mean        59.092703\n",
      "std         77.624892\n",
      "min          0.000000\n",
      "25%          8.000000\n",
      "50%         25.000000\n",
      "75%         77.000000\n",
      "max        372.000000\n",
      "Name: class, dtype: float64\n",
      "class\n",
      "0.0      1800\n",
      "1.0      1296\n",
      "2.0      1260\n",
      "3.0      1218\n",
      "4.0      1026\n",
      "         ... \n",
      "368.0      10\n",
      "369.0      10\n",
      "370.0      10\n",
      "371.0      10\n",
      "372.0      10\n",
      "Length: 373, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X = dataGene.drop(['class','OsID'], axis=1) # exclude class & OsID column\n",
    "Y = dataGene['class']\n",
    "\n",
    "#input feature names in order of descending importance scores in PCC feature selection method\n",
    "feature_names = ['Root10DaysSeedling', 'Leaf45DaysOldPlant', 'Shoot10DaysSeedling', 'Shoot35DaysSeedling', 'Root35DaysSeedling', \n",
    "                 'Leaf21DaysSeedling', 'Root14DaysSeedling', 'Shoot3DaysSeedling', 'Root24DaysSeedling', 'Root52DaysSeedling', \n",
    "                 'Root17DaysSeedling', 'Root21DaysSeedling', 'Shoot14DaysSeedling', 'Shoot21DaysSeedling', 'Shoot17DaysSeedling',\n",
    "                  'ET', 'PCC', 'log_2FoldChange', 'PPI', 'CoExpression']\n",
    "\n",
    "X_fs = X.reindex(columns=feature_names)\n",
    "\n",
    "print(\"Shape of X:\\n\",X_fs.shape)\n",
    "print(\"Shape of Y:\\n\",Y.shape)\n",
    "\n",
    "# Statistical summary of the variables\n",
    "print(\"Summary of X:\\n\",X_fs.describe())\n",
    "print(\"Summary of Y:\\n\",Y.describe())\n",
    "\n",
    "# Check for class imbalance\n",
    "print(df.groupby(Y).size())\n",
    "\n",
    "# change both input and target variables datatype to ndarray\n",
    "\n",
    "X_fs = X_fs.values # 2-D array\n",
    "\n",
    "# select target variable \n",
    "\n",
    "Y = Y.values #1-D array\n",
    "Y = Y.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class=0, n=1800 (4.378%)\n",
      "Class=1, n=1296 (3.153%)\n",
      "Class=2, n=1260 (3.065%)\n",
      "Class=3, n=1218 (2.963%)\n",
      "Class=4, n=1026 (2.496%)\n",
      "Class=5, n=1008 (2.452%)\n",
      "Class=6, n=930 (2.262%)\n",
      "Class=7, n=912 (2.218%)\n",
      "Class=8, n=880 (2.141%)\n",
      "Class=9, n=798 (1.941%)\n",
      "Class=10, n=792 (1.927%)\n",
      "Class=11, n=759 (1.846%)\n",
      "Class=12, n=729 (1.773%)\n",
      "Class=13, n=720 (1.751%)\n",
      "Class=14, n=702 (1.708%)\n",
      "Class=15, n=693 (1.686%)\n",
      "Class=16, n=672 (1.635%)\n",
      "Class=17, n=640 (1.557%)\n",
      "Class=18, n=625 (1.520%)\n",
      "Class=19, n=570 (1.387%)\n",
      "Class=20, n=546 (1.328%)\n",
      "Class=21, n=506 (1.231%)\n",
      "Class=22, n=483 (1.175%)\n",
      "Class=23, n=448 (1.090%)\n",
      "Class=24, n=432 (1.051%)\n",
      "Class=25, n=384 (0.934%)\n",
      "Class=26, n=360 (0.876%)\n",
      "Class=27, n=360 (0.876%)\n",
      "Class=28, n=320 (0.778%)\n",
      "Class=29, n=312 (0.759%)\n",
      "Class=30, n=312 (0.759%)\n",
      "Class=31, n=306 (0.744%)\n",
      "Class=32, n=304 (0.739%)\n",
      "Class=33, n=299 (0.727%)\n",
      "Class=34, n=297 (0.722%)\n",
      "Class=35, n=296 (0.720%)\n",
      "Class=36, n=280 (0.681%)\n",
      "Class=37, n=264 (0.642%)\n",
      "Class=38, n=260 (0.632%)\n",
      "Class=39, n=253 (0.615%)\n",
      "Class=40, n=252 (0.613%)\n",
      "Class=41, n=248 (0.603%)\n",
      "Class=42, n=242 (0.589%)\n",
      "Class=43, n=228 (0.555%)\n",
      "Class=44, n=216 (0.525%)\n",
      "Class=45, n=210 (0.511%)\n",
      "Class=46, n=200 (0.486%)\n",
      "Class=47, n=192 (0.467%)\n",
      "Class=48, n=180 (0.438%)\n",
      "Class=49, n=171 (0.416%)\n",
      "Class=50, n=168 (0.409%)\n",
      "Class=51, n=168 (0.409%)\n",
      "Class=52, n=162 (0.394%)\n",
      "Class=53, n=150 (0.365%)\n",
      "Class=54, n=148 (0.360%)\n",
      "Class=55, n=138 (0.336%)\n",
      "Class=56, n=135 (0.328%)\n",
      "Class=57, n=135 (0.328%)\n",
      "Class=58, n=133 (0.324%)\n",
      "Class=59, n=132 (0.321%)\n",
      "Class=60, n=132 (0.321%)\n",
      "Class=61, n=130 (0.316%)\n",
      "Class=62, n=130 (0.316%)\n",
      "Class=63, n=130 (0.316%)\n",
      "Class=64, n=128 (0.311%)\n",
      "Class=65, n=128 (0.311%)\n",
      "Class=66, n=126 (0.306%)\n",
      "Class=67, n=124 (0.302%)\n",
      "Class=68, n=124 (0.302%)\n",
      "Class=69, n=124 (0.302%)\n",
      "Class=70, n=120 (0.292%)\n",
      "Class=71, n=120 (0.292%)\n",
      "Class=72, n=118 (0.287%)\n",
      "Class=73, n=116 (0.282%)\n",
      "Class=74, n=114 (0.277%)\n",
      "Class=75, n=105 (0.255%)\n",
      "Class=76, n=104 (0.253%)\n",
      "Class=77, n=102 (0.248%)\n",
      "Class=78, n=99 (0.241%)\n",
      "Class=79, n=98 (0.238%)\n",
      "Class=80, n=98 (0.238%)\n",
      "Class=81, n=98 (0.238%)\n",
      "Class=82, n=98 (0.238%)\n",
      "Class=83, n=96 (0.234%)\n",
      "Class=84, n=96 (0.234%)\n",
      "Class=85, n=96 (0.234%)\n",
      "Class=86, n=93 (0.226%)\n",
      "Class=87, n=92 (0.224%)\n",
      "Class=88, n=92 (0.224%)\n",
      "Class=89, n=91 (0.221%)\n",
      "Class=90, n=88 (0.214%)\n",
      "Class=91, n=88 (0.214%)\n",
      "Class=92, n=86 (0.209%)\n",
      "Class=93, n=86 (0.209%)\n",
      "Class=94, n=84 (0.204%)\n",
      "Class=95, n=84 (0.204%)\n",
      "Class=96, n=84 (0.204%)\n",
      "Class=97, n=78 (0.190%)\n",
      "Class=98, n=78 (0.190%)\n",
      "Class=99, n=76 (0.185%)\n",
      "Class=100, n=75 (0.182%)\n",
      "Class=101, n=75 (0.182%)\n",
      "Class=102, n=73 (0.178%)\n",
      "Class=103, n=72 (0.175%)\n",
      "Class=104, n=72 (0.175%)\n",
      "Class=105, n=70 (0.170%)\n",
      "Class=106, n=69 (0.168%)\n",
      "Class=107, n=68 (0.165%)\n",
      "Class=108, n=67 (0.163%)\n",
      "Class=109, n=66 (0.161%)\n",
      "Class=110, n=66 (0.161%)\n",
      "Class=111, n=66 (0.161%)\n",
      "Class=112, n=66 (0.161%)\n",
      "Class=113, n=66 (0.161%)\n",
      "Class=114, n=65 (0.158%)\n",
      "Class=115, n=64 (0.156%)\n",
      "Class=116, n=63 (0.153%)\n",
      "Class=117, n=63 (0.153%)\n",
      "Class=118, n=62 (0.151%)\n",
      "Class=119, n=61 (0.148%)\n",
      "Class=120, n=60 (0.146%)\n",
      "Class=121, n=60 (0.146%)\n",
      "Class=122, n=60 (0.146%)\n",
      "Class=123, n=60 (0.146%)\n",
      "Class=124, n=60 (0.146%)\n",
      "Class=125, n=60 (0.146%)\n",
      "Class=126, n=60 (0.146%)\n",
      "Class=127, n=60 (0.146%)\n",
      "Class=128, n=60 (0.146%)\n",
      "Class=129, n=60 (0.146%)\n",
      "Class=130, n=59 (0.144%)\n",
      "Class=131, n=59 (0.144%)\n",
      "Class=132, n=58 (0.141%)\n",
      "Class=133, n=56 (0.136%)\n",
      "Class=134, n=56 (0.136%)\n",
      "Class=135, n=56 (0.136%)\n",
      "Class=136, n=56 (0.136%)\n",
      "Class=137, n=56 (0.136%)\n",
      "Class=138, n=56 (0.136%)\n",
      "Class=139, n=56 (0.136%)\n",
      "Class=140, n=56 (0.136%)\n",
      "Class=141, n=56 (0.136%)\n",
      "Class=142, n=55 (0.134%)\n",
      "Class=143, n=55 (0.134%)\n",
      "Class=144, n=54 (0.131%)\n",
      "Class=145, n=54 (0.131%)\n",
      "Class=146, n=54 (0.131%)\n",
      "Class=147, n=54 (0.131%)\n",
      "Class=148, n=54 (0.131%)\n",
      "Class=149, n=53 (0.129%)\n",
      "Class=150, n=52 (0.126%)\n",
      "Class=151, n=52 (0.126%)\n",
      "Class=152, n=52 (0.126%)\n",
      "Class=153, n=52 (0.126%)\n",
      "Class=154, n=50 (0.122%)\n",
      "Class=155, n=50 (0.122%)\n",
      "Class=156, n=49 (0.119%)\n",
      "Class=157, n=49 (0.119%)\n",
      "Class=158, n=48 (0.117%)\n",
      "Class=159, n=48 (0.117%)\n",
      "Class=160, n=48 (0.117%)\n",
      "Class=161, n=46 (0.112%)\n",
      "Class=162, n=45 (0.109%)\n",
      "Class=163, n=44 (0.107%)\n",
      "Class=164, n=44 (0.107%)\n",
      "Class=165, n=44 (0.107%)\n",
      "Class=166, n=42 (0.102%)\n",
      "Class=167, n=42 (0.102%)\n",
      "Class=168, n=42 (0.102%)\n",
      "Class=169, n=42 (0.102%)\n",
      "Class=170, n=42 (0.102%)\n",
      "Class=171, n=42 (0.102%)\n",
      "Class=172, n=42 (0.102%)\n",
      "Class=173, n=41 (0.100%)\n",
      "Class=174, n=41 (0.100%)\n",
      "Class=175, n=40 (0.097%)\n",
      "Class=176, n=40 (0.097%)\n",
      "Class=177, n=39 (0.095%)\n",
      "Class=178, n=39 (0.095%)\n",
      "Class=179, n=38 (0.092%)\n",
      "Class=180, n=37 (0.090%)\n",
      "Class=181, n=36 (0.088%)\n",
      "Class=182, n=35 (0.085%)\n",
      "Class=183, n=35 (0.085%)\n",
      "Class=184, n=35 (0.085%)\n",
      "Class=185, n=35 (0.085%)\n",
      "Class=186, n=34 (0.083%)\n",
      "Class=187, n=34 (0.083%)\n",
      "Class=188, n=34 (0.083%)\n",
      "Class=189, n=34 (0.083%)\n",
      "Class=190, n=32 (0.078%)\n",
      "Class=191, n=32 (0.078%)\n",
      "Class=192, n=32 (0.078%)\n",
      "Class=193, n=32 (0.078%)\n",
      "Class=194, n=32 (0.078%)\n",
      "Class=195, n=32 (0.078%)\n",
      "Class=196, n=31 (0.075%)\n",
      "Class=197, n=31 (0.075%)\n",
      "Class=198, n=31 (0.075%)\n",
      "Class=199, n=31 (0.075%)\n",
      "Class=200, n=30 (0.073%)\n",
      "Class=201, n=30 (0.073%)\n",
      "Class=202, n=30 (0.073%)\n",
      "Class=203, n=30 (0.073%)\n",
      "Class=204, n=30 (0.073%)\n",
      "Class=205, n=30 (0.073%)\n",
      "Class=206, n=30 (0.073%)\n",
      "Class=207, n=30 (0.073%)\n",
      "Class=208, n=30 (0.073%)\n",
      "Class=209, n=29 (0.071%)\n",
      "Class=210, n=29 (0.071%)\n",
      "Class=211, n=28 (0.068%)\n",
      "Class=212, n=28 (0.068%)\n",
      "Class=213, n=28 (0.068%)\n",
      "Class=214, n=28 (0.068%)\n",
      "Class=215, n=28 (0.068%)\n",
      "Class=216, n=28 (0.068%)\n",
      "Class=217, n=27 (0.066%)\n",
      "Class=218, n=27 (0.066%)\n",
      "Class=219, n=27 (0.066%)\n",
      "Class=220, n=27 (0.066%)\n",
      "Class=221, n=27 (0.066%)\n",
      "Class=222, n=27 (0.066%)\n",
      "Class=223, n=26 (0.063%)\n",
      "Class=224, n=26 (0.063%)\n",
      "Class=225, n=26 (0.063%)\n",
      "Class=226, n=26 (0.063%)\n",
      "Class=227, n=26 (0.063%)\n",
      "Class=228, n=25 (0.061%)\n",
      "Class=229, n=25 (0.061%)\n",
      "Class=230, n=25 (0.061%)\n",
      "Class=231, n=25 (0.061%)\n",
      "Class=232, n=24 (0.058%)\n",
      "Class=233, n=24 (0.058%)\n",
      "Class=234, n=24 (0.058%)\n",
      "Class=235, n=24 (0.058%)\n",
      "Class=236, n=24 (0.058%)\n",
      "Class=237, n=24 (0.058%)\n",
      "Class=238, n=24 (0.058%)\n",
      "Class=239, n=24 (0.058%)\n",
      "Class=240, n=24 (0.058%)\n",
      "Class=241, n=24 (0.058%)\n",
      "Class=242, n=24 (0.058%)\n",
      "Class=243, n=24 (0.058%)\n",
      "Class=244, n=23 (0.056%)\n",
      "Class=245, n=23 (0.056%)\n",
      "Class=246, n=22 (0.054%)\n",
      "Class=247, n=22 (0.054%)\n",
      "Class=248, n=22 (0.054%)\n",
      "Class=249, n=22 (0.054%)\n",
      "Class=250, n=22 (0.054%)\n",
      "Class=251, n=22 (0.054%)\n",
      "Class=252, n=22 (0.054%)\n",
      "Class=253, n=22 (0.054%)\n",
      "Class=254, n=22 (0.054%)\n",
      "Class=255, n=22 (0.054%)\n",
      "Class=256, n=22 (0.054%)\n",
      "Class=257, n=22 (0.054%)\n",
      "Class=258, n=22 (0.054%)\n",
      "Class=259, n=22 (0.054%)\n",
      "Class=260, n=22 (0.054%)\n",
      "Class=261, n=22 (0.054%)\n",
      "Class=262, n=22 (0.054%)\n",
      "Class=263, n=22 (0.054%)\n",
      "Class=264, n=21 (0.051%)\n",
      "Class=265, n=21 (0.051%)\n",
      "Class=266, n=21 (0.051%)\n",
      "Class=267, n=21 (0.051%)\n",
      "Class=268, n=21 (0.051%)\n",
      "Class=269, n=20 (0.049%)\n",
      "Class=270, n=20 (0.049%)\n",
      "Class=271, n=20 (0.049%)\n",
      "Class=272, n=20 (0.049%)\n",
      "Class=273, n=20 (0.049%)\n",
      "Class=274, n=20 (0.049%)\n",
      "Class=275, n=20 (0.049%)\n",
      "Class=276, n=20 (0.049%)\n",
      "Class=277, n=20 (0.049%)\n",
      "Class=278, n=20 (0.049%)\n",
      "Class=279, n=20 (0.049%)\n",
      "Class=280, n=19 (0.046%)\n",
      "Class=281, n=19 (0.046%)\n",
      "Class=282, n=19 (0.046%)\n",
      "Class=283, n=18 (0.044%)\n",
      "Class=284, n=18 (0.044%)\n",
      "Class=285, n=18 (0.044%)\n",
      "Class=286, n=18 (0.044%)\n",
      "Class=287, n=18 (0.044%)\n",
      "Class=288, n=18 (0.044%)\n",
      "Class=289, n=18 (0.044%)\n",
      "Class=290, n=18 (0.044%)\n",
      "Class=291, n=18 (0.044%)\n",
      "Class=292, n=17 (0.041%)\n",
      "Class=293, n=17 (0.041%)\n",
      "Class=294, n=17 (0.041%)\n",
      "Class=295, n=17 (0.041%)\n",
      "Class=296, n=17 (0.041%)\n",
      "Class=297, n=17 (0.041%)\n",
      "Class=298, n=16 (0.039%)\n",
      "Class=299, n=16 (0.039%)\n",
      "Class=300, n=16 (0.039%)\n",
      "Class=301, n=16 (0.039%)\n",
      "Class=302, n=16 (0.039%)\n",
      "Class=303, n=16 (0.039%)\n",
      "Class=304, n=16 (0.039%)\n",
      "Class=305, n=16 (0.039%)\n",
      "Class=306, n=15 (0.036%)\n",
      "Class=307, n=15 (0.036%)\n",
      "Class=308, n=15 (0.036%)\n",
      "Class=309, n=15 (0.036%)\n",
      "Class=310, n=15 (0.036%)\n",
      "Class=311, n=14 (0.034%)\n",
      "Class=312, n=14 (0.034%)\n",
      "Class=313, n=14 (0.034%)\n",
      "Class=314, n=14 (0.034%)\n",
      "Class=315, n=14 (0.034%)\n",
      "Class=316, n=14 (0.034%)\n",
      "Class=317, n=14 (0.034%)\n",
      "Class=318, n=14 (0.034%)\n",
      "Class=319, n=14 (0.034%)\n",
      "Class=320, n=14 (0.034%)\n",
      "Class=321, n=14 (0.034%)\n",
      "Class=322, n=14 (0.034%)\n",
      "Class=323, n=14 (0.034%)\n",
      "Class=324, n=14 (0.034%)\n",
      "Class=325, n=14 (0.034%)\n",
      "Class=326, n=14 (0.034%)\n",
      "Class=327, n=14 (0.034%)\n",
      "Class=328, n=13 (0.032%)\n",
      "Class=329, n=13 (0.032%)\n",
      "Class=330, n=13 (0.032%)\n",
      "Class=331, n=13 (0.032%)\n",
      "Class=332, n=13 (0.032%)\n",
      "Class=333, n=13 (0.032%)\n",
      "Class=334, n=13 (0.032%)\n",
      "Class=335, n=13 (0.032%)\n",
      "Class=336, n=13 (0.032%)\n",
      "Class=337, n=12 (0.029%)\n",
      "Class=338, n=12 (0.029%)\n",
      "Class=339, n=12 (0.029%)\n",
      "Class=340, n=12 (0.029%)\n",
      "Class=341, n=12 (0.029%)\n",
      "Class=342, n=12 (0.029%)\n",
      "Class=343, n=12 (0.029%)\n",
      "Class=344, n=12 (0.029%)\n",
      "Class=345, n=12 (0.029%)\n",
      "Class=346, n=12 (0.029%)\n",
      "Class=347, n=12 (0.029%)\n",
      "Class=348, n=12 (0.029%)\n",
      "Class=349, n=12 (0.029%)\n",
      "Class=350, n=12 (0.029%)\n",
      "Class=351, n=12 (0.029%)\n",
      "Class=352, n=12 (0.029%)\n",
      "Class=353, n=12 (0.029%)\n",
      "Class=354, n=12 (0.029%)\n",
      "Class=355, n=11 (0.027%)\n",
      "Class=356, n=11 (0.027%)\n",
      "Class=357, n=11 (0.027%)\n",
      "Class=358, n=11 (0.027%)\n",
      "Class=359, n=11 (0.027%)\n",
      "Class=360, n=11 (0.027%)\n",
      "Class=361, n=10 (0.024%)\n",
      "Class=362, n=10 (0.024%)\n",
      "Class=363, n=10 (0.024%)\n",
      "Class=364, n=10 (0.024%)\n",
      "Class=365, n=10 (0.024%)\n",
      "Class=366, n=10 (0.024%)\n",
      "Class=367, n=10 (0.024%)\n",
      "Class=368, n=10 (0.024%)\n",
      "Class=369, n=10 (0.024%)\n",
      "Class=370, n=10 (0.024%)\n",
      "Class=371, n=10 (0.024%)\n",
      "Class=372, n=10 (0.024%)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# summarize distribution\n",
    "counter = Counter(Y.flatten())\n",
    "\n",
    "# sort counter by keys\n",
    "counter = dict(sorted(counter.items()))\n",
    "\n",
    "for k,v in counter.items():\n",
    " per = v / len(Y.flatten()) * 100\n",
    " print('Class=%d, n=%d (%.3f%%)' % (k, v, per))\n",
    "# plot the distribution\n",
    "plt.bar( counter.keys(), counter.values())\n",
    "\n",
    "plt.ylabel('No of gene samples')\n",
    "plt.xlabel('Class Index of OsID')\n",
    "plt.title('Distribution of Target Classes (OsID)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_confusion_matrix(cm):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    sns.heatmap(cm,annot=True,fmt=\"d\", center=0, cmap='autumn') \n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare target data\n",
    "def prepare_targets(y_train, y_test):\n",
    "\tle = LabelEncoder()\n",
    "\t\n",
    "\t#fit the encoders only to the training data and then transform both train and test data\n",
    "\ty_train_enc = le.fit_transform(y_train)\n",
    "\ty_test_enc = le.transform(y_test)\n",
    "\n",
    "\treturn y_train_enc, y_test_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define baseline model (MLP)\n",
    "def MLP_model(input_dim):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_dim=input_dim,bias_initializer='normal', activation='relu',kernel_regularizer=regularizers.l2(0.0001)))\n",
    "    model.add(Dense(20,bias_initializer='normal',activation='relu',kernel_regularizer=regularizers.l2(0.0001)))\n",
    "    model.add(Dense(20,bias_initializer='normal',activation='relu',kernel_regularizer=regularizers.l2(0.0001)))\n",
    "    model.add(Dense(373,kernel_initializer='normal', activation='softmax')) #softmax for multi-class classification, num_classes = 373\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input features: 1\n",
      "Fold: 1\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 16s 7ms/step - loss: 4.4897 - accuracy: 0.1007 - val_loss: 4.0962 - val_accuracy: 0.1204\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 12s 6ms/step - loss: 3.7918 - accuracy: 0.1682 - val_loss: 3.7338 - val_accuracy: 0.1925\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 12s 7ms/step - loss: 3.4393 - accuracy: 0.2285 - val_loss: 3.4744 - val_accuracy: 0.2609\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 12s 7ms/step - loss: 3.1776 - accuracy: 0.2671 - val_loss: 3.2773 - val_accuracy: 0.2499\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 2.9827 - accuracy: 0.2905 - val_loss: 3.1525 - val_accuracy: 0.3105\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 12s 6ms/step - loss: 2.8385 - accuracy: 0.3106 - val_loss: 3.0506 - val_accuracy: 0.2968\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 12s 7ms/step - loss: 2.7327 - accuracy: 0.3318 - val_loss: 2.9825 - val_accuracy: 0.3127\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 2.6656 - accuracy: 0.3389 - val_loss: 2.9069 - val_accuracy: 0.3448\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 2.6018 - accuracy: 0.3543 - val_loss: 2.9095 - val_accuracy: 0.3109\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 2.5584 - accuracy: 0.3629 - val_loss: 2.8520 - val_accuracy: 0.3666\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 2.5221 - accuracy: 0.3711 - val_loss: 2.7916 - val_accuracy: 0.3993\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 2.4905 - accuracy: 0.3733 - val_loss: 2.7615 - val_accuracy: 0.3729\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 2.4691 - accuracy: 0.3801 - val_loss: 2.7581 - val_accuracy: 0.3652\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 2.4530 - accuracy: 0.3767 - val_loss: 2.6931 - val_accuracy: 0.3668\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 2.4212 - accuracy: 0.3859 - val_loss: 2.6856 - val_accuracy: 0.3364\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 2.4055 - accuracy: 0.3885 - val_loss: 2.6767 - val_accuracy: 0.3980\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 12s 6ms/step - loss: 2.3952 - accuracy: 0.3866 - val_loss: 2.6349 - val_accuracy: 0.3982\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 2.3650 - accuracy: 0.3990 - val_loss: 2.6295 - val_accuracy: 0.4024\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 2.3569 - accuracy: 0.3962 - val_loss: 2.6304 - val_accuracy: 0.4194\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 2.3535 - accuracy: 0.3961 - val_loss: 2.7222 - val_accuracy: 0.3237\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 14s 6ms/step - loss: 4.4763 - accuracy: 0.1006 - val_loss: 4.1280 - val_accuracy: 0.1364\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 3.8223 - accuracy: 0.1654 - val_loss: 3.8503 - val_accuracy: 0.1723\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 3.5086 - accuracy: 0.2205 - val_loss: 3.5719 - val_accuracy: 0.2488\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 3.2283 - accuracy: 0.2532 - val_loss: 3.3594 - val_accuracy: 0.2528\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 2.9889 - accuracy: 0.2793 - val_loss: 3.1778 - val_accuracy: 0.3397\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 2.8097 - accuracy: 0.3257 - val_loss: 3.0103 - val_accuracy: 0.3586\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 2.6670 - accuracy: 0.3494 - val_loss: 2.8990 - val_accuracy: 0.3245\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 2.5530 - accuracy: 0.3726 - val_loss: 2.7975 - val_accuracy: 0.3435\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 2.4626 - accuracy: 0.3907 - val_loss: 2.6695 - val_accuracy: 0.4416\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 2.3912 - accuracy: 0.4124 - val_loss: 2.6077 - val_accuracy: 0.4491\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 2.3256 - accuracy: 0.4285 - val_loss: 2.5443 - val_accuracy: 0.4018\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 2.2802 - accuracy: 0.4319 - val_loss: 2.5536 - val_accuracy: 0.4048\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 2.2337 - accuracy: 0.4410 - val_loss: 2.4821 - val_accuracy: 0.4268\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 12s 6ms/step - loss: 2.2046 - accuracy: 0.4476 - val_loss: 2.4152 - val_accuracy: 0.4350\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 2.1657 - accuracy: 0.4498 - val_loss: 2.4299 - val_accuracy: 0.4627\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 2.1356 - accuracy: 0.4566 - val_loss: 2.3969 - val_accuracy: 0.4220\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 2.1063 - accuracy: 0.4582 - val_loss: 2.3051 - val_accuracy: 0.4942\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 2.0896 - accuracy: 0.4646 - val_loss: 2.3072 - val_accuracy: 0.4680\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 2.0683 - accuracy: 0.4668 - val_loss: 2.3032 - val_accuracy: 0.4675\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 2.0504 - accuracy: 0.4680 - val_loss: 2.3023 - val_accuracy: 0.4389\n",
      "Average Validation Accuracy: 0.398215651512146\n",
      "Average Validation Loss: 2.29996395111084\n",
      "Average Test Accuracy: 0.4002358615398407\n",
      "Final Test Accuracy for each fold: 0.4536743462085724\n",
      "Number of input features: 2\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 14s 6ms/step - loss: 4.0995 - accuracy: 0.1821 - val_loss: 3.4167 - val_accuracy: 0.2878\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 2.8680 - accuracy: 0.3768 - val_loss: 2.6935 - val_accuracy: 0.4607\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 2.2144 - accuracy: 0.5189 - val_loss: 2.2021 - val_accuracy: 0.5976\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 1.7707 - accuracy: 0.5944 - val_loss: 1.8704 - val_accuracy: 0.6493\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 1.4994 - accuracy: 0.6370 - val_loss: 1.7087 - val_accuracy: 0.6251\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 1.3402 - accuracy: 0.6592 - val_loss: 1.5761 - val_accuracy: 0.6673\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 1.2326 - accuracy: 0.6791 - val_loss: 1.4537 - val_accuracy: 0.6920\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 1.1543 - accuracy: 0.6987 - val_loss: 1.4003 - val_accuracy: 0.7098\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 1.0934 - accuracy: 0.7075 - val_loss: 1.3179 - val_accuracy: 0.7168\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 1.0387 - accuracy: 0.7172 - val_loss: 1.3510 - val_accuracy: 0.7017\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 1.0001 - accuracy: 0.7223 - val_loss: 1.2132 - val_accuracy: 0.7248\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.9657 - accuracy: 0.7323 - val_loss: 1.2165 - val_accuracy: 0.7285\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.9275 - accuracy: 0.7445 - val_loss: 1.1794 - val_accuracy: 0.7371\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.9095 - accuracy: 0.7491 - val_loss: 1.1047 - val_accuracy: 0.7296\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.8863 - accuracy: 0.7508 - val_loss: 1.1112 - val_accuracy: 0.7622\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.8606 - accuracy: 0.7582 - val_loss: 1.0886 - val_accuracy: 0.7454\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.8432 - accuracy: 0.7614 - val_loss: 1.0348 - val_accuracy: 0.7683\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.8275 - accuracy: 0.7643 - val_loss: 1.0452 - val_accuracy: 0.7624\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.8223 - accuracy: 0.7658 - val_loss: 1.0066 - val_accuracy: 0.7646\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.7957 - accuracy: 0.7755 - val_loss: 1.0194 - val_accuracy: 0.7644\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 14s 6ms/step - loss: 4.0736 - accuracy: 0.1798 - val_loss: 3.4500 - val_accuracy: 0.2977\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 2.8194 - accuracy: 0.3911 - val_loss: 2.7334 - val_accuracy: 0.4649\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 2.1643 - accuracy: 0.5179 - val_loss: 2.2549 - val_accuracy: 0.5578\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 1.7266 - accuracy: 0.5909 - val_loss: 1.9778 - val_accuracy: 0.6541\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 1.4499 - accuracy: 0.6414 - val_loss: 1.7251 - val_accuracy: 0.6583\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 1.2831 - accuracy: 0.6720 - val_loss: 1.5851 - val_accuracy: 0.6469\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 1.1738 - accuracy: 0.6942 - val_loss: 1.4931 - val_accuracy: 0.6915\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 1.0947 - accuracy: 0.7080 - val_loss: 1.3822 - val_accuracy: 0.7113\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 1.0303 - accuracy: 0.7246 - val_loss: 1.3726 - val_accuracy: 0.6895\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.9850 - accuracy: 0.7324 - val_loss: 1.2430 - val_accuracy: 0.7168\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.9433 - accuracy: 0.7400 - val_loss: 1.2500 - val_accuracy: 0.7311\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.9241 - accuracy: 0.7425 - val_loss: 1.2110 - val_accuracy: 0.7402\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.8815 - accuracy: 0.7540 - val_loss: 1.1496 - val_accuracy: 0.7798\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.8701 - accuracy: 0.7598 - val_loss: 1.0983 - val_accuracy: 0.7681\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.8404 - accuracy: 0.7637 - val_loss: 1.1361 - val_accuracy: 0.7498\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 12s 6ms/step - loss: 0.8235 - accuracy: 0.7731 - val_loss: 1.0706 - val_accuracy: 0.7820\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.8078 - accuracy: 0.7729 - val_loss: 1.1444 - val_accuracy: 0.7069\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.7961 - accuracy: 0.7748 - val_loss: 1.0151 - val_accuracy: 0.7879\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.7777 - accuracy: 0.7803 - val_loss: 1.0134 - val_accuracy: 0.7989\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.7803 - accuracy: 0.7817 - val_loss: 1.0880 - val_accuracy: 0.7606\n",
      "Average Validation Accuracy: 0.7730096280574799\n",
      "Average Validation Loss: 0.8731164634227753\n",
      "Average Test Accuracy: 0.7657551169395447\n",
      "Final Test Accuracy for each fold: 0.7700302004814148\n",
      "Number of input features: 3\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 13s 6ms/step - loss: 3.8138 - accuracy: 0.2651 - val_loss: 2.8913 - val_accuracy: 0.4645\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 2.2555 - accuracy: 0.5383 - val_loss: 2.0795 - val_accuracy: 0.5776\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 1.5402 - accuracy: 0.6549 - val_loss: 1.5763 - val_accuracy: 0.6909\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 1.1331 - accuracy: 0.7303 - val_loss: 1.2375 - val_accuracy: 0.7611\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.9124 - accuracy: 0.7667 - val_loss: 1.0907 - val_accuracy: 0.7835\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.7937 - accuracy: 0.7912 - val_loss: 0.9965 - val_accuracy: 0.7910\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.7229 - accuracy: 0.8039 - val_loss: 0.9459 - val_accuracy: 0.7969\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.6663 - accuracy: 0.8162 - val_loss: 0.8672 - val_accuracy: 0.7987\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.6275 - accuracy: 0.8218 - val_loss: 0.8313 - val_accuracy: 0.8330\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.5923 - accuracy: 0.8330 - val_loss: 0.7974 - val_accuracy: 0.8521\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.5569 - accuracy: 0.8440 - val_loss: 0.8105 - val_accuracy: 0.8103\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.5348 - accuracy: 0.8457 - val_loss: 0.7867 - val_accuracy: 0.8167\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.5082 - accuracy: 0.8593 - val_loss: 0.6821 - val_accuracy: 0.8627\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.5046 - accuracy: 0.8562 - val_loss: 0.6623 - val_accuracy: 0.8664\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.4824 - accuracy: 0.8643 - val_loss: 0.8124 - val_accuracy: 0.8495\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.4677 - accuracy: 0.8640 - val_loss: 0.7094 - val_accuracy: 0.8517\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.4522 - accuracy: 0.8711 - val_loss: 0.6589 - val_accuracy: 0.8554\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.4488 - accuracy: 0.8775 - val_loss: 0.6440 - val_accuracy: 0.8671\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.4247 - accuracy: 0.8840 - val_loss: 0.6893 - val_accuracy: 0.8579\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.4195 - accuracy: 0.8826 - val_loss: 0.5946 - val_accuracy: 0.8917\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 13s 6ms/step - loss: 3.8374 - accuracy: 0.2660 - val_loss: 2.9746 - val_accuracy: 0.4266\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 2.1717 - accuracy: 0.5622 - val_loss: 2.0260 - val_accuracy: 0.6092\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 1.4596 - accuracy: 0.6699 - val_loss: 1.6061 - val_accuracy: 0.7105\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 1.0968 - accuracy: 0.7383 - val_loss: 1.3641 - val_accuracy: 0.7481\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.8951 - accuracy: 0.7783 - val_loss: 1.1854 - val_accuracy: 0.7965\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.7753 - accuracy: 0.8042 - val_loss: 1.0546 - val_accuracy: 0.8352\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.6842 - accuracy: 0.8305 - val_loss: 1.0302 - val_accuracy: 0.8044\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.6098 - accuracy: 0.8475 - val_loss: 0.9088 - val_accuracy: 0.8510\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.5614 - accuracy: 0.8581 - val_loss: 0.8432 - val_accuracy: 0.8565\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.5285 - accuracy: 0.8617 - val_loss: 0.8177 - val_accuracy: 0.8530\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.4971 - accuracy: 0.8675 - val_loss: 0.7465 - val_accuracy: 0.8680\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.4721 - accuracy: 0.8740 - val_loss: 0.8095 - val_accuracy: 0.8381\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.4606 - accuracy: 0.8763 - val_loss: 0.7536 - val_accuracy: 0.8398\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.4287 - accuracy: 0.8854 - val_loss: 0.7367 - val_accuracy: 0.8616\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.4171 - accuracy: 0.8884 - val_loss: 0.6670 - val_accuracy: 0.8944\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.4108 - accuracy: 0.8914 - val_loss: 0.6588 - val_accuracy: 0.8792\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.3904 - accuracy: 0.8989 - val_loss: 0.6460 - val_accuracy: 0.8851\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.3872 - accuracy: 0.8989 - val_loss: 0.6525 - val_accuracy: 0.8834\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.3797 - accuracy: 0.8985 - val_loss: 0.6440 - val_accuracy: 0.8964\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.3689 - accuracy: 0.9044 - val_loss: 0.5870 - val_accuracy: 0.8961\n",
      "Average Validation Accuracy: 0.9005557596683502\n",
      "Average Validation Loss: 0.4278615862131119\n",
      "Average Test Accuracy: 0.8925333619117737\n",
      "Final Test Accuracy for each fold: 0.8954817056655884\n",
      "Number of input features: 4\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 13s 6ms/step - loss: 3.8647 - accuracy: 0.2583 - val_loss: 2.8551 - val_accuracy: 0.4854\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 2.0737 - accuracy: 0.6167 - val_loss: 1.8348 - val_accuracy: 0.6825\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 1.3053 - accuracy: 0.7459 - val_loss: 1.2960 - val_accuracy: 0.7421\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.9038 - accuracy: 0.8027 - val_loss: 0.9842 - val_accuracy: 0.8227\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.7041 - accuracy: 0.8394 - val_loss: 0.8476 - val_accuracy: 0.8440\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.5963 - accuracy: 0.8586 - val_loss: 0.7771 - val_accuracy: 0.8444\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.5290 - accuracy: 0.8711 - val_loss: 0.7077 - val_accuracy: 0.8713\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.4865 - accuracy: 0.8774 - val_loss: 0.6708 - val_accuracy: 0.8816\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.4531 - accuracy: 0.8825 - val_loss: 0.6249 - val_accuracy: 0.8834\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.4255 - accuracy: 0.8878 - val_loss: 0.6030 - val_accuracy: 0.9017\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.4095 - accuracy: 0.8935 - val_loss: 0.6144 - val_accuracy: 0.8799\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.3871 - accuracy: 0.8995 - val_loss: 0.5794 - val_accuracy: 0.8906\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.3699 - accuracy: 0.9028 - val_loss: 0.5561 - val_accuracy: 0.9109\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.3555 - accuracy: 0.9072 - val_loss: 0.5265 - val_accuracy: 0.9069\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.3532 - accuracy: 0.9078 - val_loss: 0.5965 - val_accuracy: 0.9034\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.3357 - accuracy: 0.9112 - val_loss: 0.5103 - val_accuracy: 0.9195\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.3272 - accuracy: 0.9119 - val_loss: 0.5267 - val_accuracy: 0.9144\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.3270 - accuracy: 0.9158 - val_loss: 0.4745 - val_accuracy: 0.9195\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.3100 - accuracy: 0.9178 - val_loss: 0.5043 - val_accuracy: 0.9177\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.3039 - accuracy: 0.9203 - val_loss: 0.4562 - val_accuracy: 0.9289\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 13s 6ms/step - loss: 3.5899 - accuracy: 0.3165 - val_loss: 2.6226 - val_accuracy: 0.5494\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 1.8473 - accuracy: 0.6478 - val_loss: 1.7128 - val_accuracy: 0.7215\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 1.1282 - accuracy: 0.7733 - val_loss: 1.2390 - val_accuracy: 0.7899\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.8014 - accuracy: 0.8258 - val_loss: 0.9835 - val_accuracy: 0.8308\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.6377 - accuracy: 0.8452 - val_loss: 0.8252 - val_accuracy: 0.8411\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.5358 - accuracy: 0.8633 - val_loss: 0.7430 - val_accuracy: 0.8473\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.4770 - accuracy: 0.8781 - val_loss: 0.6889 - val_accuracy: 0.8680\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.4350 - accuracy: 0.8854 - val_loss: 0.5959 - val_accuracy: 0.8823\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.4074 - accuracy: 0.8930 - val_loss: 0.5838 - val_accuracy: 0.8693\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.3834 - accuracy: 0.8981 - val_loss: 0.5391 - val_accuracy: 0.9050\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.3612 - accuracy: 0.9034 - val_loss: 0.6070 - val_accuracy: 0.8832\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.3497 - accuracy: 0.9048 - val_loss: 0.6250 - val_accuracy: 0.8766\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.3410 - accuracy: 0.9071 - val_loss: 0.4851 - val_accuracy: 0.9215\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.3228 - accuracy: 0.9139 - val_loss: 0.4688 - val_accuracy: 0.9091\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.3131 - accuracy: 0.9201 - val_loss: 0.5358 - val_accuracy: 0.8873\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.3013 - accuracy: 0.9206 - val_loss: 0.4570 - val_accuracy: 0.9155\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2974 - accuracy: 0.9210 - val_loss: 0.4889 - val_accuracy: 0.9003\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2975 - accuracy: 0.9194 - val_loss: 0.4596 - val_accuracy: 0.9113\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.2846 - accuracy: 0.9246 - val_loss: 0.4743 - val_accuracy: 0.8994\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2790 - accuracy: 0.9261 - val_loss: 0.4426 - val_accuracy: 0.9135\n",
      "Average Validation Accuracy: 0.9292741119861603\n",
      "Average Validation Loss: 0.32166361808776855\n",
      "Average Test Accuracy: 0.9282081425189972\n",
      "Final Test Accuracy for each fold: 0.9306405186653137\n",
      "Number of input features: 5\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 13s 6ms/step - loss: 3.5586 - accuracy: 0.3419 - val_loss: 2.4013 - val_accuracy: 0.5923\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 1.6424 - accuracy: 0.6907 - val_loss: 1.3892 - val_accuracy: 0.7558\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.9178 - accuracy: 0.8071 - val_loss: 0.9297 - val_accuracy: 0.8268\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.6197 - accuracy: 0.8570 - val_loss: 0.7330 - val_accuracy: 0.8442\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.4832 - accuracy: 0.8840 - val_loss: 0.5958 - val_accuracy: 0.8763\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.4132 - accuracy: 0.8946 - val_loss: 0.5459 - val_accuracy: 0.8944\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.3666 - accuracy: 0.9044 - val_loss: 0.5208 - val_accuracy: 0.8818\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.3314 - accuracy: 0.9120 - val_loss: 0.4596 - val_accuracy: 0.9127\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.3083 - accuracy: 0.9202 - val_loss: 0.4401 - val_accuracy: 0.9096\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2979 - accuracy: 0.9195 - val_loss: 0.4498 - val_accuracy: 0.9186\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2778 - accuracy: 0.9265 - val_loss: 0.4151 - val_accuracy: 0.9166\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.2671 - accuracy: 0.9294 - val_loss: 0.3864 - val_accuracy: 0.9219\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.2604 - accuracy: 0.9310 - val_loss: 0.3497 - val_accuracy: 0.9364\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.2429 - accuracy: 0.9400 - val_loss: 0.3698 - val_accuracy: 0.9243\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2460 - accuracy: 0.9370 - val_loss: 0.3645 - val_accuracy: 0.9325\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2352 - accuracy: 0.9388 - val_loss: 0.4257 - val_accuracy: 0.9149\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.2291 - accuracy: 0.9445 - val_loss: 0.3466 - val_accuracy: 0.9318\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2142 - accuracy: 0.9459 - val_loss: 0.3471 - val_accuracy: 0.9320\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.2175 - accuracy: 0.9460 - val_loss: 0.3546 - val_accuracy: 0.9388\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2159 - accuracy: 0.9468 - val_loss: 0.3308 - val_accuracy: 0.9553\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 13s 6ms/step - loss: 3.5565 - accuracy: 0.3360 - val_loss: 2.3940 - val_accuracy: 0.5809\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 1.6053 - accuracy: 0.7088 - val_loss: 1.4730 - val_accuracy: 0.7611\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.9685 - accuracy: 0.8009 - val_loss: 1.1042 - val_accuracy: 0.8253\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.6979 - accuracy: 0.8438 - val_loss: 0.8968 - val_accuracy: 0.8363\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.5534 - accuracy: 0.8701 - val_loss: 0.7587 - val_accuracy: 0.8706\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.4529 - accuracy: 0.8871 - val_loss: 0.6298 - val_accuracy: 0.9045\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.4007 - accuracy: 0.8979 - val_loss: 0.5923 - val_accuracy: 0.8931\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.3533 - accuracy: 0.9072 - val_loss: 0.5788 - val_accuracy: 0.8959\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.3294 - accuracy: 0.9138 - val_loss: 0.5058 - val_accuracy: 0.9078\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.3012 - accuracy: 0.9212 - val_loss: 0.5922 - val_accuracy: 0.8843\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.2952 - accuracy: 0.9215 - val_loss: 0.4598 - val_accuracy: 0.9243\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2774 - accuracy: 0.9259 - val_loss: 0.4890 - val_accuracy: 0.9217\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.2673 - accuracy: 0.9311 - val_loss: 0.4292 - val_accuracy: 0.9358\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2582 - accuracy: 0.9316 - val_loss: 0.4351 - val_accuracy: 0.9285\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2474 - accuracy: 0.9369 - val_loss: 0.4348 - val_accuracy: 0.9366\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2361 - accuracy: 0.9372 - val_loss: 0.4322 - val_accuracy: 0.9318\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.2391 - accuracy: 0.9397 - val_loss: 0.4426 - val_accuracy: 0.9369\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2229 - accuracy: 0.9442 - val_loss: 0.4121 - val_accuracy: 0.9311\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2308 - accuracy: 0.9410 - val_loss: 0.4254 - val_accuracy: 0.9366\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2136 - accuracy: 0.9453 - val_loss: 0.4006 - val_accuracy: 0.9417\n",
      "Average Validation Accuracy: 0.953345537185669\n",
      "Average Validation Loss: 0.2536706328392029\n",
      "Average Test Accuracy: 0.954153448343277\n",
      "Final Test Accuracy for each fold: 0.9576914310455322\n",
      "Number of input features: 6\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 13s 6ms/step - loss: 3.4348 - accuracy: 0.3696 - val_loss: 2.2498 - val_accuracy: 0.6009\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 1.4512 - accuracy: 0.7315 - val_loss: 1.2377 - val_accuracy: 0.7927\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.7886 - accuracy: 0.8336 - val_loss: 0.8856 - val_accuracy: 0.8370\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.5239 - accuracy: 0.8759 - val_loss: 0.6655 - val_accuracy: 0.8708\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.4041 - accuracy: 0.9016 - val_loss: 0.5907 - val_accuracy: 0.8953\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.3363 - accuracy: 0.9157 - val_loss: 0.5473 - val_accuracy: 0.8911\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.2958 - accuracy: 0.9280 - val_loss: 0.4613 - val_accuracy: 0.9373\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.2612 - accuracy: 0.9356 - val_loss: 0.4434 - val_accuracy: 0.9316\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.2417 - accuracy: 0.9405 - val_loss: 0.3996 - val_accuracy: 0.9309\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.2223 - accuracy: 0.9466 - val_loss: 0.4045 - val_accuracy: 0.9305\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.2056 - accuracy: 0.9488 - val_loss: 0.3614 - val_accuracy: 0.9406\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1990 - accuracy: 0.9498 - val_loss: 0.3696 - val_accuracy: 0.9432\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1837 - accuracy: 0.9556 - val_loss: 0.3363 - val_accuracy: 0.9518\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1869 - accuracy: 0.9545 - val_loss: 0.3557 - val_accuracy: 0.9371\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1777 - accuracy: 0.9557 - val_loss: 0.3351 - val_accuracy: 0.9470\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1655 - accuracy: 0.9587 - val_loss: 0.3373 - val_accuracy: 0.9474\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1619 - accuracy: 0.9614 - val_loss: 0.3124 - val_accuracy: 0.9487\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1613 - accuracy: 0.9620 - val_loss: 0.3027 - val_accuracy: 0.9525\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1523 - accuracy: 0.9646 - val_loss: 0.3045 - val_accuracy: 0.9591\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1580 - accuracy: 0.9624 - val_loss: 0.2883 - val_accuracy: 0.9534\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 13s 6ms/step - loss: 3.6677 - accuracy: 0.3083 - val_loss: 2.5121 - val_accuracy: 0.5868\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 1.6413 - accuracy: 0.6905 - val_loss: 1.5243 - val_accuracy: 0.7549\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.9565 - accuracy: 0.8000 - val_loss: 1.0911 - val_accuracy: 0.8196\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.6651 - accuracy: 0.8472 - val_loss: 0.8875 - val_accuracy: 0.8343\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.5196 - accuracy: 0.8732 - val_loss: 0.7033 - val_accuracy: 0.8766\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.4219 - accuracy: 0.8924 - val_loss: 0.6259 - val_accuracy: 0.8961\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.3602 - accuracy: 0.9122 - val_loss: 0.5952 - val_accuracy: 0.8937\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.3178 - accuracy: 0.9184 - val_loss: 0.5255 - val_accuracy: 0.9054\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.2957 - accuracy: 0.9241 - val_loss: 0.4736 - val_accuracy: 0.9102\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.2672 - accuracy: 0.9297 - val_loss: 0.4411 - val_accuracy: 0.9300\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.2496 - accuracy: 0.9367 - val_loss: 0.4468 - val_accuracy: 0.9155\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.2377 - accuracy: 0.9396 - val_loss: 0.4220 - val_accuracy: 0.9274\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.2318 - accuracy: 0.9403 - val_loss: 0.4351 - val_accuracy: 0.9256\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.2145 - accuracy: 0.9465 - val_loss: 0.3449 - val_accuracy: 0.9404\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2040 - accuracy: 0.9477 - val_loss: 0.3837 - val_accuracy: 0.9399\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.2003 - accuracy: 0.9481 - val_loss: 0.4373 - val_accuracy: 0.9102\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1972 - accuracy: 0.9492 - val_loss: 0.3495 - val_accuracy: 0.9413\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1933 - accuracy: 0.9516 - val_loss: 0.3481 - val_accuracy: 0.9435\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1896 - accuracy: 0.9517 - val_loss: 0.3329 - val_accuracy: 0.9529\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1786 - accuracy: 0.9546 - val_loss: 0.3689 - val_accuracy: 0.9144\n",
      "Average Validation Accuracy: 0.9411823153495789\n",
      "Average Validation Loss: 0.2277183011174202\n",
      "Average Test Accuracy: 0.9425812661647797\n",
      "Final Test Accuracy for each fold: 0.957322895526886\n",
      "Number of input features: 7\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 13s 6ms/step - loss: 3.6397 - accuracy: 0.3239 - val_loss: 2.5049 - val_accuracy: 0.5641\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 1.7051 - accuracy: 0.6813 - val_loss: 1.4598 - val_accuracy: 0.7303\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.9613 - accuracy: 0.8095 - val_loss: 1.0084 - val_accuracy: 0.8262\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.6231 - accuracy: 0.8660 - val_loss: 0.7773 - val_accuracy: 0.8821\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.4619 - accuracy: 0.8944 - val_loss: 0.6537 - val_accuracy: 0.8911\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.3782 - accuracy: 0.9120 - val_loss: 0.6560 - val_accuracy: 0.8783\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.3293 - accuracy: 0.9207 - val_loss: 0.5134 - val_accuracy: 0.9190\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2969 - accuracy: 0.9271 - val_loss: 0.4988 - val_accuracy: 0.9157\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2690 - accuracy: 0.9323 - val_loss: 0.4970 - val_accuracy: 0.9162\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2534 - accuracy: 0.9376 - val_loss: 0.4879 - val_accuracy: 0.9186\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2393 - accuracy: 0.9400 - val_loss: 0.4171 - val_accuracy: 0.9428\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2263 - accuracy: 0.9443 - val_loss: 0.4413 - val_accuracy: 0.9102\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2180 - accuracy: 0.9453 - val_loss: 0.3896 - val_accuracy: 0.9468\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2107 - accuracy: 0.9457 - val_loss: 0.3582 - val_accuracy: 0.9540\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1959 - accuracy: 0.9542 - val_loss: 0.4191 - val_accuracy: 0.9259\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2001 - accuracy: 0.9512 - val_loss: 0.3752 - val_accuracy: 0.9406\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1842 - accuracy: 0.9568 - val_loss: 0.3710 - val_accuracy: 0.9494\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1853 - accuracy: 0.9572 - val_loss: 0.3647 - val_accuracy: 0.9360\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1786 - accuracy: 0.9566 - val_loss: 0.3491 - val_accuracy: 0.9446\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1759 - accuracy: 0.9583 - val_loss: 0.3111 - val_accuracy: 0.9626\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 13s 6ms/step - loss: 3.4157 - accuracy: 0.3821 - val_loss: 2.2231 - val_accuracy: 0.6308\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 1.4468 - accuracy: 0.7327 - val_loss: 1.3292 - val_accuracy: 0.7991\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.8075 - accuracy: 0.8357 - val_loss: 0.9213 - val_accuracy: 0.8469\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.5383 - accuracy: 0.8792 - val_loss: 0.7136 - val_accuracy: 0.8805\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.4165 - accuracy: 0.9072 - val_loss: 0.6117 - val_accuracy: 0.8946\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.3411 - accuracy: 0.9195 - val_loss: 0.4775 - val_accuracy: 0.9173\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2991 - accuracy: 0.9286 - val_loss: 0.4506 - val_accuracy: 0.9129\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2659 - accuracy: 0.9359 - val_loss: 0.4251 - val_accuracy: 0.9228\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2402 - accuracy: 0.9412 - val_loss: 0.3778 - val_accuracy: 0.9300\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2242 - accuracy: 0.9451 - val_loss: 0.4143 - val_accuracy: 0.9155\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2161 - accuracy: 0.9454 - val_loss: 0.3275 - val_accuracy: 0.9408\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2024 - accuracy: 0.9513 - val_loss: 0.3250 - val_accuracy: 0.9470\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1847 - accuracy: 0.9548 - val_loss: 0.3177 - val_accuracy: 0.9432\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1847 - accuracy: 0.9546 - val_loss: 0.2887 - val_accuracy: 0.9584\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1819 - accuracy: 0.9572 - val_loss: 0.2930 - val_accuracy: 0.9461\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1722 - accuracy: 0.9591 - val_loss: 0.3116 - val_accuracy: 0.9514\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1677 - accuracy: 0.9614 - val_loss: 0.2873 - val_accuracy: 0.9593\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1632 - accuracy: 0.9606 - val_loss: 0.2598 - val_accuracy: 0.9615\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1509 - accuracy: 0.9652 - val_loss: 0.2819 - val_accuracy: 0.9485\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1565 - accuracy: 0.9621 - val_loss: 0.2607 - val_accuracy: 0.9657\n",
      "Average Validation Accuracy: 0.9669607877731323\n",
      "Average Validation Loss: 0.1912882924079895\n",
      "Average Test Accuracy: 0.9666101634502411\n",
      "Final Test Accuracy for each fold: 0.9680843353271484\n",
      "Number of input features: 8\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 13s 6ms/step - loss: 3.4224 - accuracy: 0.3863 - val_loss: 2.1788 - val_accuracy: 0.6328\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 1.4387 - accuracy: 0.7297 - val_loss: 1.3064 - val_accuracy: 0.7635\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.8392 - accuracy: 0.8287 - val_loss: 0.9378 - val_accuracy: 0.8449\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.5829 - accuracy: 0.8715 - val_loss: 0.7388 - val_accuracy: 0.8693\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.4426 - accuracy: 0.8975 - val_loss: 0.6484 - val_accuracy: 0.8832\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.3593 - accuracy: 0.9124 - val_loss: 0.5444 - val_accuracy: 0.9171\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.3040 - accuracy: 0.9255 - val_loss: 0.5399 - val_accuracy: 0.8840\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2640 - accuracy: 0.9349 - val_loss: 0.4448 - val_accuracy: 0.9259\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2417 - accuracy: 0.9428 - val_loss: 0.4254 - val_accuracy: 0.9340\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2188 - accuracy: 0.9462 - val_loss: 0.4627 - val_accuracy: 0.9140\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2173 - accuracy: 0.9488 - val_loss: 0.3979 - val_accuracy: 0.9404\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1910 - accuracy: 0.9563 - val_loss: 0.4304 - val_accuracy: 0.9171\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1897 - accuracy: 0.9532 - val_loss: 0.3919 - val_accuracy: 0.9397\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1798 - accuracy: 0.9565 - val_loss: 0.3607 - val_accuracy: 0.9584\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1686 - accuracy: 0.9589 - val_loss: 0.3658 - val_accuracy: 0.9443\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1681 - accuracy: 0.9613 - val_loss: 0.3591 - val_accuracy: 0.9472\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1639 - accuracy: 0.9611 - val_loss: 0.3506 - val_accuracy: 0.9487\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1605 - accuracy: 0.9610 - val_loss: 0.3391 - val_accuracy: 0.9630\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1495 - accuracy: 0.9666 - val_loss: 0.4059 - val_accuracy: 0.9474\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1551 - accuracy: 0.9624 - val_loss: 0.3240 - val_accuracy: 0.9617\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 13s 6ms/step - loss: 3.5101 - accuracy: 0.3638 - val_loss: 2.2965 - val_accuracy: 0.6262\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 1.4672 - accuracy: 0.7219 - val_loss: 1.2663 - val_accuracy: 0.7993\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.7806 - accuracy: 0.8397 - val_loss: 0.9123 - val_accuracy: 0.8640\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.5361 - accuracy: 0.8828 - val_loss: 0.7193 - val_accuracy: 0.8770\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.4054 - accuracy: 0.9068 - val_loss: 0.6368 - val_accuracy: 0.8803\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.3462 - accuracy: 0.9163 - val_loss: 0.5300 - val_accuracy: 0.8977\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2949 - accuracy: 0.9283 - val_loss: 0.4949 - val_accuracy: 0.9118\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2716 - accuracy: 0.9329 - val_loss: 0.4643 - val_accuracy: 0.9283\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2447 - accuracy: 0.9390 - val_loss: 0.4130 - val_accuracy: 0.9448\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2281 - accuracy: 0.9431 - val_loss: 0.4390 - val_accuracy: 0.9120\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2195 - accuracy: 0.9460 - val_loss: 0.4150 - val_accuracy: 0.9329\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2043 - accuracy: 0.9493 - val_loss: 0.4093 - val_accuracy: 0.9303\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2007 - accuracy: 0.9501 - val_loss: 0.4080 - val_accuracy: 0.9322\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1903 - accuracy: 0.9527 - val_loss: 0.3975 - val_accuracy: 0.9358\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1831 - accuracy: 0.9555 - val_loss: 0.3408 - val_accuracy: 0.9571\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1756 - accuracy: 0.9575 - val_loss: 0.3440 - val_accuracy: 0.9461\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1754 - accuracy: 0.9581 - val_loss: 0.3799 - val_accuracy: 0.9314\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1684 - accuracy: 0.9595 - val_loss: 0.3249 - val_accuracy: 0.9529\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1594 - accuracy: 0.9633 - val_loss: 0.3516 - val_accuracy: 0.9435\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1651 - accuracy: 0.9602 - val_loss: 0.3376 - val_accuracy: 0.9481\n",
      "Average Validation Accuracy: 0.9641286730766296\n",
      "Average Validation Loss: 0.2003694474697113\n",
      "Average Test Accuracy: 0.9646568596363068\n",
      "Final Test Accuracy for each fold: 0.9685265421867371\n",
      "Number of input features: 9\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 14s 6ms/step - loss: 3.5840 - accuracy: 0.3480 - val_loss: 2.3111 - val_accuracy: 0.5934\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 1.5158 - accuracy: 0.7201 - val_loss: 1.3386 - val_accuracy: 0.7740\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.8557 - accuracy: 0.8270 - val_loss: 0.9367 - val_accuracy: 0.8530\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.5739 - accuracy: 0.8757 - val_loss: 0.7699 - val_accuracy: 0.8752\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.4356 - accuracy: 0.9026 - val_loss: 0.7404 - val_accuracy: 0.8730\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 12s 7ms/step - loss: 0.3587 - accuracy: 0.9163 - val_loss: 0.5878 - val_accuracy: 0.8997\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.3001 - accuracy: 0.9276 - val_loss: 0.5578 - val_accuracy: 0.9206\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2684 - accuracy: 0.9364 - val_loss: 0.4944 - val_accuracy: 0.9351\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.2468 - accuracy: 0.9416 - val_loss: 0.4553 - val_accuracy: 0.9399\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2261 - accuracy: 0.9494 - val_loss: 0.4200 - val_accuracy: 0.9472\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.2089 - accuracy: 0.9507 - val_loss: 0.4170 - val_accuracy: 0.9448\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1933 - accuracy: 0.9561 - val_loss: 0.3977 - val_accuracy: 0.9527\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1900 - accuracy: 0.9576 - val_loss: 0.4483 - val_accuracy: 0.9344\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1848 - accuracy: 0.9562 - val_loss: 0.3801 - val_accuracy: 0.9391\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1686 - accuracy: 0.9620 - val_loss: 0.3800 - val_accuracy: 0.9432\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1635 - accuracy: 0.9624 - val_loss: 0.3623 - val_accuracy: 0.9540\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1657 - accuracy: 0.9626 - val_loss: 0.3611 - val_accuracy: 0.9492\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1479 - accuracy: 0.9677 - val_loss: 0.3293 - val_accuracy: 0.9582\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1506 - accuracy: 0.9663 - val_loss: 0.3146 - val_accuracy: 0.9668\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1469 - accuracy: 0.9659 - val_loss: 0.3181 - val_accuracy: 0.9622\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 12s 5ms/step - loss: 3.6420 - accuracy: 0.3346 - val_loss: 2.4889 - val_accuracy: 0.5580\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.6369 - accuracy: 0.7055 - val_loss: 1.5105 - val_accuracy: 0.7399\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.9381 - accuracy: 0.8107 - val_loss: 1.0816 - val_accuracy: 0.8207\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.6217 - accuracy: 0.8621 - val_loss: 0.8769 - val_accuracy: 0.8651\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.4584 - accuracy: 0.8925 - val_loss: 0.7222 - val_accuracy: 0.9034\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.3695 - accuracy: 0.9098 - val_loss: 0.5892 - val_accuracy: 0.8953\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.3117 - accuracy: 0.9248 - val_loss: 0.5541 - val_accuracy: 0.9102\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2679 - accuracy: 0.9357 - val_loss: 0.5080 - val_accuracy: 0.9307\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2466 - accuracy: 0.9385 - val_loss: 0.4338 - val_accuracy: 0.9509\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.2232 - accuracy: 0.9451 - val_loss: 0.4552 - val_accuracy: 0.9494\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2094 - accuracy: 0.9496 - val_loss: 0.4237 - val_accuracy: 0.9388\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1977 - accuracy: 0.9527 - val_loss: 0.5150 - val_accuracy: 0.9252\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1844 - accuracy: 0.9548 - val_loss: 0.4051 - val_accuracy: 0.9388\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1763 - accuracy: 0.9593 - val_loss: 0.3803 - val_accuracy: 0.9512\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1723 - accuracy: 0.9586 - val_loss: 0.3741 - val_accuracy: 0.9485\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1642 - accuracy: 0.9599 - val_loss: 0.3364 - val_accuracy: 0.9564\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1611 - accuracy: 0.9609 - val_loss: 0.3423 - val_accuracy: 0.9512\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1576 - accuracy: 0.9602 - val_loss: 0.3251 - val_accuracy: 0.9600\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1458 - accuracy: 0.9653 - val_loss: 0.3296 - val_accuracy: 0.9512\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1508 - accuracy: 0.9622 - val_loss: 0.2965 - val_accuracy: 0.9622\n",
      "Average Validation Accuracy: 0.9700469076633453\n",
      "Average Validation Loss: 0.18217886984348297\n",
      "Average Test Accuracy: 0.9706272482872009\n",
      "Final Test Accuracy for each fold: 0.9725068211555481\n",
      "Number of input features: 10\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 12s 5ms/step - loss: 3.5824 - accuracy: 0.3438 - val_loss: 2.3264 - val_accuracy: 0.6066\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 1.5109 - accuracy: 0.7201 - val_loss: 1.2855 - val_accuracy: 0.7846\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.7888 - accuracy: 0.8422 - val_loss: 0.8413 - val_accuracy: 0.8574\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.4998 - accuracy: 0.8909 - val_loss: 0.6673 - val_accuracy: 0.8862\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.3741 - accuracy: 0.9120 - val_loss: 0.5146 - val_accuracy: 0.9331\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.2942 - accuracy: 0.9308 - val_loss: 0.4824 - val_accuracy: 0.9129\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.2631 - accuracy: 0.9355 - val_loss: 0.4409 - val_accuracy: 0.9340\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.2324 - accuracy: 0.9452 - val_loss: 0.3974 - val_accuracy: 0.9450\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2150 - accuracy: 0.9488 - val_loss: 0.4689 - val_accuracy: 0.9182\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1968 - accuracy: 0.9539 - val_loss: 0.3979 - val_accuracy: 0.9404\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1898 - accuracy: 0.9563 - val_loss: 0.3736 - val_accuracy: 0.9527\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1832 - accuracy: 0.9566 - val_loss: 0.3685 - val_accuracy: 0.9461\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1723 - accuracy: 0.9599 - val_loss: 0.3822 - val_accuracy: 0.9397\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1689 - accuracy: 0.9611 - val_loss: 0.3518 - val_accuracy: 0.9562\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1626 - accuracy: 0.9620 - val_loss: 0.3446 - val_accuracy: 0.9560\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1538 - accuracy: 0.9661 - val_loss: 0.3338 - val_accuracy: 0.9558\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1504 - accuracy: 0.9670 - val_loss: 0.4044 - val_accuracy: 0.9318\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1490 - accuracy: 0.9657 - val_loss: 0.3283 - val_accuracy: 0.9593\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1488 - accuracy: 0.9656 - val_loss: 0.3153 - val_accuracy: 0.9589\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1361 - accuracy: 0.9704 - val_loss: 0.2946 - val_accuracy: 0.9663\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 12s 5ms/step - loss: 3.4389 - accuracy: 0.3718 - val_loss: 2.2540 - val_accuracy: 0.6103\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 1.3814 - accuracy: 0.7420 - val_loss: 1.3338 - val_accuracy: 0.8037\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.7449 - accuracy: 0.8486 - val_loss: 0.9291 - val_accuracy: 0.8647\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.4952 - accuracy: 0.8940 - val_loss: 0.7036 - val_accuracy: 0.8924\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.3819 - accuracy: 0.9139 - val_loss: 0.6001 - val_accuracy: 0.9080\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.3204 - accuracy: 0.9244 - val_loss: 0.5427 - val_accuracy: 0.9091\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.2703 - accuracy: 0.9353 - val_loss: 0.5113 - val_accuracy: 0.9226\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.2431 - accuracy: 0.9430 - val_loss: 0.5200 - val_accuracy: 0.9074\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.2250 - accuracy: 0.9449 - val_loss: 0.4094 - val_accuracy: 0.9386\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.2110 - accuracy: 0.9475 - val_loss: 0.3805 - val_accuracy: 0.9435\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.2004 - accuracy: 0.9520 - val_loss: 0.3896 - val_accuracy: 0.9380\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1912 - accuracy: 0.9548 - val_loss: 0.3570 - val_accuracy: 0.9443\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1747 - accuracy: 0.9585 - val_loss: 0.3667 - val_accuracy: 0.9430\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1667 - accuracy: 0.9591 - val_loss: 0.3323 - val_accuracy: 0.9564\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1701 - accuracy: 0.9577 - val_loss: 0.3248 - val_accuracy: 0.9509\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1552 - accuracy: 0.9651 - val_loss: 0.3465 - val_accuracy: 0.9448\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1577 - accuracy: 0.9627 - val_loss: 0.3046 - val_accuracy: 0.9571\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1486 - accuracy: 0.9656 - val_loss: 0.3418 - val_accuracy: 0.9421\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1465 - accuracy: 0.9667 - val_loss: 0.3082 - val_accuracy: 0.9560\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1511 - accuracy: 0.9642 - val_loss: 0.2667 - val_accuracy: 0.9606\n",
      "Average Validation Accuracy: 0.972261518239975\n",
      "Average Validation Loss: 0.1730860024690628\n",
      "Average Test Accuracy: 0.9726542532444\n",
      "Final Test Accuracy for each fold: 0.9744969606399536\n",
      "Number of input features: 11\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 14s 6ms/step - loss: 3.6577 - accuracy: 0.3361 - val_loss: 2.4914 - val_accuracy: 0.5828\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 1.5528 - accuracy: 0.7123 - val_loss: 1.2874 - val_accuracy: 0.7679\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.8023 - accuracy: 0.8390 - val_loss: 0.8547 - val_accuracy: 0.8477\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.5107 - accuracy: 0.8866 - val_loss: 0.6898 - val_accuracy: 0.8851\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.3780 - accuracy: 0.9121 - val_loss: 0.5671 - val_accuracy: 0.9124\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.3125 - accuracy: 0.9274 - val_loss: 0.5367 - val_accuracy: 0.9100\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2675 - accuracy: 0.9357 - val_loss: 0.4582 - val_accuracy: 0.9248\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2423 - accuracy: 0.9406 - val_loss: 0.3976 - val_accuracy: 0.9393\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.2249 - accuracy: 0.9446 - val_loss: 0.4306 - val_accuracy: 0.9278\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.2084 - accuracy: 0.9487 - val_loss: 0.3765 - val_accuracy: 0.9410\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1923 - accuracy: 0.9542 - val_loss: 0.3508 - val_accuracy: 0.9430\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1860 - accuracy: 0.9559 - val_loss: 0.3850 - val_accuracy: 0.9322\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1848 - accuracy: 0.9555 - val_loss: 0.3192 - val_accuracy: 0.9443\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1687 - accuracy: 0.9598 - val_loss: 0.3359 - val_accuracy: 0.9439\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1633 - accuracy: 0.9609 - val_loss: 0.3020 - val_accuracy: 0.9573\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1639 - accuracy: 0.9624 - val_loss: 0.3143 - val_accuracy: 0.9538\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1536 - accuracy: 0.9651 - val_loss: 0.3120 - val_accuracy: 0.9549\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1515 - accuracy: 0.9641 - val_loss: 0.3119 - val_accuracy: 0.9549\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1444 - accuracy: 0.9669 - val_loss: 0.2962 - val_accuracy: 0.9569\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1439 - accuracy: 0.9654 - val_loss: 0.3060 - val_accuracy: 0.9553\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 18s 9ms/step - loss: 3.2082 - accuracy: 0.4184 - val_loss: 1.9736 - val_accuracy: 0.6746\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 1.2488 - accuracy: 0.7672 - val_loss: 1.1700 - val_accuracy: 0.8268\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.7134 - accuracy: 0.8560 - val_loss: 0.8678 - val_accuracy: 0.8629\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 12s 6ms/step - loss: 0.4861 - accuracy: 0.8908 - val_loss: 0.6946 - val_accuracy: 0.8917\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.3722 - accuracy: 0.9111 - val_loss: 0.5554 - val_accuracy: 0.9096\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.3054 - accuracy: 0.9239 - val_loss: 0.4760 - val_accuracy: 0.9265\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2621 - accuracy: 0.9344 - val_loss: 0.4695 - val_accuracy: 0.9325\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.2430 - accuracy: 0.9417 - val_loss: 0.4485 - val_accuracy: 0.9144\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2323 - accuracy: 0.9405 - val_loss: 0.4127 - val_accuracy: 0.9267\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2093 - accuracy: 0.9473 - val_loss: 0.4337 - val_accuracy: 0.9212\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.2000 - accuracy: 0.9507 - val_loss: 0.3747 - val_accuracy: 0.9450\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1902 - accuracy: 0.9511 - val_loss: 0.3613 - val_accuracy: 0.9393\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 12s 6ms/step - loss: 0.1831 - accuracy: 0.9542 - val_loss: 0.3183 - val_accuracy: 0.9564\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1735 - accuracy: 0.9577 - val_loss: 0.3902 - val_accuracy: 0.9430\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1739 - accuracy: 0.9572 - val_loss: 0.3250 - val_accuracy: 0.9487\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1669 - accuracy: 0.9591 - val_loss: 0.3170 - val_accuracy: 0.9437\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1614 - accuracy: 0.9600 - val_loss: 0.3006 - val_accuracy: 0.9648\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1558 - accuracy: 0.9632 - val_loss: 0.3191 - val_accuracy: 0.9520\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1577 - accuracy: 0.9624 - val_loss: 0.2821 - val_accuracy: 0.9584\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1455 - accuracy: 0.9653 - val_loss: 0.2872 - val_accuracy: 0.9637\n",
      "Average Validation Accuracy: 0.9671424031257629\n",
      "Average Validation Loss: 0.1870393231511116\n",
      "Average Test Accuracy: 0.9679737687110901\n",
      "Final Test Accuracy for each fold: 0.9688951373100281\n",
      "Number of input features: 12\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 12s 5ms/step - loss: 3.5022 - accuracy: 0.3634 - val_loss: 2.2574 - val_accuracy: 0.6418\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 1.4534 - accuracy: 0.7392 - val_loss: 1.2219 - val_accuracy: 0.8046\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.7914 - accuracy: 0.8429 - val_loss: 0.8579 - val_accuracy: 0.8581\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.5223 - accuracy: 0.8869 - val_loss: 0.6889 - val_accuracy: 0.8845\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.3967 - accuracy: 0.9111 - val_loss: 0.5919 - val_accuracy: 0.9149\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.3273 - accuracy: 0.9221 - val_loss: 0.5621 - val_accuracy: 0.9039\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.2881 - accuracy: 0.9290 - val_loss: 0.4717 - val_accuracy: 0.9250\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.2577 - accuracy: 0.9347 - val_loss: 0.4998 - val_accuracy: 0.9039\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.2373 - accuracy: 0.9426 - val_loss: 0.4718 - val_accuracy: 0.8986\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.2172 - accuracy: 0.9465 - val_loss: 0.4229 - val_accuracy: 0.9351\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.2079 - accuracy: 0.9485 - val_loss: 0.3777 - val_accuracy: 0.9413\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1952 - accuracy: 0.9529 - val_loss: 0.3522 - val_accuracy: 0.9435\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1900 - accuracy: 0.9534 - val_loss: 0.3309 - val_accuracy: 0.9531\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1742 - accuracy: 0.9568 - val_loss: 0.3762 - val_accuracy: 0.9369\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1697 - accuracy: 0.9582 - val_loss: 0.3519 - val_accuracy: 0.9472\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1655 - accuracy: 0.9608 - val_loss: 0.3127 - val_accuracy: 0.9641\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1550 - accuracy: 0.9642 - val_loss: 0.3306 - val_accuracy: 0.9509\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1628 - accuracy: 0.9604 - val_loss: 0.3314 - val_accuracy: 0.9529\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1495 - accuracy: 0.9643 - val_loss: 0.3181 - val_accuracy: 0.9545\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1478 - accuracy: 0.9652 - val_loss: 0.2969 - val_accuracy: 0.9547\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 12s 5ms/step - loss: 3.4748 - accuracy: 0.3600 - val_loss: 2.2080 - val_accuracy: 0.5864\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 1.3622 - accuracy: 0.7479 - val_loss: 1.2433 - val_accuracy: 0.8000\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.7399 - accuracy: 0.8446 - val_loss: 0.8892 - val_accuracy: 0.8532\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.5026 - accuracy: 0.8895 - val_loss: 0.7167 - val_accuracy: 0.8849\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.3936 - accuracy: 0.9091 - val_loss: 0.6234 - val_accuracy: 0.8972\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.3240 - accuracy: 0.9245 - val_loss: 0.5447 - val_accuracy: 0.9067\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.2823 - accuracy: 0.9317 - val_loss: 0.4643 - val_accuracy: 0.9276\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.2534 - accuracy: 0.9390 - val_loss: 0.4391 - val_accuracy: 0.9245\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.2283 - accuracy: 0.9444 - val_loss: 0.4131 - val_accuracy: 0.9329\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.2196 - accuracy: 0.9465 - val_loss: 0.4060 - val_accuracy: 0.9369\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.2050 - accuracy: 0.9505 - val_loss: 0.4004 - val_accuracy: 0.9415\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1984 - accuracy: 0.9523 - val_loss: 0.3805 - val_accuracy: 0.9419\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1849 - accuracy: 0.9562 - val_loss: 0.3480 - val_accuracy: 0.9415\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1741 - accuracy: 0.9583 - val_loss: 0.3594 - val_accuracy: 0.9474\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1744 - accuracy: 0.9574 - val_loss: 0.3326 - val_accuracy: 0.9432\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1648 - accuracy: 0.9604 - val_loss: 0.3234 - val_accuracy: 0.9571\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1584 - accuracy: 0.9628 - val_loss: 0.2951 - val_accuracy: 0.9507\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1587 - accuracy: 0.9614 - val_loss: 0.3134 - val_accuracy: 0.9496\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1551 - accuracy: 0.9656 - val_loss: 0.4671 - val_accuracy: 0.9179\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1500 - accuracy: 0.9660 - val_loss: 0.3088 - val_accuracy: 0.9498\n",
      "Average Validation Accuracy: 0.9614420533180237\n",
      "Average Validation Loss: 0.19085387140512466\n",
      "Average Test Accuracy: 0.9624087810516357\n",
      "Final Test Accuracy for each fold: 0.9641040563583374\n",
      "Number of input features: 13\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 12s 5ms/step - loss: 3.2853 - accuracy: 0.3935 - val_loss: 1.9979 - val_accuracy: 0.6682\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 1.2785 - accuracy: 0.7644 - val_loss: 1.0966 - val_accuracy: 0.8306\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.6979 - accuracy: 0.8556 - val_loss: 0.7619 - val_accuracy: 0.8436\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.4701 - accuracy: 0.8943 - val_loss: 0.6191 - val_accuracy: 0.9025\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.3554 - accuracy: 0.9188 - val_loss: 0.5440 - val_accuracy: 0.9017\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.3013 - accuracy: 0.9288 - val_loss: 0.5076 - val_accuracy: 0.9032\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2702 - accuracy: 0.9325 - val_loss: 0.4814 - val_accuracy: 0.9087\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.2371 - accuracy: 0.9420 - val_loss: 0.5160 - val_accuracy: 0.9094\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.2224 - accuracy: 0.9467 - val_loss: 0.3963 - val_accuracy: 0.9287\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2010 - accuracy: 0.9513 - val_loss: 0.3892 - val_accuracy: 0.9254\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1945 - accuracy: 0.9529 - val_loss: 0.3421 - val_accuracy: 0.9501\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1872 - accuracy: 0.9531 - val_loss: 0.3231 - val_accuracy: 0.9487\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1798 - accuracy: 0.9558 - val_loss: 0.3336 - val_accuracy: 0.9501\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 12s 6ms/step - loss: 0.1668 - accuracy: 0.9597 - val_loss: 0.3128 - val_accuracy: 0.9582\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 12s 6ms/step - loss: 0.1658 - accuracy: 0.9590 - val_loss: 0.3203 - val_accuracy: 0.9514\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1564 - accuracy: 0.9639 - val_loss: 0.2841 - val_accuracy: 0.9655\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 11s 6ms/step - loss: 0.1550 - accuracy: 0.9640 - val_loss: 0.3012 - val_accuracy: 0.9465\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.1496 - accuracy: 0.9643 - val_loss: 0.2925 - val_accuracy: 0.9534\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.1469 - accuracy: 0.9650 - val_loss: 0.2891 - val_accuracy: 0.9569\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1395 - accuracy: 0.9675 - val_loss: 0.3062 - val_accuracy: 0.9514\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 12s 5ms/step - loss: 3.3341 - accuracy: 0.4049 - val_loss: 2.0800 - val_accuracy: 0.6825\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 1.3239 - accuracy: 0.7559 - val_loss: 1.2476 - val_accuracy: 0.7912\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 10s 6ms/step - loss: 0.7381 - accuracy: 0.8490 - val_loss: 0.8769 - val_accuracy: 0.8539\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.4836 - accuracy: 0.8942 - val_loss: 0.7218 - val_accuracy: 0.8928\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.3652 - accuracy: 0.9133 - val_loss: 0.5557 - val_accuracy: 0.9226\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.2995 - accuracy: 0.9289 - val_loss: 0.5653 - val_accuracy: 0.9012\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.2647 - accuracy: 0.9339 - val_loss: 0.4274 - val_accuracy: 0.9355\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 10s 5ms/step - loss: 0.2369 - accuracy: 0.9408 - val_loss: 0.4263 - val_accuracy: 0.9298\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.2169 - accuracy: 0.9456 - val_loss: 0.3895 - val_accuracy: 0.9380\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1995 - accuracy: 0.9514 - val_loss: 0.4126 - val_accuracy: 0.9316\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1919 - accuracy: 0.9520 - val_loss: 0.3652 - val_accuracy: 0.9413\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1791 - accuracy: 0.9559 - val_loss: 0.3798 - val_accuracy: 0.9353\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1745 - accuracy: 0.9553 - val_loss: 0.3525 - val_accuracy: 0.9457\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1685 - accuracy: 0.9596 - val_loss: 0.3437 - val_accuracy: 0.9476\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1665 - accuracy: 0.9576 - val_loss: 0.3164 - val_accuracy: 0.9602\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1573 - accuracy: 0.9615 - val_loss: 0.3236 - val_accuracy: 0.9529\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1533 - accuracy: 0.9630 - val_loss: 0.3009 - val_accuracy: 0.9738\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1539 - accuracy: 0.9629 - val_loss: 0.3045 - val_accuracy: 0.9644\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1430 - accuracy: 0.9664 - val_loss: 0.2966 - val_accuracy: 0.9549\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1446 - accuracy: 0.9645 - val_loss: 0.2859 - val_accuracy: 0.9668\n",
      "Average Validation Accuracy: 0.9665979444980621\n",
      "Average Validation Loss: 0.18591061979532242\n",
      "Average Test Accuracy: 0.9690425395965576\n",
      "Final Test Accuracy for each fold: 0.9747917652130127\n",
      "Number of input features: 14\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 12s 5ms/step - loss: 3.2489 - accuracy: 0.4126 - val_loss: 1.9890 - val_accuracy: 0.6339\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 1.2573 - accuracy: 0.7656 - val_loss: 1.1119 - val_accuracy: 0.7991\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.7223 - accuracy: 0.8437 - val_loss: 0.8219 - val_accuracy: 0.8640\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.5176 - accuracy: 0.8800 - val_loss: 0.6970 - val_accuracy: 0.8715\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.4162 - accuracy: 0.9001 - val_loss: 0.5559 - val_accuracy: 0.8961\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.3527 - accuracy: 0.9110 - val_loss: 0.5212 - val_accuracy: 0.8904\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.3014 - accuracy: 0.9263 - val_loss: 0.6231 - val_accuracy: 0.8770\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.2733 - accuracy: 0.9308 - val_loss: 0.4248 - val_accuracy: 0.9122\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.2558 - accuracy: 0.9370 - val_loss: 0.3995 - val_accuracy: 0.9182\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.2373 - accuracy: 0.9409 - val_loss: 0.3478 - val_accuracy: 0.9415\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.2117 - accuracy: 0.9471 - val_loss: 0.4684 - val_accuracy: 0.9149\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.2064 - accuracy: 0.9484 - val_loss: 0.3898 - val_accuracy: 0.9230\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1920 - accuracy: 0.9522 - val_loss: 0.3943 - val_accuracy: 0.9303\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1865 - accuracy: 0.9561 - val_loss: 0.3091 - val_accuracy: 0.9538\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1780 - accuracy: 0.9566 - val_loss: 0.3183 - val_accuracy: 0.9404\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1705 - accuracy: 0.9595 - val_loss: 0.3087 - val_accuracy: 0.9492\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1619 - accuracy: 0.9595 - val_loss: 0.2728 - val_accuracy: 0.9608\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1599 - accuracy: 0.9617 - val_loss: 0.3036 - val_accuracy: 0.9443\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1626 - accuracy: 0.9609 - val_loss: 0.2685 - val_accuracy: 0.9644\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1522 - accuracy: 0.9641 - val_loss: 0.2755 - val_accuracy: 0.9586\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 12s 5ms/step - loss: 3.4561 - accuracy: 0.3599 - val_loss: 2.3162 - val_accuracy: 0.6117\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 1.4144 - accuracy: 0.7413 - val_loss: 1.3202 - val_accuracy: 0.8088\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.7451 - accuracy: 0.8471 - val_loss: 0.9239 - val_accuracy: 0.8702\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4834 - accuracy: 0.8988 - val_loss: 0.7511 - val_accuracy: 0.8807\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.3652 - accuracy: 0.9188 - val_loss: 0.6125 - val_accuracy: 0.9094\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.2985 - accuracy: 0.9315 - val_loss: 0.5698 - val_accuracy: 0.9197\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 8s 5ms/step - loss: 0.2559 - accuracy: 0.9419 - val_loss: 0.4955 - val_accuracy: 0.9241\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.2353 - accuracy: 0.9446 - val_loss: 0.4329 - val_accuracy: 0.9472\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2151 - accuracy: 0.9511 - val_loss: 0.4121 - val_accuracy: 0.9395\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2013 - accuracy: 0.9527 - val_loss: 0.4022 - val_accuracy: 0.9441\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1902 - accuracy: 0.9552 - val_loss: 0.3695 - val_accuracy: 0.9415\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1740 - accuracy: 0.9597 - val_loss: 0.3402 - val_accuracy: 0.9529\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1700 - accuracy: 0.9613 - val_loss: 0.3122 - val_accuracy: 0.9613\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1635 - accuracy: 0.9623 - val_loss: 0.3364 - val_accuracy: 0.9498\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1679 - accuracy: 0.9604 - val_loss: 0.3676 - val_accuracy: 0.9404\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1562 - accuracy: 0.9626 - val_loss: 0.3094 - val_accuracy: 0.9538\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1519 - accuracy: 0.9635 - val_loss: 0.3242 - val_accuracy: 0.9505\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1501 - accuracy: 0.9651 - val_loss: 0.3015 - val_accuracy: 0.9564\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 0.1471 - accuracy: 0.9638 - val_loss: 0.3136 - val_accuracy: 0.9536\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1433 - accuracy: 0.9665 - val_loss: 0.3572 - val_accuracy: 0.9318\n",
      "Average Validation Accuracy: 0.9539624452590942\n",
      "Average Validation Loss: 0.2100473716855049\n",
      "Average Test Accuracy: 0.9542640149593353\n",
      "Final Test Accuracy for each fold: 0.9671998023986816\n",
      "Number of input features: 15\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 3.4576 - accuracy: 0.3656 - val_loss: 2.2455 - val_accuracy: 0.6152\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 1.4510 - accuracy: 0.7392 - val_loss: 1.2418 - val_accuracy: 0.7934\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.7936 - accuracy: 0.8385 - val_loss: 0.8535 - val_accuracy: 0.8594\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.5242 - accuracy: 0.8829 - val_loss: 0.7195 - val_accuracy: 0.8645\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.3958 - accuracy: 0.9082 - val_loss: 0.5959 - val_accuracy: 0.9087\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.3206 - accuracy: 0.9260 - val_loss: 0.5403 - val_accuracy: 0.9118\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2794 - accuracy: 0.9344 - val_loss: 0.5039 - val_accuracy: 0.9144\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2501 - accuracy: 0.9415 - val_loss: 0.4771 - val_accuracy: 0.9285\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2271 - accuracy: 0.9484 - val_loss: 0.4506 - val_accuracy: 0.9410\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2116 - accuracy: 0.9485 - val_loss: 0.4377 - val_accuracy: 0.9340\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1998 - accuracy: 0.9527 - val_loss: 0.4363 - val_accuracy: 0.9450\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1913 - accuracy: 0.9534 - val_loss: 0.4151 - val_accuracy: 0.9256\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1883 - accuracy: 0.9546 - val_loss: 0.4018 - val_accuracy: 0.9428\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1807 - accuracy: 0.9590 - val_loss: 0.3602 - val_accuracy: 0.9542\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1714 - accuracy: 0.9590 - val_loss: 0.3430 - val_accuracy: 0.9650\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1620 - accuracy: 0.9613 - val_loss: 0.3458 - val_accuracy: 0.9567\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1626 - accuracy: 0.9622 - val_loss: 0.3458 - val_accuracy: 0.9498\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1584 - accuracy: 0.9643 - val_loss: 0.3292 - val_accuracy: 0.9692\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1458 - accuracy: 0.9678 - val_loss: 0.3285 - val_accuracy: 0.9573\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1469 - accuracy: 0.9676 - val_loss: 0.3485 - val_accuracy: 0.9503\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 3.1660 - accuracy: 0.4330 - val_loss: 1.8753 - val_accuracy: 0.6735\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 1.1833 - accuracy: 0.7607 - val_loss: 1.1094 - val_accuracy: 0.7949\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.6686 - accuracy: 0.8539 - val_loss: 0.7777 - val_accuracy: 0.8662\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.4676 - accuracy: 0.8896 - val_loss: 0.6420 - val_accuracy: 0.8992\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.3700 - accuracy: 0.9100 - val_loss: 0.5302 - val_accuracy: 0.9102\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.3139 - accuracy: 0.9236 - val_loss: 0.4985 - val_accuracy: 0.9116\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2780 - accuracy: 0.9305 - val_loss: 0.4334 - val_accuracy: 0.9197\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2468 - accuracy: 0.9392 - val_loss: 0.4094 - val_accuracy: 0.9241\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2285 - accuracy: 0.9446 - val_loss: 0.4063 - val_accuracy: 0.9226\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2223 - accuracy: 0.9464 - val_loss: 0.3552 - val_accuracy: 0.9270\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2019 - accuracy: 0.9491 - val_loss: 0.4187 - val_accuracy: 0.9226\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1987 - accuracy: 0.9512 - val_loss: 0.3172 - val_accuracy: 0.9391\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1852 - accuracy: 0.9555 - val_loss: 0.3054 - val_accuracy: 0.9457\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1824 - accuracy: 0.9561 - val_loss: 0.3294 - val_accuracy: 0.9331\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1706 - accuracy: 0.9581 - val_loss: 0.3836 - val_accuracy: 0.9195\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1640 - accuracy: 0.9593 - val_loss: 0.2917 - val_accuracy: 0.9547\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1635 - accuracy: 0.9630 - val_loss: 0.3068 - val_accuracy: 0.9377\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1620 - accuracy: 0.9611 - val_loss: 0.2689 - val_accuracy: 0.9622\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 6s 4ms/step - loss: 0.1460 - accuracy: 0.9678 - val_loss: 0.3111 - val_accuracy: 0.9487\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1571 - accuracy: 0.9619 - val_loss: 0.2827 - val_accuracy: 0.9553\n",
      "Average Validation Accuracy: 0.9617690145969391\n",
      "Average Validation Loss: 0.20360661298036575\n",
      "Average Test Accuracy: 0.9625193476676941\n",
      "Final Test Accuracy for each fold: 0.9649885892868042\n",
      "Number of input features: 16\n",
      "Fold: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 8s 3ms/step - loss: 3.2646 - accuracy: 0.4016 - val_loss: 2.0631 - val_accuracy: 0.5892\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 1.3801 - accuracy: 0.7426 - val_loss: 1.1676 - val_accuracy: 0.8141\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.7589 - accuracy: 0.8519 - val_loss: 0.8219 - val_accuracy: 0.8612\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.4751 - accuracy: 0.8954 - val_loss: 0.7423 - val_accuracy: 0.8642\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.3405 - accuracy: 0.9188 - val_loss: 0.5642 - val_accuracy: 0.9003\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2643 - accuracy: 0.9382 - val_loss: 0.5005 - val_accuracy: 0.9138\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2363 - accuracy: 0.9402 - val_loss: 0.4693 - val_accuracy: 0.9230\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2076 - accuracy: 0.9519 - val_loss: 0.4556 - val_accuracy: 0.9307\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1858 - accuracy: 0.9562 - val_loss: 0.3865 - val_accuracy: 0.9373\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1753 - accuracy: 0.9582 - val_loss: 0.3719 - val_accuracy: 0.9518\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1688 - accuracy: 0.9579 - val_loss: 0.3805 - val_accuracy: 0.9441\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1630 - accuracy: 0.9589 - val_loss: 0.3467 - val_accuracy: 0.9578\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1485 - accuracy: 0.9641 - val_loss: 0.3628 - val_accuracy: 0.9518\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1416 - accuracy: 0.9647 - val_loss: 0.3771 - val_accuracy: 0.9452\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1460 - accuracy: 0.9649 - val_loss: 0.3277 - val_accuracy: 0.9575\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1380 - accuracy: 0.9674 - val_loss: 0.4124 - val_accuracy: 0.9426\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1386 - accuracy: 0.9676 - val_loss: 0.3076 - val_accuracy: 0.9650\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1314 - accuracy: 0.9690 - val_loss: 0.3529 - val_accuracy: 0.9507\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1342 - accuracy: 0.9682 - val_loss: 0.3082 - val_accuracy: 0.9558\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1264 - accuracy: 0.9703 - val_loss: 0.3084 - val_accuracy: 0.9718\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 10s 4ms/step - loss: 3.3901 - accuracy: 0.3657 - val_loss: 2.3284 - val_accuracy: 0.6194\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 1.5145 - accuracy: 0.7168 - val_loss: 1.4411 - val_accuracy: 0.7791\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.8910 - accuracy: 0.8223 - val_loss: 1.0818 - val_accuracy: 0.8128\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.5978 - accuracy: 0.8753 - val_loss: 0.8137 - val_accuracy: 0.8719\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.4469 - accuracy: 0.9001 - val_loss: 0.7164 - val_accuracy: 0.8917\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.3638 - accuracy: 0.9141 - val_loss: 0.5973 - val_accuracy: 0.9094\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.3066 - accuracy: 0.9249 - val_loss: 0.5871 - val_accuracy: 0.9089\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2684 - accuracy: 0.9356 - val_loss: 0.4730 - val_accuracy: 0.9311\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2414 - accuracy: 0.9401 - val_loss: 0.4302 - val_accuracy: 0.9362\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2228 - accuracy: 0.9456 - val_loss: 0.4425 - val_accuracy: 0.9329\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2032 - accuracy: 0.9487 - val_loss: 0.4101 - val_accuracy: 0.9267\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1912 - accuracy: 0.9532 - val_loss: 0.3697 - val_accuracy: 0.9516\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1775 - accuracy: 0.9571 - val_loss: 0.4199 - val_accuracy: 0.9349\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1672 - accuracy: 0.9593 - val_loss: 0.3593 - val_accuracy: 0.9496\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1660 - accuracy: 0.9610 - val_loss: 0.3708 - val_accuracy: 0.9503\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 5261s 3s/step - loss: 0.1524 - accuracy: 0.9643 - val_loss: 0.3367 - val_accuracy: 0.9589\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1547 - accuracy: 0.9621 - val_loss: 0.3243 - val_accuracy: 0.9586\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1475 - accuracy: 0.9665 - val_loss: 0.3389 - val_accuracy: 0.9531\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1424 - accuracy: 0.9677 - val_loss: 0.3713 - val_accuracy: 0.9479\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.1377 - accuracy: 0.9698 - val_loss: 0.3090 - val_accuracy: 0.9551\n",
      "Average Validation Accuracy: 0.9707364439964294\n",
      "Average Validation Loss: 0.1807090863585472\n",
      "Average Test Accuracy: 0.9726542532444\n",
      "Final Test Accuracy for each fold: 0.9800987839698792\n",
      "Number of input features: 17\n",
      "Fold: 1\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1846/1846 [==============================] - 3s 1ms/step - loss: 3.3187 - accuracy: 0.3757 - val_loss: 2.0181 - val_accuracy: 0.6618\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 2s 1ms/step - loss: 1.3055 - accuracy: 0.7367 - val_loss: 1.0928 - val_accuracy: 0.7822\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 2s 1ms/step - loss: 0.7008 - accuracy: 0.8410 - val_loss: 0.7415 - val_accuracy: 0.8667\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 2s 1ms/step - loss: 0.4684 - accuracy: 0.8841 - val_loss: 0.5587 - val_accuracy: 0.8889\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 2s 1ms/step - loss: 0.3584 - accuracy: 0.9059 - val_loss: 0.4838 - val_accuracy: 0.9050\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 2s 1ms/step - loss: 0.3015 - accuracy: 0.9197 - val_loss: 0.4600 - val_accuracy: 0.9116\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 2s 1ms/step - loss: 0.2653 - accuracy: 0.9319 - val_loss: 0.4144 - val_accuracy: 0.9157\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 2s 1ms/step - loss: 0.2427 - accuracy: 0.9343 - val_loss: 0.3801 - val_accuracy: 0.9366\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 2s 1ms/step - loss: 0.2201 - accuracy: 0.9415 - val_loss: 0.3499 - val_accuracy: 0.9435\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 2s 1ms/step - loss: 0.2060 - accuracy: 0.9468 - val_loss: 0.3299 - val_accuracy: 0.9428\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 2s 1ms/step - loss: 0.1953 - accuracy: 0.9500 - val_loss: 0.3145 - val_accuracy: 0.9490\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 2s 1ms/step - loss: 0.1858 - accuracy: 0.9529 - val_loss: 0.4069 - val_accuracy: 0.9100\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 2s 1ms/step - loss: 0.1705 - accuracy: 0.9564 - val_loss: 0.3176 - val_accuracy: 0.9410\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 3s 1ms/step - loss: 0.1677 - accuracy: 0.9595 - val_loss: 0.2873 - val_accuracy: 0.9597\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 2s 1ms/step - loss: 0.1600 - accuracy: 0.9612 - val_loss: 0.4486 - val_accuracy: 0.9157\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 2s 1ms/step - loss: 0.1480 - accuracy: 0.9659 - val_loss: 0.2952 - val_accuracy: 0.9556\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 2s 1ms/step - loss: 0.1507 - accuracy: 0.9637 - val_loss: 0.3144 - val_accuracy: 0.9540\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 2s 1ms/step - loss: 0.1388 - accuracy: 0.9660 - val_loss: 0.2952 - val_accuracy: 0.9600\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 2s 1ms/step - loss: 0.1402 - accuracy: 0.9668 - val_loss: 0.3745 - val_accuracy: 0.9450\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 2s 1ms/step - loss: 0.1393 - accuracy: 0.9668 - val_loss: 0.2951 - val_accuracy: 0.9503\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 3s 1ms/step - loss: 3.3287 - accuracy: 0.3717 - val_loss: 2.1911 - val_accuracy: 0.6224\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 2s 1ms/step - loss: 1.4452 - accuracy: 0.7328 - val_loss: 1.3201 - val_accuracy: 0.8020\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 2s 1ms/step - loss: 0.8229 - accuracy: 0.8379 - val_loss: 0.9610 - val_accuracy: 0.8293\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 2s 1ms/step - loss: 0.5296 - accuracy: 0.8877 - val_loss: 0.6752 - val_accuracy: 0.8979\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 2s 1ms/step - loss: 0.3776 - accuracy: 0.9172 - val_loss: 0.6018 - val_accuracy: 0.9061\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 2s 1ms/step - loss: 0.2997 - accuracy: 0.9288 - val_loss: 0.5022 - val_accuracy: 0.9168\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 2s 1ms/step - loss: 0.2502 - accuracy: 0.9422 - val_loss: 0.4740 - val_accuracy: 0.9217\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 2s 1ms/step - loss: 0.2144 - accuracy: 0.9499 - val_loss: 0.3995 - val_accuracy: 0.9347\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 4s 2ms/step - loss: 0.2016 - accuracy: 0.9513 - val_loss: 0.4353 - val_accuracy: 0.9267\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1790 - accuracy: 0.9570 - val_loss: 0.3316 - val_accuracy: 0.9547\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1650 - accuracy: 0.9597 - val_loss: 0.4326 - val_accuracy: 0.9281\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1578 - accuracy: 0.9633 - val_loss: 0.3308 - val_accuracy: 0.9520\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1536 - accuracy: 0.9637 - val_loss: 0.3581 - val_accuracy: 0.9540\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1433 - accuracy: 0.9650 - val_loss: 0.3004 - val_accuracy: 0.9531\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1359 - accuracy: 0.9675 - val_loss: 0.3048 - val_accuracy: 0.9604\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1406 - accuracy: 0.9675 - val_loss: 0.2900 - val_accuracy: 0.9641\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1410 - accuracy: 0.9677 - val_loss: 0.2787 - val_accuracy: 0.9523\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1220 - accuracy: 0.9730 - val_loss: 0.2973 - val_accuracy: 0.9551\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1242 - accuracy: 0.9698 - val_loss: 0.3018 - val_accuracy: 0.9606\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1181 - accuracy: 0.9740 - val_loss: 0.2823 - val_accuracy: 0.9593\n",
      "Average Validation Accuracy: 0.9641653597354889\n",
      "Average Validation Loss: 0.1784941777586937\n",
      "Average Test Accuracy: 0.9656519591808319\n",
      "Final Test Accuracy for each fold: 0.9731702208518982\n",
      "Number of input features: 18\n",
      "Fold: 1\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1846/1846 [==============================] - 7s 3ms/step - loss: 3.3717 - accuracy: 0.3607 - val_loss: 2.2079 - val_accuracy: 0.6011\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 1.5270 - accuracy: 0.7036 - val_loss: 1.3105 - val_accuracy: 0.7692\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.8989 - accuracy: 0.8177 - val_loss: 0.9416 - val_accuracy: 0.8224\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.6046 - accuracy: 0.8700 - val_loss: 0.7554 - val_accuracy: 0.8728\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.4414 - accuracy: 0.9027 - val_loss: 0.6379 - val_accuracy: 0.8975\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.3408 - accuracy: 0.9232 - val_loss: 0.6460 - val_accuracy: 0.8715\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2815 - accuracy: 0.9341 - val_loss: 0.5633 - val_accuracy: 0.9105\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2365 - accuracy: 0.9453 - val_loss: 0.4792 - val_accuracy: 0.9256\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2169 - accuracy: 0.9480 - val_loss: 0.4566 - val_accuracy: 0.9342\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1944 - accuracy: 0.9540 - val_loss: 0.4085 - val_accuracy: 0.9481\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1853 - accuracy: 0.9571 - val_loss: 0.4043 - val_accuracy: 0.9536\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1717 - accuracy: 0.9604 - val_loss: 0.4216 - val_accuracy: 0.9428\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1555 - accuracy: 0.9642 - val_loss: 0.3757 - val_accuracy: 0.9589\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1597 - accuracy: 0.9646 - val_loss: 0.4357 - val_accuracy: 0.9424\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1529 - accuracy: 0.9637 - val_loss: 0.3566 - val_accuracy: 0.9615\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1475 - accuracy: 0.9659 - val_loss: 0.3555 - val_accuracy: 0.9648\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1331 - accuracy: 0.9700 - val_loss: 0.3801 - val_accuracy: 0.9575\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1353 - accuracy: 0.9708 - val_loss: 0.3689 - val_accuracy: 0.9635\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1409 - accuracy: 0.9699 - val_loss: 0.3547 - val_accuracy: 0.9624\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1204 - accuracy: 0.9750 - val_loss: 0.4915 - val_accuracy: 0.9274\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 7s 3ms/step - loss: 3.2142 - accuracy: 0.3945 - val_loss: 2.0635 - val_accuracy: 0.6341\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 1.3560 - accuracy: 0.7470 - val_loss: 1.2588 - val_accuracy: 0.8033\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.7627 - accuracy: 0.8478 - val_loss: 0.8845 - val_accuracy: 0.8462\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.4877 - accuracy: 0.8939 - val_loss: 0.7478 - val_accuracy: 0.8856\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.3435 - accuracy: 0.9263 - val_loss: 0.5985 - val_accuracy: 0.9118\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2686 - accuracy: 0.9408 - val_loss: 0.5303 - val_accuracy: 0.9309\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2277 - accuracy: 0.9508 - val_loss: 0.4676 - val_accuracy: 0.9426\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1950 - accuracy: 0.9563 - val_loss: 0.4487 - val_accuracy: 0.9421\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1797 - accuracy: 0.9606 - val_loss: 0.4148 - val_accuracy: 0.9371\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1634 - accuracy: 0.9622 - val_loss: 0.3556 - val_accuracy: 0.9586\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 0.1538 - accuracy: 0.9648 - val_loss: 0.3586 - val_accuracy: 0.9496\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1455 - accuracy: 0.9675 - val_loss: 0.3300 - val_accuracy: 0.9626\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 0.1346 - accuracy: 0.9689 - val_loss: 0.3805 - val_accuracy: 0.9520\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 0.1349 - accuracy: 0.9697 - val_loss: 0.3585 - val_accuracy: 0.9408\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 0.1289 - accuracy: 0.9708 - val_loss: 0.3377 - val_accuracy: 0.9573\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 0.1238 - accuracy: 0.9729 - val_loss: 0.3057 - val_accuracy: 0.9703\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 0.1227 - accuracy: 0.9741 - val_loss: 0.3370 - val_accuracy: 0.9516\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 0.1242 - accuracy: 0.9728 - val_loss: 0.3111 - val_accuracy: 0.9633\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 0.1212 - accuracy: 0.9750 - val_loss: 0.3503 - val_accuracy: 0.9457\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 0.1125 - accuracy: 0.9768 - val_loss: 0.3018 - val_accuracy: 0.9688\n",
      "Average Validation Accuracy: 0.9567956030368805\n",
      "Average Validation Loss: 0.2482098639011383\n",
      "Average Test Accuracy: 0.9580599963665009\n",
      "Final Test Accuracy for each fold: 0.9778875112533569\n",
      "Number of input features: 19\n",
      "Fold: 1\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1846/1846 [==============================] - 4s 2ms/step - loss: 3.4148 - accuracy: 0.3498 - val_loss: 2.1801 - val_accuracy: 0.6180\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 1.4061 - accuracy: 0.7314 - val_loss: 1.1935 - val_accuracy: 0.7729\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 0.7534 - accuracy: 0.8423 - val_loss: 0.7930 - val_accuracy: 0.8592\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 0.4757 - accuracy: 0.8904 - val_loss: 0.7178 - val_accuracy: 0.8805\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 0.3539 - accuracy: 0.9129 - val_loss: 0.5378 - val_accuracy: 0.9074\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 0.2871 - accuracy: 0.9285 - val_loss: 0.4618 - val_accuracy: 0.9228\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 0.2453 - accuracy: 0.9377 - val_loss: 0.4534 - val_accuracy: 0.9164\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 0.2232 - accuracy: 0.9411 - val_loss: 0.4833 - val_accuracy: 0.9122\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 0.2086 - accuracy: 0.9454 - val_loss: 0.3534 - val_accuracy: 0.9472\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 0.1885 - accuracy: 0.9529 - val_loss: 0.3468 - val_accuracy: 0.9419\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 0.1739 - accuracy: 0.9551 - val_loss: 0.4497 - val_accuracy: 0.9188\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 0.1645 - accuracy: 0.9581 - val_loss: 0.3271 - val_accuracy: 0.9474\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 0.1596 - accuracy: 0.9609 - val_loss: 0.3539 - val_accuracy: 0.9448\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 0.1517 - accuracy: 0.9600 - val_loss: 0.3487 - val_accuracy: 0.9483\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 0.1506 - accuracy: 0.9610 - val_loss: 0.2922 - val_accuracy: 0.9593\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 0.1389 - accuracy: 0.9664 - val_loss: 0.2839 - val_accuracy: 0.9628\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 0.1382 - accuracy: 0.9669 - val_loss: 0.2902 - val_accuracy: 0.9608\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 0.1363 - accuracy: 0.9700 - val_loss: 0.3281 - val_accuracy: 0.9509\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 3s 2ms/step - loss: 0.1289 - accuracy: 0.9708 - val_loss: 0.2910 - val_accuracy: 0.9630\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 5s 2ms/step - loss: 0.1276 - accuracy: 0.9694 - val_loss: 0.2661 - val_accuracy: 0.9639\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 9s 4ms/step - loss: 3.2479 - accuracy: 0.4009 - val_loss: 1.9789 - val_accuracy: 0.6708\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 1.2558 - accuracy: 0.7579 - val_loss: 1.2063 - val_accuracy: 0.7956\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.7424 - accuracy: 0.8433 - val_loss: 0.8905 - val_accuracy: 0.8462\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.5234 - accuracy: 0.8767 - val_loss: 0.6944 - val_accuracy: 0.8614\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.3903 - accuracy: 0.9063 - val_loss: 0.6200 - val_accuracy: 0.8939\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.3272 - accuracy: 0.9162 - val_loss: 0.5263 - val_accuracy: 0.9107\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2798 - accuracy: 0.9275 - val_loss: 0.4610 - val_accuracy: 0.9140\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2466 - accuracy: 0.9371 - val_loss: 0.4318 - val_accuracy: 0.9230\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.2183 - accuracy: 0.9417 - val_loss: 0.3729 - val_accuracy: 0.9388\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1989 - accuracy: 0.9490 - val_loss: 0.4175 - val_accuracy: 0.9239\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1891 - accuracy: 0.9511 - val_loss: 0.3590 - val_accuracy: 0.9439\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1756 - accuracy: 0.9538 - val_loss: 0.3556 - val_accuracy: 0.9386\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1655 - accuracy: 0.9595 - val_loss: 0.3212 - val_accuracy: 0.9498\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1572 - accuracy: 0.9600 - val_loss: 0.3234 - val_accuracy: 0.9562\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1568 - accuracy: 0.9596 - val_loss: 0.3402 - val_accuracy: 0.9476\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1417 - accuracy: 0.9661 - val_loss: 0.2961 - val_accuracy: 0.9593\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1442 - accuracy: 0.9663 - val_loss: 0.3154 - val_accuracy: 0.9424\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1386 - accuracy: 0.9666 - val_loss: 0.2781 - val_accuracy: 0.9617\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1317 - accuracy: 0.9694 - val_loss: 0.2873 - val_accuracy: 0.9617\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1279 - accuracy: 0.9705 - val_loss: 0.3259 - val_accuracy: 0.9498\n",
      "Average Validation Accuracy: 0.9673235714435577\n",
      "Average Validation Loss: 0.17608528584241867\n",
      "Average Test Accuracy: 0.9690425395965576\n",
      "Final Test Accuracy for each fold: 0.9741284251213074\n",
      "Number of input features: 20\n",
      "Fold: 1\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1846/1846 [==============================] - 7s 3ms/step - loss: 3.4985 - accuracy: 0.3412 - val_loss: 2.2369 - val_accuracy: 0.6130\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.4440 - accuracy: 0.7423 - val_loss: 1.2719 - val_accuracy: 0.7921\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.7984 - accuracy: 0.8474 - val_loss: 0.8694 - val_accuracy: 0.8594\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.4962 - accuracy: 0.8933 - val_loss: 0.6308 - val_accuracy: 0.9078\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.3456 - accuracy: 0.9206 - val_loss: 0.5164 - val_accuracy: 0.9219\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2704 - accuracy: 0.9359 - val_loss: 0.4781 - val_accuracy: 0.9267\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2248 - accuracy: 0.9481 - val_loss: 0.4130 - val_accuracy: 0.9432\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.2029 - accuracy: 0.9497 - val_loss: 0.4528 - val_accuracy: 0.9300\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1826 - accuracy: 0.9549 - val_loss: 0.3790 - val_accuracy: 0.9424\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1691 - accuracy: 0.9590 - val_loss: 0.3703 - val_accuracy: 0.9410\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1681 - accuracy: 0.9566 - val_loss: 0.3762 - val_accuracy: 0.9481\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1481 - accuracy: 0.9641 - val_loss: 0.3163 - val_accuracy: 0.9659\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1482 - accuracy: 0.9630 - val_loss: 0.3320 - val_accuracy: 0.9542\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1343 - accuracy: 0.9682 - val_loss: 0.3556 - val_accuracy: 0.9496\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1393 - accuracy: 0.9655 - val_loss: 0.3976 - val_accuracy: 0.9300\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1362 - accuracy: 0.9673 - val_loss: 0.3338 - val_accuracy: 0.9545\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 6s 4ms/step - loss: 0.1278 - accuracy: 0.9691 - val_loss: 0.3068 - val_accuracy: 0.9683\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1306 - accuracy: 0.9700 - val_loss: 0.3764 - val_accuracy: 0.9472\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1203 - accuracy: 0.9712 - val_loss: 0.3132 - val_accuracy: 0.9608\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1231 - accuracy: 0.9703 - val_loss: 0.3140 - val_accuracy: 0.9635\n",
      "Fold: 2\n",
      "Epoch 1/20\n",
      "1846/1846 [==============================] - 8s 4ms/step - loss: 3.3309 - accuracy: 0.3752 - val_loss: 2.2639 - val_accuracy: 0.5685\n",
      "Epoch 2/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 1.4448 - accuracy: 0.7267 - val_loss: 1.3870 - val_accuracy: 0.7696\n",
      "Epoch 3/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.8466 - accuracy: 0.8324 - val_loss: 0.9820 - val_accuracy: 0.8488\n",
      "Epoch 4/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.5745 - accuracy: 0.8834 - val_loss: 0.7563 - val_accuracy: 0.8880\n",
      "Epoch 5/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.4276 - accuracy: 0.9090 - val_loss: 0.6527 - val_accuracy: 0.9008\n",
      "Epoch 6/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.3363 - accuracy: 0.9219 - val_loss: 0.5749 - val_accuracy: 0.9052\n",
      "Epoch 7/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2833 - accuracy: 0.9324 - val_loss: 0.4610 - val_accuracy: 0.9265\n",
      "Epoch 8/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2474 - accuracy: 0.9412 - val_loss: 0.5117 - val_accuracy: 0.9094\n",
      "Epoch 9/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.2226 - accuracy: 0.9460 - val_loss: 0.4045 - val_accuracy: 0.9320\n",
      "Epoch 10/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1998 - accuracy: 0.9522 - val_loss: 0.3628 - val_accuracy: 0.9386\n",
      "Epoch 11/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1893 - accuracy: 0.9529 - val_loss: 0.3815 - val_accuracy: 0.9377\n",
      "Epoch 12/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1669 - accuracy: 0.9588 - val_loss: 0.3208 - val_accuracy: 0.9545\n",
      "Epoch 13/20\n",
      "1846/1846 [==============================] - 9s 5ms/step - loss: 0.1701 - accuracy: 0.9611 - val_loss: 0.3246 - val_accuracy: 0.9474\n",
      "Epoch 14/20\n",
      "1846/1846 [==============================] - 7s 4ms/step - loss: 0.1560 - accuracy: 0.9624 - val_loss: 0.3066 - val_accuracy: 0.9595\n",
      "Epoch 15/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1590 - accuracy: 0.9622 - val_loss: 0.2965 - val_accuracy: 0.9622\n",
      "Epoch 16/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1403 - accuracy: 0.9686 - val_loss: 0.3050 - val_accuracy: 0.9578\n",
      "Epoch 17/20\n",
      "1846/1846 [==============================] - 5s 3ms/step - loss: 0.1435 - accuracy: 0.9669 - val_loss: 0.3052 - val_accuracy: 0.9606\n",
      "Epoch 18/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1348 - accuracy: 0.9706 - val_loss: 0.2883 - val_accuracy: 0.9619\n",
      "Epoch 19/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1318 - accuracy: 0.9705 - val_loss: 0.2971 - val_accuracy: 0.9606\n",
      "Epoch 20/20\n",
      "1846/1846 [==============================] - 6s 3ms/step - loss: 0.1342 - accuracy: 0.9705 - val_loss: 0.2857 - val_accuracy: 0.9661\n",
      "Average Validation Accuracy: 0.9708455801010132\n",
      "Average Validation Loss: 0.17971321195363998\n",
      "Average Test Accuracy: 0.9722856879234314\n",
      "Final Test Accuracy for each fold: 0.9737598299980164\n"
     ]
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_fs, Y, test_size=0.33, random_state=1)\n",
    "\n",
    "# prepare output data\n",
    "y_train_enc, y_test_enc = prepare_targets(y_train, y_test)\n",
    "\n",
    "# Define the number of folds for k-fold cross-validation\n",
    "k = 2\n",
    "\n",
    "# Define the cross-validation method\n",
    "cv_method = StratifiedKFold(n_splits=k)\n",
    "\n",
    "# Initialize the list to store the history, train & validation(accuracy & loss) for each model\n",
    "models = []\n",
    "model_history = []\n",
    "model_accuracy = []\n",
    "model_train_acc = []\n",
    "model_train_loss = []\n",
    "model_val_acc = []\n",
    "model_val_loss = []\n",
    "\n",
    "\n",
    "for i in range(1,21):\n",
    "\n",
    "    models_fold = []\n",
    "    hist = []\n",
    "    train_accuracy = []\n",
    "    train_loss = []\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "    test_accuracy = []\n",
    "\n",
    "    print(\"Number of input features:\",i)\n",
    "\n",
    "    # Select the input features from the input data\n",
    "    X_train_selected = X_train[:, :i]\n",
    "    X_test_selected = X_test[:, :i]\n",
    "\n",
    "    # Loop over the folds\n",
    "    for fold, (train_index, val_index) in enumerate(cv_method.split(X_train_selected, y_train)):\n",
    "\n",
    "        print(\"Fold:\", fold+1)\n",
    "\n",
    "        # Split the data into train and validation sets using the current fold index\n",
    "        X_train_fold  = X_train_selected[train_index]\n",
    "        y_train_fold  = y_train[train_index]\n",
    "        X_val_fold = X_train_selected[val_index]\n",
    "        y_val_fold = y_train[val_index]\n",
    "\n",
    "        # Prepare the target data\n",
    "        y_train_fold_enc, y_val_fold_enc = prepare_targets(y_train_fold, y_val_fold)\n",
    "\n",
    "        # build the model\n",
    "        model = MLP_model(i)\n",
    "\n",
    "        # Fit the model to the training data for the current fold\n",
    "        history = model.fit(X_train_fold, to_categorical(y_train_fold_enc, num_classes=373), epochs=20, batch_size=5, verbose=1, validation_split = 0.33)\n",
    "    \n",
    "        # Evaluate the model on the validation data for the current fold\n",
    "        val_scores = model.evaluate(X_val_fold, to_categorical(y_val_fold_enc, num_classes=373), verbose=0)\n",
    "        val_accuracy.append(val_scores[1])\n",
    "        val_loss.append(val_scores[0])\n",
    "\n",
    "        # Evaluate the model on the test data for the current fold\n",
    "        test_scores = model.evaluate(X_test_selected, to_categorical(y_test_enc, num_classes=373), verbose=0)\n",
    "        test_accuracy.append(test_scores[1])\n",
    "\n",
    "        # add the model to the list of models\n",
    "        models_fold.append(model)\n",
    "        hist.append(history)\n",
    "\n",
    "        # store the training accuracy and loss for each fold\n",
    "        train_accuracy.append(history.history['accuracy'])\n",
    "        train_loss.append(history.history['loss'])\n",
    "        \n",
    "    # Calculate the average test and validation accuracy and loss across all folds\n",
    "    avg_test_acc = sum(test_accuracy) / len(test_accuracy)\n",
    "    avg_val_acc = sum(val_accuracy) / len(val_accuracy)\n",
    "    avg_val_loss = sum(val_loss) / len(val_loss)\n",
    "\n",
    "    # Print the average validation and test accuracy and loss\n",
    "    print(\"Average Validation Accuracy:\", avg_val_acc)\n",
    "    print(\"Average Validation Loss:\",avg_val_loss)\n",
    "    print(\"Average Test Accuracy:\", avg_test_acc)\n",
    "\n",
    "    best_fold_index = test_accuracy.index(max(test_accuracy))\n",
    "    model_accuracy.append(test_accuracy[best_fold_index])\n",
    "    models.append(models_fold[best_fold_index])\n",
    "    model_history.append(hist[best_fold_index])\n",
    "    model_train_acc.append(train_accuracy[best_fold_index])\n",
    "    model_train_loss.append(train_loss[best_fold_index])\n",
    "    model_val_acc.append(val_accuracy[best_fold_index])\n",
    "    model_val_loss.append(val_loss[best_fold_index])\n",
    "\n",
    "\n",
    "    print(\"Final Test Accuracy for each fold:\", test_accuracy[best_fold_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to show the no of input features and its corresponding model accuracy\n",
    "model_list = []\n",
    "\n",
    "#Iterate through each model's accuracy \n",
    "for i in range (len(model_accuracy)):\n",
    "    #get the number of input features for the current model\n",
    "    no_features = i + 1\n",
    "\n",
    "    #round the model accuries to 3 d.p.\n",
    "    rounded_model_acc = round(model_accuracy[i], 3)\n",
    "    \n",
    "    model_list.append([no_features, rounded_model_acc])\n",
    "\n",
    "models_df = pd.DataFrame(model_list, columns=[\"No of input features\", \"Model accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 0.9800987839698792\n"
     ]
    }
   ],
   "source": [
    "# Select the final model based on the max test accuracy across all models\n",
    "\n",
    "best_model_index = model_accuracy.index(max(model_accuracy))\n",
    "\n",
    "best_model = models[best_model_index]\n",
    "best_model_history = model_history[best_model_index]\n",
    "best_model_train_acc = model_train_acc[best_model_index]\n",
    "best_model_train_loss = model_train_loss[best_model_index]\n",
    "best_model_val_acc = model_val_acc[best_model_index]\n",
    "best_model_val_loss = model_val_loss[best_model_index]\n",
    "\n",
    "# summarize history for accuracy  \n",
    "plt.subplot(211)  \n",
    "plt.plot(best_model_history.history['accuracy'])  \n",
    "plt.plot(best_model_history.history['val_accuracy'])  \n",
    "plt.title('model accuracy')  \n",
    "plt.ylabel('accuracy')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'test'], loc='lower right')  \n",
    "\n",
    "# summarize history for loss\n",
    "plt.subplot(212)  \n",
    "plt.plot(best_model_history.history['loss'])  \n",
    "plt.plot(best_model_history.history['val_loss'])  \n",
    "plt.title('model loss')  \n",
    "plt.ylabel('loss')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.legend(['train', 'test'], loc='upper right')  \n",
    "plt.subplots_adjust(hspace=0.7)\n",
    "\n",
    "plt.show() \n",
    "\n",
    "print(\"Final Test Accuracy:\", model_accuracy[best_model_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424/424 [==============================] - 1s 2ms/step\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       591\n",
      "           1       1.00      1.00      1.00       430\n",
      "           2       1.00      1.00      1.00       419\n",
      "           3       1.00      1.00      1.00       384\n",
      "           4       0.98      1.00      0.99       339\n",
      "           5       1.00      1.00      1.00       342\n",
      "           6       1.00      1.00      1.00       310\n",
      "           7       1.00      1.00      1.00       325\n",
      "           8       1.00      1.00      1.00       294\n",
      "           9       1.00      1.00      1.00       269\n",
      "          10       1.00      1.00      1.00       296\n",
      "          11       1.00      1.00      1.00       258\n",
      "          12       1.00      1.00      1.00       247\n",
      "          13       1.00      1.00      1.00       237\n",
      "          14       0.91      1.00      0.95       239\n",
      "          15       1.00      1.00      1.00       235\n",
      "          16       1.00      1.00      1.00       213\n",
      "          17       1.00      1.00      1.00       202\n",
      "          18       1.00      1.00      1.00       196\n",
      "          19       0.95      1.00      0.98       181\n",
      "          20       1.00      1.00      1.00       177\n",
      "          21       1.00      1.00      1.00       177\n",
      "          22       1.00      1.00      1.00       155\n",
      "          23       1.00      1.00      1.00       155\n",
      "          24       1.00      1.00      1.00       144\n",
      "          25       1.00      1.00      1.00       126\n",
      "          26       1.00      1.00      1.00       108\n",
      "          27       1.00      1.00      1.00       121\n",
      "          28       1.00      1.00      1.00        95\n",
      "          29       1.00      1.00      1.00       106\n",
      "          30       1.00      1.00      1.00       102\n",
      "          31       1.00      1.00      1.00        86\n",
      "          32       1.00      1.00      1.00       108\n",
      "          33       1.00      1.00      1.00        88\n",
      "          34       1.00      1.00      1.00       102\n",
      "          35       1.00      1.00      1.00        88\n",
      "          36       1.00      1.00      1.00        83\n",
      "          37       1.00      1.00      1.00        93\n",
      "          38       1.00      1.00      1.00        76\n",
      "          39       1.00      1.00      1.00        85\n",
      "          40       1.00      1.00      1.00        86\n",
      "          41       1.00      1.00      1.00        85\n",
      "          42       1.00      1.00      1.00        68\n",
      "          43       1.00      1.00      1.00        75\n",
      "          44       1.00      1.00      1.00        71\n",
      "          45       0.74      1.00      0.85        58\n",
      "          46       1.00      1.00      1.00        71\n",
      "          47       0.58      1.00      0.74        57\n",
      "          48       1.00      1.00      1.00        67\n",
      "          49       0.77      1.00      0.87        47\n",
      "          50       1.00      1.00      1.00        48\n",
      "          51       1.00      1.00      1.00        47\n",
      "          52       1.00      1.00      1.00        43\n",
      "          53       1.00      1.00      1.00        51\n",
      "          54       1.00      1.00      1.00        44\n",
      "          55       0.88      1.00      0.94        51\n",
      "          56       1.00      1.00      1.00        45\n",
      "          57       1.00      1.00      1.00        44\n",
      "          58       1.00      1.00      1.00        41\n",
      "          59       1.00      1.00      1.00        41\n",
      "          60       1.00      1.00      1.00        52\n",
      "          61       1.00      1.00      1.00        43\n",
      "          62       1.00      1.00      1.00        37\n",
      "          63       1.00      1.00      1.00        43\n",
      "          64       0.82      1.00      0.90        42\n",
      "          65       1.00      1.00      1.00        46\n",
      "          66       1.00      1.00      1.00        43\n",
      "          67       1.00      1.00      1.00        40\n",
      "          68       1.00      1.00      1.00        44\n",
      "          69       1.00      1.00      1.00        43\n",
      "          70       1.00      1.00      1.00        38\n",
      "          71       1.00      1.00      1.00        33\n",
      "          72       1.00      1.00      1.00        45\n",
      "          73       1.00      1.00      1.00        38\n",
      "          74       1.00      1.00      1.00        42\n",
      "          75       1.00      1.00      1.00        39\n",
      "          76       0.00      0.00      0.00        30\n",
      "          77       1.00      1.00      1.00        28\n",
      "          78       0.90      1.00      0.95        28\n",
      "          79       1.00      1.00      1.00        32\n",
      "          80       1.00      1.00      1.00        33\n",
      "          81       1.00      1.00      1.00        31\n",
      "          82       1.00      1.00      1.00        35\n",
      "          83       1.00      1.00      1.00        39\n",
      "          84       1.00      1.00      1.00        27\n",
      "          85       1.00      1.00      1.00        36\n",
      "          86       1.00      1.00      1.00        31\n",
      "          87       1.00      1.00      1.00        28\n",
      "          88       1.00      1.00      1.00        20\n",
      "          89       1.00      1.00      1.00        33\n",
      "          90       1.00      1.00      1.00        24\n",
      "          91       1.00      1.00      1.00        22\n",
      "          92       1.00      1.00      1.00        26\n",
      "          93       1.00      1.00      1.00        35\n",
      "          94       0.90      1.00      0.95        27\n",
      "          95       1.00      1.00      1.00        23\n",
      "          96       1.00      1.00      1.00        27\n",
      "          97       1.00      1.00      1.00        28\n",
      "          98       0.00      0.00      0.00        16\n",
      "          99       1.00      1.00      1.00        35\n",
      "         100       1.00      1.00      1.00        28\n",
      "         101       1.00      1.00      1.00        25\n",
      "         102       1.00      1.00      1.00        26\n",
      "         103       1.00      1.00      1.00        33\n",
      "         104       1.00      1.00      1.00        26\n",
      "         105       1.00      1.00      1.00        24\n",
      "         106       1.00      1.00      1.00        22\n",
      "         107       0.74      1.00      0.85        26\n",
      "         108       1.00      1.00      1.00        25\n",
      "         109       1.00      1.00      1.00        16\n",
      "         110       0.74      1.00      0.85        20\n",
      "         111       1.00      1.00      1.00        26\n",
      "         112       1.00      1.00      1.00        18\n",
      "         113       0.68      1.00      0.81        23\n",
      "         114       1.00      1.00      1.00        25\n",
      "         115       1.00      1.00      1.00        18\n",
      "         116       1.00      1.00      1.00        19\n",
      "         117       1.00      1.00      1.00        16\n",
      "         118       1.00      1.00      1.00        26\n",
      "         119       0.76      1.00      0.86        22\n",
      "         120       1.00      1.00      1.00        17\n",
      "         121       1.00      1.00      1.00        15\n",
      "         122       1.00      1.00      1.00        18\n",
      "         123       1.00      1.00      1.00        20\n",
      "         124       1.00      1.00      1.00        14\n",
      "         125       1.00      1.00      1.00        22\n",
      "         126       1.00      1.00      1.00        19\n",
      "         127       1.00      1.00      1.00        27\n",
      "         128       1.00      1.00      1.00        26\n",
      "         129       0.84      1.00      0.91        21\n",
      "         130       1.00      1.00      1.00        18\n",
      "         131       1.00      1.00      1.00        18\n",
      "         132       1.00      1.00      1.00        20\n",
      "         133       1.00      1.00      1.00        14\n",
      "         134       1.00      1.00      1.00        19\n",
      "         135       1.00      1.00      1.00        16\n",
      "         136       1.00      1.00      1.00        23\n",
      "         137       0.00      0.00      0.00        11\n",
      "         138       1.00      1.00      1.00        14\n",
      "         139       1.00      1.00      1.00        20\n",
      "         140       1.00      1.00      1.00        23\n",
      "         141       1.00      1.00      1.00        14\n",
      "         142       1.00      0.54      0.70        13\n",
      "         143       1.00      1.00      1.00        23\n",
      "         144       1.00      1.00      1.00        17\n",
      "         145       1.00      1.00      1.00        24\n",
      "         146       1.00      1.00      1.00        16\n",
      "         147       1.00      0.58      0.73        19\n",
      "         148       0.88      1.00      0.94        22\n",
      "         149       1.00      1.00      1.00        15\n",
      "         150       1.00      1.00      1.00        11\n",
      "         151       1.00      1.00      1.00        19\n",
      "         152       1.00      1.00      1.00        20\n",
      "         153       1.00      1.00      1.00        24\n",
      "         154       1.00      1.00      1.00        11\n",
      "         155       1.00      1.00      1.00        17\n",
      "         156       1.00      1.00      1.00        18\n",
      "         157       1.00      1.00      1.00        12\n",
      "         158       1.00      1.00      1.00        18\n",
      "         159       1.00      1.00      1.00        20\n",
      "         160       1.00      1.00      1.00        20\n",
      "         161       1.00      1.00      1.00        16\n",
      "         162       1.00      1.00      1.00        15\n",
      "         163       1.00      1.00      1.00        15\n",
      "         164       1.00      1.00      1.00        13\n",
      "         165       0.00      0.00      0.00        19\n",
      "         166       1.00      1.00      1.00        11\n",
      "         167       1.00      1.00      1.00         9\n",
      "         168       1.00      1.00      1.00        11\n",
      "         169       0.64      1.00      0.78        21\n",
      "         170       1.00      1.00      1.00        15\n",
      "         171       1.00      1.00      1.00        18\n",
      "         172       1.00      1.00      1.00        11\n",
      "         173       1.00      1.00      1.00        16\n",
      "         174       1.00      1.00      1.00        10\n",
      "         175       1.00      1.00      1.00        11\n",
      "         176       1.00      1.00      1.00        10\n",
      "         177       1.00      1.00      1.00        15\n",
      "         178       1.00      1.00      1.00        11\n",
      "         179       1.00      1.00      1.00        15\n",
      "         180       1.00      1.00      1.00        13\n",
      "         181       1.00      1.00      1.00        15\n",
      "         182       1.00      1.00      1.00         9\n",
      "         183       1.00      1.00      1.00        16\n",
      "         184       1.00      1.00      1.00         8\n",
      "         185       0.00      0.00      0.00        12\n",
      "         186       1.00      1.00      1.00        15\n",
      "         187       1.00      1.00      1.00        15\n",
      "         188       0.00      0.00      0.00        13\n",
      "         189       1.00      1.00      1.00        14\n",
      "         190       1.00      1.00      1.00        11\n",
      "         191       1.00      1.00      1.00        10\n",
      "         192       1.00      1.00      1.00        17\n",
      "         193       1.00      1.00      1.00         9\n",
      "         194       0.82      1.00      0.90         9\n",
      "         195       1.00      1.00      1.00         4\n",
      "         196       1.00      1.00      1.00         7\n",
      "         197       1.00      1.00      1.00         7\n",
      "         198       1.00      1.00      1.00        12\n",
      "         199       1.00      1.00      1.00        14\n",
      "         200       1.00      1.00      1.00         6\n",
      "         201       1.00      1.00      1.00         9\n",
      "         202       1.00      1.00      1.00         7\n",
      "         203       0.00      0.00      0.00         6\n",
      "         204       1.00      1.00      1.00        11\n",
      "         205       0.00      0.00      0.00        14\n",
      "         206       1.00      1.00      1.00        12\n",
      "         207       1.00      1.00      1.00        14\n",
      "         208       1.00      1.00      1.00        12\n",
      "         209       1.00      1.00      1.00         7\n",
      "         210       1.00      1.00      1.00        19\n",
      "         211       0.00      0.00      0.00         7\n",
      "         212       1.00      1.00      1.00        11\n",
      "         213       1.00      1.00      1.00         9\n",
      "         214       1.00      1.00      1.00         7\n",
      "         215       1.00      1.00      1.00         6\n",
      "         216       1.00      1.00      1.00        12\n",
      "         217       1.00      1.00      1.00        12\n",
      "         218       0.69      1.00      0.82         9\n",
      "         219       1.00      1.00      1.00         6\n",
      "         220       1.00      1.00      1.00         8\n",
      "         221       1.00      1.00      1.00         5\n",
      "         222       0.00      0.00      0.00         4\n",
      "         223       0.56      1.00      0.72        14\n",
      "         224       1.00      1.00      1.00        13\n",
      "         225       1.00      1.00      1.00         4\n",
      "         226       1.00      1.00      1.00        10\n",
      "         227       1.00      1.00      1.00        12\n",
      "         228       1.00      1.00      1.00        13\n",
      "         229       1.00      1.00      1.00        11\n",
      "         230       1.00      1.00      1.00         5\n",
      "         231       1.00      1.00      1.00         6\n",
      "         232       0.00      0.00      0.00         8\n",
      "         233       1.00      1.00      1.00        10\n",
      "         234       1.00      1.00      1.00         4\n",
      "         235       1.00      1.00      1.00         8\n",
      "         236       1.00      1.00      1.00         9\n",
      "         237       1.00      1.00      1.00         8\n",
      "         238       1.00      1.00      1.00        10\n",
      "         239       0.00      0.00      0.00         9\n",
      "         240       1.00      1.00      1.00         7\n",
      "         241       1.00      1.00      1.00         8\n",
      "         242       1.00      1.00      1.00         6\n",
      "         243       1.00      0.57      0.73         7\n",
      "         244       1.00      1.00      1.00         9\n",
      "         245       1.00      1.00      1.00         7\n",
      "         246       0.00      0.00      0.00         9\n",
      "         247       1.00      1.00      1.00         7\n",
      "         248       1.00      1.00      1.00        10\n",
      "         249       1.00      1.00      1.00        10\n",
      "         250       1.00      1.00      1.00         7\n",
      "         251       1.00      1.00      1.00         6\n",
      "         252       1.00      1.00      1.00        11\n",
      "         253       1.00      1.00      1.00         7\n",
      "         254       1.00      1.00      1.00         8\n",
      "         255       1.00      1.00      1.00         5\n",
      "         256       1.00      1.00      1.00         7\n",
      "         257       1.00      1.00      1.00         9\n",
      "         258       1.00      1.00      1.00         6\n",
      "         259       0.00      0.00      0.00         3\n",
      "         260       1.00      1.00      1.00         4\n",
      "         261       1.00      1.00      1.00         2\n",
      "         262       1.00      1.00      1.00         5\n",
      "         263       1.00      1.00      1.00        12\n",
      "         264       1.00      1.00      1.00         5\n",
      "         265       1.00      1.00      1.00         7\n",
      "         266       1.00      1.00      1.00        10\n",
      "         267       1.00      1.00      1.00         8\n",
      "         268       0.00      0.00      0.00         9\n",
      "         269       1.00      1.00      1.00         6\n",
      "         270       1.00      1.00      1.00         4\n",
      "         271       1.00      1.00      1.00         7\n",
      "         272       1.00      1.00      1.00        10\n",
      "         273       1.00      1.00      1.00         3\n",
      "         274       1.00      1.00      1.00         9\n",
      "         275       1.00      1.00      1.00         6\n",
      "         276       1.00      1.00      1.00         5\n",
      "         277       0.00      0.00      0.00         4\n",
      "         278       0.50      1.00      0.67         3\n",
      "         279       1.00      1.00      1.00         4\n",
      "         280       0.00      0.00      0.00         5\n",
      "         281       1.00      1.00      1.00         6\n",
      "         282       1.00      1.00      1.00        11\n",
      "         283       1.00      1.00      1.00         6\n",
      "         284       1.00      1.00      1.00         2\n",
      "         285       1.00      1.00      1.00         4\n",
      "         286       1.00      1.00      1.00         7\n",
      "         287       1.00      1.00      1.00         9\n",
      "         288       0.00      0.00      0.00         7\n",
      "         289       1.00      1.00      1.00         7\n",
      "         290       1.00      1.00      1.00         6\n",
      "         291       0.83      1.00      0.91         5\n",
      "         292       0.38      1.00      0.55         6\n",
      "         293       1.00      1.00      1.00         8\n",
      "         294       1.00      1.00      1.00         7\n",
      "         295       0.00      0.00      0.00         3\n",
      "         296       1.00      1.00      1.00         6\n",
      "         297       1.00      1.00      1.00         8\n",
      "         298       0.40      1.00      0.57         4\n",
      "         299       1.00      1.00      1.00         6\n",
      "         300       1.00      1.00      1.00         5\n",
      "         301       0.16      1.00      0.27         5\n",
      "         302       1.00      1.00      1.00         5\n",
      "         303       1.00      1.00      1.00         2\n",
      "         304       1.00      1.00      1.00         8\n",
      "         305       1.00      1.00      1.00         3\n",
      "         306       1.00      1.00      1.00         6\n",
      "         307       1.00      1.00      1.00         7\n",
      "         308       1.00      1.00      1.00         4\n",
      "         309       1.00      1.00      1.00         4\n",
      "         310       0.00      0.00      0.00         9\n",
      "         311       1.00      1.00      1.00         7\n",
      "         312       1.00      1.00      1.00         6\n",
      "         313       0.00      0.00      0.00         6\n",
      "         314       0.00      0.00      0.00         8\n",
      "         315       1.00      1.00      1.00         7\n",
      "         316       1.00      1.00      1.00         3\n",
      "         317       0.30      1.00      0.46         6\n",
      "         318       1.00      1.00      1.00        10\n",
      "         319       1.00      1.00      1.00         6\n",
      "         320       1.00      1.00      1.00         2\n",
      "         321       1.00      1.00      1.00         8\n",
      "         322       1.00      1.00      1.00         4\n",
      "         323       1.00      1.00      1.00         4\n",
      "         324       1.00      1.00      1.00         8\n",
      "         325       1.00      1.00      1.00         4\n",
      "         326       1.00      1.00      1.00         7\n",
      "         327       1.00      1.00      1.00         4\n",
      "         328       0.00      0.00      0.00         6\n",
      "         329       1.00      1.00      1.00         4\n",
      "         330       0.00      0.00      0.00         3\n",
      "         331       1.00      1.00      1.00         8\n",
      "         332       1.00      1.00      1.00         1\n",
      "         333       1.00      1.00      1.00         3\n",
      "         334       1.00      1.00      1.00         4\n",
      "         335       1.00      1.00      1.00         3\n",
      "         336       0.00      0.00      0.00         6\n",
      "         337       1.00      1.00      1.00         2\n",
      "         338       1.00      1.00      1.00         7\n",
      "         339       1.00      1.00      1.00         4\n",
      "         340       1.00      1.00      1.00         6\n",
      "         341       1.00      1.00      1.00         7\n",
      "         342       0.00      0.00      0.00         2\n",
      "         343       1.00      1.00      1.00         5\n",
      "         344       1.00      1.00      1.00         4\n",
      "         345       0.00      0.00      0.00         1\n",
      "         346       1.00      1.00      1.00         2\n",
      "         347       0.00      0.00      0.00         4\n",
      "         348       1.00      1.00      1.00         7\n",
      "         349       1.00      1.00      1.00         4\n",
      "         350       1.00      1.00      1.00         6\n",
      "         351       1.00      1.00      1.00         4\n",
      "         352       1.00      1.00      1.00         5\n",
      "         353       1.00      1.00      1.00         4\n",
      "         354       1.00      1.00      1.00         3\n",
      "         355       1.00      1.00      1.00         1\n",
      "         356       1.00      1.00      1.00         4\n",
      "         357       0.00      0.00      0.00         1\n",
      "         358       1.00      1.00      1.00         3\n",
      "         359       1.00      1.00      1.00         4\n",
      "         360       0.00      0.00      0.00         3\n",
      "         361       0.00      0.00      0.00         3\n",
      "         362       1.00      1.00      1.00         3\n",
      "         363       0.00      0.00      0.00         2\n",
      "         364       0.00      0.00      0.00         3\n",
      "         365       1.00      1.00      1.00         3\n",
      "         366       0.00      0.00      0.00         2\n",
      "         367       1.00      1.00      1.00         3\n",
      "         368       1.00      1.00      1.00         1\n",
      "         369       1.00      0.50      0.67         2\n",
      "         370       1.00      1.00      1.00         2\n",
      "         372       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.98     13567\n",
      "   macro avg       0.89      0.90      0.89     13567\n",
      "weighted avg       0.97      0.98      0.97     13567\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Python\\Pyhton3.10.0\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Select only the optimal number of input features for X_test\n",
    "X_test = X_test[:,:(best_model_index+1)]\n",
    "\n",
    "# Evaluate the best model on the test data\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# np.argmax() is used to convert the one-hot encoded predictions and test labels to class labels.\n",
    "y_pred_label = np.argmax(y_pred, axis = 1)\n",
    "\n",
    "# Print the classification report\n",
    "print(\"Classification report:\\n\", classification_report(y_test_enc, y_pred_label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           OsID  True Class  Predicted Class  True/False\n",
      "0  Os11g0704500         328              301       False\n",
      "1  Os09g0279600         161              161        True\n",
      "2  Os03g0669100          17               17        True\n",
      "3  Os05g0542500          34               34        True\n",
      "4  Os09g0522000           7                7        True\n"
     ]
    }
   ],
   "source": [
    "# extract class labels from test data\n",
    "class_test = y_test_enc\n",
    "\n",
    "# Invert OsID_labels dictionary\n",
    "inv_OsID_labels = {v: k for k, v in OsID_labels.items()}\n",
    "\n",
    "# map OsID values to the class labels\n",
    "OsID_test = [inv_OsID_labels.get(value, 'Unknown') for value in class_test]\n",
    "\n",
    "# create dataframe with OsID, true class, predicted class, and true/false columns\n",
    "results = pd.DataFrame({\n",
    "    'OsID': OsID_test,\n",
    "    'True Class': y_test_enc,\n",
    "    'Predicted Class': y_pred_label,\n",
    "    'True/False': class_test == y_pred_label\n",
    "})\n",
    "\n",
    "# display dataframe\n",
    "print(results.head())\n",
    "\n",
    "# save results_df to a CSV file\n",
    "results.to_csv('MLP_gene classification.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54645332476aec3a1589d49135d9c8280fdb5d7db877f5b7af7a1b58b8f996bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
